{"meta":{"version":1,"warehouse":"2.2.0"},"models":{"Asset":[{"_id":"source/CNAME","path":"CNAME","modified":1,"renderable":0},{"_id":"source/about/resume_cn.pdf","path":"about/resume_cn.pdf","modified":1,"renderable":0},{"_id":"source/about/resume_en.pdf","path":"about/resume_en.pdf","modified":1,"renderable":0},{"_id":"themes/next/source/css/main.styl","path":"css/main.styl","modified":1,"renderable":1},{"_id":"themes/next/source/images/algolia_logo.svg","path":"images/algolia_logo.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc-sa.svg","path":"images/cc-by-nc-sa.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc-nd.svg","path":"images/cc-by-nc-nd.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc.svg","path":"images/cc-by-nc.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by-nd.svg","path":"images/cc-by-nd.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by.svg","path":"images/cc-by.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by-sa.svg","path":"images/cc-by-sa.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-zero.svg","path":"images/cc-zero.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/favicon.ico","path":"images/favicon.ico","modified":1,"renderable":1},{"_id":"themes/next/source/images/loading.gif","path":"images/loading.gif","modified":1,"renderable":1},{"_id":"themes/next/source/images/placeholder.gif","path":"images/placeholder.gif","modified":1,"renderable":1},{"_id":"themes/next/source/images/quote-l.svg","path":"images/quote-l.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/quote-r.svg","path":"images/quote-r.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/searchicon.png","path":"images/searchicon.png","modified":1,"renderable":1},{"_id":"source/uploads/avatar.jpg","path":"uploads/avatar.jpg","modified":1,"renderable":0},{"_id":"themes/next/source/images/alipay-reward-image.jpg","path":"images/alipay-reward-image.jpg","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/affix.js","path":"js/src/affix.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/algolia-search.js","path":"js/src/algolia-search.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/bootstrap.js","path":"js/src/bootstrap.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/exturl.js","path":"js/src/exturl.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/hook-duoshuo.js","path":"js/src/hook-duoshuo.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/js.cookie.js","path":"js/src/js.cookie.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/motion.js","path":"js/src/motion.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/post-details.js","path":"js/src/post-details.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/scrollspy.js","path":"js/src/scrollspy.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/scroll-cookie.js","path":"js/src/scroll-cookie.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/utils.js","path":"js/src/utils.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.css","path":"lib/algolia-instant-search/instantsearch.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/canvas-nest/canvas-nest.min.js","path":"lib/canvas-nest/canvas-nest.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/canvas-ribbon/canvas-ribbon.js","path":"lib/canvas-ribbon/canvas-ribbon.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fastclick/LICENSE","path":"lib/fastclick/LICENSE","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fastclick/README.md","path":"lib/fastclick/README.md","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fastclick/bower.json","path":"lib/fastclick/bower.json","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/HELP-US-OUT.txt","path":"lib/font-awesome/HELP-US-OUT.txt","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/bower.json","path":"lib/font-awesome/bower.json","modified":1,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/CONTRIBUTING.md","path":"lib/jquery_lazyload/CONTRIBUTING.md","modified":1,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/README.md","path":"lib/jquery_lazyload/README.md","modified":1,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/bower.json","path":"lib/jquery_lazyload/bower.json","modified":1,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/jquery.lazyload.js","path":"lib/jquery_lazyload/jquery.lazyload.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/jquery.scrollstop.js","path":"lib/jquery_lazyload/jquery.scrollstop.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-barber-shop.min.css","path":"lib/pace/pace-theme-barber-shop.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-big-counter.min.css","path":"lib/pace/pace-theme-big-counter.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-bounce.min.css","path":"lib/pace/pace-theme-bounce.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-center-atom.min.css","path":"lib/pace/pace-theme-center-atom.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-center-circle.min.css","path":"lib/pace/pace-theme-center-circle.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-center-radar.min.css","path":"lib/pace/pace-theme-center-radar.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-center-simple.min.css","path":"lib/pace/pace-theme-center-simple.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-corner-indicator.min.css","path":"lib/pace/pace-theme-corner-indicator.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-fill-left.min.css","path":"lib/pace/pace-theme-fill-left.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-flash.min.css","path":"lib/pace/pace-theme-flash.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-loading-bar.min.css","path":"lib/pace/pace-theme-loading-bar.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-minimal.min.css","path":"lib/pace/pace-theme-minimal.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-mac-osx.min.css","path":"lib/pace/pace-theme-mac-osx.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace.min.js","path":"lib/pace/pace.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/three/canvas_lines.min.js","path":"lib/three/canvas_lines.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/three/three-waves.min.js","path":"lib/three/three-waves.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/three/canvas_sphere.min.js","path":"lib/three/canvas_sphere.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/velocity/bower.json","path":"lib/velocity/bower.json","modified":1,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.ui.js","path":"lib/velocity/velocity.ui.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.min.js","path":"lib/velocity/velocity.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.ui.min.js","path":"lib/velocity/velocity.ui.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/images/wechat-reward-image.jpg","path":"images/wechat-reward-image.jpg","modified":1,"renderable":1},{"_id":"themes/next/source/lib/jquery/index.js","path":"lib/jquery/index.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/han.css","path":"lib/Han/dist/han.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/han.min.css","path":"lib/Han/dist/han.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/han.min.js","path":"lib/Han/dist/han.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/schemes/pisces.js","path":"js/src/schemes/pisces.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_loading.gif","path":"lib/fancybox/source/fancybox_loading.gif","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/blank.gif","path":"lib/fancybox/source/blank.gif","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_loading@2x.gif","path":"lib/fancybox/source/fancybox_loading@2x.gif","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_overlay.png","path":"lib/fancybox/source/fancybox_overlay.png","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_sprite.png","path":"lib/fancybox/source/fancybox_sprite.png","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_sprite@2x.png","path":"lib/fancybox/source/fancybox_sprite@2x.png","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.css","path":"lib/fancybox/source/jquery.fancybox.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.js","path":"lib/fancybox/source/jquery.fancybox.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.pack.js","path":"lib/fancybox/source/jquery.fancybox.pack.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fastclick/lib/fastclick.js","path":"lib/fastclick/lib/fastclick.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fastclick/lib/fastclick.min.js","path":"lib/fastclick/lib/fastclick.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css","path":"lib/font-awesome/css/font-awesome.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css.map","path":"lib/font-awesome/css/font-awesome.css.map","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.min.css","path":"lib/font-awesome/css/font-awesome.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.min.js","path":"lib/ua-parser-js/dist/ua-parser.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.pack.js","path":"lib/ua-parser-js/dist/ua-parser.pack.js","modified":1,"renderable":1},{"_id":"themes/next/source/uploads/avatar.jpg","path":"uploads/avatar.jpg","modified":1,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/han.js","path":"lib/Han/dist/han.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff","path":"lib/font-awesome/fonts/fontawesome-webfont.woff","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff2","path":"lib/font-awesome/fonts/fontawesome-webfont.woff2","modified":1,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.js","path":"lib/velocity/velocity.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/font/han-space.otf","path":"lib/Han/dist/font/han-space.otf","modified":1,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/font/han-space.woff","path":"lib/Han/dist/font/han-space.woff","modified":1,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/font/han.otf","path":"lib/Han/dist/font/han.otf","modified":1,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/font/han.woff","path":"lib/Han/dist/font/han.woff","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-buttons.css","path":"lib/fancybox/source/helpers/jquery.fancybox-buttons.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/fancybox_buttons.png","path":"lib/fancybox/source/helpers/fancybox_buttons.png","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-buttons.js","path":"lib/fancybox/source/helpers/jquery.fancybox-buttons.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-media.js","path":"lib/fancybox/source/helpers/jquery.fancybox-media.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-thumbs.css","path":"lib/fancybox/source/helpers/jquery.fancybox-thumbs.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-thumbs.js","path":"lib/fancybox/source/helpers/jquery.fancybox-thumbs.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/FontAwesome.otf","path":"lib/font-awesome/fonts/FontAwesome.otf","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.eot","path":"lib/font-awesome/fonts/fontawesome-webfont.eot","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.ttf","path":"lib/font-awesome/fonts/fontawesome-webfont.ttf","modified":1,"renderable":1},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.js","path":"lib/algolia-instant-search/instantsearch.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/three/three.min.js","path":"lib/three/three.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.svg","path":"lib/font-awesome/fonts/fontawesome-webfont.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/avatar.gif","path":"images/avatar.gif","modified":1,"renderable":1}],"Cache":[{"_id":"source/CNAME","hash":"ab075e42f34723bce8bab60d8b6da686e2c40faf","modified":1588077535395},{"_id":"themes/next/LICENSE","hash":"ec44503d7e617144909e54533754f0147845f0c5","modified":1588077535403},{"_id":"themes/next/README.en.md","hash":"fd7a00ae9026fb4f87dd7eed9ce049d0db447140","modified":1588077535403},{"_id":"themes/next/README.md","hash":"06aaf1241e9e1619956c86d8b1397a643840a9d1","modified":1588077535403},{"_id":"themes/next/_config.yml","hash":"fac953669b7b971521ea844872323176b9c5a314","modified":1588077535403},{"_id":"themes/next/bower.json","hash":"63c38f50fb54b25bf5101f566189f9e5b3a6ef0e","modified":1588077535403},{"_id":"themes/next/gulpfile.coffee","hash":"412defab3d93d404b7c26aaa0279e2e586e97454","modified":1588077535403},{"_id":"themes/next/package.json","hash":"85a77bafb3d1e958b82e52528b7a95fcd59efda9","modified":1588077535407},{"_id":"source/_posts/Apri.md","hash":"e46a2ac1405e03c0b0d787209dac231c41999ba2","modified":1588077535395},{"_id":"source/_posts/IT salary in USA.md","hash":"3f8d0ab495504c4f653af17315725ed4245f52b9","modified":1588077535395},{"_id":"source/_posts/LInux.md","hash":"4fa5d3d0d1ea3313f0c8b77a8b9c6be4472e7e89","modified":1588077535395},{"_id":"source/_posts/MXnet_Config.md","hash":"f38c40598865394833052d8c16f17c83cc36e0b1","modified":1588077535395},{"_id":"source/_posts/Machine Learning in NBA.md","hash":"81f70ba31ae801b420f2df13adb2ebb7cb836edf","modified":1588077535395},{"_id":"source/_posts/MalongTEch-Learning-Semantic-Soft-Segmentation.md","hash":"657334901cdc361ad6572b66ad99ff92cf6f2bac","modified":1588077535395},{"_id":"source/_posts/MalongTech-Learning-3D-Image-Segmentation.md","hash":"25f0f6b65a92dc0e2c930c73a24218a3782d3df6","modified":1588077535395},{"_id":"source/_posts/MalongTech-Learning-Image-Segmentation.md","hash":"c6af03a0cc401ed7843b9dd1be51bf2d5c876800","modified":1588077535395},{"_id":"source/_posts/MalongTech-Learning-Object-Detection.md","hash":"fbc943b5b82a8ee28c933a628c59b15031886a46","modified":1588077535395},{"_id":"source/about/index.md","hash":"04a7b4058187f12ffeffcc55f78492c489ed1f0c","modified":1588077535395},{"_id":"source/about/resume_cn.pdf","hash":"87a665f33c05dfe969edf2001c4474a3f9ee5195","modified":1588077535395},{"_id":"source/about/resume_en.pdf","hash":"10b6e617530d73b17a81c9f855d1656adb3673f1","modified":1588077535395},{"_id":"source/categories/index.md","hash":"36e51eebd296314d61d0461e591e4bd96a0f42eb","modified":1588077535395},{"_id":"source/tags/index.md","hash":"5bd05991aa1304e46cb2e7fd82e953747c177986","modified":1588077535395},{"_id":"themes/next/languages/de.yml","hash":"4be3e7d296d5592e0d111dfa6cbbff02602c972d","modified":1588077535403},{"_id":"themes/next/languages/default.yml","hash":"d912814caac150da1611c96843371a87714e52f9","modified":1588077535403},{"_id":"themes/next/languages/en.yml","hash":"b3ee45143bc014578db6b8ac0573f7c7b143a743","modified":1588077535403},{"_id":"themes/next/languages/fr-FR.yml","hash":"0d5bd8bbbeafb72506124ed35e7509debc753612","modified":1588077535403},{"_id":"themes/next/languages/id.yml","hash":"c0848e93bf33a1333ff232905b6b392b1e056dd1","modified":1588077535403},{"_id":"themes/next/languages/ja.yml","hash":"1a608dc799c0f9c36b626bac6fe3404acb45b86d","modified":1588077535403},{"_id":"themes/next/languages/ko.yml","hash":"5c811514aef401317a9ec38b95679d6d2ef0ad42","modified":1588077535403},{"_id":"themes/next/languages/pt-BR.yml","hash":"cc8b5a67ec87b0d5aec6e253bab67ec3cfe3069c","modified":1588077535403},{"_id":"themes/next/languages/pt.yml","hash":"943475a7d681f37ede579cd62da9c50568ca0f8d","modified":1588077535403},{"_id":"themes/next/languages/zh-Hans.yml","hash":"3111ce4cc5f30868b3628f9f805d2aef3b75d1c1","modified":1588077535403},{"_id":"themes/next/languages/zh-hk.yml","hash":"b58c0d85daa4d62b0c9753a59de0739aa0120735","modified":1588077535403},{"_id":"themes/next/languages/ru.yml","hash":"84d41a111e497236b2c1fa16e9b91668a1f37037","modified":1588077535403},{"_id":"themes/next/languages/zh-tw.yml","hash":"8ce0a32411de111ae39d08e4bc936767dacdeb08","modified":1588077535403},{"_id":"themes/next/layout/_layout.swig","hash":"b88585f9e1b7071f6670b20b77b656edd087ccc9","modified":1588077535407},{"_id":"themes/next/layout/archive.swig","hash":"c2be7c95af6205c7501a261f2fc9702c57107f89","modified":1588077535407},{"_id":"themes/next/layout/category.swig","hash":"3cbb3f72429647411f9e85f2544bdf0e3ad2e6b2","modified":1588077535407},{"_id":"themes/next/layout/post.swig","hash":"182a99b1f6db0350106c6bb480fede0bbdb7e40f","modified":1588077535407},{"_id":"themes/next/layout/index.swig","hash":"4bf29f44ca9519a005671f2f2a79a48a148b435b","modified":1588077535407},{"_id":"themes/next/layout/schedule.swig","hash":"87ad6055df01fa2e63e51887d34a2d8f0fbd2f5a","modified":1588077535407},{"_id":"themes/next/layout/tag.swig","hash":"34e1c016cbdf94a31f9c5d494854ff46b2a182e9","modified":1588077535407},{"_id":"themes/next/layout/page.swig","hash":"dbff0302b4bfabb51556a197bf65190eb30361f0","modified":1588077535407},{"_id":"themes/next/scripts/merge-configs.js","hash":"3ce1be32bb77ee19da25e8dae7dc04e2afc46ca1","modified":1588077535407},{"_id":"themes/next/scripts/merge.js","hash":"39b84b937b2a9608b94e5872349a47200e1800ff","modified":1588077535407},{"_id":"themes/next/test/helpers.js","hash":"f25e7f3265eb5a6e1ccbb5e5012fa9bebf134105","modified":1588077535427},{"_id":"themes/next/test/intern.js","hash":"db90b1063356727d72be0d77054fdc32fa882a66","modified":1588077535427},{"_id":"themes/next/test/.jshintrc","hash":"c9fca43ae0d99718e45a6f5ce736a18ba5fc8fb6","modified":1588077535427},{"_id":"themes/next/source/fonts/.gitkeep","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1588077535411},{"_id":"source/_posts/LogBook.md","hash":"e54572cc145d33a4f633564f26bc3f86cffb249d","modified":1588077535395},{"_id":"themes/next/layout/_custom/header.swig","hash":"ba8ab5a0280b953aa97435ff8946cbcbb2755a27","modified":1588077535407},{"_id":"themes/next/layout/_custom/sidebar.swig","hash":"ba8ab5a0280b953aa97435ff8946cbcbb2755a27","modified":1588077535407},{"_id":"themes/next/layout/_macro/post-copyright.swig","hash":"0f59a51b5cea3e8a7c078db486626cddc2978622","modified":1588077535407},{"_id":"themes/next/layout/_macro/post-collapse.swig","hash":"8c56dd26157cbc580ae41d97ac34b90ab48ced3f","modified":1588077535407},{"_id":"themes/next/layout/_macro/reward.swig","hash":"5b1e91c2f6f88fbecd426cd0727e7b7854c6cc1d","modified":1588077535407},{"_id":"themes/next/layout/_macro/wechat-subscriber.swig","hash":"e2e4eae391476da994045ed4c7faf5e05aca2cd7","modified":1588077535407},{"_id":"themes/next/layout/_partials/comments.swig","hash":"e748c83209ca737be156f8179807d12bc652c1f2","modified":1588077535407},{"_id":"themes/next/layout/_macro/sidebar.swig","hash":"f10ca698e8ea0c31ff72a6cffa832c3cd703a133","modified":1588077535407},{"_id":"themes/next/layout/_partials/footer.swig","hash":"683616f4a80796051e2346d80acf838a589450b0","modified":1588077535407},{"_id":"themes/next/layout/_macro/post.swig","hash":"b6b86a199f5a3692ec492123fe8cb53a8ca08169","modified":1588077535407},{"_id":"themes/next/layout/_partials/page-header.swig","hash":"77c61e0baea3544df361b7338c3cd13dc84dde22","modified":1588077535407},{"_id":"themes/next/layout/_partials/pagination.swig","hash":"1634fb887842698e01ff6e632597fe03c75d2d01","modified":1588077535407},{"_id":"themes/next/layout/_partials/head.swig","hash":"476e6c2452732c2741f518004d336bc348e710d7","modified":1588077535407},{"_id":"themes/next/layout/_partials/search.swig","hash":"b4ebe4a52a3b51efe549dd1cdee846103664f5eb","modified":1588077535407},{"_id":"themes/next/layout/_scripts/boostrap.swig","hash":"c0f5a0955f69ca4ed9ee64a2d5f8aa75064935ad","modified":1588077535407},{"_id":"themes/next/layout/_scripts/commons.swig","hash":"931808ad9b8d8390c0dcf9bdeb0954eeb9185d68","modified":1588077535407},{"_id":"themes/next/layout/_scripts/vendors.swig","hash":"53c894e6f3573c662dc4e4f7b5a6f1a32f1a8c94","modified":1588077535407},{"_id":"themes/next/layout/_partials/header.swig","hash":"d6bf1d1554d91eaf1bfc40ba8905ae81673e5f45","modified":1588077535407},{"_id":"themes/next/layout/_third-party/duoshuo-hot-articles.swig","hash":"ba75672183d94f1de7c8bd0eeee497a58c70e889","modified":1588077535407},{"_id":"themes/next/layout/_third-party/exturl.swig","hash":"8301c9600bb3e47f7fb98b0e0332ef3c51bb1688","modified":1588077535407},{"_id":"themes/next/layout/_third-party/mathjax.swig","hash":"a0bd3388587fd943baae0d84ca779a707fbcad89","modified":1588077535407},{"_id":"themes/next/layout/_third-party/rating.swig","hash":"554ec568e9d2c71e4a624a8de3cb5929050811d6","modified":1588077535407},{"_id":"themes/next/layout/_third-party/schedule.swig","hash":"db15d7e1552aa2d2386a6b8a33b3b3a40bf9e43d","modified":1588077535407},{"_id":"themes/next/layout/_third-party/scroll-cookie.swig","hash":"9a188938d46931d5f3882a140aa1c48b3a893f0c","modified":1588077535407},{"_id":"themes/next/scripts/tags/button.js","hash":"aaf71be6b483fca7a65cd6296c2cf1c2271c26a6","modified":1588077535407},{"_id":"themes/next/scripts/tags/center-quote.js","hash":"99b66949f18398689b904907af23c013be1b978f","modified":1588077535407},{"_id":"themes/next/scripts/tags/full-image.js","hash":"c9f833158c66bd72f627a0559cf96550e867aa72","modified":1588077535407},{"_id":"themes/next/scripts/tags/exturl.js","hash":"5022c0ba9f1d13192677cf1fd66005c57c3d0f53","modified":1588077535407},{"_id":"themes/next/scripts/tags/group-pictures.js","hash":"ac681b0d0d8d39ba3817336c0270c6787c2b6b70","modified":1588077535407},{"_id":"themes/next/scripts/tags/label.js","hash":"6f00952d70aadece844ce7fd27adc52816cc7374","modified":1588077535407},{"_id":"themes/next/scripts/tags/lazy-image.js","hash":"bcba2ff25cd7850ce6da322d8bd85a8dd00b5ceb","modified":1588077535407},{"_id":"themes/next/scripts/tags/tabs.js","hash":"aa7fc94a5ec27737458d9fe1a75c0db7593352fd","modified":1588077535407},{"_id":"themes/next/scripts/tags/note.js","hash":"f7eae135f35cdab23728e9d0d88b76e00715faa0","modified":1588077535407},{"_id":"themes/next/source/css/main.styl","hash":"a91dbb7ef799f0a171b5e726c801139efe545176","modified":1588077535411},{"_id":"themes/next/source/images/algolia_logo.svg","hash":"45eeea0b5fba833e21e38ea10ed5ab385ceb4f01","modified":1588077535411},{"_id":"themes/next/source/images/cc-by-nc-sa.svg","hash":"6f55543d1fb9cbc436c101d24f802dec7b41efc3","modified":1588077535419},{"_id":"themes/next/source/images/cc-by-nc-nd.svg","hash":"bc3588c9b2d7c68830524783120ff6cf957cf668","modified":1588077535419},{"_id":"themes/next/source/images/cc-by-nc.svg","hash":"6f076713fb9bf934aa2c1046bdf2cf2e37bc1eab","modified":1588077535419},{"_id":"themes/next/source/images/cc-by-nd.svg","hash":"42cd73da328077ccc92f859bb8f3cf621b3484f8","modified":1588077535419},{"_id":"themes/next/source/images/cc-by.svg","hash":"e92a33c32d1dac8ed94849b2b4e6456e887efe70","modified":1588077535419},{"_id":"themes/next/source/images/cc-by-sa.svg","hash":"70c1535f43e54e5ff35ca81419e77e4c0c301398","modified":1588077535419},{"_id":"themes/next/source/images/cc-zero.svg","hash":"9bfb52b2f63527a7049247bf00d44e6dc1170e7d","modified":1588077535419},{"_id":"themes/next/source/images/favicon.ico","hash":"17c483c2915dbd1c84cc42d124a7199bdc7ade1f","modified":1588077535419},{"_id":"themes/next/source/images/loading.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1588077535419},{"_id":"themes/next/source/images/placeholder.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1588077535419},{"_id":"themes/next/source/images/quote-l.svg","hash":"cd108d6f44351cadf8e6742565217f88818a0458","modified":1588077535419},{"_id":"themes/next/source/images/quote-r.svg","hash":"2a2a250b32a87c69dcc1b1976c74b747bedbfb41","modified":1588077535419},{"_id":"themes/next/source/images/searchicon.png","hash":"67727a6a969be0b2659b908518fa6706eed307b8","modified":1588077535419},{"_id":"themes/next/layout/_scripts/schemes/muse.swig","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1588077535407},{"_id":"themes/next/layout/_scripts/schemes/mist.swig","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1588077535407},{"_id":"themes/next/source/css/_mixins/Mist.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1588077535411},{"_id":"themes/next/source/css/_mixins/Muse.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1588077535411},{"_id":"themes/next/source/css/_mixins/custom.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1588077535411},{"_id":"themes/next/source/css/_variables/Muse.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1588077535411},{"_id":"themes/next/source/css/_variables/custom.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1588077535411},{"_id":"themes/next/source/images/alipay-reward-image.jpg","hash":"ee38a3c5ac61f24467a668ba5e7246053b7b1b8c","modified":1588077535411},{"_id":"source/uploads/avatar.jpg","hash":"e3e7311f9624008234681ff57e411471dd80f379","modified":1588077535395},{"_id":"themes/next/layout/_partials/head/external-fonts.swig","hash":"f5e487b0d213ca0bd94aa30bc23b240d65081627","modified":1588077535407},{"_id":"themes/next/layout/_partials/head/custom-head.swig","hash":"a223919d2e1bf17ca4d6abb2c86f2efca9883dc1","modified":1588077535407},{"_id":"themes/next/layout/_partials/search/swiftype.swig","hash":"a8c7f9ca7c605d039a1f3bf4e4d3183700a3dd62","modified":1588077535407},{"_id":"themes/next/layout/_partials/search/tinysou.swig","hash":"b25002a83cbd2ca0c4a5df87ad5bff26477c0457","modified":1588077535407},{"_id":"themes/next/layout/_partials/search/localsearch.swig","hash":"b2f0d247b213e4cf8de47af6a304d98070cc7256","modified":1588077535407},{"_id":"themes/next/layout/_partials/share/add-this.swig","hash":"9e3d133ac5bcc6cb51702c83b2611a49811abad1","modified":1588077535407},{"_id":"themes/next/layout/_partials/share/baidushare.swig","hash":"5a4f01331d79c6f0eac119e92e2861ff61f1b35a","modified":1588077535407},{"_id":"themes/next/layout/_partials/share/jiathis.swig","hash":"12684840de632eb16e53ffa863166306a756fd4f","modified":1588077535407},{"_id":"themes/next/layout/_partials/share/duoshuo_share.swig","hash":"d4fbffd7fa8f2090eb32a871872665d90a885fac","modified":1588077535407},{"_id":"themes/next/layout/_scripts/schemes/gemini.swig","hash":"a9a3995b9615adfb8d6b127c78c6771627bee19a","modified":1588077535407},{"_id":"themes/next/layout/_scripts/schemes/pisces.swig","hash":"a9a3995b9615adfb8d6b127c78c6771627bee19a","modified":1588077535407},{"_id":"themes/next/layout/_scripts/pages/post-details.swig","hash":"9b84ab576982b2c3bb0291da49143bc77fba3cc6","modified":1588077535407},{"_id":"themes/next/layout/_third-party/analytics/application-insights.swig","hash":"71397a5823e8ec8aad3b68aace13150623b3e19d","modified":1588077535407},{"_id":"themes/next/layout/_third-party/analytics/busuanzi-counter.swig","hash":"16cb23818909f57dac1a5ada66869971c33d7bd8","modified":1588077535407},{"_id":"themes/next/layout/_third-party/analytics/baidu-analytics.swig","hash":"753d262911c27baf663fcaf199267133528656af","modified":1588077535407},{"_id":"themes/next/layout/_third-party/analytics/cnzz-analytics.swig","hash":"a10b7f19d7b5725527514622899df413a34a89db","modified":1588077535407},{"_id":"themes/next/layout/_third-party/analytics/facebook-sdk.swig","hash":"7d94845f96197d9d84a405fa5d4ede75fb81b225","modified":1588077535407},{"_id":"themes/next/layout/_third-party/analytics/google-analytics.swig","hash":"b1e13df83fb2b1d5d513b30b7aa6158b0837daab","modified":1588077535407},{"_id":"themes/next/layout/_third-party/analytics/index.swig","hash":"441f1a1b4e2f652d3b975995bd9d44ff4866f057","modified":1588077535407},{"_id":"themes/next/layout/_third-party/analytics/tencent-mta.swig","hash":"5a8027328f060f965b3014060bebec1d7cf149c1","modified":1588077535407},{"_id":"themes/next/layout/_third-party/analytics/tencent-analytics.swig","hash":"8a399df90dadba5ad4e781445b58f4765aeb701e","modified":1588077535407},{"_id":"themes/next/layout/_third-party/analytics/lean-analytics.swig","hash":"e6d10ee4fb70b3ae1cd37e9e36e000306734aa2e","modified":1588077535407},{"_id":"themes/next/layout/_third-party/comments/changyan.swig","hash":"4c501ea0b9c494181eb3c607c5526a5754e7fbd8","modified":1588077535407},{"_id":"themes/next/layout/_third-party/comments/disqus.swig","hash":"b83a51bbe0f1e2ded9819070840b0ea145f003a6","modified":1588077535407},{"_id":"themes/next/layout/_third-party/analytics/vkontakte-api.swig","hash":"f9a1647a8f1866deeb94052d1f87a5df99cb1e70","modified":1588077535407},{"_id":"themes/next/layout/_third-party/comments/duoshuo.swig","hash":"1600f340e0225361580c44890568dc07dbcf2c89","modified":1588077535407},{"_id":"themes/next/layout/_third-party/comments/hypercomments.swig","hash":"af7f3e43cbdc4f88c13f101f0f341af96ace3383","modified":1588077535407},{"_id":"themes/next/layout/_third-party/comments/livere.swig","hash":"9246162d4bc7e949ce1d12d135cbbaf5dc3024ec","modified":1588077535407},{"_id":"themes/next/layout/_third-party/comments/index.swig","hash":"be2aaeb8f05979e2ba501248480d5294256d61f2","modified":1588077535407},{"_id":"themes/next/layout/_third-party/comments/youyan.swig","hash":"7e65ff8fe586cd655b0e9d1ad2912663ff9bd36c","modified":1588077535407},{"_id":"themes/next/layout/_third-party/search/index.swig","hash":"34599633658f3b0ffb487728b7766e1c7b551f5a","modified":1588077535407},{"_id":"themes/next/layout/_third-party/search/localsearch.swig","hash":"93479642fd076a1257fecc25fcf5d20ccdefe509","modified":1588077535407},{"_id":"themes/next/layout/_third-party/seo/baidu-push.swig","hash":"d8c98938719284fa06492c114d99a1904652a555","modified":1588077535407},{"_id":"themes/next/layout/_third-party/search/tinysou.swig","hash":"fe95dd3d166634c466e19aa756e65ad6e8254d3e","modified":1588077535407},{"_id":"themes/next/source/css/_custom/custom.styl","hash":"3403fdd8efde1a0afd11ae8a5a97673f5903087f","modified":1588077535411},{"_id":"themes/next/source/css/_mixins/Gemini.styl","hash":"07f7da320689f828f6e36a6123807964a45157a0","modified":1588077535411},{"_id":"themes/next/source/css/_mixins/Pisces.styl","hash":"bf009e85212749405c27d89b49f401911355ecc7","modified":1588077535411},{"_id":"themes/next/source/css/_mixins/base.styl","hash":"0e55cbd93852dc3f8ccb44df74d35d9918f847e0","modified":1588077535411},{"_id":"themes/next/source/css/_variables/Gemini.styl","hash":"2a4e844dec690365774c2f6e8984706fee39ea63","modified":1588077535411},{"_id":"themes/next/source/css/_variables/Mist.styl","hash":"e55265c8a8a6ae0c3c08e3509de92ee62c3cb5f6","modified":1588077535411},{"_id":"themes/next/source/css/_variables/base.styl","hash":"a627633d3bb70b8501572b18037def478beb7017","modified":1588077535411},{"_id":"themes/next/source/css/_variables/Pisces.styl","hash":"89f88b9c9a191dd980f799fc36b83b63290d3ac9","modified":1588077535411},{"_id":"themes/next/source/js/src/affix.js","hash":"1b509c3b5b290a6f4607f0f06461a0c33acb69b1","modified":1588077535419},{"_id":"themes/next/source/js/src/algolia-search.js","hash":"cb431b54ba9c692165a1f5a12e4c564a560f8058","modified":1588077535419},{"_id":"themes/next/source/js/src/bootstrap.js","hash":"96c8b5fe1999de1b3a46730d9812787dfcd65884","modified":1588077535419},{"_id":"themes/next/source/js/src/exturl.js","hash":"a2a0f0de07e46211f74942a468f42ee270aa555c","modified":1588077535419},{"_id":"themes/next/source/js/src/hook-duoshuo.js","hash":"b35a7dc47b634197b93487cea8671a40a9fdffce","modified":1588077535419},{"_id":"themes/next/source/js/src/js.cookie.js","hash":"1512c751d219577d338ac0780fb2bbd9075d5298","modified":1588077535419},{"_id":"themes/next/source/js/src/motion.js","hash":"dda8c76fce91d7f140c06de2583ba806810f12c2","modified":1588077535419},{"_id":"themes/next/source/js/src/post-details.js","hash":"50fa390554f0fb467d8eb84ac8eff2cffb13fe67","modified":1588077535419},{"_id":"themes/next/source/js/src/scrollspy.js","hash":"b7657be25fc52ec67c75ab5481bdcb483573338b","modified":1588077535419},{"_id":"themes/next/source/js/src/scroll-cookie.js","hash":"02cf91514e41200bc9df5d8bdbeb58575ec06074","modified":1588077535419},{"_id":"themes/next/source/js/src/utils.js","hash":"f90c7611dc665b5e321cb81c0bd689445bab438a","modified":1588077535419},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.css","hash":"90ef19edc982645b118b095615838d9c5eaba0de","modified":1588077535419},{"_id":"themes/next/source/lib/canvas-nest/canvas-nest.min.js","hash":"0387e75e23b1db108a755073fe52a0d03eb391a7","modified":1588077535419},{"_id":"themes/next/source/lib/canvas-ribbon/canvas-ribbon.js","hash":"82fee688910efc644d3d1c3305c6ae28ba3f38f9","modified":1588077535419},{"_id":"themes/next/source/lib/fancybox/.bower.json","hash":"cc40a9b11e52348e554c84e4a5c058056f6b7aeb","modified":1588077535419},{"_id":"themes/next/source/lib/fancybox/.gitattributes","hash":"2db21acfbd457452462f71cc4048a943ee61b8e0","modified":1588077535419},{"_id":"themes/next/source/lib/jquery/.bower.json","hash":"865d6c1328ab209a4376b9d2b7a7824369565f28","modified":1588077535423},{"_id":"themes/next/source/lib/fastclick/LICENSE","hash":"6f474ea75c42442da7bbcf2e9143ce98258efd8d","modified":1588077535419},{"_id":"themes/next/source/lib/fastclick/.bower.json","hash":"bf3eef9d647cd7c9b62feda3bc708c6cdd7c0877","modified":1588077535419},{"_id":"themes/next/source/lib/fastclick/README.md","hash":"68a9b9d53126405b0fa5f3324f1fb96dbcc547aa","modified":1588077535419},{"_id":"themes/next/source/lib/fastclick/bower.json","hash":"a9b3ee1e4db71a0e4ea6d5bed292d176dd68b261","modified":1588077535419},{"_id":"themes/next/source/lib/font-awesome/.bower.json","hash":"b4aefc910578d76b267e86dfffdd5121c8db9aec","modified":1588077535419},{"_id":"themes/next/source/lib/font-awesome/.gitignore","hash":"03ddbf76c1dd1afb93eed0b670d2eee747472ef1","modified":1588077535419},{"_id":"themes/next/source/lib/font-awesome/.npmignore","hash":"c31ff06a740955e44edd4403902e653ccabfd4db","modified":1588077535419},{"_id":"themes/next/source/lib/font-awesome/HELP-US-OUT.txt","hash":"ee33b2798b1e714b904d663436c6b3521011d1fa","modified":1588077535419},{"_id":"themes/next/source/lib/font-awesome/bower.json","hash":"71e7183634dc1b9449f590f15ebd7201add22ca7","modified":1588077535419},{"_id":"themes/next/source/lib/jquery_lazyload/.bower.json","hash":"90fa628f156d8045357ff11eaf32e61abacf10e8","modified":1588077535423},{"_id":"themes/next/source/lib/jquery_lazyload/CONTRIBUTING.md","hash":"4ded6fee668544778e97e38c2b211fc56c848e77","modified":1588077535423},{"_id":"themes/next/source/lib/jquery_lazyload/README.md","hash":"b930297cb98b8e1dbd5abe9bc1ed9d5935d18ce8","modified":1588077535423},{"_id":"themes/next/source/lib/jquery_lazyload/bower.json","hash":"e0acf1db27b0cc16128a59c46db1db406b5c4c58","modified":1588077535423},{"_id":"themes/next/source/lib/jquery_lazyload/jquery.lazyload.js","hash":"f4a570908f6c89c6edfb1c74959e733eaadea4f2","modified":1588077535423},{"_id":"themes/next/source/lib/jquery_lazyload/jquery.scrollstop.js","hash":"bf773ad48a0b9aa77681a89d7569eefc0f7b7b18","modified":1588077535423},{"_id":"themes/next/source/lib/pace/pace-theme-barber-shop.min.css","hash":"ee0d51446cb4ffe1bb96bd7bc8c8e046dddfcf46","modified":1588077535423},{"_id":"themes/next/source/lib/pace/pace-theme-big-counter.min.css","hash":"5b561dc328af4c4d512e20a76fe964d113a32ba8","modified":1588077535423},{"_id":"themes/next/source/lib/pace/pace-theme-bounce.min.css","hash":"f6bdb9a785b7979dd8ec5c60e278af955ef1e585","modified":1588077535423},{"_id":"themes/next/source/lib/pace/pace-theme-center-atom.min.css","hash":"dcf79c24fe5350fb73d8038573a104e73639e9d3","modified":1588077535423},{"_id":"themes/next/source/lib/pace/pace-theme-center-circle.min.css","hash":"a4066769c78affbfbc5e30a600e2c7862cd532e0","modified":1588077535423},{"_id":"themes/next/source/lib/pace/pace-theme-center-radar.min.css","hash":"ab7cba998bf4c03b13df342bf43647fa4f419783","modified":1588077535423},{"_id":"themes/next/source/lib/pace/pace-theme-center-simple.min.css","hash":"67f44c947548bd4d77e7590d3f59e236cbf9e98a","modified":1588077535423},{"_id":"themes/next/source/lib/pace/pace-theme-corner-indicator.min.css","hash":"b3c64c973f31884e3d8145989476707333406b9a","modified":1588077535423},{"_id":"themes/next/source/lib/pace/pace-theme-fill-left.min.css","hash":"0bec1e235a4a2cccda3f993b205424e1441a44ae","modified":1588077535423},{"_id":"themes/next/source/lib/pace/pace-theme-flash.min.css","hash":"13ace22c40312d7bbd8d9c1e50eff897a7a497d8","modified":1588077535423},{"_id":"themes/next/source/lib/pace/pace-theme-loading-bar.min.css","hash":"7ee28875dfc1230d76c537f6605766e8d4011e9f","modified":1588077535423},{"_id":"themes/next/source/lib/pace/pace-theme-minimal.min.css","hash":"9cd783cceb8a191f3c8b5d81f7a430ecc3e489d3","modified":1588077535423},{"_id":"themes/next/source/lib/pace/pace-theme-mac-osx.min.css","hash":"9f2e7b51b084da407863826b25265b31150b3821","modified":1588077535423},{"_id":"themes/next/source/lib/pace/pace.min.js","hash":"8aaa675f577d5501f5f22d5ccb07c2b76310b690","modified":1588077535423},{"_id":"themes/next/source/lib/three/canvas_lines.min.js","hash":"2d9a9f38c493fdf7c0b833bb9184b6a1645c11b2","modified":1588077535423},{"_id":"themes/next/source/lib/three/three-waves.min.js","hash":"8148492dd49aa876d32bb7d5b728d3f5bf6f5074","modified":1588077535423},{"_id":"themes/next/source/lib/three/canvas_sphere.min.js","hash":"46a50b91c98b639c9a2b9265c5a1e66a5c656881","modified":1588077535423},{"_id":"themes/next/source/lib/velocity/.bower.json","hash":"63da5e80ebb61bb66a2794d5936315ca44231f0c","modified":1588077535423},{"_id":"themes/next/source/lib/velocity/bower.json","hash":"92d92860418c4216aa59eb4cb4a556290a7ad9c3","modified":1588077535423},{"_id":"themes/next/source/lib/velocity/velocity.ui.js","hash":"dbbfb50f6502f6b81dcc9fee7b31f1e812da3464","modified":1588077535423},{"_id":"themes/next/source/lib/velocity/velocity.min.js","hash":"bf172816a9c57f9040e3d19c24e181a142daf92b","modified":1588077535423},{"_id":"themes/next/source/lib/velocity/velocity.ui.min.js","hash":"dde584994ac13dc601836e86f4cf490e418d9723","modified":1588077535423},{"_id":"themes/next/source/images/wechat-reward-image.jpg","hash":"a81a23bcba5d49564c5afe2210957a86a8175431","modified":1588077535419},{"_id":"themes/next/source/lib/jquery/index.js","hash":"17a740d68a1c330876c198b6a4d9319f379f3af2","modified":1588077535423},{"_id":"themes/next/layout/_third-party/search/algolia-search/assets.swig","hash":"218cc936ba3518a3591b2c9eda46bc701edf7710","modified":1588077535407},{"_id":"themes/next/layout/_third-party/search/algolia-search/dom.swig","hash":"2530de0f3125a912756f6c0e9090cd012134a4c5","modified":1588077535407},{"_id":"themes/next/source/css/_schemes/Muse/_layout.styl","hash":"31127dcbf4c7b4ada53ffbf1638b5fe325b7cbc0","modified":1588077535411},{"_id":"themes/next/source/css/_schemes/Muse/_menu.styl","hash":"f23ac53ab901c48859dd29eee6e386b60ff956ba","modified":1588077535411},{"_id":"themes/next/source/css/_schemes/Muse/_logo.styl","hash":"748dbfbf9c08e719ddc775958003c64b00d39dab","modified":1588077535411},{"_id":"themes/next/source/css/_schemes/Muse/_search.styl","hash":"09c965022c13b84ed8a661fee8ac2a6d550495ae","modified":1588077535411},{"_id":"themes/next/source/css/_schemes/Mist/_base.styl","hash":"25d5e45a355ee2093f3b8b8eeac125ebf3905026","modified":1588077535411},{"_id":"themes/next/source/css/_schemes/Muse/index.styl","hash":"5dbc0d0c897e46760e5dbee416530d485c747bba","modified":1588077535411},{"_id":"themes/next/source/css/_schemes/Mist/_logo.styl","hash":"b1025c421406d2c24cc92a02ae28c1915b01e240","modified":1588077535411},{"_id":"themes/next/source/css/_schemes/Mist/_header.styl","hash":"d0bfd1bef988c76f7d7dd72d88af6f0908a8b0db","modified":1588077535411},{"_id":"themes/next/source/css/_schemes/Mist/_menu.styl","hash":"26666c1f472bf5f3fb9bc62081cca22b4de15ccb","modified":1588077535411},{"_id":"themes/next/source/css/_schemes/Mist/_posts-expanded.styl","hash":"ce272226a1570f5f7c70243b751a5b0fe1671a88","modified":1588077535411},{"_id":"themes/next/source/css/_schemes/Mist/_search.styl","hash":"09c965022c13b84ed8a661fee8ac2a6d550495ae","modified":1588077535411},{"_id":"themes/next/source/css/_schemes/Mist/index.styl","hash":"9b913b73d31d21f057f97115ffab93cfa578b884","modified":1588077535411},{"_id":"themes/next/source/css/_schemes/Gemini/index.styl","hash":"a509016ac0227a1903d7f0ca3a825cf9ac7fde33","modified":1588077535411},{"_id":"themes/next/source/css/_common/outline/outline.styl","hash":"12662536c7a07fff548abe94171f34b768dd610f","modified":1588077535407},{"_id":"themes/next/source/css/_schemes/Pisces/_brand.styl","hash":"bce344d3a665b4c55230d2a91eac2ad16d6f32fd","modified":1588077535411},{"_id":"themes/next/source/css/_schemes/Pisces/_layout.styl","hash":"416988dca389e6e2fdfa51fa7f4ee07eb53f82fb","modified":1588077535411},{"_id":"themes/next/source/css/_schemes/Pisces/_menu.styl","hash":"4642e30010af8b2b037f5b43146b10a934941958","modified":1588077535411},{"_id":"themes/next/source/css/_schemes/Pisces/_posts.styl","hash":"1f6e2ce674735269599acc6d77b3ea18d31967fc","modified":1588077535411},{"_id":"themes/next/source/css/_schemes/Pisces/_sidebar.styl","hash":"c48d4a561d047b3705924949b3ab7b57bee94ecd","modified":1588077535411},{"_id":"themes/next/source/css/_schemes/Pisces/index.styl","hash":"86197902dfd3bededba10ba62b8f9f22e0420bde","modified":1588077535411},{"_id":"themes/next/source/css/_common/components/back-to-top-sidebar.styl","hash":"b0e2a0e27a32f72cb283fe4b33d010d485113379","modified":1588077535407},{"_id":"themes/next/source/css/_common/components/back-to-top.styl","hash":"237d185ac62ec9877e300947fa0109c44fb8db19","modified":1588077535407},{"_id":"themes/next/source/css/_common/components/buttons.styl","hash":"22828f5141c0cecb9ef25a110e194cdfa3a36423","modified":1588077535407},{"_id":"themes/next/source/css/_common/components/comments.styl","hash":"ff4489cd582f518bba6909a301ac1292a38b4e96","modified":1588077535407},{"_id":"themes/next/source/css/_common/components/components.styl","hash":"7ad4081466b397e2a6204141bb7768b7c01bd93c","modified":1588077535407},{"_id":"themes/next/source/css/_common/components/pagination.styl","hash":"88559b13ce94311405b170a0506ded91273beceb","modified":1588077535407},{"_id":"themes/next/source/css/_common/components/tag-cloud.styl","hash":"6eb4bcc3056bd279d000607e8b4dad50d368ca69","modified":1588077535407},{"_id":"themes/next/source/css/_common/scaffolding/base.styl","hash":"eec22651977ea25b5e65e8cb1b4906eef69ec588","modified":1588077535407},{"_id":"themes/next/source/css/_common/scaffolding/helpers.styl","hash":"1da5c800d025345f212a3bf1be035060f4e5e6ed","modified":1588077535407},{"_id":"themes/next/source/css/_common/scaffolding/mobile.styl","hash":"9a45ed506274f655b11995c408cc566b16dada79","modified":1588077535407},{"_id":"themes/next/source/css/_common/scaffolding/normalize.styl","hash":"3f40e8a9fe8e7bd5cfc4cf4cbbbcb9539462e973","modified":1588077535411},{"_id":"themes/next/source/css/_common/scaffolding/scaffolding.styl","hash":"a17e2b871a335f290afb392a08f94fd35f59c715","modified":1588077535411},{"_id":"themes/next/source/css/_common/scaffolding/tables.styl","hash":"ea9069645696f86c5df64208490876fe150c8cae","modified":1588077535411},{"_id":"themes/next/source/lib/Han/dist/han.css","hash":"6c26cdb36687d4f0a11dabf5290a909c3506be5c","modified":1588077535419},{"_id":"themes/next/source/lib/Han/dist/han.min.css","hash":"6d586bfcfb7ae48f1b12f76eec82d3ad31947501","modified":1588077535419},{"_id":"themes/next/source/lib/Han/dist/han.min.js","hash":"16b03db23a52623348f37c04544f2792032c1fb6","modified":1588077535419},{"_id":"themes/next/source/js/src/schemes/pisces.js","hash":"c4358416f0a116d7f4037542fa3b385947e80908","modified":1588077535419},{"_id":"themes/next/source/lib/fancybox/source/fancybox_loading.gif","hash":"1a755fb2599f3a313cc6cfdb14df043f8c14a99c","modified":1588077535419},{"_id":"themes/next/source/lib/fancybox/source/blank.gif","hash":"2daeaa8b5f19f0bc209d976c02bd6acb51b00b0a","modified":1588077535419},{"_id":"themes/next/source/lib/fancybox/source/fancybox_loading@2x.gif","hash":"273b123496a42ba45c3416adb027cd99745058b0","modified":1588077535419},{"_id":"themes/next/source/lib/fancybox/source/fancybox_overlay.png","hash":"b3a4ee645ba494f52840ef8412015ba0f465dbe0","modified":1588077535419},{"_id":"themes/next/source/lib/fancybox/source/fancybox_sprite.png","hash":"17df19f97628e77be09c352bf27425faea248251","modified":1588077535419},{"_id":"themes/next/source/lib/fancybox/source/fancybox_sprite@2x.png","hash":"30c58913f327e28f466a00f4c1ac8001b560aed8","modified":1588077535419},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.css","hash":"5f163444617b6cf267342f06ac166a237bb62df9","modified":1588077535419},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.js","hash":"1cf3d47b5ccb7cb6e9019c64f2a88d03a64853e4","modified":1588077535419},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.pack.js","hash":"53360764b429c212f424399384417ccc233bb3be","modified":1588077535419},{"_id":"themes/next/source/lib/fastclick/lib/fastclick.js","hash":"1d6aeda0480d0e4cb6198edf7719d601d4ae2ccc","modified":1588077535419},{"_id":"themes/next/source/lib/fastclick/lib/fastclick.min.js","hash":"2cae0f5a6c5d6f3cb993015e6863f9483fc4de18","modified":1588077535419},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css","hash":"3655f1fdf1e584c4d8e8d39026093ca306a5a341","modified":1588077535423},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css.map","hash":"1573904b82807abbb32c97a3632c6c6808eaac50","modified":1588077535423},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.min.css","hash":"88af80502c44cd52ca81ffe7dc7276b7eccb06cf","modified":1588077535423},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.min.js","hash":"41ea797c68dbcff2f6fb3aba1d1043a22e7cc0f6","modified":1588077535423},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.pack.js","hash":"a817b6c158cbc5bab3582713de9fe18a18a80552","modified":1588077535423},{"_id":"themes/next/source/uploads/avatar.jpg","hash":"e3e7311f9624008234681ff57e411471dd80f379","modified":1588077535427},{"_id":"themes/next/source/lib/Han/dist/han.js","hash":"4ac683b2bc8531c84d98f51b86957be0e6f830f3","modified":1588077535419},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff","hash":"28b782240b3e76db824e12c02754a9731a167527","modified":1588077535423},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff2","hash":"d6f48cba7d076fb6f2fd6ba993a75b9dc1ecbf0c","modified":1588077535423},{"_id":"themes/next/source/lib/velocity/velocity.js","hash":"4237c6e9d59da349639de20e559e87c2c0218cfd","modified":1588077535423},{"_id":"themes/next/source/css/_schemes/Muse/sidebar/sidebar-blogroll.styl","hash":"1f09be9bb38411f0629b58c3b23873589a6dbcaa","modified":1588077535411},{"_id":"themes/next/source/css/_schemes/Mist/outline/outline.styl","hash":"a07aa12cc36ac5c819670c2a3c17d07ed7a08986","modified":1588077535411},{"_id":"themes/next/source/css/_schemes/Mist/sidebar/sidebar-blogroll.styl","hash":"1f09be9bb38411f0629b58c3b23873589a6dbcaa","modified":1588077535411},{"_id":"themes/next/source/css/_common/components/footer/footer.styl","hash":"4c4ef6e997d0c6e21de39c2daa0c768e12c8c6fa","modified":1588077535407},{"_id":"themes/next/source/css/_common/components/header/header.styl","hash":"53cde051e0337f4bf42fb8d6d7a79fa3fa6d4ef2","modified":1588077535407},{"_id":"themes/next/source/css/_common/components/header/headerband.styl","hash":"d63e0cacc53dd375fcc113465a4328c59ff5f2c1","modified":1588077535407},{"_id":"themes/next/source/css/_common/components/header/menu.styl","hash":"1a0d059799a298fe17c49a44298d32cebde93785","modified":1588077535407},{"_id":"themes/next/source/css/_common/components/header/site-meta.styl","hash":"0656e753f182c9f47fef7304c847b7587a85ef0d","modified":1588077535407},{"_id":"themes/next/source/css/_common/components/header/site-nav.styl","hash":"1727702eac5d326b5c81a667944a245016668231","modified":1588077535407},{"_id":"themes/next/source/css/_common/components/highlight/diff.styl","hash":"167986d0f649516671ddf7193eebba7b421cd115","modified":1588077535407},{"_id":"themes/next/source/css/_common/components/highlight/highlight.styl","hash":"b3b783511bbd94af7e941abf8ff411885db7395b","modified":1588077535407},{"_id":"themes/next/source/css/_common/components/highlight/theme.styl","hash":"7fe4d4d656e86276c17cb4e48a560cb6a4def703","modified":1588077535407},{"_id":"themes/next/source/css/_common/components/pages/archive.styl","hash":"104b5c79cd891506e0beaf938b083685f1da8637","modified":1588077535407},{"_id":"themes/next/source/css/_common/components/pages/categories.styl","hash":"7fb593f90d74a99c21840679933b9ef6fdc16a61","modified":1588077535407},{"_id":"themes/next/source/css/_common/components/pages/pages.styl","hash":"f9760ecf186954cee3ba4a149be334e9ba296b89","modified":1588077535407},{"_id":"themes/next/source/css/_common/components/pages/post-detail.styl","hash":"4e3838d7ac81d9ad133960f0f7ed58a44a015285","modified":1588077535407},{"_id":"themes/next/source/css/_common/components/pages/schedule.styl","hash":"8cf318644acc8b4978537c263290363e21c7f5af","modified":1588077535407},{"_id":"themes/next/source/css/_common/components/post/post-button.styl","hash":"4783f85872bc7e218c1522a5c1c68cd27a5922db","modified":1588077535407},{"_id":"themes/next/source/css/_common/components/post/post-collapse.styl","hash":"875cbe88d5c7f6248990e2beb97c9828920e7e24","modified":1588077535407},{"_id":"themes/next/source/css/_common/components/post/post-copyright.styl","hash":"caf263d1928496688c0e1419801eafd7e6919ce5","modified":1588077535407},{"_id":"themes/next/source/css/_common/components/post/post-eof.styl","hash":"a200c0a1c5a895ac9dc41e0641a5dfcd766be99b","modified":1588077535407},{"_id":"themes/next/source/css/_common/components/post/post-expand.styl","hash":"58f9e6aba94733244a87d2ba5966c5a009486509","modified":1588077535407},{"_id":"themes/next/source/css/_common/components/post/post-gallery.styl","hash":"cd9e214e502697f2f2db84eb721bac57a49b0fce","modified":1588077535407},{"_id":"themes/next/source/css/_common/components/post/post-meta.styl","hash":"658accf8e196721f295003da66941e6d1f7b81b0","modified":1588077535407},{"_id":"themes/next/source/css/_common/components/post/post-nav.styl","hash":"27deb3d3a243d30022055dac7dad851024099a8b","modified":1588077535407},{"_id":"themes/next/source/css/_common/components/post/post-reward.styl","hash":"f363a544aa800a2a5ed97c40887fe9743f67b03b","modified":1588077535407},{"_id":"themes/next/source/css/_common/components/post/post-rtl.styl","hash":"b2495ae5e04dcca610aacadc47881d9e716cd440","modified":1588077535407},{"_id":"themes/next/source/css/_common/components/post/post-tags.styl","hash":"5a982d8ef3b3623ea5f59e63728990f5623c1b57","modified":1588077535407},{"_id":"themes/next/source/css/_common/components/post/post-title.styl","hash":"350469437b20ecfd6f3ca45e400478f8e3f71cfb","modified":1588077535407},{"_id":"themes/next/source/css/_common/components/post/post-type.styl","hash":"01567edaea6978628aa5521a122a85434c418bfd","modified":1588077535407},{"_id":"themes/next/source/css/_common/components/post/post-widgets.styl","hash":"2cb09973d29a8e34e2a3425ac6e0938296970d8e","modified":1588077535407},{"_id":"themes/next/source/css/_common/components/post/post-wordcount.styl","hash":"268c9704481fdb0b4d1e646196386143990fe235","modified":1588077535407},{"_id":"themes/next/source/css/_common/components/post/post.styl","hash":"efc65bba7f2423439e9bca7d32ef7728c21e5c97","modified":1588077535407},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-author-links.styl","hash":"5f3510419161ec22ca88cce6a181ddad61de9e86","modified":1588077535407},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-author.styl","hash":"761eba9811b050b25d548cc0854de4824b41eb08","modified":1588077535407},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-blogroll.styl","hash":"ac060861b27b764bc4012fc362a25a332df4045a","modified":1588077535407},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-dimmer.styl","hash":"11c22f0fb3f6beb13e5a425ec064a4ff974c13b7","modified":1588077535407},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-feed-link.styl","hash":"61f8cea3c01acd600e90e1bc2a07def405503748","modified":1588077535407},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-nav.styl","hash":"1153bb71edf253765145559674390e16dd67c633","modified":1588077535407},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-toc.styl","hash":"28a8737c090fbffd188d73a00b42e90b9ee57df2","modified":1588077535407},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-toggle.styl","hash":"a1521d48bb06d8d703753f52a198baa197af7da2","modified":1588077535407},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar.styl","hash":"8e00d9a0bdf35ffc0a7fa387fa294b953c2d28fc","modified":1588077535407},{"_id":"themes/next/source/css/_common/components/sidebar/site-state.styl","hash":"e71652d3216e289c8548b1ea2357822c1476a425","modified":1588077535407},{"_id":"themes/next/source/css/_common/components/tags/blockquote-center.styl","hash":"2fe76476432b31993338cb45cdb3b29a518b6379","modified":1588077535407},{"_id":"themes/next/source/css/_common/components/tags/exturl.styl","hash":"a3bdd71237afc112b2aa255f278cab6baeb25351","modified":1588077535407},{"_id":"themes/next/source/css/_common/components/tags/full-image.styl","hash":"3159b55f35c40bd08e55b00148c523760a708c51","modified":1588077535407},{"_id":"themes/next/source/css/_common/components/tags/group-pictures.styl","hash":"2ad1a2a9bbf6742d1b0762c4c623b68113d1e0fe","modified":1588077535407},{"_id":"themes/next/source/css/_common/components/tags/label.styl","hash":"2ab1322fe52ab5aafd49e68f5bd890e8380ee927","modified":1588077535407},{"_id":"themes/next/source/css/_common/components/tags/note-modern.styl","hash":"962b654f8f7cbd18a298126a403d236ed4540516","modified":1588077535407},{"_id":"themes/next/source/css/_common/components/tags/note.styl","hash":"9a409b798decdefdaf7a23f0b11004a8c27e82f3","modified":1588077535407},{"_id":"themes/next/source/css/_common/components/tags/tabs.styl","hash":"154a87a32d2fead480d5e909c37f6c476671c5e6","modified":1588077535407},{"_id":"themes/next/source/css/_common/components/tags/tags.styl","hash":"b80604868e4f5cf20fccafd7ee415c20c804f700","modified":1588077535407},{"_id":"themes/next/source/css/_common/components/third-party/algolia-search.styl","hash":"bba4f3bdb7517cd85376df3e1209b570c0548c69","modified":1588077535407},{"_id":"themes/next/source/css/_common/components/third-party/baidushare.styl","hash":"5dbeed535d63a50265d96b396a5440f9bb31e4ba","modified":1588077535407},{"_id":"themes/next/source/css/_common/components/third-party/busuanzi-counter.styl","hash":"a6e7d698702c2e383dde3fde2abde27951679084","modified":1588077535407},{"_id":"themes/next/source/css/_common/components/third-party/duoshuo.styl","hash":"717cc7f82be9cc151e23a7678601ff2fd3a7fa1d","modified":1588077535407},{"_id":"themes/next/source/css/_common/components/third-party/han.styl","hash":"10599e16414a8b7a76c4e79e6617b5fe3d4d1adf","modified":1588077535407},{"_id":"themes/next/source/css/_common/components/third-party/localsearch.styl","hash":"16087276945fa038f199692e3eabb1c52b8ea633","modified":1588077535407},{"_id":"themes/next/source/css/_common/components/third-party/jiathis.styl","hash":"15975ba7456b96916b1dbac448a1a0d2c38b8f3d","modified":1588077535407},{"_id":"themes/next/source/css/_common/components/third-party/third-party.styl","hash":"37e406ec42b7a53c72395bdbaa434270019e7179","modified":1588077535407},{"_id":"themes/next/source/lib/Han/dist/font/han-space.otf","hash":"07436f011b44051f61b8329c99de4bec64e86f4b","modified":1588077535419},{"_id":"themes/next/source/lib/Han/dist/font/han-space.woff","hash":"7a635062b10bf5662ae1d218ba0980171005d060","modified":1588077535419},{"_id":"themes/next/source/lib/Han/dist/font/han.otf","hash":"f1f6bb8f461f5672e000380195d3d2358a28494c","modified":1588077535419},{"_id":"themes/next/source/lib/Han/dist/font/han.woff","hash":"f38ff9b2eecaa17b50b66aa2dae87e9e7436d195","modified":1588077535419},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-buttons.css","hash":"1a9d8e5c22b371fcc69d4dbbb823d9c39f04c0c8","modified":1588077535419},{"_id":"themes/next/source/lib/fancybox/source/helpers/fancybox_buttons.png","hash":"e385b139516c6813dcd64b8fc431c364ceafe5f3","modified":1588077535419},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-buttons.js","hash":"91e41741c2e93f732c82aaacec4cfc6e3f3ec876","modified":1588077535419},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-media.js","hash":"3bdf69ed2469e4fb57f5a95f17300eef891ff90d","modified":1588077535419},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-thumbs.css","hash":"4ac329c16a5277592fc12a37cca3d72ca4ec292f","modified":1588077535419},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-thumbs.js","hash":"53e194f4a72e649c04fb586dd57762b8c022800b","modified":1588077535419},{"_id":"themes/next/source/lib/font-awesome/fonts/FontAwesome.otf","hash":"048707bc52ac4b6563aaa383bfe8660a0ddc908c","modified":1588077535423},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.eot","hash":"d980c2ce873dc43af460d4d572d441304499f400","modified":1588077535423},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.ttf","hash":"13b1eab65a983c7a73bc7997c479d66943f7c6cb","modified":1588077535423},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.js","hash":"90a1b22129efc172e2dfcceeeb76bff58bc3192f","modified":1588077535419},{"_id":"themes/next/source/lib/three/three.min.js","hash":"26273b1cb4914850a89529b48091dc584f2c57b8","modified":1588077535423},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.svg","hash":"b5483b11f8ba213e733b5b8af9927a04fec996f6","modified":1588077535423},{"_id":"themes/next/source/images/avatar.gif","hash":"5b72c7a6939173933d8b2da234f4fa7ebfe42382","modified":1588077535419},{"_id":"public/search.xml","hash":"b3e2879e02fd6dbbcba34ceeb7eff3d942e3a2a8","modified":1588077567742},{"_id":"public/about/index.html","hash":"495686e4ea087d6c7678b1a30ff8a9f87da7c72d","modified":1588077567747},{"_id":"public/categories/index.html","hash":"3ebdad01980253b897ce69bc540f59a5b65e8f24","modified":1588077567747},{"_id":"public/tags/index.html","hash":"089d2ac294f5ab405ac65a20264dae6cc0cc454c","modified":1588077567747},{"_id":"public/2018/11/30/MalongTech-Learning-3D-Image-Segmentation/index.html","hash":"a3c1df3b20705bf0b6e58e585df32f83a9c0aa54","modified":1588077567747},{"_id":"public/2018/11/29/MalongTEch-Learning-Semantic-Soft-Segmentation/index.html","hash":"9a6f344a783a793b954c97aca16928efa4d3a9ae","modified":1588077567747},{"_id":"public/2018/11/26/MalongTech-Learning-Image-Segmentation/index.html","hash":"8a94758e3d5f60459dc5fa1126b911b8cde476e0","modified":1588077567747},{"_id":"public/2018/11/12/MalongTech-Learning-Object-Detection/index.html","hash":"1cf847daa4021139faeb707f9de7deda81bfa28a","modified":1588077567747},{"_id":"public/2018/11/12/LInux/index.html","hash":"9f806b2a70945c64fa31fd86022d4c1bc9bf3826","modified":1588077567747},{"_id":"public/2018/05/25/LogBook/index.html","hash":"4b2dd40906e4c3fffd17f071fdd16db7c288c540","modified":1588077567748},{"_id":"public/2018/04/30/Apri/index.html","hash":"d81760a163345028acf06b60d735dc5acdb97b0a","modified":1588077567748},{"_id":"public/2018/01/31/Machine Learning in NBA/index.html","hash":"57f82560722a8c8316f6ad87abf97a6864d67625","modified":1588077567748},{"_id":"public/2018/01/28/MXnet_Config/index.html","hash":"b23df582e6fbd00415b05d32f4b28b376963ddcb","modified":1588077567748},{"_id":"public/2018/01/28/IT salary in USA/index.html","hash":"5b7c219cfedf22c57aef87191e475a82103fb264","modified":1588077567748},{"_id":"public/archives/index.html","hash":"1f8f5d8a923c6d30fc13155f386be1ec24111865","modified":1588077567748},{"_id":"public/archives/2018/index.html","hash":"179a744cf317b7c4103ddb670a23a16ef17ee42b","modified":1588077567748},{"_id":"public/archives/2018/01/index.html","hash":"77fb4a4b0c4fe60ff6c5b0c5c69159b32732d626","modified":1588077567748},{"_id":"public/archives/2018/04/index.html","hash":"716d281645980a17d03796c92c385227bfd6c87e","modified":1588077567748},{"_id":"public/archives/2018/05/index.html","hash":"de1a23066fa46886b8f010ce267489ea4e1d4442","modified":1588077567748},{"_id":"public/archives/2018/11/index.html","hash":"35111fec922b29b7466e5f729f425882b3f717c1","modified":1588077567749},{"_id":"public/index.html","hash":"338254a39e47b1974542263aad1470452cb68686","modified":1588077567749},{"_id":"public/categories/My-Life/index.html","hash":"c4aa6d9b05083781e4779a4d1121c17c34e298b5","modified":1588077567749},{"_id":"public/categories/Data-Visualisation/index.html","hash":"c5594dd8f32d04d032ab7c3374b65c09e002729b","modified":1588077567749},{"_id":"public/categories/System/index.html","hash":"09261c058d217eb8f3e286efb0a8fb47acc4c914","modified":1588077567749},{"_id":"public/categories/Machine-Learning/index.html","hash":"756a2c225e1a66f5ac05ebaabd2e1ce44607260d","modified":1588077567749},{"_id":"public/categories//index.html","hash":"cd30cd8677340f83440a9e682e87acf6aae1faed","modified":1588077567749},{"_id":"public/categories/Msc-Project/index.html","hash":"d8b5b172885c200e20f3c153851eb2d773f284dd","modified":1588077567749},{"_id":"public/tags/A/index.html","hash":"b306a5a49f06f6093eaa5be2b85f555f4ef42775","modified":1588077567749},{"_id":"public/tags/data-analysis/index.html","hash":"3747074801c435e18487a5cc7bc135459b170677","modified":1588077567749},{"_id":"public/tags/Linux/index.html","hash":"4a2259247cfb079dff25911be215f5f3fbe773e2","modified":1588077567749},{"_id":"public/tags/MXnet/index.html","hash":"32c63b6d5f91711b7a8d544bdd4e66163711730d","modified":1588077567749},{"_id":"public/tags/Machine-Learning/index.html","hash":"01475b7e1c0b4a5f5c5b7ed8f33b1912224298b3","modified":1588077567750},{"_id":"public/tags/NBA/index.html","hash":"85c0d9f68eb724c2684ce089610cf1b40f2d6b94","modified":1588077567750},{"_id":"public/tags/Deep-Learning/index.html","hash":"444c0b1ce2b61f87fb2b7bf760d723045a9fe473","modified":1588077567750},{"_id":"public/tags/Object-Detection/index.html","hash":"bb9d816bc739df783a85463eaee14cd6e8a944d5","modified":1588077567750},{"_id":"public/CNAME","hash":"ab075e42f34723bce8bab60d8b6da686e2c40faf","modified":1588077567756},{"_id":"public/about/resume_en.pdf","hash":"10b6e617530d73b17a81c9f855d1656adb3673f1","modified":1588077567756},{"_id":"public/about/resume_cn.pdf","hash":"87a665f33c05dfe969edf2001c4474a3f9ee5195","modified":1588077567756},{"_id":"public/images/algolia_logo.svg","hash":"45eeea0b5fba833e21e38ea10ed5ab385ceb4f01","modified":1588077567756},{"_id":"public/images/cc-by-nc-sa.svg","hash":"6f55543d1fb9cbc436c101d24f802dec7b41efc3","modified":1588077567756},{"_id":"public/images/cc-by-nc-nd.svg","hash":"bc3588c9b2d7c68830524783120ff6cf957cf668","modified":1588077567756},{"_id":"public/images/cc-by-nc.svg","hash":"6f076713fb9bf934aa2c1046bdf2cf2e37bc1eab","modified":1588077567756},{"_id":"public/images/cc-by-nd.svg","hash":"42cd73da328077ccc92f859bb8f3cf621b3484f8","modified":1588077567756},{"_id":"public/images/cc-by.svg","hash":"e92a33c32d1dac8ed94849b2b4e6456e887efe70","modified":1588077567756},{"_id":"public/images/cc-by-sa.svg","hash":"70c1535f43e54e5ff35ca81419e77e4c0c301398","modified":1588077567756},{"_id":"public/images/cc-zero.svg","hash":"9bfb52b2f63527a7049247bf00d44e6dc1170e7d","modified":1588077567757},{"_id":"public/images/favicon.ico","hash":"17c483c2915dbd1c84cc42d124a7199bdc7ade1f","modified":1588077567757},{"_id":"public/images/loading.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1588077567757},{"_id":"public/images/placeholder.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1588077567757},{"_id":"public/images/quote-l.svg","hash":"cd108d6f44351cadf8e6742565217f88818a0458","modified":1588077567757},{"_id":"public/images/quote-r.svg","hash":"2a2a250b32a87c69dcc1b1976c74b747bedbfb41","modified":1588077567757},{"_id":"public/images/searchicon.png","hash":"67727a6a969be0b2659b908518fa6706eed307b8","modified":1588077567757},{"_id":"public/lib/fastclick/LICENSE","hash":"6f474ea75c42442da7bbcf2e9143ce98258efd8d","modified":1588077567757},{"_id":"public/lib/font-awesome/HELP-US-OUT.txt","hash":"ee33b2798b1e714b904d663436c6b3521011d1fa","modified":1588077567757},{"_id":"public/lib/fancybox/source/fancybox_loading.gif","hash":"1a755fb2599f3a313cc6cfdb14df043f8c14a99c","modified":1588077567757},{"_id":"public/lib/fancybox/source/blank.gif","hash":"2daeaa8b5f19f0bc209d976c02bd6acb51b00b0a","modified":1588077567757},{"_id":"public/lib/fancybox/source/fancybox_loading@2x.gif","hash":"273b123496a42ba45c3416adb027cd99745058b0","modified":1588077567757},{"_id":"public/lib/fancybox/source/fancybox_overlay.png","hash":"b3a4ee645ba494f52840ef8412015ba0f465dbe0","modified":1588077567757},{"_id":"public/lib/fancybox/source/fancybox_sprite.png","hash":"17df19f97628e77be09c352bf27425faea248251","modified":1588077567757},{"_id":"public/lib/fancybox/source/fancybox_sprite@2x.png","hash":"30c58913f327e28f466a00f4c1ac8001b560aed8","modified":1588077567757},{"_id":"public/lib/font-awesome/css/font-awesome.css.map","hash":"1573904b82807abbb32c97a3632c6c6808eaac50","modified":1588077567757},{"_id":"public/lib/Han/dist/font/han-space.otf","hash":"07436f011b44051f61b8329c99de4bec64e86f4b","modified":1588077567758},{"_id":"public/lib/Han/dist/font/han-space.woff","hash":"7a635062b10bf5662ae1d218ba0980171005d060","modified":1588077567758},{"_id":"public/lib/Han/dist/font/han.otf","hash":"f1f6bb8f461f5672e000380195d3d2358a28494c","modified":1588077567758},{"_id":"public/lib/Han/dist/font/han.woff","hash":"f38ff9b2eecaa17b50b66aa2dae87e9e7436d195","modified":1588077567758},{"_id":"public/lib/fancybox/source/helpers/fancybox_buttons.png","hash":"e385b139516c6813dcd64b8fc431c364ceafe5f3","modified":1588077567758},{"_id":"public/images/alipay-reward-image.jpg","hash":"ee38a3c5ac61f24467a668ba5e7246053b7b1b8c","modified":1588077568154},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.woff","hash":"28b782240b3e76db824e12c02754a9731a167527","modified":1588077568154},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.woff2","hash":"d6f48cba7d076fb6f2fd6ba993a75b9dc1ecbf0c","modified":1588077568154},{"_id":"public/js/src/affix.js","hash":"978e0422b5bf1b560236d8d10ebc1adcf66392e3","modified":1588077568158},{"_id":"public/js/src/bootstrap.js","hash":"6117f97b4984b8e33f21c726132da64ba678e4ed","modified":1588077568158},{"_id":"public/js/src/exturl.js","hash":"e42e2aaab7bf4c19a0c8e779140e079c6aa5c0b1","modified":1588077568158},{"_id":"public/js/src/algolia-search.js","hash":"b172f697ed339a24b1e80261075232978d164c35","modified":1588077568158},{"_id":"public/js/src/hook-duoshuo.js","hash":"a6119070c0119f33e08b29da7d2cce2635eb40a0","modified":1588077568158},{"_id":"public/js/src/motion.js","hash":"dc0365b2fb315a8b43d3ef19b59d3a82a366fcc1","modified":1588077568159},{"_id":"public/js/src/js.cookie.js","hash":"9b37973a90fd50e71ea91682265715e45ae82c75","modified":1588077568159},{"_id":"public/js/src/post-details.js","hash":"0693695a9512641daff63d99da772625a058ab18","modified":1588077568159},{"_id":"public/js/src/scrollspy.js","hash":"fe4da1b9fe73518226446f5f27d2831e4426fc35","modified":1588077568159},{"_id":"public/js/src/scroll-cookie.js","hash":"09dc828cbf5f31158ff6250d2bf7c3cde6365c67","modified":1588077568159},{"_id":"public/js/src/utils.js","hash":"2917c39c75b14b6dab7e1c46ab4d87b4df9fcd5d","modified":1588077568159},{"_id":"public/lib/algolia-instant-search/instantsearch.min.css","hash":"90ef19edc982645b118b095615838d9c5eaba0de","modified":1588077568159},{"_id":"public/lib/canvas-nest/canvas-nest.min.js","hash":"0387e75e23b1db108a755073fe52a0d03eb391a7","modified":1588077568159},{"_id":"public/lib/canvas-ribbon/canvas-ribbon.js","hash":"7fd2f3e2773555392ef40df40cae3bedb884f17a","modified":1588077568159},{"_id":"public/lib/fastclick/bower.json","hash":"4dcecf83afddba148464d5339c93f6d0aa9f42e9","modified":1588077568159},{"_id":"public/lib/font-awesome/bower.json","hash":"64394a2a9aa00f8e321d8daa5e51a420f0e96dad","modified":1588077568159},{"_id":"public/lib/jquery_lazyload/bower.json","hash":"ae3c3b61e6e7f9e1d7e3585ad854380ecc04cf53","modified":1588077568159},{"_id":"public/lib/jquery_lazyload/jquery.lazyload.js","hash":"481fd478650e12b67c201a0ea41e92743f8b45a3","modified":1588077568159},{"_id":"public/lib/jquery_lazyload/jquery.scrollstop.js","hash":"0e9a81785a011c98be5ea821a8ed7d411818cfd1","modified":1588077568159},{"_id":"public/lib/pace/pace-theme-barber-shop.min.css","hash":"ee0d51446cb4ffe1bb96bd7bc8c8e046dddfcf46","modified":1588077568159},{"_id":"public/lib/pace/pace-theme-big-counter.min.css","hash":"5b561dc328af4c4d512e20a76fe964d113a32ba8","modified":1588077568160},{"_id":"public/lib/pace/pace-theme-bounce.min.css","hash":"f6bdb9a785b7979dd8ec5c60e278af955ef1e585","modified":1588077568160},{"_id":"public/lib/pace/pace-theme-center-atom.min.css","hash":"dcf79c24fe5350fb73d8038573a104e73639e9d3","modified":1588077568160},{"_id":"public/lib/pace/pace-theme-center-circle.min.css","hash":"a4066769c78affbfbc5e30a600e2c7862cd532e0","modified":1588077568160},{"_id":"public/lib/pace/pace-theme-center-radar.min.css","hash":"ab7cba998bf4c03b13df342bf43647fa4f419783","modified":1588077568160},{"_id":"public/lib/pace/pace-theme-center-simple.min.css","hash":"67f44c947548bd4d77e7590d3f59e236cbf9e98a","modified":1588077568160},{"_id":"public/lib/pace/pace-theme-corner-indicator.min.css","hash":"b3c64c973f31884e3d8145989476707333406b9a","modified":1588077568160},{"_id":"public/lib/pace/pace-theme-fill-left.min.css","hash":"0bec1e235a4a2cccda3f993b205424e1441a44ae","modified":1588077568160},{"_id":"public/lib/pace/pace-theme-flash.min.css","hash":"13ace22c40312d7bbd8d9c1e50eff897a7a497d8","modified":1588077568160},{"_id":"public/lib/pace/pace-theme-loading-bar.min.css","hash":"7ee28875dfc1230d76c537f6605766e8d4011e9f","modified":1588077568160},{"_id":"public/lib/pace/pace-theme-minimal.min.css","hash":"9cd783cceb8a191f3c8b5d81f7a430ecc3e489d3","modified":1588077568160},{"_id":"public/lib/pace/pace-theme-mac-osx.min.css","hash":"9f2e7b51b084da407863826b25265b31150b3821","modified":1588077568160},{"_id":"public/lib/pace/pace.min.js","hash":"9944dfb7814b911090e96446cea4d36e2b487234","modified":1588077568160},{"_id":"public/lib/velocity/bower.json","hash":"0ef14e7ccdfba5db6eb3f8fc6aa3b47282c36409","modified":1588077568160},{"_id":"public/lib/velocity/velocity.ui.min.js","hash":"ed5e534cd680a25d8d14429af824f38a2c7d9908","modified":1588077568160},{"_id":"public/js/src/schemes/pisces.js","hash":"79da92119bc246fe05d1626ac98426a83ec90a94","modified":1588077568160},{"_id":"public/lib/fancybox/source/jquery.fancybox.css","hash":"5f163444617b6cf267342f06ac166a237bb62df9","modified":1588077568161},{"_id":"public/lib/fastclick/lib/fastclick.min.js","hash":"2cae0f5a6c5d6f3cb993015e6863f9483fc4de18","modified":1588077568161},{"_id":"public/lib/ua-parser-js/dist/ua-parser.pack.js","hash":"214dad442a92d36af77ed0ca1d9092b16687f02f","modified":1588077568161},{"_id":"public/lib/ua-parser-js/dist/ua-parser.min.js","hash":"38628e75e4412cc6f11074e03e1c6d257aae495b","modified":1588077568161},{"_id":"public/lib/fancybox/source/helpers/jquery.fancybox-buttons.css","hash":"1a9d8e5c22b371fcc69d4dbbb823d9c39f04c0c8","modified":1588077568162},{"_id":"public/lib/fancybox/source/helpers/jquery.fancybox-buttons.js","hash":"91e41741c2e93f732c82aaacec4cfc6e3f3ec876","modified":1588077568162},{"_id":"public/lib/fancybox/source/helpers/jquery.fancybox-media.js","hash":"3bdf69ed2469e4fb57f5a95f17300eef891ff90d","modified":1588077568162},{"_id":"public/lib/fancybox/source/helpers/jquery.fancybox-thumbs.css","hash":"4ac329c16a5277592fc12a37cca3d72ca4ec292f","modified":1588077568162},{"_id":"public/lib/fancybox/source/helpers/jquery.fancybox-thumbs.js","hash":"53e194f4a72e649c04fb586dd57762b8c022800b","modified":1588077568162},{"_id":"public/lib/fastclick/README.html","hash":"da3c74d484c73cc7df565e8abbfa4d6a5a18d4da","modified":1588077568162},{"_id":"public/lib/jquery_lazyload/CONTRIBUTING.html","hash":"a6358170d346af13b1452ac157b60505bec7015c","modified":1588077568162},{"_id":"public/lib/jquery_lazyload/README.html","hash":"bde24335f6bc09d8801c0dcd7274f71b466552bd","modified":1588077568162},{"_id":"public/css/main.css","hash":"b1aae4aec9d11681d892254a39b87fc33d44e186","modified":1588077568162},{"_id":"public/lib/three/canvas_lines.min.js","hash":"dce4a3b65f8bf958f973690caa7ec4952f353b0c","modified":1588077568162},{"_id":"public/lib/three/three-waves.min.js","hash":"d968cba6b3a50b3626a02d67b544f349d83b147c","modified":1588077568162},{"_id":"public/lib/three/canvas_sphere.min.js","hash":"d8ea241a53c135a650f7335d2b6982b899fd58a9","modified":1588077568162},{"_id":"public/lib/velocity/velocity.ui.js","hash":"6a1d101eab3de87527bb54fcc8c7b36b79d8f0df","modified":1588077568163},{"_id":"public/lib/velocity/velocity.min.js","hash":"2f1afadc12e4cf59ef3b405308d21baa97e739c6","modified":1588077568163},{"_id":"public/lib/Han/dist/han.css","hash":"bd40da3fba8735df5850956814e312bd7b3193d7","modified":1588077568163},{"_id":"public/lib/Han/dist/han.min.css","hash":"a0c9e32549a8b8cf327ab9227b037f323cdb60ee","modified":1588077568163},{"_id":"public/lib/jquery/index.js","hash":"41b4bfbaa96be6d1440db6e78004ade1c134e276","modified":1588077568163},{"_id":"public/lib/Han/dist/han.min.js","hash":"f559c68a25065a14f47da954a7617d87263e409d","modified":1588077568163},{"_id":"public/lib/fancybox/source/jquery.fancybox.pack.js","hash":"53360764b429c212f424399384417ccc233bb3be","modified":1588077568163},{"_id":"public/lib/fancybox/source/jquery.fancybox.js","hash":"1cf3d47b5ccb7cb6e9019c64f2a88d03a64853e4","modified":1588077568163},{"_id":"public/lib/fastclick/lib/fastclick.js","hash":"06cef196733a710e77ad7e386ced6963f092dc55","modified":1588077568163},{"_id":"public/lib/font-awesome/css/font-awesome.css","hash":"0140952c64e3f2b74ef64e050f2fe86eab6624c8","modified":1588077568163},{"_id":"public/lib/font-awesome/css/font-awesome.min.css","hash":"512c7d79033e3028a9be61b540cf1a6870c896f8","modified":1588077568163},{"_id":"public/lib/Han/dist/han.js","hash":"e345397e0585c9fed1449e614ec13e0224acf2ab","modified":1588077568163},{"_id":"public/lib/velocity/velocity.js","hash":"9f08181baea0cc0e906703b7e5df9111b9ef3373","modified":1588077568164},{"_id":"public/lib/algolia-instant-search/instantsearch.min.js","hash":"9ccc6f8144f54e86df9a3fd33a18368d81cf3a4f","modified":1588077568164},{"_id":"public/lib/three/three.min.js","hash":"73f4cdc17e51a72b9bf5b9291f65386d615c483b","modified":1588077568164},{"_id":"public/lib/font-awesome/fonts/FontAwesome.otf","hash":"048707bc52ac4b6563aaa383bfe8660a0ddc908c","modified":1588077568164},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.eot","hash":"d980c2ce873dc43af460d4d572d441304499f400","modified":1588077568164},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.ttf","hash":"13b1eab65a983c7a73bc7997c479d66943f7c6cb","modified":1588077568165},{"_id":"public/images/wechat-reward-image.jpg","hash":"a81a23bcba5d49564c5afe2210957a86a8175431","modified":1588077568204},{"_id":"public/uploads/avatar.jpg","hash":"e3e7311f9624008234681ff57e411471dd80f379","modified":1588077568208},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.svg","hash":"b5483b11f8ba213e733b5b8af9927a04fec996f6","modified":1588077568211},{"_id":"public/images/avatar.gif","hash":"5b72c7a6939173933d8b2da234f4fa7ebfe42382","modified":1588077568229}],"Category":[{"name":"My Life","_id":"ck9jwdnh6000cjlrcwlvmu5ot"},{"name":"Data Visualisation","_id":"ck9jwdnha000ejlrc7w8nwbju"},{"name":"System","_id":"ck9jwdnha000gjlrcmtigkx3m"},{"name":"Machine Learning","_id":"ck9jwdnhb000ijlrcrmjbet7n"},{"name":"","_id":"ck9jwdnhd000mjlrcleeolcba"},{"name":"Msc Project","_id":"ck9jwdnmm001fjlrcfjvwudkl"}],"Data":[],"Page":[{"title":"About KAMISAMA","date":"2018-04-25T20:34:58.000Z","comments":0,"_content":"\n---\n#    LeBron James\n\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/about/LBJ1.gif)\n\n---\n\n---\n# /Resume\n\n[](resume_cn.pdf)\n[Resume_En](resume_en.pdf)\n\n---\n","source":"about/index.md","raw":"---\ntitle: About KAMISAMA\ndate: 2018-04-26 04:34:58\ncomments: false\n---\n\n---\n#    LeBron James\n\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/about/LBJ1.gif)\n\n---\n\n---\n# /Resume\n\n[](resume_cn.pdf)\n[Resume_En](resume_en.pdf)\n\n---\n","updated":"2020-04-28T12:38:55.395Z","path":"about/index.html","layout":"page","_id":"ck9jwdnfv0000jlrcrr2nd8x5","content":"<hr>\n<h1 id=\"--LeBron-James\"><a href=\"#--LeBron-James\" class=\"headerlink\" title=\"   LeBron James\"></a>   LeBron James</h1><p><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/about/LBJ1.gif\" alt=\"image\"></p>\n<hr>\n<hr>\n<h1 id=\"-Resume\"><a href=\"#-Resume\" class=\"headerlink\" title=\"/Resume\"></a>/Resume</h1><p><a href=\"resume_cn.pdf\"></a><br><a href=\"resume_en.pdf\">Resume_En</a></p>\n<hr>\n","site":{"data":{}},"excerpt":"","more":"<hr>\n<h1 id=\"--LeBron-James\"><a href=\"#--LeBron-James\" class=\"headerlink\" title=\"   LeBron James\"></a>   LeBron James</h1><p><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/about/LBJ1.gif\" alt=\"image\"></p>\n<hr>\n<hr>\n<h1 id=\"-Resume\"><a href=\"#-Resume\" class=\"headerlink\" title=\"/Resume\"></a>/Resume</h1><p><a href=\"resume_cn.pdf\"></a><br><a href=\"resume_en.pdf\">Resume_En</a></p>\n<hr>\n"},{"title":"Categories","date":"2018-04-25T19:21:32.000Z","type":"categories","comments":0,"_content":"","source":"categories/index.md","raw":"---\ntitle: Categories\ndate: 2018-04-26 03:21:32\ntype: \"categories\"\ncomments: false\n---\n","updated":"2020-04-28T12:38:55.395Z","path":"categories/index.html","layout":"page","_id":"ck9jwdng10002jlrcoib9utu0","content":"","site":{"data":{}},"excerpt":"","more":""},{"title":"Tags","date":"2018-04-25T19:03:03.000Z","type":"tags","comments":0,"_content":"","source":"tags/index.md","raw":"---\ntitle: Tags\ndate: 2018-04-26 03:03:03\ntype: \"tags\"\ncomments: false\n---\n","updated":"2020-04-28T12:38:55.395Z","path":"tags/index.html","layout":"page","_id":"ck9jwdng20004jlrcjb5bj9ob","content":"","site":{"data":{}},"excerpt":"","more":""}],"Post":[{"title":"","date":"2018-04-30T12:04:14.000Z","_content":"\n## YouTube Source\n[AMV Your Lie in April](https://www.youtube.com/watch?v=-fFqguvUmwM)\n\n<iframe width=\"731\" height=\"411\" src=\"https://www.youtube.com/embed/-fFqguvUmwM\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>\n\n## BiliBili Source\n[](https://www.bilibili.com/video/av2113169/)\n\n## 4\n<center>\n****\n****\n\n</center>\n\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/YourLieInApril/1.png)\n<!-- more -->\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/YourLieInApril/2.png)\n\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/YourLieInApril/3.png)\n\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/YourLieInApril/4.png)\n\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/YourLieInApril/5.png)\n\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/YourLieInApril/6.png)\n\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/YourLieInApril/7.png)\n\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/YourLieInApril/8.png)\n\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/YourLieInApril/9.png)","source":"_posts/Apri.md","raw":"---\ntitle: \ndate: 2018-04-30 20:04:14\ntags: [A]\ncategories: My Life\n---\n\n## YouTube Source\n[AMV Your Lie in April](https://www.youtube.com/watch?v=-fFqguvUmwM)\n\n<iframe width=\"731\" height=\"411\" src=\"https://www.youtube.com/embed/-fFqguvUmwM\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>\n\n## BiliBili Source\n[](https://www.bilibili.com/video/av2113169/)\n\n## 4\n<center>\n****\n****\n\n</center>\n\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/YourLieInApril/1.png)\n<!-- more -->\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/YourLieInApril/2.png)\n\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/YourLieInApril/3.png)\n\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/YourLieInApril/4.png)\n\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/YourLieInApril/5.png)\n\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/YourLieInApril/6.png)\n\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/YourLieInApril/7.png)\n\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/YourLieInApril/8.png)\n\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/YourLieInApril/9.png)","slug":"Apri","published":1,"updated":"2020-04-28T12:38:55.395Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck9jwdnfx0001jlrcgips8q0o","content":"<h2 id=\"YouTube-Source\"><a href=\"#YouTube-Source\" class=\"headerlink\" title=\"YouTube Source\"></a>YouTube Source</h2><p><a href=\"https://www.youtube.com/watch?v=-fFqguvUmwM\" target=\"_blank\" rel=\"noopener\">AMV Your Lie in April</a></p>\n<iframe width=\"731\" height=\"411\" src=\"https://www.youtube.com/embed/-fFqguvUmwM\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>\n\n<h2 id=\"BiliBili-Source\"><a href=\"#BiliBili-Source\" class=\"headerlink\" title=\"BiliBili Source\"></a>BiliBili Source</h2><p><a href=\"https://www.bilibili.com/video/av2113169/\" target=\"_blank\" rel=\"noopener\"></a></p>\n<h2 id=\"4\"><a href=\"#4\" class=\"headerlink\" title=\"4\"></a>4</h2><center><br><strong></strong><br><strong></strong><br><br></center>\n\n<p><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/YourLieInApril/1.png\" alt=\"image\"><br><a id=\"more\"></a><br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/YourLieInApril/2.png\" alt=\"image\"></p>\n<p><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/YourLieInApril/3.png\" alt=\"image\"></p>\n<p><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/YourLieInApril/4.png\" alt=\"image\"></p>\n<p><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/YourLieInApril/5.png\" alt=\"image\"></p>\n<p><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/YourLieInApril/6.png\" alt=\"image\"></p>\n<p><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/YourLieInApril/7.png\" alt=\"image\"></p>\n<p><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/YourLieInApril/8.png\" alt=\"image\"></p>\n<p><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/YourLieInApril/9.png\" alt=\"image\"></p>\n","site":{"data":{}},"excerpt":"<h2 id=\"YouTube-Source\"><a href=\"#YouTube-Source\" class=\"headerlink\" title=\"YouTube Source\"></a>YouTube Source</h2><p><a href=\"https://www.youtube.com/watch?v=-fFqguvUmwM\" target=\"_blank\" rel=\"noopener\">AMV Your Lie in April</a></p>\n<iframe width=\"731\" height=\"411\" src=\"https://www.youtube.com/embed/-fFqguvUmwM\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>\n\n<h2 id=\"BiliBili-Source\"><a href=\"#BiliBili-Source\" class=\"headerlink\" title=\"BiliBili Source\"></a>BiliBili Source</h2><p><a href=\"https://www.bilibili.com/video/av2113169/\" target=\"_blank\" rel=\"noopener\"></a></p>\n<h2 id=\"4\"><a href=\"#4\" class=\"headerlink\" title=\"4\"></a>4</h2><center><br><strong></strong><br><strong></strong><br><br></center>\n\n<p><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/YourLieInApril/1.png\" alt=\"image\"><br>","more":"<br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/YourLieInApril/2.png\" alt=\"image\"></p>\n<p><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/YourLieInApril/3.png\" alt=\"image\"></p>\n<p><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/YourLieInApril/4.png\" alt=\"image\"></p>\n<p><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/YourLieInApril/5.png\" alt=\"image\"></p>\n<p><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/YourLieInApril/6.png\" alt=\"image\"></p>\n<p><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/YourLieInApril/7.png\" alt=\"image\"></p>\n<p><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/YourLieInApril/8.png\" alt=\"image\"></p>\n<p><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/YourLieInApril/9.png\" alt=\"image\"></p>"},{"title":"IT salary in USA","date":"2018-01-28T00:23:32.000Z","_content":"\n## Result\n**Chart with Anlysis**\n\n**Website: SALARY DISTRIBUTION OF DIFFERENT JOBS IN USA**\n\n![image](https://raw.githubusercontent.com/Trouble404/IT-Salary-in-USA/master/readme_add_pic/page1.PNG)<!-- more -->\n![image](https://raw.githubusercontent.com/Trouble404/IT-Salary-in-USA/master/readme_add_pic/page2.PNG)\n![image](https://raw.githubusercontent.com/Trouble404/IT-Salary-in-USA/master/readme_add_pic/page3.PNG)\n---\n\n---\n\n## Data process\n### Datasets used to anlysis\n*    U.S. Technology Jobs on Dice.com  Found in [Kaggle](https://www.kaggle.com/PromptCloudHQ/us-technology-jobs-on-dicecom/data)\n*    US jobs on Monster.com  Found in [Kaggle](https://www.kaggle.com/PromptCloudHQ/us-jobs-on-monstercom)\n### Data Clean\n*    Obtain **salay** and coresspoding **states** information in US jos on Monster.com  [clean process](https://github.com/Trouble404/IT-Salary-in-USA/blob/master/chart/chart1/chart1_data.ipynb)\n*    Obtain **salay** and coresspoding **cities** information in US jos on Monster.com  [clean process](https://github.com/Trouble404/IT-Salary-in-USA/blob/master/chart/chart2/chart2_data.ipynb)\n*    Obtain **skills** information in U.S Technology Jobs on Dice.com  [clean process](https://github.com/Trouble404/IT-Salary-in-USA/blob/master/chart/chart3/chart3_data.ipynb)\n\n---\n\n## Chart Plotting\n### Chart 1: Map chart of average salay of 3 types jobs\n[Tableau address](https://public.tableau.com/profile/j.zhang#!/vizhome/chart1_23/dashborad1)\n\n<img src=\"https://raw.githubusercontent.com/Trouble404/IT-Salary-in-USA/master/chart/chart1/Chart1.png\" width=\"1000px\" />\n\n### Chart 2: Bubble chart of average salay of 3 types jobs\n[Tableau address](https://public.tableau.com/profile/j.zhang#!/vizhome/chart2_10/dasbord1)\n\n<img src=\"https://raw.githubusercontent.com/Trouble404/IT-Salary-in-USA/master/chart/chart2/Chart2.png\" width=\"500px\" />\n\n### Chart 3 : Cloud words chart of skills used in IT-jobs\n[Cloud words tool](https://timdream.org/wordcloud/#wikipedia:Cloud)\n\n<img src=\"https://raw.githubusercontent.com/Trouble404/IT-Salary-in-USA/master/chart/chart3/Chart3.png\" width=\"500px\" />\n\n---\n","source":"_posts/IT salary in USA.md","raw":"---\ntitle: IT salary in USA\ndate: 2018-01-28 08:23:32\ntags: [data analysis]\ncategories: Data Visualisation\n---\n\n## Result\n**Chart with Anlysis**\n\n**Website: SALARY DISTRIBUTION OF DIFFERENT JOBS IN USA**\n\n![image](https://raw.githubusercontent.com/Trouble404/IT-Salary-in-USA/master/readme_add_pic/page1.PNG)<!-- more -->\n![image](https://raw.githubusercontent.com/Trouble404/IT-Salary-in-USA/master/readme_add_pic/page2.PNG)\n![image](https://raw.githubusercontent.com/Trouble404/IT-Salary-in-USA/master/readme_add_pic/page3.PNG)\n---\n\n---\n\n## Data process\n### Datasets used to anlysis\n*    U.S. Technology Jobs on Dice.com  Found in [Kaggle](https://www.kaggle.com/PromptCloudHQ/us-technology-jobs-on-dicecom/data)\n*    US jobs on Monster.com  Found in [Kaggle](https://www.kaggle.com/PromptCloudHQ/us-jobs-on-monstercom)\n### Data Clean\n*    Obtain **salay** and coresspoding **states** information in US jos on Monster.com  [clean process](https://github.com/Trouble404/IT-Salary-in-USA/blob/master/chart/chart1/chart1_data.ipynb)\n*    Obtain **salay** and coresspoding **cities** information in US jos on Monster.com  [clean process](https://github.com/Trouble404/IT-Salary-in-USA/blob/master/chart/chart2/chart2_data.ipynb)\n*    Obtain **skills** information in U.S Technology Jobs on Dice.com  [clean process](https://github.com/Trouble404/IT-Salary-in-USA/blob/master/chart/chart3/chart3_data.ipynb)\n\n---\n\n## Chart Plotting\n### Chart 1: Map chart of average salay of 3 types jobs\n[Tableau address](https://public.tableau.com/profile/j.zhang#!/vizhome/chart1_23/dashborad1)\n\n<img src=\"https://raw.githubusercontent.com/Trouble404/IT-Salary-in-USA/master/chart/chart1/Chart1.png\" width=\"1000px\" />\n\n### Chart 2: Bubble chart of average salay of 3 types jobs\n[Tableau address](https://public.tableau.com/profile/j.zhang#!/vizhome/chart2_10/dasbord1)\n\n<img src=\"https://raw.githubusercontent.com/Trouble404/IT-Salary-in-USA/master/chart/chart2/Chart2.png\" width=\"500px\" />\n\n### Chart 3 : Cloud words chart of skills used in IT-jobs\n[Cloud words tool](https://timdream.org/wordcloud/#wikipedia:Cloud)\n\n<img src=\"https://raw.githubusercontent.com/Trouble404/IT-Salary-in-USA/master/chart/chart3/Chart3.png\" width=\"500px\" />\n\n---\n","slug":"IT salary in USA","published":1,"updated":"2020-04-28T12:38:55.395Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck9jwdng10003jlrc7j1i0314","content":"<h2 id=\"Result\"><a href=\"#Result\" class=\"headerlink\" title=\"Result\"></a>Result</h2><p><strong>Chart with Anlysis</strong></p>\n<p><strong>Website: SALARY DISTRIBUTION OF DIFFERENT JOBS IN USA</strong></p>\n<p><img src=\"https://raw.githubusercontent.com/Trouble404/IT-Salary-in-USA/master/readme_add_pic/page1.PNG\" alt=\"image\"><a id=\"more\"></a><br><img src=\"https://raw.githubusercontent.com/Trouble404/IT-Salary-in-USA/master/readme_add_pic/page2.PNG\" alt=\"image\"></p>\n<h2 id><a href=\"#\" class=\"headerlink\" title></a><img src=\"https://raw.githubusercontent.com/Trouble404/IT-Salary-in-USA/master/readme_add_pic/page3.PNG\" alt=\"image\"></h2><hr>\n<h2 id=\"Data-process\"><a href=\"#Data-process\" class=\"headerlink\" title=\"Data process\"></a>Data process</h2><h3 id=\"Datasets-used-to-anlysis\"><a href=\"#Datasets-used-to-anlysis\" class=\"headerlink\" title=\"Datasets used to anlysis\"></a>Datasets used to anlysis</h3><ul>\n<li>U.S. Technology Jobs on Dice.com  Found in <a href=\"https://www.kaggle.com/PromptCloudHQ/us-technology-jobs-on-dicecom/data\" target=\"_blank\" rel=\"noopener\">Kaggle</a></li>\n<li>US jobs on Monster.com  Found in <a href=\"https://www.kaggle.com/PromptCloudHQ/us-jobs-on-monstercom\" target=\"_blank\" rel=\"noopener\">Kaggle</a><h3 id=\"Data-Clean\"><a href=\"#Data-Clean\" class=\"headerlink\" title=\"Data Clean\"></a>Data Clean</h3></li>\n<li>Obtain <strong>salay</strong> and coresspoding <strong>states</strong> information in US jos on Monster.com  <a href=\"https://github.com/Trouble404/IT-Salary-in-USA/blob/master/chart/chart1/chart1_data.ipynb\" target=\"_blank\" rel=\"noopener\">clean process</a></li>\n<li>Obtain <strong>salay</strong> and coresspoding <strong>cities</strong> information in US jos on Monster.com  <a href=\"https://github.com/Trouble404/IT-Salary-in-USA/blob/master/chart/chart2/chart2_data.ipynb\" target=\"_blank\" rel=\"noopener\">clean process</a></li>\n<li>Obtain <strong>skills</strong> information in U.S Technology Jobs on Dice.com  <a href=\"https://github.com/Trouble404/IT-Salary-in-USA/blob/master/chart/chart3/chart3_data.ipynb\" target=\"_blank\" rel=\"noopener\">clean process</a></li>\n</ul>\n<hr>\n<h2 id=\"Chart-Plotting\"><a href=\"#Chart-Plotting\" class=\"headerlink\" title=\"Chart Plotting\"></a>Chart Plotting</h2><h3 id=\"Chart-1-Map-chart-of-average-salay-of-3-types-jobs\"><a href=\"#Chart-1-Map-chart-of-average-salay-of-3-types-jobs\" class=\"headerlink\" title=\"Chart 1: Map chart of average salay of 3 types jobs\"></a>Chart 1: Map chart of average salay of 3 types jobs</h3><p><a href=\"https://public.tableau.com/profile/j.zhang#!/vizhome/chart1_23/dashborad1\" target=\"_blank\" rel=\"noopener\">Tableau address</a></p>\n<p><img src=\"https://raw.githubusercontent.com/Trouble404/IT-Salary-in-USA/master/chart/chart1/Chart1.png\" width=\"1000px\"></p>\n<h3 id=\"Chart-2-Bubble-chart-of-average-salay-of-3-types-jobs\"><a href=\"#Chart-2-Bubble-chart-of-average-salay-of-3-types-jobs\" class=\"headerlink\" title=\"Chart 2: Bubble chart of average salay of 3 types jobs\"></a>Chart 2: Bubble chart of average salay of 3 types jobs</h3><p><a href=\"https://public.tableau.com/profile/j.zhang#!/vizhome/chart2_10/dasbord1\" target=\"_blank\" rel=\"noopener\">Tableau address</a></p>\n<p><img src=\"https://raw.githubusercontent.com/Trouble404/IT-Salary-in-USA/master/chart/chart2/Chart2.png\" width=\"500px\"></p>\n<h3 id=\"Chart-3-Cloud-words-chart-of-skills-used-in-IT-jobs\"><a href=\"#Chart-3-Cloud-words-chart-of-skills-used-in-IT-jobs\" class=\"headerlink\" title=\"Chart 3 : Cloud words chart of skills used in IT-jobs\"></a>Chart 3 : Cloud words chart of skills used in IT-jobs</h3><p><a href=\"https://timdream.org/wordcloud/#wikipedia:Cloud\" target=\"_blank\" rel=\"noopener\">Cloud words tool</a></p>\n<p><img src=\"https://raw.githubusercontent.com/Trouble404/IT-Salary-in-USA/master/chart/chart3/Chart3.png\" width=\"500px\"></p>\n<hr>\n","site":{"data":{}},"excerpt":"<h2 id=\"Result\"><a href=\"#Result\" class=\"headerlink\" title=\"Result\"></a>Result</h2><p><strong>Chart with Anlysis</strong></p>\n<p><strong>Website: SALARY DISTRIBUTION OF DIFFERENT JOBS IN USA</strong></p>\n<p><img src=\"https://raw.githubusercontent.com/Trouble404/IT-Salary-in-USA/master/readme_add_pic/page1.PNG\" alt=\"image\">","more":"<br><img src=\"https://raw.githubusercontent.com/Trouble404/IT-Salary-in-USA/master/readme_add_pic/page2.PNG\" alt=\"image\"></p>\n<h2 id><a href=\"#\" class=\"headerlink\" title></a><img src=\"https://raw.githubusercontent.com/Trouble404/IT-Salary-in-USA/master/readme_add_pic/page3.PNG\" alt=\"image\"></h2><hr>\n<h2 id=\"Data-process\"><a href=\"#Data-process\" class=\"headerlink\" title=\"Data process\"></a>Data process</h2><h3 id=\"Datasets-used-to-anlysis\"><a href=\"#Datasets-used-to-anlysis\" class=\"headerlink\" title=\"Datasets used to anlysis\"></a>Datasets used to anlysis</h3><ul>\n<li>U.S. Technology Jobs on Dice.com  Found in <a href=\"https://www.kaggle.com/PromptCloudHQ/us-technology-jobs-on-dicecom/data\" target=\"_blank\" rel=\"noopener\">Kaggle</a></li>\n<li>US jobs on Monster.com  Found in <a href=\"https://www.kaggle.com/PromptCloudHQ/us-jobs-on-monstercom\" target=\"_blank\" rel=\"noopener\">Kaggle</a><h3 id=\"Data-Clean\"><a href=\"#Data-Clean\" class=\"headerlink\" title=\"Data Clean\"></a>Data Clean</h3></li>\n<li>Obtain <strong>salay</strong> and coresspoding <strong>states</strong> information in US jos on Monster.com  <a href=\"https://github.com/Trouble404/IT-Salary-in-USA/blob/master/chart/chart1/chart1_data.ipynb\" target=\"_blank\" rel=\"noopener\">clean process</a></li>\n<li>Obtain <strong>salay</strong> and coresspoding <strong>cities</strong> information in US jos on Monster.com  <a href=\"https://github.com/Trouble404/IT-Salary-in-USA/blob/master/chart/chart2/chart2_data.ipynb\" target=\"_blank\" rel=\"noopener\">clean process</a></li>\n<li>Obtain <strong>skills</strong> information in U.S Technology Jobs on Dice.com  <a href=\"https://github.com/Trouble404/IT-Salary-in-USA/blob/master/chart/chart3/chart3_data.ipynb\" target=\"_blank\" rel=\"noopener\">clean process</a></li>\n</ul>\n<hr>\n<h2 id=\"Chart-Plotting\"><a href=\"#Chart-Plotting\" class=\"headerlink\" title=\"Chart Plotting\"></a>Chart Plotting</h2><h3 id=\"Chart-1-Map-chart-of-average-salay-of-3-types-jobs\"><a href=\"#Chart-1-Map-chart-of-average-salay-of-3-types-jobs\" class=\"headerlink\" title=\"Chart 1: Map chart of average salay of 3 types jobs\"></a>Chart 1: Map chart of average salay of 3 types jobs</h3><p><a href=\"https://public.tableau.com/profile/j.zhang#!/vizhome/chart1_23/dashborad1\" target=\"_blank\" rel=\"noopener\">Tableau address</a></p>\n<p><img src=\"https://raw.githubusercontent.com/Trouble404/IT-Salary-in-USA/master/chart/chart1/Chart1.png\" width=\"1000px\"></p>\n<h3 id=\"Chart-2-Bubble-chart-of-average-salay-of-3-types-jobs\"><a href=\"#Chart-2-Bubble-chart-of-average-salay-of-3-types-jobs\" class=\"headerlink\" title=\"Chart 2: Bubble chart of average salay of 3 types jobs\"></a>Chart 2: Bubble chart of average salay of 3 types jobs</h3><p><a href=\"https://public.tableau.com/profile/j.zhang#!/vizhome/chart2_10/dasbord1\" target=\"_blank\" rel=\"noopener\">Tableau address</a></p>\n<p><img src=\"https://raw.githubusercontent.com/Trouble404/IT-Salary-in-USA/master/chart/chart2/Chart2.png\" width=\"500px\"></p>\n<h3 id=\"Chart-3-Cloud-words-chart-of-skills-used-in-IT-jobs\"><a href=\"#Chart-3-Cloud-words-chart-of-skills-used-in-IT-jobs\" class=\"headerlink\" title=\"Chart 3 : Cloud words chart of skills used in IT-jobs\"></a>Chart 3 : Cloud words chart of skills used in IT-jobs</h3><p><a href=\"https://timdream.org/wordcloud/#wikipedia:Cloud\" target=\"_blank\" rel=\"noopener\">Cloud words tool</a></p>\n<p><img src=\"https://raw.githubusercontent.com/Trouble404/IT-Salary-in-USA/master/chart/chart3/Chart3.png\" width=\"500px\"></p>\n<hr>"},{"title":"Ubuntu","date":"2018-11-12T01:04:14.000Z","_content":"\n## Ubuntu \n\n### Anaconda3\n[](https://www.anaconda.com/download/#linux)  \n**For Linux Installer**<!-- more -->\n\n\n1. /path/filename \n```\nsha256sum /path/filename\n```\n\n2. \n```\nbash ~/path/filename\n```\n\n3. YES\n\n4. \n\n\n```\nvim ~/.bashrc\n```\n\"i\"\n```\nexport PATH=~/anaconda3/bin:$PATH\n```\n\n```\nsource ~/.bashrc\n```\n\n5. \n```\nanaconda-navigator\n```\n6. \n\n### Anaconda\n**VPN!!!**\n\n1. ```\nconda create -n pytorch python=3.5\n```\n\n2. ```\nsource activate pytorch\n```\n\n3. Pytorchtorchvision [](https://pytorch.org/)```\nconda install pytorch-cpu torchvision-cpu -c pytorch\n```\n\n4. Jupyter-Notebook \n```\nconda install nb_conda\n```\n\n### \n1. pip>10.0\n```\npip install -i https://pypi.tuna.tsinghua.edu.cn/simple pip -U\n```\n2. \n```\npip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple\n```\n\n3. Anaconda \n```\nconda config --add channels 'https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/'\nconda config --set show_channel_urls yes\n```\n\n\n\n\n\n\n\n\n\n","source":"_posts/LInux.md","raw":"---\ntitle: Ubuntu\ndate: 2018-11-12 09:04:14\ntags: [Linux]\ncategories: System\n---\n\n## Ubuntu \n\n### Anaconda3\n[](https://www.anaconda.com/download/#linux)  \n**For Linux Installer**<!-- more -->\n\n\n1. /path/filename \n```\nsha256sum /path/filename\n```\n\n2. \n```\nbash ~/path/filename\n```\n\n3. YES\n\n4. \n\n\n```\nvim ~/.bashrc\n```\n\"i\"\n```\nexport PATH=~/anaconda3/bin:$PATH\n```\n\n```\nsource ~/.bashrc\n```\n\n5. \n```\nanaconda-navigator\n```\n6. \n\n### Anaconda\n**VPN!!!**\n\n1. ```\nconda create -n pytorch python=3.5\n```\n\n2. ```\nsource activate pytorch\n```\n\n3. Pytorchtorchvision [](https://pytorch.org/)```\nconda install pytorch-cpu torchvision-cpu -c pytorch\n```\n\n4. Jupyter-Notebook \n```\nconda install nb_conda\n```\n\n### \n1. pip>10.0\n```\npip install -i https://pypi.tuna.tsinghua.edu.cn/simple pip -U\n```\n2. \n```\npip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple\n```\n\n3. Anaconda \n```\nconda config --add channels 'https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/'\nconda config --set show_channel_urls yes\n```\n\n\n\n\n\n\n\n\n\n","slug":"LInux","published":1,"updated":"2020-04-28T12:38:55.395Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck9jwdng30005jlrcozi1jzkz","content":"<h2 id=\"Ubuntu-\"><a href=\"#Ubuntu-\" class=\"headerlink\" title=\"Ubuntu \"></a>Ubuntu </h2><h3 id=\"Anaconda3\"><a href=\"#Anaconda3\" class=\"headerlink\" title=\"Anaconda3\"></a>Anaconda3</h3><p><a href=\"https://www.anaconda.com/download/#linux\" target=\"_blank\" rel=\"noopener\"></a><br><strong>For Linux Installer</strong><a id=\"more\"></a></p>\n<p></p>\n<ol>\n<li><p>/path/filename </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sha256sum /path/filename</span><br></pre></td></tr></table></figure>\n</li>\n<li><p></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">bash ~/path/filename</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>YES</p>\n</li>\n<li><p></p>\n</li>\n</ol>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">vim ~/.bashrc</span><br></pre></td></tr></table></figure>\n<p>i<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">export PATH=~/anaconda3/bin:$PATH</span><br></pre></td></tr></table></figure></p>\n<p><br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">source ~/.bashrc</span><br></pre></td></tr></table></figure></p>\n<ol start=\"5\">\n<li><p></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">anaconda-navigator</span><br></pre></td></tr></table></figure>\n</li>\n<li><p></p>\n</li>\n</ol>\n<h3 id=\"Anaconda\"><a href=\"#Anaconda\" class=\"headerlink\" title=\"Anaconda\"></a>Anaconda</h3><p><strong>VPN!!!</strong></p>\n<ol>\n<li><p><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">conda create -n pytorch python=3.5</span><br></pre></td></tr></table></figure></p>\n</li>\n<li><p><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">source activate pytorch</span><br></pre></td></tr></table></figure></p>\n</li>\n<li><p>Pytorchtorchvision <a href=\"https://pytorch.org/\" target=\"_blank\" rel=\"noopener\"></a><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">conda install pytorch-cpu torchvision-cpu -c pytorch</span><br></pre></td></tr></table></figure></p>\n</li>\n<li><p>Jupyter-Notebook </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">conda install nb_conda</span><br></pre></td></tr></table></figure>\n</li>\n</ol>\n<h3 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h3><ol>\n<li><p>pip&gt;10.0</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">pip install -i https://pypi.tuna.tsinghua.edu.cn/simple pip -U</span><br></pre></td></tr></table></figure>\n</li>\n<li><p></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>Anaconda </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">conda config --add channels &apos;https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/&apos;</span><br><span class=\"line\">conda config --set show_channel_urls yes</span><br></pre></td></tr></table></figure>\n</li>\n</ol>\n","site":{"data":{}},"excerpt":"<h2 id=\"Ubuntu-\"><a href=\"#Ubuntu-\" class=\"headerlink\" title=\"Ubuntu \"></a>Ubuntu </h2><h3 id=\"Anaconda3\"><a href=\"#Anaconda3\" class=\"headerlink\" title=\"Anaconda3\"></a>Anaconda3</h3><p><a href=\"https://www.anaconda.com/download/#linux\" target=\"_blank\" rel=\"noopener\"></a><br><strong>For Linux Installer</strong>","more":"</p>\n<p></p>\n<ol>\n<li><p>/path/filename </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sha256sum /path/filename</span><br></pre></td></tr></table></figure>\n</li>\n<li><p></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">bash ~/path/filename</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>YES</p>\n</li>\n<li><p></p>\n</li>\n</ol>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">vim ~/.bashrc</span><br></pre></td></tr></table></figure>\n<p>i<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">export PATH=~/anaconda3/bin:$PATH</span><br></pre></td></tr></table></figure></p>\n<p><br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">source ~/.bashrc</span><br></pre></td></tr></table></figure></p>\n<ol start=\"5\">\n<li><p></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">anaconda-navigator</span><br></pre></td></tr></table></figure>\n</li>\n<li><p></p>\n</li>\n</ol>\n<h3 id=\"Anaconda\"><a href=\"#Anaconda\" class=\"headerlink\" title=\"Anaconda\"></a>Anaconda</h3><p><strong>VPN!!!</strong></p>\n<ol>\n<li><p><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">conda create -n pytorch python=3.5</span><br></pre></td></tr></table></figure></p>\n</li>\n<li><p><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">source activate pytorch</span><br></pre></td></tr></table></figure></p>\n</li>\n<li><p>Pytorchtorchvision <a href=\"https://pytorch.org/\" target=\"_blank\" rel=\"noopener\"></a><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">conda install pytorch-cpu torchvision-cpu -c pytorch</span><br></pre></td></tr></table></figure></p>\n</li>\n<li><p>Jupyter-Notebook </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">conda install nb_conda</span><br></pre></td></tr></table></figure>\n</li>\n</ol>\n<h3 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h3><ol>\n<li><p>pip&gt;10.0</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">pip install -i https://pypi.tuna.tsinghua.edu.cn/simple pip -U</span><br></pre></td></tr></table></figure>\n</li>\n<li><p></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>Anaconda </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">conda config --add channels &apos;https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/&apos;</span><br><span class=\"line\">conda config --set show_channel_urls yes</span><br></pre></td></tr></table></figure>\n</li>\n</ol>"},{"title":"MXnet ","date":"2018-01-28T12:04:14.000Z","_content":"\n\n# MXnet GPU version configuration (Windows 10. GTX960M)\n## Tools\n* [Microsoft Visual Studio 2015](https://www.visualstudio.com/zh-hans/vs/older-downloads/)\n* [CUDA 9.0](http://docs.nvidia.com/cuda/cuda-installation-guide-microsoft-windows/)\n* [cuDNN7](https://developer.nvidia.com/cudnn)\n* [CMake](https://cmake.org/)\n* [OpenCV3.0](https://sourceforge.net/projects/opencvlibrary/files/opencv-win/3.0.0/opencv-3.0.0.exe/download)\n* [OpenBLAS](https://sourceforge.net/projects/openblas/files/v0.2.14/)\n* [Anaconda](https://www.anaconda.com/download/)\n* [Graphviz](https://graphviz.gitlab.io/_pages/Download/Download_windows.html)\n<!-- more -->\n---\n\n## Methods\n* Step 1: Install VS2015\n\n* Step 2: Install CUDA 9.0\n\n* Step 3: Install cuDNN7 cudnnbinPATH\n\n* Step 4: Install Opencv OpenCV_DIRopencv/build,\\opencv\\build\\x64\\vc12\\bin\\opencv\\build\\x86\\vc12\\binPATH\n\n* Step 5: Install openBLAS mingw64_dll.zipOpenBLAS-v0.2.14-Win64-int64.zip.  OpenBLAS_HOMEopenBLAS,DLLpath.  \"C:\\Program files (x86)\\OpenBLAS\\\" \n\n* Step 6: Install Anaconda PATH\n\n* Step 7: Install MXnet MXnet  CD\n  {% codeblock %}\n  git clone --recursive https://github.com/dmlc/mxnet\n  {% endcodeblock %}\n\n  build  (  makeconfig.mk  USE_CUDNN = 0 to USE_CUDNN = 1, USE_BLAS = openBLAS )\n\n* Step 8: Install Cmake Configure and Genreate VS **mxnet.sln**  Configure\n![image](https://raw.githubusercontent.com/Trouble404/Kaggle-Dog-breed-Identification/master/readme_pic_add/cmake.PNG)\n\n* Step 9: VS2015 mxnet.sln release 64  mxnet_build\\Release**libmxnet.dll**\n\n* Step 10: Install graphviz library path\n\n* Step 11: Anaconda \n  {% codeblock %}\n  conda create  --name MXNet python=2.7\n  {% endcodeblock %}\n\n  MXNETpython3,  \n  {% codeblock %}\n  activate MXNet\n  {% endcodeblock %}\n\n* Step 12: cd mxnetpython  \n\n  ![iamge](https://raw.githubusercontent.com/Trouble404/Kaggle-Dog-breed-Identification/master/readme_pic_add/dll.PNG)\n\n  \n  {% codeblock %}\n  python setup.py install\n  {% endcodeblock %}\n\n\n\n* Step 13: AnacondaMXNet Lib\\site-packages\\mxnet-.egg\\mxnet  path\n\n* Step 14: \n  {% codeblock %}\n  conda install nb_conda\n  {% endcodeblock %}\n\n  juptyer notebook jupyter book\n\n![iamge](https://raw.githubusercontent.com/Trouble404/Kaggle-Dog-breed-Identification/master/readme_pic_add/test.png)\n\n\n","source":"_posts/MXnet_Config.md","raw":"---\ntitle: MXnet \ndate: 2018-01-28 20:04:14\ntags: [MXnet]\ncategories: Machine Learning\n---\n\n\n# MXnet GPU version configuration (Windows 10. GTX960M)\n## Tools\n* [Microsoft Visual Studio 2015](https://www.visualstudio.com/zh-hans/vs/older-downloads/)\n* [CUDA 9.0](http://docs.nvidia.com/cuda/cuda-installation-guide-microsoft-windows/)\n* [cuDNN7](https://developer.nvidia.com/cudnn)\n* [CMake](https://cmake.org/)\n* [OpenCV3.0](https://sourceforge.net/projects/opencvlibrary/files/opencv-win/3.0.0/opencv-3.0.0.exe/download)\n* [OpenBLAS](https://sourceforge.net/projects/openblas/files/v0.2.14/)\n* [Anaconda](https://www.anaconda.com/download/)\n* [Graphviz](https://graphviz.gitlab.io/_pages/Download/Download_windows.html)\n<!-- more -->\n---\n\n## Methods\n* Step 1: Install VS2015\n\n* Step 2: Install CUDA 9.0\n\n* Step 3: Install cuDNN7 cudnnbinPATH\n\n* Step 4: Install Opencv OpenCV_DIRopencv/build,\\opencv\\build\\x64\\vc12\\bin\\opencv\\build\\x86\\vc12\\binPATH\n\n* Step 5: Install openBLAS mingw64_dll.zipOpenBLAS-v0.2.14-Win64-int64.zip.  OpenBLAS_HOMEopenBLAS,DLLpath.  \"C:\\Program files (x86)\\OpenBLAS\\\" \n\n* Step 6: Install Anaconda PATH\n\n* Step 7: Install MXnet MXnet  CD\n  {% codeblock %}\n  git clone --recursive https://github.com/dmlc/mxnet\n  {% endcodeblock %}\n\n  build  (  makeconfig.mk  USE_CUDNN = 0 to USE_CUDNN = 1, USE_BLAS = openBLAS )\n\n* Step 8: Install Cmake Configure and Genreate VS **mxnet.sln**  Configure\n![image](https://raw.githubusercontent.com/Trouble404/Kaggle-Dog-breed-Identification/master/readme_pic_add/cmake.PNG)\n\n* Step 9: VS2015 mxnet.sln release 64  mxnet_build\\Release**libmxnet.dll**\n\n* Step 10: Install graphviz library path\n\n* Step 11: Anaconda \n  {% codeblock %}\n  conda create  --name MXNet python=2.7\n  {% endcodeblock %}\n\n  MXNETpython3,  \n  {% codeblock %}\n  activate MXNet\n  {% endcodeblock %}\n\n* Step 12: cd mxnetpython  \n\n  ![iamge](https://raw.githubusercontent.com/Trouble404/Kaggle-Dog-breed-Identification/master/readme_pic_add/dll.PNG)\n\n  \n  {% codeblock %}\n  python setup.py install\n  {% endcodeblock %}\n\n\n\n* Step 13: AnacondaMXNet Lib\\site-packages\\mxnet-.egg\\mxnet  path\n\n* Step 14: \n  {% codeblock %}\n  conda install nb_conda\n  {% endcodeblock %}\n\n  juptyer notebook jupyter book\n\n![iamge](https://raw.githubusercontent.com/Trouble404/Kaggle-Dog-breed-Identification/master/readme_pic_add/test.png)\n\n\n","slug":"MXnet_Config","published":1,"updated":"2020-04-28T12:38:55.395Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck9jwdng40006jlrcowjfpwux","content":"<h1 id=\"MXnet-GPU-version-configuration-Windows-10-GTX960M\"><a href=\"#MXnet-GPU-version-configuration-Windows-10-GTX960M\" class=\"headerlink\" title=\"MXnet GPU version configuration (Windows 10. GTX960M)\"></a>MXnet GPU version configuration (Windows 10. GTX960M)</h1><h2 id=\"Tools\"><a href=\"#Tools\" class=\"headerlink\" title=\"Tools\"></a>Tools</h2><ul>\n<li><a href=\"https://www.visualstudio.com/zh-hans/vs/older-downloads/\" target=\"_blank\" rel=\"noopener\">Microsoft Visual Studio 2015</a></li>\n<li><a href=\"http://docs.nvidia.com/cuda/cuda-installation-guide-microsoft-windows/\" target=\"_blank\" rel=\"noopener\">CUDA 9.0</a></li>\n<li><a href=\"https://developer.nvidia.com/cudnn\" target=\"_blank\" rel=\"noopener\">cuDNN7</a></li>\n<li><a href=\"https://cmake.org/\" target=\"_blank\" rel=\"noopener\">CMake</a></li>\n<li><a href=\"https://sourceforge.net/projects/opencvlibrary/files/opencv-win/3.0.0/opencv-3.0.0.exe/download\" target=\"_blank\" rel=\"noopener\">OpenCV3.0</a></li>\n<li><a href=\"https://sourceforge.net/projects/openblas/files/v0.2.14/\" target=\"_blank\" rel=\"noopener\">OpenBLAS</a></li>\n<li><a href=\"https://www.anaconda.com/download/\" target=\"_blank\" rel=\"noopener\">Anaconda</a></li>\n<li><a href=\"https://graphviz.gitlab.io/_pages/Download/Download_windows.html\" target=\"_blank\" rel=\"noopener\">Graphviz</a><a id=\"more\"></a>\n</li>\n</ul>\n<hr>\n<h2 id=\"Methods\"><a href=\"#Methods\" class=\"headerlink\" title=\"Methods\"></a>Methods</h2><ul>\n<li><p>Step 1: Install VS2015</p>\n</li>\n<li><p>Step 2: Install CUDA 9.0</p>\n</li>\n<li><p>Step 3: Install cuDNN7 cudnnbinPATH</p>\n</li>\n<li><p>Step 4: Install Opencv OpenCV_DIRopencv/build,\\opencv\\build\\x64\\vc12\\bin\\opencv\\build\\x86\\vc12\\binPATH</p>\n</li>\n<li><p>Step 5: Install openBLAS mingw64_dll.zipOpenBLAS-v0.2.14-Win64-int64.zip.  OpenBLAS_HOMEopenBLAS,DLLpath.  C:\\Program files (x86)\\OpenBLAS\\ </p>\n</li>\n<li><p>Step 6: Install Anaconda PATH</p>\n</li>\n<li><p>Step 7: Install MXnet MXnet  CD</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git clone --recursive https://github.com/dmlc/mxnet</span><br></pre></td></tr></table></figure>\n<p>build  (  makeconfig.mk  USE_CUDNN = 0 to USE_CUDNN = 1, USE_BLAS = openBLAS )</p>\n</li>\n<li><p>Step 8: Install Cmake Configure and Genreate VS <strong>mxnet.sln</strong>  Configure<br><img src=\"https://raw.githubusercontent.com/Trouble404/Kaggle-Dog-breed-Identification/master/readme_pic_add/cmake.PNG\" alt=\"image\"></p>\n</li>\n<li><p>Step 9: VS2015 mxnet.sln release 64  mxnet_build\\Release<strong>libmxnet.dll</strong></p>\n</li>\n<li><p>Step 10: Install graphviz library path</p>\n</li>\n<li><p>Step 11: Anaconda </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">conda create  --name MXNet python=2.7</span><br></pre></td></tr></table></figure>\n<p>MXNETpython3,  </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">activate MXNet</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>Step 12: cd mxnetpython  </p>\n<p><img src=\"https://raw.githubusercontent.com/Trouble404/Kaggle-Dog-breed-Identification/master/readme_pic_add/dll.PNG\" alt=\"iamge\"></p>\n<p></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">python setup.py install</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<ul>\n<li><p>Step 13: AnacondaMXNet Lib\\site-packages\\mxnet-.egg\\mxnet  path</p>\n</li>\n<li><p>Step 14: </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">conda install nb_conda</span><br></pre></td></tr></table></figure>\n<p>juptyer notebook  jupyter book</p>\n</li>\n</ul>\n<p><img src=\"https://raw.githubusercontent.com/Trouble404/Kaggle-Dog-breed-Identification/master/readme_pic_add/test.png\" alt=\"iamge\"></p>\n","site":{"data":{}},"excerpt":"<h1 id=\"MXnet-GPU-version-configuration-Windows-10-GTX960M\"><a href=\"#MXnet-GPU-version-configuration-Windows-10-GTX960M\" class=\"headerlink\" title=\"MXnet GPU version configuration (Windows 10. GTX960M)\"></a>MXnet GPU version configuration (Windows 10. GTX960M)</h1><h2 id=\"Tools\"><a href=\"#Tools\" class=\"headerlink\" title=\"Tools\"></a>Tools</h2><ul>\n<li><a href=\"https://www.visualstudio.com/zh-hans/vs/older-downloads/\" target=\"_blank\" rel=\"noopener\">Microsoft Visual Studio 2015</a></li>\n<li><a href=\"http://docs.nvidia.com/cuda/cuda-installation-guide-microsoft-windows/\" target=\"_blank\" rel=\"noopener\">CUDA 9.0</a></li>\n<li><a href=\"https://developer.nvidia.com/cudnn\" target=\"_blank\" rel=\"noopener\">cuDNN7</a></li>\n<li><a href=\"https://cmake.org/\" target=\"_blank\" rel=\"noopener\">CMake</a></li>\n<li><a href=\"https://sourceforge.net/projects/opencvlibrary/files/opencv-win/3.0.0/opencv-3.0.0.exe/download\" target=\"_blank\" rel=\"noopener\">OpenCV3.0</a></li>\n<li><a href=\"https://sourceforge.net/projects/openblas/files/v0.2.14/\" target=\"_blank\" rel=\"noopener\">OpenBLAS</a></li>\n<li><a href=\"https://www.anaconda.com/download/\" target=\"_blank\" rel=\"noopener\">Anaconda</a></li>\n<li><a href=\"https://graphviz.gitlab.io/_pages/Download/Download_windows.html\" target=\"_blank\" rel=\"noopener\">Graphviz</a>","more":"</li>\n</ul>\n<hr>\n<h2 id=\"Methods\"><a href=\"#Methods\" class=\"headerlink\" title=\"Methods\"></a>Methods</h2><ul>\n<li><p>Step 1: Install VS2015</p>\n</li>\n<li><p>Step 2: Install CUDA 9.0</p>\n</li>\n<li><p>Step 3: Install cuDNN7 cudnnbinPATH</p>\n</li>\n<li><p>Step 4: Install Opencv OpenCV_DIRopencv/build,\\opencv\\build\\x64\\vc12\\bin\\opencv\\build\\x86\\vc12\\binPATH</p>\n</li>\n<li><p>Step 5: Install openBLAS mingw64_dll.zipOpenBLAS-v0.2.14-Win64-int64.zip.  OpenBLAS_HOMEopenBLAS,DLLpath.  C:\\Program files (x86)\\OpenBLAS\\ </p>\n</li>\n<li><p>Step 6: Install Anaconda PATH</p>\n</li>\n<li><p>Step 7: Install MXnet MXnet  CD</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git clone --recursive https://github.com/dmlc/mxnet</span><br></pre></td></tr></table></figure>\n<p>build  (  makeconfig.mk  USE_CUDNN = 0 to USE_CUDNN = 1, USE_BLAS = openBLAS )</p>\n</li>\n<li><p>Step 8: Install Cmake Configure and Genreate VS <strong>mxnet.sln</strong>  Configure<br><img src=\"https://raw.githubusercontent.com/Trouble404/Kaggle-Dog-breed-Identification/master/readme_pic_add/cmake.PNG\" alt=\"image\"></p>\n</li>\n<li><p>Step 9: VS2015 mxnet.sln release 64  mxnet_build\\Release<strong>libmxnet.dll</strong></p>\n</li>\n<li><p>Step 10: Install graphviz library path</p>\n</li>\n<li><p>Step 11: Anaconda </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">conda create  --name MXNet python=2.7</span><br></pre></td></tr></table></figure>\n<p>MXNETpython3,  </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">activate MXNet</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>Step 12: cd mxnetpython  </p>\n<p><img src=\"https://raw.githubusercontent.com/Trouble404/Kaggle-Dog-breed-Identification/master/readme_pic_add/dll.PNG\" alt=\"iamge\"></p>\n<p></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">python setup.py install</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<ul>\n<li><p>Step 13: AnacondaMXNet Lib\\site-packages\\mxnet-.egg\\mxnet  path</p>\n</li>\n<li><p>Step 14: </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">conda install nb_conda</span><br></pre></td></tr></table></figure>\n<p>juptyer notebook  jupyter book</p>\n</li>\n</ul>\n<p><img src=\"https://raw.githubusercontent.com/Trouble404/Kaggle-Dog-breed-Identification/master/readme_pic_add/test.png\" alt=\"iamge\"></p>"},{"title":"Using Machine Learning in NBA","date":"2018-01-31T04:23:32.000Z","_content":"\n## Fan-map plotting\n*   Obtain the game data in 2000~2017 in NBA\n*   Obtain the follwers of 2015~2017 NBA rookies by [twittR](https://www.rdocumentation.org/packages/twitteR/versions/1.1.9) package\n*   Obtain location information and plot the fan-map by [Tableau](https://www.tableau.com/zh-cn)\n<!-- more -->\n---\n\n## Machine learning apply\n### all-star players in 2017 prediction\n*    [PCA ](https://github.com/Trouble404/NBA-with-Machine-Learning/blob/master/nerual_network/pca/nba%20all%20star/pca%20of%20nba%20all%20players_00-17.ipynb) in all-star player prediction\n*  [Keras](https://keras-cn.readthedocs.io/en/latest/)APIKerasPython[Tensorflow](https://github.com/tensorflow/tensorflow)\n*    [All-star players in 2017 prediction](https://github.com/Trouble404/NBA-with-Machine-Learning/blob/master/nerual_network/prediction/nba_all_star_prediction/nba%20all%20star%20prediction.ipynb)\n*    [Tensorboard](https://www.tensorflow.org/get_started/summaries_and_tensorboard) is used to compare and choose better model.\n![image](https://raw.githubusercontent.com/Trouble404/NBA-with-Machine-Learning/master/readme_add_pic/full-list.PNG)\n\n### Best rookies in 2017 prediction\n*    [PCA ](https://github.com/Trouble404/NBA-with-Machine-Learning/blob/master/nerual_network/pca/nba%20rookies%20best/pca%20process.ipynb) in best rookies prediction\n* [Keras](https://keras-cn.readthedocs.io/en/latest/)APIKerasPython[Tensorflow](https://github.com/tensorflow/tensorflow)\n*    [Best rookies in 2017 prediction](https://github.com/Trouble404/NBA-with-Machine-Learning/blob/master/nerual_network/prediction/rookies_best/normal%20prediction/Best%205%20rookie.ipynb)\n*    [First and Second rookies in 2017 prediction](https://github.com/Trouble404/NBA-with-Machine-Learning/blob/master/nerual_network/prediction/rookies_first_second/first_second_rookie-tensorboard.ipynb) **Muti-classes classification application**\n*    [Tensorboard](https://www.tensorflow.org/get_started/summaries_and_tensorboard) is used to compare and choose better model.\n![image](https://raw.githubusercontent.com/Trouble404/NBA-with-Machine-Learning/master/readme_add_pic/firstrookies.PNG)\n\n### Potential all-star players in 2015~16 prediction\n*    [PCA ](https://github.com/Trouble404/NBA-with-Machine-Learning/blob/master/nerual_network/pca/nba%20rookies%20all%20star/pca%20process.ipynb) in potential rookies prediction\n* [Keras](https://keras-cn.readthedocs.io/en/latest/)APIKerasPython[Tensorflow](https://github.com/tensorflow/tensorflow)\n*    [Potential rookies in 2015~16 prediction](https://github.com/Trouble404/NBA-with-Machine-Learning/blob/master/nerual_network/prediction/rookies_all_star_prediction/All%20star%20rookie%20without%20pca.ipynb)\n*    [Tensorboard](https://www.tensorflow.org/get_started/summaries_and_tensorboard) is used to compare and choose better model.\n![image](https://raw.githubusercontent.com/Trouble404/NBA-with-Machine-Learning/master/readme_add_pic/rookies-allstar.PNG)\n\n---\n\n## Result\n**Website: [2017 NBA ALL-STAR AND BEST ROOKIES PREDICTION WITH FAN-MAP](https://d2v4olxsjbfep7.cloudfront.net/panels.html)**\n---\n\nexample: \n*  Neural network model\n\n![image](https://raw.githubusercontent.com/Trouble404/NBA-with-Machine-Learning/master/readme_add_pic/model.png)\n\n*  2D PCA (linear unseparable)\n\n![image](https://raw.githubusercontent.com/Trouble404/NBA-with-Machine-Learning/master/readme_add_pic/pca.PNG)\n\n\n*  Tensorboard check\n\n![image](https://raw.githubusercontent.com/Trouble404/NBA-with-Machine-Learning/master/readme_add_pic/tensor_board.PNG)\n","source":"_posts/Machine Learning in NBA.md","raw":"---\ntitle: Using Machine Learning in NBA\ndate: 2018-01-31 12:23:32\ntags: [Machine Learning, NBA]\ncategories: Machine Learning\n---\n\n## Fan-map plotting\n*   Obtain the game data in 2000~2017 in NBA\n*   Obtain the follwers of 2015~2017 NBA rookies by [twittR](https://www.rdocumentation.org/packages/twitteR/versions/1.1.9) package\n*   Obtain location information and plot the fan-map by [Tableau](https://www.tableau.com/zh-cn)\n<!-- more -->\n---\n\n## Machine learning apply\n### all-star players in 2017 prediction\n*    [PCA ](https://github.com/Trouble404/NBA-with-Machine-Learning/blob/master/nerual_network/pca/nba%20all%20star/pca%20of%20nba%20all%20players_00-17.ipynb) in all-star player prediction\n*  [Keras](https://keras-cn.readthedocs.io/en/latest/)APIKerasPython[Tensorflow](https://github.com/tensorflow/tensorflow)\n*    [All-star players in 2017 prediction](https://github.com/Trouble404/NBA-with-Machine-Learning/blob/master/nerual_network/prediction/nba_all_star_prediction/nba%20all%20star%20prediction.ipynb)\n*    [Tensorboard](https://www.tensorflow.org/get_started/summaries_and_tensorboard) is used to compare and choose better model.\n![image](https://raw.githubusercontent.com/Trouble404/NBA-with-Machine-Learning/master/readme_add_pic/full-list.PNG)\n\n### Best rookies in 2017 prediction\n*    [PCA ](https://github.com/Trouble404/NBA-with-Machine-Learning/blob/master/nerual_network/pca/nba%20rookies%20best/pca%20process.ipynb) in best rookies prediction\n* [Keras](https://keras-cn.readthedocs.io/en/latest/)APIKerasPython[Tensorflow](https://github.com/tensorflow/tensorflow)\n*    [Best rookies in 2017 prediction](https://github.com/Trouble404/NBA-with-Machine-Learning/blob/master/nerual_network/prediction/rookies_best/normal%20prediction/Best%205%20rookie.ipynb)\n*    [First and Second rookies in 2017 prediction](https://github.com/Trouble404/NBA-with-Machine-Learning/blob/master/nerual_network/prediction/rookies_first_second/first_second_rookie-tensorboard.ipynb) **Muti-classes classification application**\n*    [Tensorboard](https://www.tensorflow.org/get_started/summaries_and_tensorboard) is used to compare and choose better model.\n![image](https://raw.githubusercontent.com/Trouble404/NBA-with-Machine-Learning/master/readme_add_pic/firstrookies.PNG)\n\n### Potential all-star players in 2015~16 prediction\n*    [PCA ](https://github.com/Trouble404/NBA-with-Machine-Learning/blob/master/nerual_network/pca/nba%20rookies%20all%20star/pca%20process.ipynb) in potential rookies prediction\n* [Keras](https://keras-cn.readthedocs.io/en/latest/)APIKerasPython[Tensorflow](https://github.com/tensorflow/tensorflow)\n*    [Potential rookies in 2015~16 prediction](https://github.com/Trouble404/NBA-with-Machine-Learning/blob/master/nerual_network/prediction/rookies_all_star_prediction/All%20star%20rookie%20without%20pca.ipynb)\n*    [Tensorboard](https://www.tensorflow.org/get_started/summaries_and_tensorboard) is used to compare and choose better model.\n![image](https://raw.githubusercontent.com/Trouble404/NBA-with-Machine-Learning/master/readme_add_pic/rookies-allstar.PNG)\n\n---\n\n## Result\n**Website: [2017 NBA ALL-STAR AND BEST ROOKIES PREDICTION WITH FAN-MAP](https://d2v4olxsjbfep7.cloudfront.net/panels.html)**\n---\n\nexample: \n*  Neural network model\n\n![image](https://raw.githubusercontent.com/Trouble404/NBA-with-Machine-Learning/master/readme_add_pic/model.png)\n\n*  2D PCA (linear unseparable)\n\n![image](https://raw.githubusercontent.com/Trouble404/NBA-with-Machine-Learning/master/readme_add_pic/pca.PNG)\n\n\n*  Tensorboard check\n\n![image](https://raw.githubusercontent.com/Trouble404/NBA-with-Machine-Learning/master/readme_add_pic/tensor_board.PNG)\n","slug":"Machine Learning in NBA","published":1,"updated":"2020-04-28T12:38:55.395Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck9jwdng50007jlrcdxl2eea8","content":"<h2 id=\"Fan-map-plotting\"><a href=\"#Fan-map-plotting\" class=\"headerlink\" title=\"Fan-map plotting\"></a>Fan-map plotting</h2><ul>\n<li>Obtain the game data in 2000~2017 in NBA</li>\n<li>Obtain the follwers of 2015~2017 NBA rookies by <a href=\"https://www.rdocumentation.org/packages/twitteR/versions/1.1.9\" target=\"_blank\" rel=\"noopener\">twittR</a> package</li>\n<li>Obtain location information and plot the fan-map by <a href=\"https://www.tableau.com/zh-cn\" target=\"_blank\" rel=\"noopener\">Tableau</a><a id=\"more\"></a>\n</li>\n</ul>\n<hr>\n<h2 id=\"Machine-learning-apply\"><a href=\"#Machine-learning-apply\" class=\"headerlink\" title=\"Machine learning apply\"></a>Machine learning apply</h2><h3 id=\"all-star-players-in-2017-prediction\"><a href=\"#all-star-players-in-2017-prediction\" class=\"headerlink\" title=\"all-star players in 2017 prediction\"></a>all-star players in 2017 prediction</h3><ul>\n<li><a href=\"https://github.com/Trouble404/NBA-with-Machine-Learning/blob/master/nerual_network/pca/nba%20all%20star/pca%20of%20nba%20all%20players_00-17.ipynb\" target=\"_blank\" rel=\"noopener\">PCA </a> in all-star player prediction</li>\n<li><a href=\"https://keras-cn.readthedocs.io/en/latest/\" target=\"_blank\" rel=\"noopener\">Keras</a>APIKerasPython<a href=\"https://github.com/tensorflow/tensorflow\" target=\"_blank\" rel=\"noopener\">Tensorflow</a></li>\n<li><a href=\"https://github.com/Trouble404/NBA-with-Machine-Learning/blob/master/nerual_network/prediction/nba_all_star_prediction/nba%20all%20star%20prediction.ipynb\" target=\"_blank\" rel=\"noopener\">All-star players in 2017 prediction</a></li>\n<li><a href=\"https://www.tensorflow.org/get_started/summaries_and_tensorboard\" target=\"_blank\" rel=\"noopener\">Tensorboard</a> is used to compare and choose better model.<br><img src=\"https://raw.githubusercontent.com/Trouble404/NBA-with-Machine-Learning/master/readme_add_pic/full-list.PNG\" alt=\"image\"></li>\n</ul>\n<h3 id=\"Best-rookies-in-2017-prediction\"><a href=\"#Best-rookies-in-2017-prediction\" class=\"headerlink\" title=\"Best rookies in 2017 prediction\"></a>Best rookies in 2017 prediction</h3><ul>\n<li><a href=\"https://github.com/Trouble404/NBA-with-Machine-Learning/blob/master/nerual_network/pca/nba%20rookies%20best/pca%20process.ipynb\" target=\"_blank\" rel=\"noopener\">PCA </a> in best rookies prediction</li>\n<li><a href=\"https://keras-cn.readthedocs.io/en/latest/\" target=\"_blank\" rel=\"noopener\">Keras</a>APIKerasPython<a href=\"https://github.com/tensorflow/tensorflow\" target=\"_blank\" rel=\"noopener\">Tensorflow</a></li>\n<li><a href=\"https://github.com/Trouble404/NBA-with-Machine-Learning/blob/master/nerual_network/prediction/rookies_best/normal%20prediction/Best%205%20rookie.ipynb\" target=\"_blank\" rel=\"noopener\">Best rookies in 2017 prediction</a></li>\n<li><a href=\"https://github.com/Trouble404/NBA-with-Machine-Learning/blob/master/nerual_network/prediction/rookies_first_second/first_second_rookie-tensorboard.ipynb\" target=\"_blank\" rel=\"noopener\">First and Second rookies in 2017 prediction</a> <strong>Muti-classes classification application</strong></li>\n<li><a href=\"https://www.tensorflow.org/get_started/summaries_and_tensorboard\" target=\"_blank\" rel=\"noopener\">Tensorboard</a> is used to compare and choose better model.<br><img src=\"https://raw.githubusercontent.com/Trouble404/NBA-with-Machine-Learning/master/readme_add_pic/firstrookies.PNG\" alt=\"image\"></li>\n</ul>\n<h3 id=\"Potential-all-star-players-in-2015-16-prediction\"><a href=\"#Potential-all-star-players-in-2015-16-prediction\" class=\"headerlink\" title=\"Potential all-star players in 2015~16 prediction\"></a>Potential all-star players in 2015~16 prediction</h3><ul>\n<li><a href=\"https://github.com/Trouble404/NBA-with-Machine-Learning/blob/master/nerual_network/pca/nba%20rookies%20all%20star/pca%20process.ipynb\" target=\"_blank\" rel=\"noopener\">PCA </a> in potential rookies prediction</li>\n<li><a href=\"https://keras-cn.readthedocs.io/en/latest/\" target=\"_blank\" rel=\"noopener\">Keras</a>APIKerasPython<a href=\"https://github.com/tensorflow/tensorflow\" target=\"_blank\" rel=\"noopener\">Tensorflow</a></li>\n<li><a href=\"https://github.com/Trouble404/NBA-with-Machine-Learning/blob/master/nerual_network/prediction/rookies_all_star_prediction/All%20star%20rookie%20without%20pca.ipynb\" target=\"_blank\" rel=\"noopener\">Potential rookies in 2015~16 prediction</a></li>\n<li><a href=\"https://www.tensorflow.org/get_started/summaries_and_tensorboard\" target=\"_blank\" rel=\"noopener\">Tensorboard</a> is used to compare and choose better model.<br><img src=\"https://raw.githubusercontent.com/Trouble404/NBA-with-Machine-Learning/master/readme_add_pic/rookies-allstar.PNG\" alt=\"image\"></li>\n</ul>\n<hr>\n<h2 id=\"Result\"><a href=\"#Result\" class=\"headerlink\" title=\"Result\"></a>Result</h2><h2 id=\"Website-2017-NBA-ALL-STAR-AND-BEST-ROOKIES-PREDICTION-WITH-FAN-MAP\"><a href=\"#Website-2017-NBA-ALL-STAR-AND-BEST-ROOKIES-PREDICTION-WITH-FAN-MAP\" class=\"headerlink\" title=\"Website: 2017 NBA ALL-STAR AND BEST ROOKIES PREDICTION WITH FAN-MAP\"></a><strong>Website: <a href=\"https://d2v4olxsjbfep7.cloudfront.net/panels.html\" target=\"_blank\" rel=\"noopener\">2017 NBA ALL-STAR AND BEST ROOKIES PREDICTION WITH FAN-MAP</a></strong></h2><p>example: </p>\n<ul>\n<li>Neural network model</li>\n</ul>\n<p><img src=\"https://raw.githubusercontent.com/Trouble404/NBA-with-Machine-Learning/master/readme_add_pic/model.png\" alt=\"image\"></p>\n<ul>\n<li>2D PCA (linear unseparable)</li>\n</ul>\n<p><img src=\"https://raw.githubusercontent.com/Trouble404/NBA-with-Machine-Learning/master/readme_add_pic/pca.PNG\" alt=\"image\"></p>\n<ul>\n<li>Tensorboard check</li>\n</ul>\n<p><img src=\"https://raw.githubusercontent.com/Trouble404/NBA-with-Machine-Learning/master/readme_add_pic/tensor_board.PNG\" alt=\"image\"></p>\n","site":{"data":{}},"excerpt":"<h2 id=\"Fan-map-plotting\"><a href=\"#Fan-map-plotting\" class=\"headerlink\" title=\"Fan-map plotting\"></a>Fan-map plotting</h2><ul>\n<li>Obtain the game data in 2000~2017 in NBA</li>\n<li>Obtain the follwers of 2015~2017 NBA rookies by <a href=\"https://www.rdocumentation.org/packages/twitteR/versions/1.1.9\" target=\"_blank\" rel=\"noopener\">twittR</a> package</li>\n<li>Obtain location information and plot the fan-map by <a href=\"https://www.tableau.com/zh-cn\" target=\"_blank\" rel=\"noopener\">Tableau</a>","more":"</li>\n</ul>\n<hr>\n<h2 id=\"Machine-learning-apply\"><a href=\"#Machine-learning-apply\" class=\"headerlink\" title=\"Machine learning apply\"></a>Machine learning apply</h2><h3 id=\"all-star-players-in-2017-prediction\"><a href=\"#all-star-players-in-2017-prediction\" class=\"headerlink\" title=\"all-star players in 2017 prediction\"></a>all-star players in 2017 prediction</h3><ul>\n<li><a href=\"https://github.com/Trouble404/NBA-with-Machine-Learning/blob/master/nerual_network/pca/nba%20all%20star/pca%20of%20nba%20all%20players_00-17.ipynb\" target=\"_blank\" rel=\"noopener\">PCA </a> in all-star player prediction</li>\n<li><a href=\"https://keras-cn.readthedocs.io/en/latest/\" target=\"_blank\" rel=\"noopener\">Keras</a>APIKerasPython<a href=\"https://github.com/tensorflow/tensorflow\" target=\"_blank\" rel=\"noopener\">Tensorflow</a></li>\n<li><a href=\"https://github.com/Trouble404/NBA-with-Machine-Learning/blob/master/nerual_network/prediction/nba_all_star_prediction/nba%20all%20star%20prediction.ipynb\" target=\"_blank\" rel=\"noopener\">All-star players in 2017 prediction</a></li>\n<li><a href=\"https://www.tensorflow.org/get_started/summaries_and_tensorboard\" target=\"_blank\" rel=\"noopener\">Tensorboard</a> is used to compare and choose better model.<br><img src=\"https://raw.githubusercontent.com/Trouble404/NBA-with-Machine-Learning/master/readme_add_pic/full-list.PNG\" alt=\"image\"></li>\n</ul>\n<h3 id=\"Best-rookies-in-2017-prediction\"><a href=\"#Best-rookies-in-2017-prediction\" class=\"headerlink\" title=\"Best rookies in 2017 prediction\"></a>Best rookies in 2017 prediction</h3><ul>\n<li><a href=\"https://github.com/Trouble404/NBA-with-Machine-Learning/blob/master/nerual_network/pca/nba%20rookies%20best/pca%20process.ipynb\" target=\"_blank\" rel=\"noopener\">PCA </a> in best rookies prediction</li>\n<li><a href=\"https://keras-cn.readthedocs.io/en/latest/\" target=\"_blank\" rel=\"noopener\">Keras</a>APIKerasPython<a href=\"https://github.com/tensorflow/tensorflow\" target=\"_blank\" rel=\"noopener\">Tensorflow</a></li>\n<li><a href=\"https://github.com/Trouble404/NBA-with-Machine-Learning/blob/master/nerual_network/prediction/rookies_best/normal%20prediction/Best%205%20rookie.ipynb\" target=\"_blank\" rel=\"noopener\">Best rookies in 2017 prediction</a></li>\n<li><a href=\"https://github.com/Trouble404/NBA-with-Machine-Learning/blob/master/nerual_network/prediction/rookies_first_second/first_second_rookie-tensorboard.ipynb\" target=\"_blank\" rel=\"noopener\">First and Second rookies in 2017 prediction</a> <strong>Muti-classes classification application</strong></li>\n<li><a href=\"https://www.tensorflow.org/get_started/summaries_and_tensorboard\" target=\"_blank\" rel=\"noopener\">Tensorboard</a> is used to compare and choose better model.<br><img src=\"https://raw.githubusercontent.com/Trouble404/NBA-with-Machine-Learning/master/readme_add_pic/firstrookies.PNG\" alt=\"image\"></li>\n</ul>\n<h3 id=\"Potential-all-star-players-in-2015-16-prediction\"><a href=\"#Potential-all-star-players-in-2015-16-prediction\" class=\"headerlink\" title=\"Potential all-star players in 2015~16 prediction\"></a>Potential all-star players in 2015~16 prediction</h3><ul>\n<li><a href=\"https://github.com/Trouble404/NBA-with-Machine-Learning/blob/master/nerual_network/pca/nba%20rookies%20all%20star/pca%20process.ipynb\" target=\"_blank\" rel=\"noopener\">PCA </a> in potential rookies prediction</li>\n<li><a href=\"https://keras-cn.readthedocs.io/en/latest/\" target=\"_blank\" rel=\"noopener\">Keras</a>APIKerasPython<a href=\"https://github.com/tensorflow/tensorflow\" target=\"_blank\" rel=\"noopener\">Tensorflow</a></li>\n<li><a href=\"https://github.com/Trouble404/NBA-with-Machine-Learning/blob/master/nerual_network/prediction/rookies_all_star_prediction/All%20star%20rookie%20without%20pca.ipynb\" target=\"_blank\" rel=\"noopener\">Potential rookies in 2015~16 prediction</a></li>\n<li><a href=\"https://www.tensorflow.org/get_started/summaries_and_tensorboard\" target=\"_blank\" rel=\"noopener\">Tensorboard</a> is used to compare and choose better model.<br><img src=\"https://raw.githubusercontent.com/Trouble404/NBA-with-Machine-Learning/master/readme_add_pic/rookies-allstar.PNG\" alt=\"image\"></li>\n</ul>\n<hr>\n<h2 id=\"Result\"><a href=\"#Result\" class=\"headerlink\" title=\"Result\"></a>Result</h2><h2 id=\"Website-2017-NBA-ALL-STAR-AND-BEST-ROOKIES-PREDICTION-WITH-FAN-MAP\"><a href=\"#Website-2017-NBA-ALL-STAR-AND-BEST-ROOKIES-PREDICTION-WITH-FAN-MAP\" class=\"headerlink\" title=\"Website: 2017 NBA ALL-STAR AND BEST ROOKIES PREDICTION WITH FAN-MAP\"></a><strong>Website: <a href=\"https://d2v4olxsjbfep7.cloudfront.net/panels.html\" target=\"_blank\" rel=\"noopener\">2017 NBA ALL-STAR AND BEST ROOKIES PREDICTION WITH FAN-MAP</a></strong></h2><p>example: </p>\n<ul>\n<li>Neural network model</li>\n</ul>\n<p><img src=\"https://raw.githubusercontent.com/Trouble404/NBA-with-Machine-Learning/master/readme_add_pic/model.png\" alt=\"image\"></p>\n<ul>\n<li>2D PCA (linear unseparable)</li>\n</ul>\n<p><img src=\"https://raw.githubusercontent.com/Trouble404/NBA-with-Machine-Learning/master/readme_add_pic/pca.PNG\" alt=\"image\"></p>\n<ul>\n<li>Tensorboard check</li>\n</ul>\n<p><img src=\"https://raw.githubusercontent.com/Trouble404/NBA-with-Machine-Learning/master/readme_add_pic/tensor_board.PNG\" alt=\"image\"></p>"},{"title":" - Semantic Soft Segmentation","date":"2018-11-29T03:00:00.000Z","_content":"\n#  - Semantic Soft Segmentation\n\n<center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Semantic-soft-Segmentation/1.png)</center>\n\n---\n\n[Semantic Soft Segmentation - SIGGRAPH2018](http://people.inf.ethz.ch/aksoyy/papers/TOG18-sss.pdf)\n\n[Yagiz Aksoy](http://people.inf.ethz.ch/aksoyy/),[Tae-Hyun Oh](http://taehyunoh.com/),[Sylvain Paris](http://people.csail.mit.edu/sparis/),[Marc Pollefeys](https://www.inf.ethz.ch/personal/marc.pollefeys/)and[Wojciech Matusik](http://people.csail.mit.edu/wojciech/)\n\nMIT CSAIL, Adobe Research<!-- more -->\n\n---\n\n[[Paper - Semantic Soft Segmentation - SIGGRAPH2018]](http://people.inf.ethz.ch/aksoyy/papers/TOG18-sss.pdf)\n\n[[Supplementary Material - Semantic Soft Segmentation - SIGGRAPH2018]](http://people.inf.ethz.ch/aksoyy/papers/TOG18-sss-supp.pdf)\n\n[[HomePage]](http://people.inf.ethz.ch/aksoyy/sss/)\n\n[[Github - SIGGRAPH18SSS - Semantic feature generator- ]](https://github.com/iyah4888/SIGGRAPH18SSS)\n\n[[Github - Spectral segmentation implementation - ]](https://github.com/yaksoy/SemanticSoftSegmentation)\n\n[[YouTube - Video]](https://youtu.be/QYIQbfnS9jA)\n\n(Semantic Soft Segments). (magnetic lasso) (magic wand) .\n\n(spectral segmentation)  soft segmentation (Graph Structure).  Laplacian (eigendecomposition)  soft segments.\n\n\n1. .\n2. .\n\nSemantic Soft Segmentation(soft transitions) .\n\n\n\n*   Soft segmentation - .\n*   Natural image matting - .  trimap.\n*   Targeted edit propagation\n*   Semantic segmentation - \n\n## \n\n****\n soft .\nalpha. alpha=0 (fully opaque)alpha=1 (fully transparent)alpha  0-1 .\n\n$$(R,G,B)_{input} = \\sum_{i} \\alpha_{i}(R,G,B)_{i}$$\n$$\\sum_{i}\\alpha_{i}=1$$\n\n RGB  alpha .\n\n<center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Semantic-soft-Segmentation/2.png)</center>\n\n### 1\\.  - Nonlocal Color Affinity\n\n. Nonloal Color Affinityisolated\n<center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Semantic-soft-Segmentation/3.png)</center>\n\n\n1.  SLIC()  2500 ;\n2.  20% .\n\n### 2\\.  - High-Level Semantic Affinity\n\n nonlocal color affinity .\n.\n\n<center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Semantic-soft-Segmentation/4.png)</center>\n\n### 3\\.  - Creating the Layers\n\n Laplacian .\n\n1.  Laplacian \n3. (Constrained sparsification)\n3. (Relaxed sparsification)\n\n<center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Semantic-soft-Segmentation/5.png)</center>\n\n### 4\\.  - Semantic Feature Vectors\n.\n.\n\n DeepLab-ResNet-101  L2 ( N-Pair loss).\n\n<center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Semantic-soft-Segmentation/6.png)</center>\n\n### 5\\. \n(deep learning)3~4 ","source":"_posts/MalongTEch-Learning-Semantic-Soft-Segmentation.md","raw":"---\ntitle:  - Semantic Soft Segmentation\ndate: 2018-11-29 11:00:00\ntags: [Deep Learning]\ncategories: \n---\n\n#  - Semantic Soft Segmentation\n\n<center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Semantic-soft-Segmentation/1.png)</center>\n\n---\n\n[Semantic Soft Segmentation - SIGGRAPH2018](http://people.inf.ethz.ch/aksoyy/papers/TOG18-sss.pdf)\n\n[Yagiz Aksoy](http://people.inf.ethz.ch/aksoyy/),[Tae-Hyun Oh](http://taehyunoh.com/),[Sylvain Paris](http://people.csail.mit.edu/sparis/),[Marc Pollefeys](https://www.inf.ethz.ch/personal/marc.pollefeys/)and[Wojciech Matusik](http://people.csail.mit.edu/wojciech/)\n\nMIT CSAIL, Adobe Research<!-- more -->\n\n---\n\n[[Paper - Semantic Soft Segmentation - SIGGRAPH2018]](http://people.inf.ethz.ch/aksoyy/papers/TOG18-sss.pdf)\n\n[[Supplementary Material - Semantic Soft Segmentation - SIGGRAPH2018]](http://people.inf.ethz.ch/aksoyy/papers/TOG18-sss-supp.pdf)\n\n[[HomePage]](http://people.inf.ethz.ch/aksoyy/sss/)\n\n[[Github - SIGGRAPH18SSS - Semantic feature generator- ]](https://github.com/iyah4888/SIGGRAPH18SSS)\n\n[[Github - Spectral segmentation implementation - ]](https://github.com/yaksoy/SemanticSoftSegmentation)\n\n[[YouTube - Video]](https://youtu.be/QYIQbfnS9jA)\n\n(Semantic Soft Segments). (magnetic lasso) (magic wand) .\n\n(spectral segmentation)  soft segmentation (Graph Structure).  Laplacian (eigendecomposition)  soft segments.\n\n\n1. .\n2. .\n\nSemantic Soft Segmentation(soft transitions) .\n\n\n\n*   Soft segmentation - .\n*   Natural image matting - .  trimap.\n*   Targeted edit propagation\n*   Semantic segmentation - \n\n## \n\n****\n soft .\nalpha. alpha=0 (fully opaque)alpha=1 (fully transparent)alpha  0-1 .\n\n$$(R,G,B)_{input} = \\sum_{i} \\alpha_{i}(R,G,B)_{i}$$\n$$\\sum_{i}\\alpha_{i}=1$$\n\n RGB  alpha .\n\n<center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Semantic-soft-Segmentation/2.png)</center>\n\n### 1\\.  - Nonlocal Color Affinity\n\n. Nonloal Color Affinityisolated\n<center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Semantic-soft-Segmentation/3.png)</center>\n\n\n1.  SLIC()  2500 ;\n2.  20% .\n\n### 2\\.  - High-Level Semantic Affinity\n\n nonlocal color affinity .\n.\n\n<center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Semantic-soft-Segmentation/4.png)</center>\n\n### 3\\.  - Creating the Layers\n\n Laplacian .\n\n1.  Laplacian \n3. (Constrained sparsification)\n3. (Relaxed sparsification)\n\n<center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Semantic-soft-Segmentation/5.png)</center>\n\n### 4\\.  - Semantic Feature Vectors\n.\n.\n\n DeepLab-ResNet-101  L2 ( N-Pair loss).\n\n<center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Semantic-soft-Segmentation/6.png)</center>\n\n### 5\\. \n(deep learning)3~4 ","slug":"MalongTEch-Learning-Semantic-Soft-Segmentation","published":1,"updated":"2020-04-28T12:38:55.395Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck9jwdng50008jlrcbfxepafd","content":"<h1 id=\"-Semantic-Soft-Segmentation\"><a href=\"#-Semantic-Soft-Segmentation\" class=\"headerlink\" title=\" - Semantic Soft Segmentation\"></a> - Semantic Soft Segmentation</h1><center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Semantic-soft-Segmentation/1.png\" alt=\"image\"></center>\n\n<hr>\n<p><a href=\"http://people.inf.ethz.ch/aksoyy/papers/TOG18-sss.pdf\" target=\"_blank\" rel=\"noopener\">Semantic Soft Segmentation - SIGGRAPH2018</a></p>\n<p><a href=\"http://people.inf.ethz.ch/aksoyy/\" target=\"_blank\" rel=\"noopener\">Yagiz Aksoy</a>, <a href=\"http://taehyunoh.com/\" target=\"_blank\" rel=\"noopener\">Tae-Hyun Oh</a>, <a href=\"http://people.csail.mit.edu/sparis/\" target=\"_blank\" rel=\"noopener\">Sylvain Paris</a>, <a href=\"https://www.inf.ethz.ch/personal/marc.pollefeys/\" target=\"_blank\" rel=\"noopener\">Marc Pollefeys</a> and <a href=\"http://people.csail.mit.edu/wojciech/\" target=\"_blank\" rel=\"noopener\">Wojciech Matusik</a></p>\n<p>MIT CSAIL, Adobe Research<a id=\"more\"></a></p>\n<hr>\n<p><a href=\"http://people.inf.ethz.ch/aksoyy/papers/TOG18-sss.pdf\" target=\"_blank\" rel=\"noopener\">[Paper - Semantic Soft Segmentation - SIGGRAPH2018]</a></p>\n<p><a href=\"http://people.inf.ethz.ch/aksoyy/papers/TOG18-sss-supp.pdf\" target=\"_blank\" rel=\"noopener\">[Supplementary Material - Semantic Soft Segmentation - SIGGRAPH2018]</a></p>\n<p><a href=\"http://people.inf.ethz.ch/aksoyy/sss/\" target=\"_blank\" rel=\"noopener\">[HomePage]</a></p>\n<p><a href=\"https://github.com/iyah4888/SIGGRAPH18SSS\" target=\"_blank\" rel=\"noopener\">[Github - SIGGRAPH18SSS - Semantic feature generator- ]</a></p>\n<p><a href=\"https://github.com/yaksoy/SemanticSoftSegmentation\" target=\"_blank\" rel=\"noopener\">[Github - Spectral segmentation implementation - ]</a></p>\n<p><a href=\"https://youtu.be/QYIQbfnS9jA\" target=\"_blank\" rel=\"noopener\">[YouTube - Video]</a></p>\n<p>(Semantic Soft Segments). (magnetic lasso) (magic wand) .</p>\n<p>(spectral segmentation)  soft segmentation (Graph Structure).  Laplacian (eigendecomposition)  soft segments.</p>\n<p></p>\n<ol>\n<li>.</li>\n<li>.</li>\n</ol>\n<p>Semantic Soft Segmentation(soft transitions) .</p>\n<p></p>\n<ul>\n<li>Soft segmentation - .</li>\n<li>Natural image matting - .  trimap.</li>\n<li>Targeted edit propagation</li>\n<li>Semantic segmentation - </li>\n</ul>\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h2><p><strong></strong><br> soft .<br>alpha. alpha=0 (fully opaque)alpha=1 (fully transparent)alpha  0-1 .</p>\n<p>$$(R,G,B)<em>{input} = \\sum</em>{i} \\alpha_{i}(R,G,B)<em>{i}$$<br>$$\\sum</em>{i}\\alpha_{i}=1$$</p>\n<p> RGB  alpha .</p>\n<center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Semantic-soft-Segmentation/2.png\" alt=\"image\"></center>\n\n<h3 id=\"1--Nonlocal-Color-Affinity\"><a href=\"#1--Nonlocal-Color-Affinity\" class=\"headerlink\" title=\"1.  - Nonlocal Color Affinity\"></a>1.  - Nonlocal Color Affinity</h3><p>. Nonloal Color Affinityisolated</p>\n<center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Semantic-soft-Segmentation/3.png\" alt=\"image\"></center>\n\n<p></p>\n<ol>\n<li> SLIC()  2500 ;</li>\n<li> 20% .</li>\n</ol>\n<h3 id=\"2--High-Level-Semantic-Affinity\"><a href=\"#2--High-Level-Semantic-Affinity\" class=\"headerlink\" title=\"2.  - High-Level Semantic Affinity\"></a>2.  - High-Level Semantic Affinity</h3><p> nonlocal color affinity .<br>.</p>\n<center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Semantic-soft-Segmentation/4.png\" alt=\"image\"></center>\n\n<h3 id=\"3--Creating-the-Layers\"><a href=\"#3--Creating-the-Layers\" class=\"headerlink\" title=\"3.  - Creating the Layers\"></a>3.  - Creating the Layers</h3><p> Laplacian .</p>\n<ol>\n<li> Laplacian </li>\n<li>(Constrained sparsification)</li>\n<li>(Relaxed sparsification)</li>\n</ol>\n<center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Semantic-soft-Segmentation/5.png\" alt=\"image\"></center>\n\n<h3 id=\"4--Semantic-Feature-Vectors\"><a href=\"#4--Semantic-Feature-Vectors\" class=\"headerlink\" title=\"4.  - Semantic Feature Vectors\"></a>4.  - Semantic Feature Vectors</h3><p>.<br>.</p>\n<p> DeepLab-ResNet-101  L2 ( N-Pair loss).</p>\n<center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Semantic-soft-Segmentation/6.png\" alt=\"image\"></center>\n\n<h3 id=\"5-\"><a href=\"#5-\" class=\"headerlink\" title=\"5. \"></a>5. </h3><p>(deep learning)3~4 </p>\n","site":{"data":{}},"excerpt":"<h1 id=\"-Semantic-Soft-Segmentation\"><a href=\"#-Semantic-Soft-Segmentation\" class=\"headerlink\" title=\" - Semantic Soft Segmentation\"></a> - Semantic Soft Segmentation</h1><center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Semantic-soft-Segmentation/1.png\" alt=\"image\"></center>\n\n<hr>\n<p><a href=\"http://people.inf.ethz.ch/aksoyy/papers/TOG18-sss.pdf\" target=\"_blank\" rel=\"noopener\">Semantic Soft Segmentation - SIGGRAPH2018</a></p>\n<p><a href=\"http://people.inf.ethz.ch/aksoyy/\" target=\"_blank\" rel=\"noopener\">Yagiz Aksoy</a>, <a href=\"http://taehyunoh.com/\" target=\"_blank\" rel=\"noopener\">Tae-Hyun Oh</a>, <a href=\"http://people.csail.mit.edu/sparis/\" target=\"_blank\" rel=\"noopener\">Sylvain Paris</a>, <a href=\"https://www.inf.ethz.ch/personal/marc.pollefeys/\" target=\"_blank\" rel=\"noopener\">Marc Pollefeys</a> and <a href=\"http://people.csail.mit.edu/wojciech/\" target=\"_blank\" rel=\"noopener\">Wojciech Matusik</a></p>\n<p>MIT CSAIL, Adobe Research","more":"</p>\n<hr>\n<p><a href=\"http://people.inf.ethz.ch/aksoyy/papers/TOG18-sss.pdf\" target=\"_blank\" rel=\"noopener\">[Paper - Semantic Soft Segmentation - SIGGRAPH2018]</a></p>\n<p><a href=\"http://people.inf.ethz.ch/aksoyy/papers/TOG18-sss-supp.pdf\" target=\"_blank\" rel=\"noopener\">[Supplementary Material - Semantic Soft Segmentation - SIGGRAPH2018]</a></p>\n<p><a href=\"http://people.inf.ethz.ch/aksoyy/sss/\" target=\"_blank\" rel=\"noopener\">[HomePage]</a></p>\n<p><a href=\"https://github.com/iyah4888/SIGGRAPH18SSS\" target=\"_blank\" rel=\"noopener\">[Github - SIGGRAPH18SSS - Semantic feature generator- ]</a></p>\n<p><a href=\"https://github.com/yaksoy/SemanticSoftSegmentation\" target=\"_blank\" rel=\"noopener\">[Github - Spectral segmentation implementation - ]</a></p>\n<p><a href=\"https://youtu.be/QYIQbfnS9jA\" target=\"_blank\" rel=\"noopener\">[YouTube - Video]</a></p>\n<p>(Semantic Soft Segments). (magnetic lasso) (magic wand) .</p>\n<p>(spectral segmentation)  soft segmentation (Graph Structure).  Laplacian (eigendecomposition)  soft segments.</p>\n<p></p>\n<ol>\n<li>.</li>\n<li>.</li>\n</ol>\n<p>Semantic Soft Segmentation(soft transitions) .</p>\n<p></p>\n<ul>\n<li>Soft segmentation - .</li>\n<li>Natural image matting - .  trimap.</li>\n<li>Targeted edit propagation</li>\n<li>Semantic segmentation - </li>\n</ul>\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h2><p><strong></strong><br> soft .<br>alpha. alpha=0 (fully opaque)alpha=1 (fully transparent)alpha  0-1 .</p>\n<p>$$(R,G,B)<em>{input} = \\sum</em>{i} \\alpha_{i}(R,G,B)<em>{i}$$<br>$$\\sum</em>{i}\\alpha_{i}=1$$</p>\n<p> RGB  alpha .</p>\n<center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Semantic-soft-Segmentation/2.png\" alt=\"image\"></center>\n\n<h3 id=\"1--Nonlocal-Color-Affinity\"><a href=\"#1--Nonlocal-Color-Affinity\" class=\"headerlink\" title=\"1.  - Nonlocal Color Affinity\"></a>1.  - Nonlocal Color Affinity</h3><p>. Nonloal Color Affinityisolated</p>\n<center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Semantic-soft-Segmentation/3.png\" alt=\"image\"></center>\n\n<p></p>\n<ol>\n<li> SLIC()  2500 ;</li>\n<li> 20% .</li>\n</ol>\n<h3 id=\"2--High-Level-Semantic-Affinity\"><a href=\"#2--High-Level-Semantic-Affinity\" class=\"headerlink\" title=\"2.  - High-Level Semantic Affinity\"></a>2.  - High-Level Semantic Affinity</h3><p> nonlocal color affinity .<br>.</p>\n<center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Semantic-soft-Segmentation/4.png\" alt=\"image\"></center>\n\n<h3 id=\"3--Creating-the-Layers\"><a href=\"#3--Creating-the-Layers\" class=\"headerlink\" title=\"3.  - Creating the Layers\"></a>3.  - Creating the Layers</h3><p> Laplacian .</p>\n<ol>\n<li> Laplacian </li>\n<li>(Constrained sparsification)</li>\n<li>(Relaxed sparsification)</li>\n</ol>\n<center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Semantic-soft-Segmentation/5.png\" alt=\"image\"></center>\n\n<h3 id=\"4--Semantic-Feature-Vectors\"><a href=\"#4--Semantic-Feature-Vectors\" class=\"headerlink\" title=\"4.  - Semantic Feature Vectors\"></a>4.  - Semantic Feature Vectors</h3><p>.<br>.</p>\n<p> DeepLab-ResNet-101  L2 ( N-Pair loss).</p>\n<center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Semantic-soft-Segmentation/6.png\" alt=\"image\"></center>\n\n<h3 id=\"5-\"><a href=\"#5-\" class=\"headerlink\" title=\"5. \"></a>5. </h3><p>(deep learning)3~4 </p>"},{"title":"MalongTech :-3D or ","date":"2018-11-30T03:00:00.000Z","_content":"\n# \n\n<center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/3D-Segmentation/1.png)</center>\n\n| 2D | 3D|\n| ---------- | -----------|\n| A Novel Domain Adaptation Framework for Medical Image Segmentation   | 3D U-Net: Learning Dense Volumetric Segmentation from Sparrse Annotation   |\n| DeepMedic for Brain Tumor Segmentation | Joint Sequence Learning and Cross-Modality Convolution for 3D Biomedical Segmentation  |\n|  | Simultaneous Super-Resolution and Cross-Modality Synthesis of 3D Medical Image using Weekly-Supervised Joint Convolutional Sparse Coding |\n<!-- more -->\n\n---\n\n## A Novel Domain Adaptation Framework for Medical Image Segmentation\n\n[](https://arxiv.org/pdf/1810.05732.pdf)\n\n****\n* ()\n* 4(Modality)\n  1. (T1)\n  2. T1(T2)\n  3. (T2)\n  4. (FLAIR)\n\n****:\n* A biophysics based domain adaptation method(MR)\n* An automatic method to ()\n\n****\n* **Data Augmentation:** (PDE)correct intensities distributionMR()\n* \n* **Extened segmentation:** (in-house diffeomorphic registration code)(MRI)<center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/3D-Segmentation/2.png)</center>\n\n  DNN() \n  \n  :\n    * Affine registration of each atlas image to the brats image\n    * Diffeomorphic registration of each atlas image to the BraTS image(BraTS)\n    * Majority voting to fuse labels of all deformed atlases to get the final healthy tissuse segmentation\n  \n* ****\n\n  1. 3D U-Net()\n\n    3D\n  2. U-Net()\n\n    2DU-Netdomain adaptation results\n    <center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/3D-Segmentation/3.png)</center>\n    \n* ****\n\n  2Ddomain trasformations3D2D3Defficient\n\n---\n\n## 3D U-Net: Learning Dense Volumetric Segmentation from Sparse Annotation\n\n[](https://arxiv.org/pdf/1606.06650.pdf)\n\n3D2D()3D()3D Unet2D:\n* Semi-automated setup: \n* Fully-automated setup: representative\n<center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/3D-Segmentation/4.png)</center>\n\n****\n* 2D U-Netencoder, decoderfull-resolution\n* 3D3D convolutions, 3D max pooling  3D up-convolutional\n* bottlenecks\n\n****\n<center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/3D-Segmentation/5.png)</center>\n\n* Encoder: $3 \\times 3 \\times 3$ReLU, strides2$2 \\times 2 \\times 2$\n* Decoder: strides2$2 \\times 2 \\times 2$upconvolution$3 \\times 3 \\times 3$ReLU\n* Batch Normalization BN$x_{new}=\\alpha \\cdot x + \\beta$, \n* weighted softmanx loss function, unlabeled pixels0\n\n---\n\n## Joint Sequence Learning and Cross-Modality Convolution for 3D Biomedical Segmentation\n\n[](https://arxiv.org/pdf/1704.07754.pdf)\n\n<center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/3D-Segmentation/9.png)</center>\n\n\n1. \n2. \n\n encoder-decoder LSTM2D\n\nMRICT4(Modality)2D3D3D2D3D2DLSTMLSTM3D\n\n\n****\n<center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/3D-Segmentation/6.png)</center>\n\nframework\n\n*   MRIslice2D\n\n*   bmulti-modalencoderencoder\n\n*   encode2D\n\n*   convolution LSTM2D2D2Ddependency3Ddecoderbackground\n\n****\n<center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/3D-Segmentation/7.png)</center>\n\n1. **MME(Multi-Modal Encoder)**\n\n  SegNet, batch-normalization\n  \n2. **MRF(Multi-Resolution Fusion)**\n    <center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/3D-Segmentation/8.png)</center>\n    encoderdecoderfeature multiplication\n    \n3. **CMC(Cross-Modality COnvolution)**\n\n  CMC\n  \n  channelstackblock, channel C sliceHWCC4HWC4411\n\n4. **Slice Sequence Learning**\n\n  convolution LSTMLSTMLSTM2D\n  \n  convLSTM\n  \n5. **Decoder**\n\n  soft-max\n  \n****\n1. **Single Slice Training**:\n\n  median frequency balancingcross-entropy loss(98%)\n  $$\\alpha_{c}=\\frac{median\\_freq}{freq(c)}$$\n  \n2. **Two-Phase Training**:\n\n  * median frequency balancing\n  * median frequency\n\n****\n<center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/3D-Segmentation/10.png)</center>","source":"_posts/MalongTech-Learning-3D-Image-Segmentation.md","raw":"---\ntitle: MalongTech :-3D or \ndate: 2018-11-30 11:00:00\ntags: [Deep Learning]\ncategories: \n---\n\n# \n\n<center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/3D-Segmentation/1.png)</center>\n\n| 2D | 3D|\n| ---------- | -----------|\n| A Novel Domain Adaptation Framework for Medical Image Segmentation   | 3D U-Net: Learning Dense Volumetric Segmentation from Sparrse Annotation   |\n| DeepMedic for Brain Tumor Segmentation | Joint Sequence Learning and Cross-Modality Convolution for 3D Biomedical Segmentation  |\n|  | Simultaneous Super-Resolution and Cross-Modality Synthesis of 3D Medical Image using Weekly-Supervised Joint Convolutional Sparse Coding |\n<!-- more -->\n\n---\n\n## A Novel Domain Adaptation Framework for Medical Image Segmentation\n\n[](https://arxiv.org/pdf/1810.05732.pdf)\n\n****\n* ()\n* 4(Modality)\n  1. (T1)\n  2. T1(T2)\n  3. (T2)\n  4. (FLAIR)\n\n****:\n* A biophysics based domain adaptation method(MR)\n* An automatic method to ()\n\n****\n* **Data Augmentation:** (PDE)correct intensities distributionMR()\n* \n* **Extened segmentation:** (in-house diffeomorphic registration code)(MRI)<center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/3D-Segmentation/2.png)</center>\n\n  DNN() \n  \n  :\n    * Affine registration of each atlas image to the brats image\n    * Diffeomorphic registration of each atlas image to the BraTS image(BraTS)\n    * Majority voting to fuse labels of all deformed atlases to get the final healthy tissuse segmentation\n  \n* ****\n\n  1. 3D U-Net()\n\n    3D\n  2. U-Net()\n\n    2DU-Netdomain adaptation results\n    <center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/3D-Segmentation/3.png)</center>\n    \n* ****\n\n  2Ddomain trasformations3D2D3Defficient\n\n---\n\n## 3D U-Net: Learning Dense Volumetric Segmentation from Sparse Annotation\n\n[](https://arxiv.org/pdf/1606.06650.pdf)\n\n3D2D()3D()3D Unet2D:\n* Semi-automated setup: \n* Fully-automated setup: representative\n<center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/3D-Segmentation/4.png)</center>\n\n****\n* 2D U-Netencoder, decoderfull-resolution\n* 3D3D convolutions, 3D max pooling  3D up-convolutional\n* bottlenecks\n\n****\n<center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/3D-Segmentation/5.png)</center>\n\n* Encoder: $3 \\times 3 \\times 3$ReLU, strides2$2 \\times 2 \\times 2$\n* Decoder: strides2$2 \\times 2 \\times 2$upconvolution$3 \\times 3 \\times 3$ReLU\n* Batch Normalization BN$x_{new}=\\alpha \\cdot x + \\beta$, \n* weighted softmanx loss function, unlabeled pixels0\n\n---\n\n## Joint Sequence Learning and Cross-Modality Convolution for 3D Biomedical Segmentation\n\n[](https://arxiv.org/pdf/1704.07754.pdf)\n\n<center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/3D-Segmentation/9.png)</center>\n\n\n1. \n2. \n\n encoder-decoder LSTM2D\n\nMRICT4(Modality)2D3D3D2D3D2DLSTMLSTM3D\n\n\n****\n<center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/3D-Segmentation/6.png)</center>\n\nframework\n\n*   MRIslice2D\n\n*   bmulti-modalencoderencoder\n\n*   encode2D\n\n*   convolution LSTM2D2D2Ddependency3Ddecoderbackground\n\n****\n<center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/3D-Segmentation/7.png)</center>\n\n1. **MME(Multi-Modal Encoder)**\n\n  SegNet, batch-normalization\n  \n2. **MRF(Multi-Resolution Fusion)**\n    <center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/3D-Segmentation/8.png)</center>\n    encoderdecoderfeature multiplication\n    \n3. **CMC(Cross-Modality COnvolution)**\n\n  CMC\n  \n  channelstackblock, channel C sliceHWCC4HWC4411\n\n4. **Slice Sequence Learning**\n\n  convolution LSTMLSTMLSTM2D\n  \n  convLSTM\n  \n5. **Decoder**\n\n  soft-max\n  \n****\n1. **Single Slice Training**:\n\n  median frequency balancingcross-entropy loss(98%)\n  $$\\alpha_{c}=\\frac{median\\_freq}{freq(c)}$$\n  \n2. **Two-Phase Training**:\n\n  * median frequency balancing\n  * median frequency\n\n****\n<center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/3D-Segmentation/10.png)</center>","slug":"MalongTech-Learning-3D-Image-Segmentation","published":1,"updated":"2020-04-28T12:38:55.395Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck9jwdng60009jlrclgm4jkxk","content":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/3D-Segmentation/1.png\" alt=\"image\"></center>\n\n<table>\n<thead>\n<tr>\n<th>2D</th>\n<th>3D</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>A Novel Domain Adaptation Framework for Medical Image Segmentation</td>\n<td>3D U-Net: Learning Dense Volumetric Segmentation from Sparrse Annotation</td>\n</tr>\n<tr>\n<td>DeepMedic for Brain Tumor Segmentation</td>\n<td>Joint Sequence Learning and Cross-Modality Convolution for 3D Biomedical Segmentation</td>\n</tr>\n<tr>\n<td></td>\n<td>Simultaneous Super-Resolution and Cross-Modality Synthesis of 3D Medical Image using Weekly-Supervised Joint Convolutional Sparse Coding</td>\n</tr>\n</tbody>\n</table>\n<a id=\"more\"></a>\n<hr>\n<h2 id=\"A-Novel-Domain-Adaptation-Framework-for-Medical-Image-Segmentation\"><a href=\"#A-Novel-Domain-Adaptation-Framework-for-Medical-Image-Segmentation\" class=\"headerlink\" title=\"A Novel Domain Adaptation Framework for Medical Image Segmentation\"></a>A Novel Domain Adaptation Framework for Medical Image Segmentation</h2><p><a href=\"https://arxiv.org/pdf/1810.05732.pdf\" target=\"_blank\" rel=\"noopener\"></a></p>\n<p><strong></strong></p>\n<ul>\n<li>()</li>\n<li>4(Modality)<ol>\n<li>(T1)</li>\n<li>T1(T2)</li>\n<li>(T2)</li>\n<li>(FLAIR)</li>\n</ol>\n</li>\n</ul>\n<p><strong></strong>:</p>\n<ul>\n<li>A biophysics based domain adaptation method(MR)</li>\n<li>An automatic method to ()</li>\n</ul>\n<p><strong></strong></p>\n<ul>\n<li><strong>Data Augmentation:</strong> (PDE)correct intensities distributionMR()</li>\n<li></li>\n<li><p><strong>Extened segmentation:</strong> (in-house diffeomorphic registration code)(MRI)<center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/3D-Segmentation/2.png\" alt=\"image\"></center></p>\n<p>DNN() </p>\n<p>:</p>\n<ul>\n<li>Affine registration of each atlas image to the brats image</li>\n<li>Diffeomorphic registration of each atlas image to the BraTS image(BraTS)</li>\n<li>Majority voting to fuse labels of all deformed atlases to get the final healthy tissuse segmentation</li>\n</ul>\n</li>\n<li><p><strong></strong></p>\n<ol>\n<li><p>3D U-Net()</p>\n<p>3D</p>\n</li>\n<li><p>U-Net()</p>\n<p>2DU-Netdomain adaptation results</p>\n<center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/3D-Segmentation/3.png\" alt=\"image\"></center>\n</li>\n</ol>\n</li>\n<li><p><strong></strong></p>\n<p>2Ddomain trasformations3D2D3Defficient</p>\n</li>\n</ul>\n<hr>\n<h2 id=\"3D-U-Net-Learning-Dense-Volumetric-Segmentation-from-Sparse-Annotation\"><a href=\"#3D-U-Net-Learning-Dense-Volumetric-Segmentation-from-Sparse-Annotation\" class=\"headerlink\" title=\"3D U-Net: Learning Dense Volumetric Segmentation from Sparse Annotation\"></a>3D U-Net: Learning Dense Volumetric Segmentation from Sparse Annotation</h2><p><a href=\"https://arxiv.org/pdf/1606.06650.pdf\" target=\"_blank\" rel=\"noopener\"></a></p>\n<p>3D2D()3D()3D Unet2D:</p>\n<ul>\n<li>Semi-automated setup: </li>\n<li>Fully-automated setup: representative<center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/3D-Segmentation/4.png\" alt=\"image\"></center>\n\n</li>\n</ul>\n<p><strong></strong></p>\n<ul>\n<li>2D U-Netencoder, decoderfull-resolution</li>\n<li>3D3D convolutions, 3D max pooling  3D up-convolutional</li>\n<li>bottlenecks</li>\n</ul>\n<p><strong></strong></p>\n<center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/3D-Segmentation/5.png\" alt=\"image\"></center>\n\n<ul>\n<li>Encoder: $3 \\times 3 \\times 3$ReLU, strides2$2 \\times 2 \\times 2$</li>\n<li>Decoder: strides2$2 \\times 2 \\times 2$upconvolution$3 \\times 3 \\times 3$ReLU</li>\n<li>Batch Normalization BN$x_{new}=\\alpha \\cdot x + \\beta$, </li>\n<li>weighted softmanx loss function, unlabeled pixels0</li>\n</ul>\n<hr>\n<h2 id=\"Joint-Sequence-Learning-and-Cross-Modality-Convolution-for-3D-Biomedical-Segmentation\"><a href=\"#Joint-Sequence-Learning-and-Cross-Modality-Convolution-for-3D-Biomedical-Segmentation\" class=\"headerlink\" title=\"Joint Sequence Learning and Cross-Modality Convolution for 3D Biomedical Segmentation\"></a>Joint Sequence Learning and Cross-Modality Convolution for 3D Biomedical Segmentation</h2><p><a href=\"https://arxiv.org/pdf/1704.07754.pdf\" target=\"_blank\" rel=\"noopener\"></a></p>\n<center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/3D-Segmentation/9.png\" alt=\"image\"></center>\n\n<p></p>\n<ol>\n<li></li>\n<li></li>\n</ol>\n<p> encoder-decoder LSTM2D</p>\n<p>MRICT4(Modality)2D3D3D2D3D2DLSTMLSTM3D</p>\n<p><strong></strong></p>\n<center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/3D-Segmentation/6.png\" alt=\"image\"></center>\n\n<p>framework</p>\n<ul>\n<li><p>MRIslice2D</p>\n</li>\n<li><p>bmulti-modalencoderencoder</p>\n</li>\n<li><p>encode2D</p>\n</li>\n<li><p>convolution LSTM2D2D2Ddependency3Ddecoderbackground</p>\n</li>\n</ul>\n<p><strong></strong></p>\n<center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/3D-Segmentation/7.png\" alt=\"image\"></center>\n\n<ol>\n<li><p><strong>MME(Multi-Modal Encoder)</strong></p>\n<p>SegNet, batch-normalization</p>\n</li>\n<li><p><strong>MRF(Multi-Resolution Fusion)</strong><br> <center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/3D-Segmentation/8.png\" alt=\"image\"></center><br> encoderdecoderfeature multiplication</p>\n</li>\n<li><p><strong>CMC(Cross-Modality COnvolution)</strong></p>\n<p>CMC</p>\n<p>channelstackblock, channel C sliceHWCC4HWC4411</p>\n</li>\n<li><p><strong>Slice Sequence Learning</strong></p>\n<p>convolution LSTMLSTMLSTM2D</p>\n<p>convLSTM</p>\n</li>\n<li><p><strong>Decoder</strong></p>\n<p>soft-max</p>\n</li>\n</ol>\n<p><strong></strong></p>\n<ol>\n<li><p><strong>Single Slice Training</strong>:</p>\n<p>median frequency balancingcross-entropy loss(98%)<br>$$\\alpha_{c}=\\frac{median_freq}{freq(c)}$$</p>\n</li>\n<li><p><strong>Two-Phase Training</strong>:</p>\n<ul>\n<li>median frequency balancing</li>\n<li>median frequency</li>\n</ul>\n</li>\n</ol>\n<p><strong></strong></p>\n<center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/3D-Segmentation/10.png\" alt=\"image\"></center>","site":{"data":{}},"excerpt":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/3D-Segmentation/1.png\" alt=\"image\"></center>\n\n<table>\n<thead>\n<tr>\n<th>2D</th>\n<th>3D</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>A Novel Domain Adaptation Framework for Medical Image Segmentation</td>\n<td>3D U-Net: Learning Dense Volumetric Segmentation from Sparrse Annotation</td>\n</tr>\n<tr>\n<td>DeepMedic for Brain Tumor Segmentation</td>\n<td>Joint Sequence Learning and Cross-Modality Convolution for 3D Biomedical Segmentation</td>\n</tr>\n<tr>\n<td></td>\n<td>Simultaneous Super-Resolution and Cross-Modality Synthesis of 3D Medical Image using Weekly-Supervised Joint Convolutional Sparse Coding</td>\n</tr>\n</tbody>\n</table>","more":"<hr>\n<h2 id=\"A-Novel-Domain-Adaptation-Framework-for-Medical-Image-Segmentation\"><a href=\"#A-Novel-Domain-Adaptation-Framework-for-Medical-Image-Segmentation\" class=\"headerlink\" title=\"A Novel Domain Adaptation Framework for Medical Image Segmentation\"></a>A Novel Domain Adaptation Framework for Medical Image Segmentation</h2><p><a href=\"https://arxiv.org/pdf/1810.05732.pdf\" target=\"_blank\" rel=\"noopener\"></a></p>\n<p><strong></strong></p>\n<ul>\n<li>()</li>\n<li>4(Modality)<ol>\n<li>(T1)</li>\n<li>T1(T2)</li>\n<li>(T2)</li>\n<li>(FLAIR)</li>\n</ol>\n</li>\n</ul>\n<p><strong></strong>:</p>\n<ul>\n<li>A biophysics based domain adaptation method(MR)</li>\n<li>An automatic method to ()</li>\n</ul>\n<p><strong></strong></p>\n<ul>\n<li><strong>Data Augmentation:</strong> (PDE)correct intensities distributionMR()</li>\n<li></li>\n<li><p><strong>Extened segmentation:</strong> (in-house diffeomorphic registration code)(MRI)<center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/3D-Segmentation/2.png\" alt=\"image\"></center></p>\n<p>DNN() </p>\n<p>:</p>\n<ul>\n<li>Affine registration of each atlas image to the brats image</li>\n<li>Diffeomorphic registration of each atlas image to the BraTS image(BraTS)</li>\n<li>Majority voting to fuse labels of all deformed atlases to get the final healthy tissuse segmentation</li>\n</ul>\n</li>\n<li><p><strong></strong></p>\n<ol>\n<li><p>3D U-Net()</p>\n<p>3D</p>\n</li>\n<li><p>U-Net()</p>\n<p>2DU-Netdomain adaptation results</p>\n<center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/3D-Segmentation/3.png\" alt=\"image\"></center>\n</li>\n</ol>\n</li>\n<li><p><strong></strong></p>\n<p>2Ddomain trasformations3D2D3Defficient</p>\n</li>\n</ul>\n<hr>\n<h2 id=\"3D-U-Net-Learning-Dense-Volumetric-Segmentation-from-Sparse-Annotation\"><a href=\"#3D-U-Net-Learning-Dense-Volumetric-Segmentation-from-Sparse-Annotation\" class=\"headerlink\" title=\"3D U-Net: Learning Dense Volumetric Segmentation from Sparse Annotation\"></a>3D U-Net: Learning Dense Volumetric Segmentation from Sparse Annotation</h2><p><a href=\"https://arxiv.org/pdf/1606.06650.pdf\" target=\"_blank\" rel=\"noopener\"></a></p>\n<p>3D2D()3D()3D Unet2D:</p>\n<ul>\n<li>Semi-automated setup: </li>\n<li>Fully-automated setup: representative<center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/3D-Segmentation/4.png\" alt=\"image\"></center>\n\n</li>\n</ul>\n<p><strong></strong></p>\n<ul>\n<li>2D U-Netencoder, decoderfull-resolution</li>\n<li>3D3D convolutions, 3D max pooling  3D up-convolutional</li>\n<li>bottlenecks</li>\n</ul>\n<p><strong></strong></p>\n<center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/3D-Segmentation/5.png\" alt=\"image\"></center>\n\n<ul>\n<li>Encoder: $3 \\times 3 \\times 3$ReLU, strides2$2 \\times 2 \\times 2$</li>\n<li>Decoder: strides2$2 \\times 2 \\times 2$upconvolution$3 \\times 3 \\times 3$ReLU</li>\n<li>Batch Normalization BN$x_{new}=\\alpha \\cdot x + \\beta$, </li>\n<li>weighted softmanx loss function, unlabeled pixels0</li>\n</ul>\n<hr>\n<h2 id=\"Joint-Sequence-Learning-and-Cross-Modality-Convolution-for-3D-Biomedical-Segmentation\"><a href=\"#Joint-Sequence-Learning-and-Cross-Modality-Convolution-for-3D-Biomedical-Segmentation\" class=\"headerlink\" title=\"Joint Sequence Learning and Cross-Modality Convolution for 3D Biomedical Segmentation\"></a>Joint Sequence Learning and Cross-Modality Convolution for 3D Biomedical Segmentation</h2><p><a href=\"https://arxiv.org/pdf/1704.07754.pdf\" target=\"_blank\" rel=\"noopener\"></a></p>\n<center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/3D-Segmentation/9.png\" alt=\"image\"></center>\n\n<p></p>\n<ol>\n<li></li>\n<li></li>\n</ol>\n<p> encoder-decoder LSTM2D</p>\n<p>MRICT4(Modality)2D3D3D2D3D2DLSTMLSTM3D</p>\n<p><strong></strong></p>\n<center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/3D-Segmentation/6.png\" alt=\"image\"></center>\n\n<p>framework</p>\n<ul>\n<li><p>MRIslice2D</p>\n</li>\n<li><p>bmulti-modalencoderencoder</p>\n</li>\n<li><p>encode2D</p>\n</li>\n<li><p>convolution LSTM2D2D2Ddependency3Ddecoderbackground</p>\n</li>\n</ul>\n<p><strong></strong></p>\n<center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/3D-Segmentation/7.png\" alt=\"image\"></center>\n\n<ol>\n<li><p><strong>MME(Multi-Modal Encoder)</strong></p>\n<p>SegNet, batch-normalization</p>\n</li>\n<li><p><strong>MRF(Multi-Resolution Fusion)</strong><br> <center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/3D-Segmentation/8.png\" alt=\"image\"></center><br> encoderdecoderfeature multiplication</p>\n</li>\n<li><p><strong>CMC(Cross-Modality COnvolution)</strong></p>\n<p>CMC</p>\n<p>channelstackblock, channel C sliceHWCC4HWC4411</p>\n</li>\n<li><p><strong>Slice Sequence Learning</strong></p>\n<p>convolution LSTMLSTMLSTM2D</p>\n<p>convLSTM</p>\n</li>\n<li><p><strong>Decoder</strong></p>\n<p>soft-max</p>\n</li>\n</ol>\n<p><strong></strong></p>\n<ol>\n<li><p><strong>Single Slice Training</strong>:</p>\n<p>median frequency balancingcross-entropy loss(98%)<br>$$\\alpha_{c}=\\frac{median_freq}{freq(c)}$$</p>\n</li>\n<li><p><strong>Two-Phase Training</strong>:</p>\n<ul>\n<li>median frequency balancing</li>\n<li>median frequency</li>\n</ul>\n</li>\n</ol>\n<p><strong></strong></p>\n<center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/3D-Segmentation/10.png\" alt=\"image\"></center>"},{"title":"MalongTech :-2D","date":"2018-11-26T03:00:00.000Z","_content":"\n# Image Segmentation\n\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Image-Segmentation-Learning/1.jpg)\n\n## Introduction\nSegmentation<!-- more -->\n\n\n\n****\n* 1. 2. 3. 4. 5. 6. \n* \n* \n* \n* \n\n## Tranditional Methods\n[](https://zhuanlan.zhihu.com/p/30732385)\n* ****: 1. 2. 3. .\n* ****: 1. Canny2. Harris3. SIFT3. SURF.\n* ****: 1. 2. 3. .\n* ****: 1. GraphCut; 2. GrabCut; 3. Random Walk.\n* ****: 1. Snake2. Active Shape Model; 3. Active Apperance Models; 4. Constrained local model.\n\n## Datasets\n1. Pascal VOC: 206929\n2. CityScapes30500020000\n3. MS COCO803320150\n4. ImageNet: [DeepLesion](https://www.52cv.net/?p=883), 1000032000\n\n## \n$k+1$$L_{o}$$L_{k}$,, $P_{ij}$**i****j**$p_{ii}$****$p_{ij}$$p_{ji}$\n1. Pixel Accuracy(PA, ): \n$$PA=\\frac{\\sum_{i=0}^{k}p_{ii}}{\\sum_{i=0}^{k}\\sum_{j=0}^{k}p_{ij}}$$\n2. Mean Pixel Accuracy(MAP,): PA\n$$MAP=\\frac{1}{k+1}\\sum_{i=0}^{k}\\frac{p_{ii}}{\\sum_{j=0}^{k}p_{ij}}$$\n3. Mean Intersection over Union(MIoU, ): (ground truth)(predicted segmentation). (intersection)()IoU\n$$MIoU = \\frac{1}{k+1}\\sum_{i=0}^{k}\\frac{p_{ii}}{\\sum_{j=0}^{k}p_{ij}+\\sum_{j=0}^{k}(p_{ji}-p_{ii})}$$\n4. Frequency Weighted Intersection over Union(FWIoU, ): MIoU\n$$FWIoU = \\frac{1}{\\sum_{i=0}^{k}\\sum_{j=0}^{k}p_{ij}}\\sum_{i=0}^{k}\\frac{p_{ii}}{\\sum_{j=0}^{k}p_{ij}+\\sum_{j=0}^{k}(p_{ji}-p_{ii})}$$\n\n# \n## Fully Convolutional Networks\n[](https://people.eecs.berkeley.edu/~jonlong/long_shelhamer_fcn.pdf)\n\n\n FCN \n<center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Image-Segmentation-Learning/2.png)</center>\n\n, AlexNet, 1000, , tabby catFCNsemantic segmentationFCNfeature map, , , softmax, \n\n**->**\n\nK=409677512F=7,P=0,S=1,K=4096.\n\n224x224x37x7x512AlexNet409610003\n* [7x7x512]F=7[1x1x4096]\n* F=1[1x1x4096]\n* F=1[1x1x1000]\n\n**end to end, pixels to pixels network**\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Image-Segmentation-Learning/3.png)\npooling$\\frac{H}{32} \\times \\frac{W}{32}$heatmapheatmapupsampling1000heatmapupsamplinglabeltrick1000\n\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Image-Segmentation-Learning/4.png)\n\n1/32heatMap1/16featureMap1/8featureMap1/32heatMapupsamplingconv5conv4upsamplingconv3upsampling\n\n****\n* 832\n* spatial regularization\n\n****\nupsampling****\n* \n\n<center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Image-Segmentation-Learning/5.png)</center>\n$\\frac{y-y_{0}}{y_{1}-y_{0}}=\\frac{x-x_{0}}{x_{1}-x_{0}}$ $\\alpha$$\\alpha =\\frac{y-y_{0}}{y_{1}-y_{0}}=\\frac{x-x_{0}}{x_{1}-x_{0}}$. y: $y=(1-\\alpha)y_{0}+\\alpha y_{1} = (1-\\frac{x-x_{0}}{x_{1}-x_{0}})y_{0}+\\frac{x-x_{0}}{x_{1}-x_{0}}y_{1}=\\frac{x_{1}-x}{x_{1}-x_{0}}y_{0}+\\frac{x-x_{0}}{x_{1}-x_{0}}y_{1}=\\frac{x_{1}-x}{x_{1}-x_{0}}f(x_{1})+\\frac{x-x_{0}}{x_{1}-x_{0}}f(x_{0})$\n\n* \n\n<center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Image-Segmentation-Learning/6.png)</center>\n\nX$Q_{12}$,$Q_{22}$$R_{2}$$Q_{11}$$Q_{21}$$R_{1}$\n\n$f(R_{1}=\\frac{x_{2}-x}{x_{2}-x_{1}}f(Q_{11})+\\frac{x-x_{1}}{x_{2}-x_{1}}f(Q_{21})$; $f(R_{2}=\\frac{x_{2}-x}{x_{2}-x_{1}}f(Q_{12})+\\frac{x-x_{1}}{x_{2}-x_{1}}f(Q_{22})$\n\nY$R_{1$}$R_{2}$yP\n\n$f(P)=\\frac{y_{2}-y}{y_{2}-y_{1}}f(R_{1})+\\frac{y-y_{1}}{y_{2}-y_{1}}f(R_{2})$\n\n---\n\n## U-Net: Convolutional Networks for Biomedical Image Segmentation\n[](https://arxiv.org/pdf/1505.04597.pdf)\nCiresanpatch  patches\n\n\n\npatchpatch,; patchesmax-pooling,patches\n\n**U-Net Architecture**\n<center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Image-Segmentation-Learning/7.jpg)</center>\n\n1. ()\n2. maxpooling\n3. :concatenatepooling\n4. 1X1heatmap,heatmap,softmaxsoftmax\n\n**Overlap-tile strategy**\n<center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Image-Segmentation-Learning/8.png)</center>\n\npatchpatchUnetoverlapoverlapoverlappatch \n\n****\n\nsoftmax\n$$E=\\sum_{x\\in \\Omega}w(x)log(p_{\\ell(x)}(x))$$\n\n$w(x)$:\n$$w(x)=w_{c}(x)+w_{0} \\times exp(-\\frac{(d_{1}(x)+d_{2}(x))}{2\\sigma^{2}})$$\n\n**Data Augmentation**\n3\\*3(smooth deformations) 10(Bicubic interpolation)contracting pathdrop-out \n\n****\nf(x,y)16\n\n---\n\n## DeepLab V1\n[](https://arxiv.org/pdf/1412.7062v3.pdf)\n\nDeepLab([DCNNs](https://www.cnblogs.com/wangxiaocvpr/p/8763510.html))([DenseCRFs](https://zhuanlan.zhihu.com/p/33397147)).\n\nDCNN\n\n*   \n*   (invariance)\n\nDCNNDeepLab`atrous`()\n\nDCNNDeepLab(DenseCRF)\n\n CRFs  tricks  Multi-Scale features U-Net  FPN  MLP 128  33  128  11  5128=640  CRF \n\n**CRF->**\n\n$i$$x_{i}$(21$(i \\in 1,2,..,21)$  $y_{i}$()(CRF)$y_{i}$$i$$x_{i}$.:\n<center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Image-Segmentation-Learning/9.png)</center>\n\n(xI):\n$$p(x|I)=\\frac{1}{Z}exp(-E(x|I))$$\n\nCRF$E(x)$:\n$$E(x)=\\sum_{i} \\theta_{i}(x_{i})+\\sum_{ij}\\theta_{ij}(x_{i},x_{j})$$\n\n$\\theta_{i}(x_{i})$$\\theta_{ij}(x_{i},x_{j})$\n\n* i$P(x_{i})$DCNNi.\n\n$$\\theta_{i}(x_{i})=-logP(x_{i})$$\n\n* \n\n$$\\theta_{ij}(x_{i},x_{j})=\\mu(x_{i},x_{j})\\sum_{m=1}^{K}w_{m}k^{m}(f_{i},f_{j})$$\n\nDeepLab(p)(I),(p):\n$$w_{1}exp(-\\frac{\\lVert p_{i}-p_{j} \\rVert^{2}}{2\\sigma_{\\alpha}^{2}}-\\frac{\\lVert I_{i}-I_{j} \\rVert^{2}}{2\\sigma_{\\beta}^{2}})+w_{2}exp(-\\frac{\\lVert p_{i}-p_{j} \\rVert^{2}}{2\\sigma_{\\gamma}^{2}})$$\n\n****\n\n|  |  |\n| --- | --- |\n|  | PASCAL VOC 2012 segmentation benchmark |\n| DCNN | VGG16 |\n| DCNN |  |\n|  | SGDbatch=20 |\n|  | 0.0010.0120000.1 |\n|  | 0.9 0.0005 |\n\n---\n\n## DepLab V2\n[](https://arxiv.org/pdf/1606.00915.pdf)\n\nDeepLabv2  DeepLabv1 DeepLabv1 \n1. \n2. \n3. DCNN \n\n, DeepLabv23:\n1. DCNN\n2. (atrous spatial pyramid pooling (ASPP))ASPP\n3. DCNNDCNNDCNNCRF\n\n****\n<center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Image-Segmentation-Learning/10.png)</center>\n\n*   DCNN(ASPP)`Aeroplane Coarse Score map`\n*   `Bi-linear Interpolation`\n*   CRF`Final Output`\n\n****\n1. ****\n  \n  $y[i]$, $x[i]$, K$w[k]$, \n  $$y[k]=\\sum_{k=1}^{K}x[i+r\\cdot k]w[k]$$\n  r, $r=1$(a); (b)$r=2$\n  <center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Image-Segmentation-Learning/11.png)</center>\n  \n  (),\n  * 21/4\n  * (2)\n  <center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Image-Segmentation-Learning/12.png)</center>\n  \n  r$r-1$$k\\times k$$k_{e}=k+(k-1)(r-1)$DCNN()DCNN\n2. **ASPP**\n\n  DeepLabv2SPPNetAtrous Spatial Pyramid Pooling (ASPP)\n  <center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Image-Segmentation-Learning/13.png)</center>\n  \n  Input Feature Map4$r=6,12,18,24$, $3 \\times 3$. .\n3. **CRF**\n\n  <center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Image-Segmentation-Learning/14.png)</center>\n  \n****\n\n|  |  |\n| --- | --- |\n| DCNN | VGG16**ResNet101** |\n| DCNN | ground truth8 |\n|  | SGDbatch=20 |\n|  | 0.0010.0120000.1 |\n|  | 0.9 0.0005 |\n\n---\n\n## DepLab V3\n[](https://arxiv.org/pdf/1706.05587.pdf)\n\n\n\n*   DCNN\n*   \n\nDeepLabv3\n\n*   \n*   ASPPBN\n*   11, ASPP\n*   DeepLabv3\n\n****\n1. ****\n2. ****\n\n  <center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Image-Segmentation-Learning/15.png)</center>\n  \n  <center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Image-Segmentation-Learning/18.png)</center>\n\n  , ResNetblockblock4(a)out_stride = 8 (b)out_stride = 16.\n  \n3. **Atrous Spatial Pyramid Pooling**\n\n  DeepLabv2ASPPDeepLabv3ASPPBN(0)\n  <center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Image-Segmentation-Learning/16.png)</center>\n  \n  $3 \\times 3$$65 \\times 65$$3 \\times 3$$1 \\times 1$\n  \n  <center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Image-Segmentation-Learning/17.png)</center>\n  ASPP\n  1. $1 \\times 1$$3 \\times 3$$rate = (6,12,18)$256BNoutput_stride=16output_stride=8$1 \\times 1$\n  2. $1 \\times 1$\n\n****\n\n|  |  |\n| --- | --- |\n|  | PASCAL VOC 2012 |\n|  | TensorFlow |\n|  | 513 |\n|  | poly $learning rate = base\\_lr(1-\\frac{iter}{max\\_iters})^{power}$ |\n| **BN** | output_stride=16batchsize=16BN0.99970.00730KBNoutput_stride=80.00130Koutput_stride=16output_stride=8output_stride=16 |\n| **** | ,GroundTruth8GroundTruth8GroundTruth |\n\n---\n\n## DepLab-V$3^{+}$\n[](https://arxiv.org/pdf/1802.02611.pdf)\n\npooling or convolutions with stridefeature. poolingfeaturefeature\n\n****\n<center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Image-Segmentation-Learning/19.png)</center>\n\nDeepLabV$3^{+}$encoder-decoderDeepLab V3decoder(c)4encoderconcatenate4XceptionencoderAtrous Spatial Pyramid Poolingdecoderdepth-wise separable convolution\n\n1. **Encoder**\n\n  * ResNet: encoderDeepLab V3ResNet101()blockstrideoutput stride8(16)block4Atrous Spatial Pyramid Poolingconcatenate11256\n  * **Xecption**: XceptionMSRA teamXceptionAligned Xception\n\n    * Aligned Xceptionentry flow network\n    * max poolingstrideseparable convolutionatrous separable convolution\n    * 33depath-wise convolutionBNReLU\n<center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Image-Segmentation-Learning/21.png)</center>\n\n2. **Decoder**\n\n  * decoder4encoderconcatenateconcatenate(256512)encoder256concatenate11concatenate334\n  *  \n    * $1 \\times 1$48\n    * 33233\n    * encoderConv2\n\n<center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Image-Segmentation-Learning/20.png)</center>\n\n****\n\n* **Aligned Xception**\n  <center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Image-Segmentation-Learning/22.png)</center>\n  \n  train_stride=16eval_stride=8mIOU84.56% train_strideeval_stride161.53%60","source":"_posts/MalongTech-Learning-Image-Segmentation.md","raw":"---\ntitle: MalongTech :-2D\ndate: 2018-11-26 11:00:00\ntags: [Deep Learning]\ncategories: \n---\n\n# Image Segmentation\n\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Image-Segmentation-Learning/1.jpg)\n\n## Introduction\nSegmentation<!-- more -->\n\n\n\n****\n* 1. 2. 3. 4. 5. 6. \n* \n* \n* \n* \n\n## Tranditional Methods\n[](https://zhuanlan.zhihu.com/p/30732385)\n* ****: 1. 2. 3. .\n* ****: 1. Canny2. Harris3. SIFT3. SURF.\n* ****: 1. 2. 3. .\n* ****: 1. GraphCut; 2. GrabCut; 3. Random Walk.\n* ****: 1. Snake2. Active Shape Model; 3. Active Apperance Models; 4. Constrained local model.\n\n## Datasets\n1. Pascal VOC: 206929\n2. CityScapes30500020000\n3. MS COCO803320150\n4. ImageNet: [DeepLesion](https://www.52cv.net/?p=883), 1000032000\n\n## \n$k+1$$L_{o}$$L_{k}$,, $P_{ij}$**i****j**$p_{ii}$****$p_{ij}$$p_{ji}$\n1. Pixel Accuracy(PA, ): \n$$PA=\\frac{\\sum_{i=0}^{k}p_{ii}}{\\sum_{i=0}^{k}\\sum_{j=0}^{k}p_{ij}}$$\n2. Mean Pixel Accuracy(MAP,): PA\n$$MAP=\\frac{1}{k+1}\\sum_{i=0}^{k}\\frac{p_{ii}}{\\sum_{j=0}^{k}p_{ij}}$$\n3. Mean Intersection over Union(MIoU, ): (ground truth)(predicted segmentation). (intersection)()IoU\n$$MIoU = \\frac{1}{k+1}\\sum_{i=0}^{k}\\frac{p_{ii}}{\\sum_{j=0}^{k}p_{ij}+\\sum_{j=0}^{k}(p_{ji}-p_{ii})}$$\n4. Frequency Weighted Intersection over Union(FWIoU, ): MIoU\n$$FWIoU = \\frac{1}{\\sum_{i=0}^{k}\\sum_{j=0}^{k}p_{ij}}\\sum_{i=0}^{k}\\frac{p_{ii}}{\\sum_{j=0}^{k}p_{ij}+\\sum_{j=0}^{k}(p_{ji}-p_{ii})}$$\n\n# \n## Fully Convolutional Networks\n[](https://people.eecs.berkeley.edu/~jonlong/long_shelhamer_fcn.pdf)\n\n\n FCN \n<center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Image-Segmentation-Learning/2.png)</center>\n\n, AlexNet, 1000, , tabby catFCNsemantic segmentationFCNfeature map, , , softmax, \n\n**->**\n\nK=409677512F=7,P=0,S=1,K=4096.\n\n224x224x37x7x512AlexNet409610003\n* [7x7x512]F=7[1x1x4096]\n* F=1[1x1x4096]\n* F=1[1x1x1000]\n\n**end to end, pixels to pixels network**\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Image-Segmentation-Learning/3.png)\npooling$\\frac{H}{32} \\times \\frac{W}{32}$heatmapheatmapupsampling1000heatmapupsamplinglabeltrick1000\n\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Image-Segmentation-Learning/4.png)\n\n1/32heatMap1/16featureMap1/8featureMap1/32heatMapupsamplingconv5conv4upsamplingconv3upsampling\n\n****\n* 832\n* spatial regularization\n\n****\nupsampling****\n* \n\n<center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Image-Segmentation-Learning/5.png)</center>\n$\\frac{y-y_{0}}{y_{1}-y_{0}}=\\frac{x-x_{0}}{x_{1}-x_{0}}$ $\\alpha$$\\alpha =\\frac{y-y_{0}}{y_{1}-y_{0}}=\\frac{x-x_{0}}{x_{1}-x_{0}}$. y: $y=(1-\\alpha)y_{0}+\\alpha y_{1} = (1-\\frac{x-x_{0}}{x_{1}-x_{0}})y_{0}+\\frac{x-x_{0}}{x_{1}-x_{0}}y_{1}=\\frac{x_{1}-x}{x_{1}-x_{0}}y_{0}+\\frac{x-x_{0}}{x_{1}-x_{0}}y_{1}=\\frac{x_{1}-x}{x_{1}-x_{0}}f(x_{1})+\\frac{x-x_{0}}{x_{1}-x_{0}}f(x_{0})$\n\n* \n\n<center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Image-Segmentation-Learning/6.png)</center>\n\nX$Q_{12}$,$Q_{22}$$R_{2}$$Q_{11}$$Q_{21}$$R_{1}$\n\n$f(R_{1}=\\frac{x_{2}-x}{x_{2}-x_{1}}f(Q_{11})+\\frac{x-x_{1}}{x_{2}-x_{1}}f(Q_{21})$; $f(R_{2}=\\frac{x_{2}-x}{x_{2}-x_{1}}f(Q_{12})+\\frac{x-x_{1}}{x_{2}-x_{1}}f(Q_{22})$\n\nY$R_{1$}$R_{2}$yP\n\n$f(P)=\\frac{y_{2}-y}{y_{2}-y_{1}}f(R_{1})+\\frac{y-y_{1}}{y_{2}-y_{1}}f(R_{2})$\n\n---\n\n## U-Net: Convolutional Networks for Biomedical Image Segmentation\n[](https://arxiv.org/pdf/1505.04597.pdf)\nCiresanpatch  patches\n\n\n\npatchpatch,; patchesmax-pooling,patches\n\n**U-Net Architecture**\n<center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Image-Segmentation-Learning/7.jpg)</center>\n\n1. ()\n2. maxpooling\n3. :concatenatepooling\n4. 1X1heatmap,heatmap,softmaxsoftmax\n\n**Overlap-tile strategy**\n<center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Image-Segmentation-Learning/8.png)</center>\n\npatchpatchUnetoverlapoverlapoverlappatch \n\n****\n\nsoftmax\n$$E=\\sum_{x\\in \\Omega}w(x)log(p_{\\ell(x)}(x))$$\n\n$w(x)$:\n$$w(x)=w_{c}(x)+w_{0} \\times exp(-\\frac{(d_{1}(x)+d_{2}(x))}{2\\sigma^{2}})$$\n\n**Data Augmentation**\n3\\*3(smooth deformations) 10(Bicubic interpolation)contracting pathdrop-out \n\n****\nf(x,y)16\n\n---\n\n## DeepLab V1\n[](https://arxiv.org/pdf/1412.7062v3.pdf)\n\nDeepLab([DCNNs](https://www.cnblogs.com/wangxiaocvpr/p/8763510.html))([DenseCRFs](https://zhuanlan.zhihu.com/p/33397147)).\n\nDCNN\n\n*   \n*   (invariance)\n\nDCNNDeepLab`atrous`()\n\nDCNNDeepLab(DenseCRF)\n\n CRFs  tricks  Multi-Scale features U-Net  FPN  MLP 128  33  128  11  5128=640  CRF \n\n**CRF->**\n\n$i$$x_{i}$(21$(i \\in 1,2,..,21)$  $y_{i}$()(CRF)$y_{i}$$i$$x_{i}$.:\n<center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Image-Segmentation-Learning/9.png)</center>\n\n(xI):\n$$p(x|I)=\\frac{1}{Z}exp(-E(x|I))$$\n\nCRF$E(x)$:\n$$E(x)=\\sum_{i} \\theta_{i}(x_{i})+\\sum_{ij}\\theta_{ij}(x_{i},x_{j})$$\n\n$\\theta_{i}(x_{i})$$\\theta_{ij}(x_{i},x_{j})$\n\n* i$P(x_{i})$DCNNi.\n\n$$\\theta_{i}(x_{i})=-logP(x_{i})$$\n\n* \n\n$$\\theta_{ij}(x_{i},x_{j})=\\mu(x_{i},x_{j})\\sum_{m=1}^{K}w_{m}k^{m}(f_{i},f_{j})$$\n\nDeepLab(p)(I),(p):\n$$w_{1}exp(-\\frac{\\lVert p_{i}-p_{j} \\rVert^{2}}{2\\sigma_{\\alpha}^{2}}-\\frac{\\lVert I_{i}-I_{j} \\rVert^{2}}{2\\sigma_{\\beta}^{2}})+w_{2}exp(-\\frac{\\lVert p_{i}-p_{j} \\rVert^{2}}{2\\sigma_{\\gamma}^{2}})$$\n\n****\n\n|  |  |\n| --- | --- |\n|  | PASCAL VOC 2012 segmentation benchmark |\n| DCNN | VGG16 |\n| DCNN |  |\n|  | SGDbatch=20 |\n|  | 0.0010.0120000.1 |\n|  | 0.9 0.0005 |\n\n---\n\n## DepLab V2\n[](https://arxiv.org/pdf/1606.00915.pdf)\n\nDeepLabv2  DeepLabv1 DeepLabv1 \n1. \n2. \n3. DCNN \n\n, DeepLabv23:\n1. DCNN\n2. (atrous spatial pyramid pooling (ASPP))ASPP\n3. DCNNDCNNDCNNCRF\n\n****\n<center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Image-Segmentation-Learning/10.png)</center>\n\n*   DCNN(ASPP)`Aeroplane Coarse Score map`\n*   `Bi-linear Interpolation`\n*   CRF`Final Output`\n\n****\n1. ****\n  \n  $y[i]$, $x[i]$, K$w[k]$, \n  $$y[k]=\\sum_{k=1}^{K}x[i+r\\cdot k]w[k]$$\n  r, $r=1$(a); (b)$r=2$\n  <center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Image-Segmentation-Learning/11.png)</center>\n  \n  (),\n  * 21/4\n  * (2)\n  <center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Image-Segmentation-Learning/12.png)</center>\n  \n  r$r-1$$k\\times k$$k_{e}=k+(k-1)(r-1)$DCNN()DCNN\n2. **ASPP**\n\n  DeepLabv2SPPNetAtrous Spatial Pyramid Pooling (ASPP)\n  <center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Image-Segmentation-Learning/13.png)</center>\n  \n  Input Feature Map4$r=6,12,18,24$, $3 \\times 3$. .\n3. **CRF**\n\n  <center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Image-Segmentation-Learning/14.png)</center>\n  \n****\n\n|  |  |\n| --- | --- |\n| DCNN | VGG16**ResNet101** |\n| DCNN | ground truth8 |\n|  | SGDbatch=20 |\n|  | 0.0010.0120000.1 |\n|  | 0.9 0.0005 |\n\n---\n\n## DepLab V3\n[](https://arxiv.org/pdf/1706.05587.pdf)\n\n\n\n*   DCNN\n*   \n\nDeepLabv3\n\n*   \n*   ASPPBN\n*   11, ASPP\n*   DeepLabv3\n\n****\n1. ****\n2. ****\n\n  <center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Image-Segmentation-Learning/15.png)</center>\n  \n  <center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Image-Segmentation-Learning/18.png)</center>\n\n  , ResNetblockblock4(a)out_stride = 8 (b)out_stride = 16.\n  \n3. **Atrous Spatial Pyramid Pooling**\n\n  DeepLabv2ASPPDeepLabv3ASPPBN(0)\n  <center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Image-Segmentation-Learning/16.png)</center>\n  \n  $3 \\times 3$$65 \\times 65$$3 \\times 3$$1 \\times 1$\n  \n  <center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Image-Segmentation-Learning/17.png)</center>\n  ASPP\n  1. $1 \\times 1$$3 \\times 3$$rate = (6,12,18)$256BNoutput_stride=16output_stride=8$1 \\times 1$\n  2. $1 \\times 1$\n\n****\n\n|  |  |\n| --- | --- |\n|  | PASCAL VOC 2012 |\n|  | TensorFlow |\n|  | 513 |\n|  | poly $learning rate = base\\_lr(1-\\frac{iter}{max\\_iters})^{power}$ |\n| **BN** | output_stride=16batchsize=16BN0.99970.00730KBNoutput_stride=80.00130Koutput_stride=16output_stride=8output_stride=16 |\n| **** | ,GroundTruth8GroundTruth8GroundTruth |\n\n---\n\n## DepLab-V$3^{+}$\n[](https://arxiv.org/pdf/1802.02611.pdf)\n\npooling or convolutions with stridefeature. poolingfeaturefeature\n\n****\n<center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Image-Segmentation-Learning/19.png)</center>\n\nDeepLabV$3^{+}$encoder-decoderDeepLab V3decoder(c)4encoderconcatenate4XceptionencoderAtrous Spatial Pyramid Poolingdecoderdepth-wise separable convolution\n\n1. **Encoder**\n\n  * ResNet: encoderDeepLab V3ResNet101()blockstrideoutput stride8(16)block4Atrous Spatial Pyramid Poolingconcatenate11256\n  * **Xecption**: XceptionMSRA teamXceptionAligned Xception\n\n    * Aligned Xceptionentry flow network\n    * max poolingstrideseparable convolutionatrous separable convolution\n    * 33depath-wise convolutionBNReLU\n<center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Image-Segmentation-Learning/21.png)</center>\n\n2. **Decoder**\n\n  * decoder4encoderconcatenateconcatenate(256512)encoder256concatenate11concatenate334\n  *  \n    * $1 \\times 1$48\n    * 33233\n    * encoderConv2\n\n<center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Image-Segmentation-Learning/20.png)</center>\n\n****\n\n* **Aligned Xception**\n  <center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Image-Segmentation-Learning/22.png)</center>\n  \n  train_stride=16eval_stride=8mIOU84.56% train_strideeval_stride161.53%60","slug":"MalongTech-Learning-Image-Segmentation","published":1,"updated":"2020-04-28T12:38:55.395Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck9jwdng7000ajlrckzg7vut2","content":"<h1 id=\"Image-Segmentation\"><a href=\"#Image-Segmentation\" class=\"headerlink\" title=\"Image Segmentation\"></a>Image Segmentation</h1><p><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Image-Segmentation-Learning/1.jpg\" alt=\"image\"></p>\n<h2 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h2><p>Segmentation<a id=\"more\"></a></p>\n<p></p>\n<p><strong></strong></p>\n<ul>\n<li>1. 2. 3. 4. 5. 6. </li>\n<li></li>\n<li></li>\n<li></li>\n<li></li>\n</ul>\n<h2 id=\"Tranditional-Methods\"><a href=\"#Tranditional-Methods\" class=\"headerlink\" title=\"Tranditional Methods\"></a>Tranditional Methods</h2><p><a href=\"https://zhuanlan.zhihu.com/p/30732385\" target=\"_blank\" rel=\"noopener\"></a></p>\n<ul>\n<li><strong></strong>: 1. 2. 3. .</li>\n<li><strong></strong>: 1. Canny2. Harris3. SIFT3. SURF.</li>\n<li><strong></strong>: 1. 2. 3. .</li>\n<li><strong></strong>: 1. GraphCut; 2. GrabCut; 3. Random Walk.</li>\n<li><strong></strong>: 1. Snake2. Active Shape Model; 3. Active Apperance Models; 4. Constrained local model.</li>\n</ul>\n<h2 id=\"Datasets\"><a href=\"#Datasets\" class=\"headerlink\" title=\"Datasets\"></a>Datasets</h2><ol>\n<li>Pascal VOC: 206929</li>\n<li>CityScapes30500020000</li>\n<li>MS COCO803320150</li>\n<li>ImageNet: <a href=\"https://www.52cv.net/?p=883\" target=\"_blank\" rel=\"noopener\">DeepLesion</a>, 1000032000</li>\n</ol>\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h2><p>$k+1$$L_{o}$$L_{k}$,, $P_{ij}$<strong>i</strong><strong>j</strong>$p_{ii}$<strong></strong>$p_{ij}$$p_{ji}$</p>\n<ol>\n<li>Pixel Accuracy(PA, ): <br>$$PA=\\frac{\\sum_{i=0}^{k}p_{ii}}{\\sum_{i=0}^{k}\\sum_{j=0}^{k}p_{ij}}$$</li>\n<li>Mean Pixel Accuracy(MAP,): PA<br>$$MAP=\\frac{1}{k+1}\\sum_{i=0}^{k}\\frac{p_{ii}}{\\sum_{j=0}^{k}p_{ij}}$$</li>\n<li>Mean Intersection over Union(MIoU, ): (ground truth)(predicted segmentation). (intersection)()IoU<br>$$MIoU = \\frac{1}{k+1}\\sum_{i=0}^{k}\\frac{p_{ii}}{\\sum_{j=0}^{k}p_{ij}+\\sum_{j=0}^{k}(p_{ji}-p_{ii})}$$</li>\n<li>Frequency Weighted Intersection over Union(FWIoU, ): MIoU<br>$$FWIoU = \\frac{1}{\\sum_{i=0}^{k}\\sum_{j=0}^{k}p_{ij}}\\sum_{i=0}^{k}\\frac{p_{ii}}{\\sum_{j=0}^{k}p_{ij}+\\sum_{j=0}^{k}(p_{ji}-p_{ii})}$$</li>\n</ol>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><h2 id=\"Fully-Convolutional-Networks\"><a href=\"#Fully-Convolutional-Networks\" class=\"headerlink\" title=\"Fully Convolutional Networks\"></a>Fully Convolutional Networks</h2><p><a href=\"https://people.eecs.berkeley.edu/~jonlong/long_shelhamer_fcn.pdf\" target=\"_blank\" rel=\"noopener\"></a><br></p>\n<p> FCN </p>\n<center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Image-Segmentation-Learning/2.png\" alt=\"image\"></center>\n\n<p>, AlexNet, 1000, , tabby catFCNsemantic segmentationFCNfeature map, , , softmax, </p>\n<p><strong>-&gt;</strong></p>\n<p> K=4096  77512 F=7,P=0,S=1,K=4096 .</p>\n<p> 224x224x3  7x7x512 AlexNet409610003</p>\n<ul>\n<li>[7x7x512]F=7[1x1x4096]</li>\n<li>F=1[1x1x4096]</li>\n<li>F=1[1x1x1000]</li>\n</ul>\n<p><strong>end to end, pixels to pixels network</strong><br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Image-Segmentation-Learning/3.png\" alt=\"image\"><br>pooling$\\frac{H}{32} \\times \\frac{W}{32}$heatmapheatmapupsampling1000heatmapupsamplinglabeltrick1000</p>\n<p><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Image-Segmentation-Learning/4.png\" alt=\"image\"></p>\n<p>1/32heatMap1/16featureMap1/8featureMap1/32heatMapupsamplingconv5conv4upsamplingconv3upsampling</p>\n<p><strong></strong></p>\n<ul>\n<li>832</li>\n<li>spatial regularization</li>\n</ul>\n<p><strong></strong><br>upsampling<strong></strong></p>\n<ul>\n<li><p><br><br><center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Image-Segmentation-Learning/5.png\" alt=\"image\"></center><br>$\\frac{y-y_{0}}{y_{1}-y_{0}}=\\frac{x-x_{0}}{x_{1}-x_{0}}$ $\\alpha$$\\alpha =\\frac{y-y_{0}}{y_{1}-y_{0}}=\\frac{x-x_{0}}{x_{1}-x_{0}}$. y: $y=(1-\\alpha)y_{0}+\\alpha y_{1} = (1-\\frac{x-x_{0}}{x_{1}-x_{0}})y_{0}+\\frac{x-x_{0}}{x_{1}-x_{0}}y_{1}=\\frac{x_{1}-x}{x_{1}-x_{0}}y_{0}+\\frac{x-x_{0}}{x_{1}-x_{0}}y_{1}=\\frac{x_{1}-x}{x_{1}-x_{0}}f(x_{1})+\\frac{x-x_{0}}{x_{1}-x_{0}}f(x_{0})$</p>\n</li>\n<li><p><br></p>\n<center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Image-Segmentation-Learning/6.png\" alt=\"image\"></center>\n\n</li>\n</ul>\n<p>X$Q_{12}$,$Q_{22}$$R_{2}$$Q_{11}$$Q_{21}$$R_{1}$</p>\n<p>$f(R_{1}=\\frac{x_{2}-x}{x_{2}-x_{1}}f(Q_{11})+\\frac{x-x_{1}}{x_{2}-x_{1}}f(Q_{21})$; $f(R_{2}=\\frac{x_{2}-x}{x_{2}-x_{1}}f(Q_{12})+\\frac{x-x_{1}}{x_{2}-x_{1}}f(Q_{22})$</p>\n<p>Y$R_{1$}$R_{2}$yP</p>\n<p>$f(P)=\\frac{y_{2}-y}{y_{2}-y_{1}}f(R_{1})+\\frac{y-y_{1}}{y_{2}-y_{1}}f(R_{2})$</p>\n<hr>\n<h2 id=\"U-Net-Convolutional-Networks-for-Biomedical-Image-Segmentation\"><a href=\"#U-Net-Convolutional-Networks-for-Biomedical-Image-Segmentation\" class=\"headerlink\" title=\"U-Net: Convolutional Networks for Biomedical Image Segmentation\"></a>U-Net: Convolutional Networks for Biomedical Image Segmentation</h2><p><a href=\"https://arxiv.org/pdf/1505.04597.pdf\" target=\"_blank\" rel=\"noopener\"></a><br>Ciresanpatch  patches</p>\n<p></p>\n<p>patchpatch,; patchesmax-pooling,patches</p>\n<p><strong>U-Net Architecture</strong></p>\n<center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Image-Segmentation-Learning/7.jpg\" alt=\"image\"></center>\n\n<ol>\n<li>()</li>\n<li>maxpooling</li>\n<li>:concatenatepooling</li>\n<li>1X1heatmap,heatmap,softmaxsoftmax</li>\n</ol>\n<p><strong>Overlap-tile strategy</strong></p>\n<center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Image-Segmentation-Learning/8.png\" alt=\"image\"></center>\n\n<p>patchpatchUnetoverlapoverlapoverlappatch </p>\n<p><strong></strong></p>\n<p>softmax<br>$$E=\\sum_{x\\in \\Omega}w(x)log(p_{\\ell(x)}(x))$$</p>\n<p>$w(x)$:<br>$$w(x)=w_{c}(x)+w_{0} \\times exp(-\\frac{(d_{1}(x)+d_{2}(x))}{2\\sigma^{2}})$$</p>\n<p><strong>Data Augmentation</strong><br>3*3(smooth deformations) 10(Bicubic interpolation)contracting pathdrop-out </p>\n<p><strong></strong><br>f(x,y)16</p>\n<hr>\n<h2 id=\"DeepLab-V1\"><a href=\"#DeepLab-V1\" class=\"headerlink\" title=\"DeepLab V1\"></a>DeepLab V1</h2><p><a href=\"https://arxiv.org/pdf/1412.7062v3.pdf\" target=\"_blank\" rel=\"noopener\"></a></p>\n<p>DeepLab(<a href=\"https://www.cnblogs.com/wangxiaocvpr/p/8763510.html\" target=\"_blank\" rel=\"noopener\">DCNNs</a>)(<a href=\"https://zhuanlan.zhihu.com/p/33397147\" target=\"_blank\" rel=\"noopener\">DenseCRFs</a>).</p>\n<p>DCNN</p>\n<ul>\n<li></li>\n<li>(invariance)</li>\n</ul>\n<p>DCNNDeepLab<code>atrous</code>()</p>\n<p>DCNNDeepLab(DenseCRF)</p>\n<p> CRFs  tricks  Multi-Scale features U-Net  FPN  MLP 128  33  128  11  5128=640  CRF </p>\n<p><strong>CRF-&gt;</strong></p>\n<p>$i$$x_{i}$(21$(i \\in 1,2,..,21)$  $y_{i}$()(CRF)$y_{i}$$i$$x_{i}$.:</p>\n<center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Image-Segmentation-Learning/9.png\" alt=\"image\"></center>\n\n<p>(xI):<br>$$p(x|I)=\\frac{1}{Z}exp(-E(x|I))$$</p>\n<p>CRF$E(x)$:<br>$$E(x)=\\sum_{i} \\theta_{i}(x_{i})+\\sum_{ij}\\theta_{ij}(x_{i},x_{j})$$</p>\n<p>$\\theta_{i}(x_{i})$$\\theta_{ij}(x_{i},x_{j})$</p>\n<ul>\n<li>i$P(x_{i})$DCNNi.</li>\n</ul>\n<p>$$\\theta_{i}(x_{i})=-logP(x_{i})$$</p>\n<ul>\n<li></li>\n</ul>\n<p>$$\\theta_{ij}(x_{i},x_{j})=\\mu(x_{i},x_{j})\\sum_{m=1}^{K}w_{m}k^{m}(f_{i},f_{j})$$</p>\n<p>DeepLab(p)(I),(p):<br>$$w_{1}exp(-\\frac{\\lVert p_{i}-p_{j} \\rVert^{2}}{2\\sigma_{\\alpha}^{2}}-\\frac{\\lVert I_{i}-I_{j} \\rVert^{2}}{2\\sigma_{\\beta}^{2}})+w_{2}exp(-\\frac{\\lVert p_{i}-p_{j} \\rVert^{2}}{2\\sigma_{\\gamma}^{2}})$$</p>\n<p><strong></strong></p>\n<table>\n<thead>\n<tr>\n<th></th>\n<th></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td></td>\n<td>PASCAL VOC 2012 segmentation benchmark</td>\n</tr>\n<tr>\n<td>DCNN</td>\n<td>VGG16</td>\n</tr>\n<tr>\n<td>DCNN</td>\n<td></td>\n</tr>\n<tr>\n<td></td>\n<td>SGDbatch=20</td>\n</tr>\n<tr>\n<td></td>\n<td>0.0010.0120000.1</td>\n</tr>\n<tr>\n<td></td>\n<td>0.9 0.0005</td>\n</tr>\n</tbody>\n</table>\n<hr>\n<h2 id=\"DepLab-V2\"><a href=\"#DepLab-V2\" class=\"headerlink\" title=\"DepLab V2\"></a>DepLab V2</h2><p><a href=\"https://arxiv.org/pdf/1606.00915.pdf\" target=\"_blank\" rel=\"noopener\"></a></p>\n<p>DeepLabv2  DeepLabv1 DeepLabv1 </p>\n<ol>\n<li></li>\n<li></li>\n<li>DCNN </li>\n</ol>\n<p>, DeepLabv23:</p>\n<ol>\n<li>DCNN</li>\n<li>(atrous spatial pyramid pooling (ASPP))ASPP</li>\n<li>DCNNDCNNDCNNCRF</li>\n</ol>\n<p><strong></strong></p>\n<center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Image-Segmentation-Learning/10.png\" alt=\"image\"></center>\n\n<ul>\n<li>DCNN(ASPP)<code>Aeroplane Coarse Score map</code></li>\n<li><code>Bi-linear Interpolation</code></li>\n<li>CRF<code>Final Output</code></li>\n</ul>\n<p><strong></strong></p>\n<ol>\n<li><p><strong></strong></p>\n<p>$y[i]$, $x[i]$, K$w[k]$, <br>$$y[k]=\\sum_{k=1}^{K}x[i+r\\cdot k]w[k]$$<br>r, $r=1$(a); (b)$r=2$</p>\n<center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Image-Segmentation-Learning/11.png\" alt=\"image\"></center>\n\n<p>(),</p>\n<ul>\n<li>21/4</li>\n<li>(2)<center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Image-Segmentation-Learning/12.png\" alt=\"image\"></center>\n\n</li>\n</ul>\n<p>r$r-1$$k\\times k$$k_{e}=k+(k-1)(r-1)$DCNN()DCNN</p>\n</li>\n<li><p><strong>ASPP</strong></p>\n<p>DeepLabv2SPPNetAtrous Spatial Pyramid Pooling (ASPP)</p>\n<center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Image-Segmentation-Learning/13.png\" alt=\"image\"></center>\n\n<p>Input Feature Map4$r=6,12,18,24$, $3 \\times 3$. .</p>\n</li>\n<li><p><strong>CRF</strong></p>\n<center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Image-Segmentation-Learning/14.png\" alt=\"image\"></center>\n\n</li>\n</ol>\n<p><strong></strong></p>\n<table>\n<thead>\n<tr>\n<th></th>\n<th></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>DCNN</td>\n<td>VGG16<strong>ResNet101</strong></td>\n</tr>\n<tr>\n<td>DCNN</td>\n<td>ground truth8</td>\n</tr>\n<tr>\n<td></td>\n<td>SGDbatch=20</td>\n</tr>\n<tr>\n<td></td>\n<td>0.0010.0120000.1</td>\n</tr>\n<tr>\n<td></td>\n<td>0.9 0.0005</td>\n</tr>\n</tbody>\n</table>\n<hr>\n<h2 id=\"DepLab-V3\"><a href=\"#DepLab-V3\" class=\"headerlink\" title=\"DepLab V3\"></a>DepLab V3</h2><p><a href=\"https://arxiv.org/pdf/1706.05587.pdf\" target=\"_blank\" rel=\"noopener\"></a></p>\n<p></p>\n<ul>\n<li>DCNN</li>\n<li></li>\n</ul>\n<p>DeepLabv3</p>\n<ul>\n<li></li>\n<li>ASPPBN</li>\n<li>11, ASPP</li>\n<li>DeepLabv3</li>\n</ul>\n<p><strong></strong></p>\n<ol>\n<li><strong></strong></li>\n<li><p><strong></strong></p>\n<center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Image-Segmentation-Learning/15.png\" alt=\"image\"></center>\n\n<center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Image-Segmentation-Learning/18.png\" alt=\"image\"></center>\n\n<p>, ResNetblockblock4(a)out_stride = 8 (b)out_stride = 16.</p>\n</li>\n<li><p><strong>Atrous Spatial Pyramid Pooling</strong></p>\n<p>DeepLabv2ASPPDeepLabv3ASPPBN(0)</p>\n<center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Image-Segmentation-Learning/16.png\" alt=\"image\"></center>\n\n<p>$3 \\times 3$$65 \\times 65$$3 \\times 3$$1 \\times 1$</p>\n<p><center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Image-Segmentation-Learning/17.png\" alt=\"image\"></center><br>ASPP</p>\n<ol>\n<li>$1 \\times 1$$3 \\times 3$$rate = (6,12,18)$256BNoutput_stride=16output_stride=8$1 \\times 1$</li>\n<li>$1 \\times 1$</li>\n</ol>\n</li>\n</ol>\n<p><strong></strong></p>\n<table>\n<thead>\n<tr>\n<th></th>\n<th></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td></td>\n<td>PASCAL VOC 2012</td>\n</tr>\n<tr>\n<td></td>\n<td>TensorFlow</td>\n</tr>\n<tr>\n<td></td>\n<td>513</td>\n</tr>\n<tr>\n<td></td>\n<td>poly $learning rate = base_lr(1-\\frac{iter}{max_iters})^{power}$</td>\n</tr>\n<tr>\n<td><strong>BN</strong></td>\n<td>output_stride=16batchsize=16BN0.99970.00730KBNoutput_stride=80.00130Koutput_stride=16output_stride=8output_stride=16</td>\n</tr>\n<tr>\n<td><strong></strong></td>\n<td>,GroundTruth8GroundTruth8GroundTruth</td>\n</tr>\n</tbody>\n</table>\n<hr>\n<h2 id=\"DepLab-V-3\"><a href=\"#DepLab-V-3\" class=\"headerlink\" title=\"DepLab-V$3^{+}$\"></a>DepLab-V$3^{+}$</h2><p><a href=\"https://arxiv.org/pdf/1802.02611.pdf\" target=\"_blank\" rel=\"noopener\"></a></p>\n<p>pooling or convolutions with stridefeature. poolingfeaturefeature</p>\n<p><strong></strong></p>\n<center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Image-Segmentation-Learning/19.png\" alt=\"image\"></center>\n\n<p>DeepLabV$3^{+}$encoder-decoderDeepLab V3decoder(c)4encoderconcatenate4XceptionencoderAtrous Spatial Pyramid Poolingdecoderdepth-wise separable convolution</p>\n<ol>\n<li><p><strong>Encoder</strong></p>\n<ul>\n<li>ResNet: encoderDeepLab V3ResNet101()blockstrideoutput stride8(16)block4Atrous Spatial Pyramid Poolingconcatenate11256</li>\n<li><p><strong>Xecption</strong>: XceptionMSRA teamXceptionAligned Xception</p>\n<ul>\n<li>Aligned Xceptionentry flow network</li>\n<li>max poolingstrideseparable convolutionatrous separable convolution</li>\n<li>33depath-wise convolutionBNReLU<center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Image-Segmentation-Learning/21.png\" alt=\"image\"></center>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n<li><p><strong>Decoder</strong></p>\n<ul>\n<li>decoder4encoderconcatenateconcatenate(256512)encoder256concatenate11concatenate334</li>\n<li> <ul>\n<li>$1 \\times 1$48</li>\n<li>33233</li>\n<li>encoderConv2</li>\n</ul>\n</li>\n</ul>\n</li>\n</ol>\n<center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Image-Segmentation-Learning/20.png\" alt=\"image\"></center>\n\n<p><strong></strong></p>\n<ul>\n<li><p><strong>Aligned Xception</strong></p>\n<center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Image-Segmentation-Learning/22.png\" alt=\"image\"></center>\n\n<p>train_stride=16eval_stride=8mIOU84.56% train_strideeval_stride161.53%60</p>\n</li>\n</ul>\n","site":{"data":{}},"excerpt":"<h1 id=\"Image-Segmentation\"><a href=\"#Image-Segmentation\" class=\"headerlink\" title=\"Image Segmentation\"></a>Image Segmentation</h1><p><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Image-Segmentation-Learning/1.jpg\" alt=\"image\"></p>\n<h2 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h2><p>Segmentation","more":"</p>\n<p></p>\n<p><strong></strong></p>\n<ul>\n<li>1. 2. 3. 4. 5. 6. </li>\n<li></li>\n<li></li>\n<li></li>\n<li></li>\n</ul>\n<h2 id=\"Tranditional-Methods\"><a href=\"#Tranditional-Methods\" class=\"headerlink\" title=\"Tranditional Methods\"></a>Tranditional Methods</h2><p><a href=\"https://zhuanlan.zhihu.com/p/30732385\" target=\"_blank\" rel=\"noopener\"></a></p>\n<ul>\n<li><strong></strong>: 1. 2. 3. .</li>\n<li><strong></strong>: 1. Canny2. Harris3. SIFT3. SURF.</li>\n<li><strong></strong>: 1. 2. 3. .</li>\n<li><strong></strong>: 1. GraphCut; 2. GrabCut; 3. Random Walk.</li>\n<li><strong></strong>: 1. Snake2. Active Shape Model; 3. Active Apperance Models; 4. Constrained local model.</li>\n</ul>\n<h2 id=\"Datasets\"><a href=\"#Datasets\" class=\"headerlink\" title=\"Datasets\"></a>Datasets</h2><ol>\n<li>Pascal VOC: 206929</li>\n<li>CityScapes30500020000</li>\n<li>MS COCO803320150</li>\n<li>ImageNet: <a href=\"https://www.52cv.net/?p=883\" target=\"_blank\" rel=\"noopener\">DeepLesion</a>, 1000032000</li>\n</ol>\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h2><p>$k+1$$L_{o}$$L_{k}$,, $P_{ij}$<strong>i</strong><strong>j</strong>$p_{ii}$<strong></strong>$p_{ij}$$p_{ji}$</p>\n<ol>\n<li>Pixel Accuracy(PA, ): <br>$$PA=\\frac{\\sum_{i=0}^{k}p_{ii}}{\\sum_{i=0}^{k}\\sum_{j=0}^{k}p_{ij}}$$</li>\n<li>Mean Pixel Accuracy(MAP,): PA<br>$$MAP=\\frac{1}{k+1}\\sum_{i=0}^{k}\\frac{p_{ii}}{\\sum_{j=0}^{k}p_{ij}}$$</li>\n<li>Mean Intersection over Union(MIoU, ): (ground truth)(predicted segmentation). (intersection)()IoU<br>$$MIoU = \\frac{1}{k+1}\\sum_{i=0}^{k}\\frac{p_{ii}}{\\sum_{j=0}^{k}p_{ij}+\\sum_{j=0}^{k}(p_{ji}-p_{ii})}$$</li>\n<li>Frequency Weighted Intersection over Union(FWIoU, ): MIoU<br>$$FWIoU = \\frac{1}{\\sum_{i=0}^{k}\\sum_{j=0}^{k}p_{ij}}\\sum_{i=0}^{k}\\frac{p_{ii}}{\\sum_{j=0}^{k}p_{ij}+\\sum_{j=0}^{k}(p_{ji}-p_{ii})}$$</li>\n</ol>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><h2 id=\"Fully-Convolutional-Networks\"><a href=\"#Fully-Convolutional-Networks\" class=\"headerlink\" title=\"Fully Convolutional Networks\"></a>Fully Convolutional Networks</h2><p><a href=\"https://people.eecs.berkeley.edu/~jonlong/long_shelhamer_fcn.pdf\" target=\"_blank\" rel=\"noopener\"></a><br></p>\n<p> FCN </p>\n<center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Image-Segmentation-Learning/2.png\" alt=\"image\"></center>\n\n<p>, AlexNet, 1000, , tabby catFCNsemantic segmentationFCNfeature map, , , softmax, </p>\n<p><strong>-&gt;</strong></p>\n<p> K=4096  77512 F=7,P=0,S=1,K=4096 .</p>\n<p> 224x224x3  7x7x512 AlexNet409610003</p>\n<ul>\n<li>[7x7x512]F=7[1x1x4096]</li>\n<li>F=1[1x1x4096]</li>\n<li>F=1[1x1x1000]</li>\n</ul>\n<p><strong>end to end, pixels to pixels network</strong><br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Image-Segmentation-Learning/3.png\" alt=\"image\"><br>pooling$\\frac{H}{32} \\times \\frac{W}{32}$heatmapheatmapupsampling1000heatmapupsamplinglabeltrick1000</p>\n<p><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Image-Segmentation-Learning/4.png\" alt=\"image\"></p>\n<p>1/32heatMap1/16featureMap1/8featureMap1/32heatMapupsamplingconv5conv4upsamplingconv3upsampling</p>\n<p><strong></strong></p>\n<ul>\n<li>832</li>\n<li>spatial regularization</li>\n</ul>\n<p><strong></strong><br>upsampling<strong></strong></p>\n<ul>\n<li><p><br><br><center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Image-Segmentation-Learning/5.png\" alt=\"image\"></center><br>$\\frac{y-y_{0}}{y_{1}-y_{0}}=\\frac{x-x_{0}}{x_{1}-x_{0}}$ $\\alpha$$\\alpha =\\frac{y-y_{0}}{y_{1}-y_{0}}=\\frac{x-x_{0}}{x_{1}-x_{0}}$. y: $y=(1-\\alpha)y_{0}+\\alpha y_{1} = (1-\\frac{x-x_{0}}{x_{1}-x_{0}})y_{0}+\\frac{x-x_{0}}{x_{1}-x_{0}}y_{1}=\\frac{x_{1}-x}{x_{1}-x_{0}}y_{0}+\\frac{x-x_{0}}{x_{1}-x_{0}}y_{1}=\\frac{x_{1}-x}{x_{1}-x_{0}}f(x_{1})+\\frac{x-x_{0}}{x_{1}-x_{0}}f(x_{0})$</p>\n</li>\n<li><p><br></p>\n<center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Image-Segmentation-Learning/6.png\" alt=\"image\"></center>\n\n</li>\n</ul>\n<p>X$Q_{12}$,$Q_{22}$$R_{2}$$Q_{11}$$Q_{21}$$R_{1}$</p>\n<p>$f(R_{1}=\\frac{x_{2}-x}{x_{2}-x_{1}}f(Q_{11})+\\frac{x-x_{1}}{x_{2}-x_{1}}f(Q_{21})$; $f(R_{2}=\\frac{x_{2}-x}{x_{2}-x_{1}}f(Q_{12})+\\frac{x-x_{1}}{x_{2}-x_{1}}f(Q_{22})$</p>\n<p>Y$R_{1$}$R_{2}$yP</p>\n<p>$f(P)=\\frac{y_{2}-y}{y_{2}-y_{1}}f(R_{1})+\\frac{y-y_{1}}{y_{2}-y_{1}}f(R_{2})$</p>\n<hr>\n<h2 id=\"U-Net-Convolutional-Networks-for-Biomedical-Image-Segmentation\"><a href=\"#U-Net-Convolutional-Networks-for-Biomedical-Image-Segmentation\" class=\"headerlink\" title=\"U-Net: Convolutional Networks for Biomedical Image Segmentation\"></a>U-Net: Convolutional Networks for Biomedical Image Segmentation</h2><p><a href=\"https://arxiv.org/pdf/1505.04597.pdf\" target=\"_blank\" rel=\"noopener\"></a><br>Ciresanpatch  patches</p>\n<p></p>\n<p>patchpatch,; patchesmax-pooling,patches</p>\n<p><strong>U-Net Architecture</strong></p>\n<center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Image-Segmentation-Learning/7.jpg\" alt=\"image\"></center>\n\n<ol>\n<li>()</li>\n<li>maxpooling</li>\n<li>:concatenatepooling</li>\n<li>1X1heatmap,heatmap,softmaxsoftmax</li>\n</ol>\n<p><strong>Overlap-tile strategy</strong></p>\n<center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Image-Segmentation-Learning/8.png\" alt=\"image\"></center>\n\n<p>patchpatchUnetoverlapoverlapoverlappatch </p>\n<p><strong></strong></p>\n<p>softmax<br>$$E=\\sum_{x\\in \\Omega}w(x)log(p_{\\ell(x)}(x))$$</p>\n<p>$w(x)$:<br>$$w(x)=w_{c}(x)+w_{0} \\times exp(-\\frac{(d_{1}(x)+d_{2}(x))}{2\\sigma^{2}})$$</p>\n<p><strong>Data Augmentation</strong><br>3*3(smooth deformations) 10(Bicubic interpolation)contracting pathdrop-out </p>\n<p><strong></strong><br>f(x,y)16</p>\n<hr>\n<h2 id=\"DeepLab-V1\"><a href=\"#DeepLab-V1\" class=\"headerlink\" title=\"DeepLab V1\"></a>DeepLab V1</h2><p><a href=\"https://arxiv.org/pdf/1412.7062v3.pdf\" target=\"_blank\" rel=\"noopener\"></a></p>\n<p>DeepLab(<a href=\"https://www.cnblogs.com/wangxiaocvpr/p/8763510.html\" target=\"_blank\" rel=\"noopener\">DCNNs</a>)(<a href=\"https://zhuanlan.zhihu.com/p/33397147\" target=\"_blank\" rel=\"noopener\">DenseCRFs</a>).</p>\n<p>DCNN</p>\n<ul>\n<li></li>\n<li>(invariance)</li>\n</ul>\n<p>DCNNDeepLab<code>atrous</code>()</p>\n<p>DCNNDeepLab(DenseCRF)</p>\n<p> CRFs  tricks  Multi-Scale features U-Net  FPN  MLP 128  33  128  11  5128=640  CRF </p>\n<p><strong>CRF-&gt;</strong></p>\n<p>$i$$x_{i}$(21$(i \\in 1,2,..,21)$  $y_{i}$()(CRF)$y_{i}$$i$$x_{i}$.:</p>\n<center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Image-Segmentation-Learning/9.png\" alt=\"image\"></center>\n\n<p>(xI):<br>$$p(x|I)=\\frac{1}{Z}exp(-E(x|I))$$</p>\n<p>CRF$E(x)$:<br>$$E(x)=\\sum_{i} \\theta_{i}(x_{i})+\\sum_{ij}\\theta_{ij}(x_{i},x_{j})$$</p>\n<p>$\\theta_{i}(x_{i})$$\\theta_{ij}(x_{i},x_{j})$</p>\n<ul>\n<li>i$P(x_{i})$DCNNi.</li>\n</ul>\n<p>$$\\theta_{i}(x_{i})=-logP(x_{i})$$</p>\n<ul>\n<li></li>\n</ul>\n<p>$$\\theta_{ij}(x_{i},x_{j})=\\mu(x_{i},x_{j})\\sum_{m=1}^{K}w_{m}k^{m}(f_{i},f_{j})$$</p>\n<p>DeepLab(p)(I),(p):<br>$$w_{1}exp(-\\frac{\\lVert p_{i}-p_{j} \\rVert^{2}}{2\\sigma_{\\alpha}^{2}}-\\frac{\\lVert I_{i}-I_{j} \\rVert^{2}}{2\\sigma_{\\beta}^{2}})+w_{2}exp(-\\frac{\\lVert p_{i}-p_{j} \\rVert^{2}}{2\\sigma_{\\gamma}^{2}})$$</p>\n<p><strong></strong></p>\n<table>\n<thead>\n<tr>\n<th></th>\n<th></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td></td>\n<td>PASCAL VOC 2012 segmentation benchmark</td>\n</tr>\n<tr>\n<td>DCNN</td>\n<td>VGG16</td>\n</tr>\n<tr>\n<td>DCNN</td>\n<td></td>\n</tr>\n<tr>\n<td></td>\n<td>SGDbatch=20</td>\n</tr>\n<tr>\n<td></td>\n<td>0.0010.0120000.1</td>\n</tr>\n<tr>\n<td></td>\n<td>0.9 0.0005</td>\n</tr>\n</tbody>\n</table>\n<hr>\n<h2 id=\"DepLab-V2\"><a href=\"#DepLab-V2\" class=\"headerlink\" title=\"DepLab V2\"></a>DepLab V2</h2><p><a href=\"https://arxiv.org/pdf/1606.00915.pdf\" target=\"_blank\" rel=\"noopener\"></a></p>\n<p>DeepLabv2  DeepLabv1 DeepLabv1 </p>\n<ol>\n<li></li>\n<li></li>\n<li>DCNN </li>\n</ol>\n<p>, DeepLabv23:</p>\n<ol>\n<li>DCNN</li>\n<li>(atrous spatial pyramid pooling (ASPP))ASPP</li>\n<li>DCNNDCNNDCNNCRF</li>\n</ol>\n<p><strong></strong></p>\n<center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Image-Segmentation-Learning/10.png\" alt=\"image\"></center>\n\n<ul>\n<li>DCNN(ASPP)<code>Aeroplane Coarse Score map</code></li>\n<li><code>Bi-linear Interpolation</code></li>\n<li>CRF<code>Final Output</code></li>\n</ul>\n<p><strong></strong></p>\n<ol>\n<li><p><strong></strong></p>\n<p>$y[i]$, $x[i]$, K$w[k]$, <br>$$y[k]=\\sum_{k=1}^{K}x[i+r\\cdot k]w[k]$$<br>r, $r=1$(a); (b)$r=2$</p>\n<center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Image-Segmentation-Learning/11.png\" alt=\"image\"></center>\n\n<p>(),</p>\n<ul>\n<li>21/4</li>\n<li>(2)<center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Image-Segmentation-Learning/12.png\" alt=\"image\"></center>\n\n</li>\n</ul>\n<p>r$r-1$$k\\times k$$k_{e}=k+(k-1)(r-1)$DCNN()DCNN</p>\n</li>\n<li><p><strong>ASPP</strong></p>\n<p>DeepLabv2SPPNetAtrous Spatial Pyramid Pooling (ASPP)</p>\n<center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Image-Segmentation-Learning/13.png\" alt=\"image\"></center>\n\n<p>Input Feature Map4$r=6,12,18,24$, $3 \\times 3$. .</p>\n</li>\n<li><p><strong>CRF</strong></p>\n<center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Image-Segmentation-Learning/14.png\" alt=\"image\"></center>\n\n</li>\n</ol>\n<p><strong></strong></p>\n<table>\n<thead>\n<tr>\n<th></th>\n<th></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>DCNN</td>\n<td>VGG16<strong>ResNet101</strong></td>\n</tr>\n<tr>\n<td>DCNN</td>\n<td>ground truth8</td>\n</tr>\n<tr>\n<td></td>\n<td>SGDbatch=20</td>\n</tr>\n<tr>\n<td></td>\n<td>0.0010.0120000.1</td>\n</tr>\n<tr>\n<td></td>\n<td>0.9 0.0005</td>\n</tr>\n</tbody>\n</table>\n<hr>\n<h2 id=\"DepLab-V3\"><a href=\"#DepLab-V3\" class=\"headerlink\" title=\"DepLab V3\"></a>DepLab V3</h2><p><a href=\"https://arxiv.org/pdf/1706.05587.pdf\" target=\"_blank\" rel=\"noopener\"></a></p>\n<p></p>\n<ul>\n<li>DCNN</li>\n<li></li>\n</ul>\n<p>DeepLabv3</p>\n<ul>\n<li></li>\n<li>ASPPBN</li>\n<li>11, ASPP</li>\n<li>DeepLabv3</li>\n</ul>\n<p><strong></strong></p>\n<ol>\n<li><strong></strong></li>\n<li><p><strong></strong></p>\n<center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Image-Segmentation-Learning/15.png\" alt=\"image\"></center>\n\n<center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Image-Segmentation-Learning/18.png\" alt=\"image\"></center>\n\n<p>, ResNetblockblock4(a)out_stride = 8 (b)out_stride = 16.</p>\n</li>\n<li><p><strong>Atrous Spatial Pyramid Pooling</strong></p>\n<p>DeepLabv2ASPPDeepLabv3ASPPBN(0)</p>\n<center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Image-Segmentation-Learning/16.png\" alt=\"image\"></center>\n\n<p>$3 \\times 3$$65 \\times 65$$3 \\times 3$$1 \\times 1$</p>\n<p><center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Image-Segmentation-Learning/17.png\" alt=\"image\"></center><br>ASPP</p>\n<ol>\n<li>$1 \\times 1$$3 \\times 3$$rate = (6,12,18)$256BNoutput_stride=16output_stride=8$1 \\times 1$</li>\n<li>$1 \\times 1$</li>\n</ol>\n</li>\n</ol>\n<p><strong></strong></p>\n<table>\n<thead>\n<tr>\n<th></th>\n<th></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td></td>\n<td>PASCAL VOC 2012</td>\n</tr>\n<tr>\n<td></td>\n<td>TensorFlow</td>\n</tr>\n<tr>\n<td></td>\n<td>513</td>\n</tr>\n<tr>\n<td></td>\n<td>poly $learning rate = base_lr(1-\\frac{iter}{max_iters})^{power}$</td>\n</tr>\n<tr>\n<td><strong>BN</strong></td>\n<td>output_stride=16batchsize=16BN0.99970.00730KBNoutput_stride=80.00130Koutput_stride=16output_stride=8output_stride=16</td>\n</tr>\n<tr>\n<td><strong></strong></td>\n<td>,GroundTruth8GroundTruth8GroundTruth</td>\n</tr>\n</tbody>\n</table>\n<hr>\n<h2 id=\"DepLab-V-3\"><a href=\"#DepLab-V-3\" class=\"headerlink\" title=\"DepLab-V$3^{+}$\"></a>DepLab-V$3^{+}$</h2><p><a href=\"https://arxiv.org/pdf/1802.02611.pdf\" target=\"_blank\" rel=\"noopener\"></a></p>\n<p>pooling or convolutions with stridefeature. poolingfeaturefeature</p>\n<p><strong></strong></p>\n<center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Image-Segmentation-Learning/19.png\" alt=\"image\"></center>\n\n<p>DeepLabV$3^{+}$encoder-decoderDeepLab V3decoder(c)4encoderconcatenate4XceptionencoderAtrous Spatial Pyramid Poolingdecoderdepth-wise separable convolution</p>\n<ol>\n<li><p><strong>Encoder</strong></p>\n<ul>\n<li>ResNet: encoderDeepLab V3ResNet101()blockstrideoutput stride8(16)block4Atrous Spatial Pyramid Poolingconcatenate11256</li>\n<li><p><strong>Xecption</strong>: XceptionMSRA teamXceptionAligned Xception</p>\n<ul>\n<li>Aligned Xceptionentry flow network</li>\n<li>max poolingstrideseparable convolutionatrous separable convolution</li>\n<li>33depath-wise convolutionBNReLU<center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Image-Segmentation-Learning/21.png\" alt=\"image\"></center>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n<li><p><strong>Decoder</strong></p>\n<ul>\n<li>decoder4encoderconcatenateconcatenate(256512)encoder256concatenate11concatenate334</li>\n<li> <ul>\n<li>$1 \\times 1$48</li>\n<li>33233</li>\n<li>encoderConv2</li>\n</ul>\n</li>\n</ul>\n</li>\n</ol>\n<center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Image-Segmentation-Learning/20.png\" alt=\"image\"></center>\n\n<p><strong></strong></p>\n<ul>\n<li><p><strong>Aligned Xception</strong></p>\n<center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Image-Segmentation-Learning/22.png\" alt=\"image\"></center>\n\n<p>train_stride=16eval_stride=8mIOU84.56% train_strideeval_stride161.53%60</p>\n</li>\n</ul>"},{"title":"MalongTech :","date":"2018-11-12T03:00:00.000Z","_content":"\n# Object detection\n\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/14.png)\n\n| one-stage  | two-stage|\n| ---------- | -----------|\n| YOLO V1,V2,V3   | FPN   |\n| SSD   | RFCN   |\n| RetinalNet | LIghthead |\n<!-- more -->\n## One Stage\n\n### YOLO V1\n** You only look once unified real-time object detection**\n\n[PAPER ADDRESS](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Redmon_You_Only_Look_CVPR_2016_paper.pdf)\n\nYOLOobject detectionbounding box\n\n****: \n1. YOLOTitan X GPU45 fps, 155 fps\n2. YOLOsliding windowregion proposalFast R-CNNYOLO\n3. \n\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/1.png)\n\n****:\n1. accuracy  state-of-the-art \n2. \n3. \n4. \n\n****\n1. $448\\times448$.\n2. \n3. NMX()\n\n**Unified Detection**\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/2.png)\n1. $S\\times S$\n2. B5x,t,w,hconfidencex,ybounding boxgrid cellgrid cell01whwhwh01\n3. C\n4. Pascal VOC S=7,B=2,C=20.  $S\\times S\\times (B \\times 5 + C)$  $7\\times 7\\times 30$ \n\nConfidence$Pr(Object) * IOU_{pred}^{turth} $\n\nbounding boxconfidence scoregrid cellobjectconfidence0confidence scoreboxground truthIOUobjectground truthgrid cellgrid cellobjectobjectgrid cell \ngrid cellCgrid cellobject$Pr(Class_{i}|Object)$\n\nbounding boxconfidencescorebounding boxconfidence score\n\nbounding boxconfidence score20\\*(7\\*7\\*2)scorebounding box20200.20NMSbounding boxNMS:bounding boxbounding boxIOUIOU0.500.5scorebounding boxbounding boxbounding boxIOUbounding box20scorescorescore0bounding boxsocre0bounding box\n\n\n****\nGoogLeNet,\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/3.png)\n\n1. ImageNet20224\\*22442448\\*448\n2. Reluleaky Relux<00.1\\*x0\n3. sum-squared errorlocalization errorbounding boxclassificaton errorgrid cellgrid cellconfidence score0localization errorobjectboxconfidence lossloss50.5objectboxconfidence loss1\n4. .\n5. Loss Function![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/4.png)\n\nNMobjecobject4xywh1label7\\*7\\*301\\*305bounding box41confidence610bounding box41confidence20grid cell30 confidenceground truthbounding boxIOUobject0,1confidence01object1object0 loss function\n\n7\\*7\\*30IOUbounding boxconfidencebounding box\n\n**YOLO**\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/5.png)\nyoloFaster RCNN,YOLOFaster RCNN\n\n### YOLO V2 and YOLO 9000\n[](https://arxiv.org/pdf/1612.08242.pdf)\n\n\n\n**YOLO V1**\n1. YOLOYOLOFast R-CNNYOLOYOLO\n2. YOLOv2YOLO\n    * Batch Normalization: map2%\n    * High Resolution Classifier: ImageNet44844810map4%,\n    * Convolutional With Anchor Boxes: YOLO416416448448YOLO324161313map88%.\n    * Dimension Clusters: Anchors k-meansk-meansIOU$d(box,centroid)=1-IOU(box,centroid)$. voccocok=5![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/6.png)\n    * Direct location prediction: RPN(x,y)Sigmoid0~1![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/7.png)55$t_{x}$,$t_{y}$,$t_{w}$,$t_{h}$,$t_{o}$,$(c_{x},c_{y})$,$p_{w}$,$p_{h}$. $Pr(object)\\times IOU(b,object)=\\sigma(t_{o}) $ map 5%\n    * Fine-Grained Features(): ResNet2626512131320481%\n    * Multi-Scale Training:  103306083232![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/8.png)\n\n3. Darknet-19 19555.8VGGYOLOtop-5ImageNet\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/9.png)\n4. Darknet0.140.00050.9ImageNet 1000160over-fitting. 224224448\n5. 10243311VOC552012533512\n6. \n    * YOLOv2\n    * Hierarchical classification()ImageNetWordNetWordTreeWordTreeWordTreesynsets![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/10.png)\n    * YOLO 9000anchors3: COCOImageNet9000WordTree9418ImageNetCOCOImageNet4:1YOLO9000ImageNetCOCOCOCO3\n\n### YOLO V3\n[](https://pjreddie.com/media/files/papers/YOLOv3.pdf)\n\n\n1. YOLO V3V2Softmax lossLogistic loss(logistic)GT.\n2. Anchor bbox priorV25anchorV39anchorIOU.\n3. DetectionV2detectionV33Feature map13\\*132eltwise sum(feature pyramid networks)Feature map26*\\2652\\*52V341652Feature mapV2data13Feature map\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/11.png)\n4.  DarkNet-53: YOLOv2Darknet-193311shortcut connection53![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/12.png)\n5. YOLO V3Pascal Titan X608x60820FPS COCO test-dev  mAP@0.5  57.9%RetinaNet4 YOLO V3 ![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/13.png)\n6. \n    * Anchor box\n    * x,y\n    * focal loss\n    * IOU\n\n\n### SSD: Single Shot MultiBox Detector\n[](https://arxiv.org/pdf/1512.02325.pdf)\n\nSSD\n\n****:\n1. \n2. \n3. \n4. \n\n****:\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/15.png)\nSSD\n* ****\n* ****\n* ****Faster R-CNN[2]![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/16.png)\n*  PS: VGG\n\n****\n1. ****IOU0.5\n2. ****![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/17.png)\n3. ****: ![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/18.png)\n4. ****: 3:1\n5. ****![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/20.png)\n6. **** 16x\n\n****:\nSSDSSD300300512512SSD\n\n### RetinaNet Focal Loss for Dense Object Detection\n[](https://arxiv.org/abs/1708.02002)\n\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/32.png)\n\ncentral issusproposalYOLOPASCAL VOCYOLO V213135845losslossone-stageRetinaNettrade-off\n\n$FL(p_{t}=-(1-p_{t})^{\\gamma}log(p_{t})$\n\n**Focal Loss**\nFocal Loss\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/33.png)\n\n\n\n$CE(p,y)=-log(p_{y})$\n\n\n$CE(p)=-\\alpha_{t}log(p_{t})$\n\n($\\alpha$)\n\n**$FL(p_{t}=-(1-p_{t})^{\\gamma}log(p_{t})$**\n\nPytorch\n\n$L=-\\sum_{i}^{C}onehot\\odot(1-p_{t})^{\\gamma}log(p_{t})$\n\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\n\ndef one_hot(index, classes):\n    size = index.size() + (classes,)\n    view = index.size() + (1,)\n    mask = torch.Tensor(*size).fill_(0)\n    index = index.view(*view)\n    ones = 1.\n    if isinstance(index, Variable):\n        ones = Variable(torch.Tensor(index.size()).fill_(1))\n        mask = Variable(mask, volatile=index.volatile)\n    return mask.scatter_(1, index, ones)\n    \nclass FocalLoss(nn.Module):\n    def __init__(self, gamma=0, eps=1e-7):\n        super(FocalLoss, self).__init__()\n        self.gamma = gamma\n        self.eps = eps\n        \n    def forward(self, input, target):\n        y = one_hot(target, input.size(-1))\n        logit = F.softmax(input)\n        logit = logit.clamp(self.eps, 1. - self.eps)\n        loss = -1 * y * torch.log(logit) # cross entropy\n        loss = loss * (1 - logit) ** self.gamma # focal loss\n        return loss.sum()\n```\n\n****:\n\n1.  0.01Focal Lossimagenetbase net$\\sigma$=0.010.$b=-log(\\frac{(1-\\pi)}{\\pi})$, $\\pi$anchor\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/34.png)\n\nResNetFeature Pyramid NetFPNone-stage.RetinaNet \n* (small FCN attached to each FPN level)\n* (attach another samll FCN to each pyramid level)\n* \n\nAnchors: \nP3P7$32^{2}$$512^{2}$, [1:2,1:1,2:1]. $[2^{0},2^{\\frac{1}{3}},2^{\\frac{1}{2}}]$\n\nIOU:\n\\[0,0.4)\\[0.4,0.5)0.5\n\n## Two Stage\n\n### R-FCN Object Detection via Region-based Fully Convolutional Networks\n\n[](https://link.jianshu.com/?t=https%3A%2F%2Farxiv.org%2Fpdf%2F1605.06409.pdf)\n\nR-FCN  Position-sensitive score map  ROI pooling  Faster R-CNN \n\nPS: \n* ROIResNet-101conv5mAP68.9%ROIconv5Faster R-CNNmAP76.4%\n\n* Faster R-CNNROIproposal300proposalROI300, \n\n****\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/19.png)\n\n* Backbone architecture:ResNet-10110010002048102411\n* $k^{2}(C+1)$Conv: ResNet101W\\*H\\*1024$k^{2}(C+1)$1024\\*1\\*1$k^{2}(C+1)$W\\*Hposition sensitivescore mappredictionk = 3ROI3\\*39\n* ROI pooling: SPPROIfeature map![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/21.png)\n* Vote:k\\*kbinscoresoftmax![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/22.png)\n\n****:\n*  $L(s,t_{x,y,w,h})=L_{cls}(s_{c^{\\star}}+ \\lambda [c^{\\star}>0]L_{reg}(t,t^{\\star})$ \nIOU0.5ROI\n* **OHEM**): easy exampleshard examplesS-OHEM: Stratified Online Hard Example Mining for Object Detection. **S-OHEM** OHEMstratified samplingOHEMS-OHEMA-Fast-RCNN: Hard positive generation via adversary for object detectionOHEMS-OHEMA-Fast-RCNN**GAN**\n* ****:FCNResNet-1013216conv5stride=2stride=1conv5hole algorithm .\n\n****\nM  5\\*5  3\\*3  M ![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/23.png)\n\n 9  9 position-sensitive score maps![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/24.png)\n ROIs  3\\*3  3\\*3  [0][0] ![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/25.png)\n ROIs  ROI position-sensitive ROI-pool\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/26.png)\n ROI \n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/27.png)\n C  C+1 33  (C+1)33  softmax \n\n\n### FPN Feature Pyramid Networks for Object Detection\n[](http://openaccess.thecvf.com/content_cvpr_2017/papers/Lin_Feature_Pyramid_Networks_CVPR_2017_paper.pdf)\n\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/28.png)\n\na. \n\nb. ConvNET\n\nc. \n\nd. FPN\n\nFPN feature map  feature map  element-wise  feature map  feature map \n\n**FPN**:\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/29.png)\n\n1. feature map\n2. bottom-up  top-down  bottom-up Top-down\n3. top-downbottom-upx2bottom-up1x1\n4. \n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/30.png)\n\n\n**RPNFPN**\n1. FPNRPN3x3 conv/ 1x1convs$[P_{2},P_{3},P_{4},P_{5},P_{6}]$$[32^{2},64^{2},128^{2},256^{2},512^{2}]$ {1:2,1:1,2:1}15\n2. RPN\n2. ROIboxROIP5P4\n$k=[k_{0}+log_{2}(\\frac{\\sqrt{wh}}{224}]$ \n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/31.png)\n\n### Light-Head R-CNN: In Defense of Two-Stage Object Detector\n[](https://arxiv.org/abs/1711.07264)\n\ntwo-stage  classification+regression \n* Faster R-CNN: two fully connected layers for RoI recognition\n* R-FCN: produces a large score maps.\n\n detection  two-stage detectorLight-Head R-CNNLight-Head R-CNN  head  R-CNN subnetROI pooling networkROI warping\n\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/35.png)\n\n****\n* **Thin feature map**: ![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/36.png)headfeature mapchannelr-fcnscore map$P\\times P(C+1)$$P\\times P\\times \\alpha$$\\alpha$10score mapchannelr-fcnvoteroi poolingfc\n* **Large separable convolution**: \n\n\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/38.png)![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/39.png)  \n* **Cheap R-CNN**: ![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/37.png) \n\n****\n* **Large**: (1) ResNEt-101; (2) atrous algorithm; (3) RPN chanels 512; (4) Large separable convolution with $C_{mid}=256$\n* **Small**: (1) Xception like; (2) abbandon atrous algorithm; (3) RPN convolution to 256; (4) Large separable convolution with $C_{mid}=64$; (5) Apply PSPooling with alignment techniques as RoI warping(better results involve RoI-align).","source":"_posts/MalongTech-Learning-Object-Detection.md","raw":"---\ntitle: MalongTech :\ndate: 2018-11-12 11:00:00\ntags: [Deep Learning]\ncategories: \n---\n\n# Object detection\n\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/14.png)\n\n| one-stage  | two-stage|\n| ---------- | -----------|\n| YOLO V1,V2,V3   | FPN   |\n| SSD   | RFCN   |\n| RetinalNet | LIghthead |\n<!-- more -->\n## One Stage\n\n### YOLO V1\n** You only look once unified real-time object detection**\n\n[PAPER ADDRESS](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Redmon_You_Only_Look_CVPR_2016_paper.pdf)\n\nYOLOobject detectionbounding box\n\n****: \n1. YOLOTitan X GPU45 fps, 155 fps\n2. YOLOsliding windowregion proposalFast R-CNNYOLO\n3. \n\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/1.png)\n\n****:\n1. accuracy  state-of-the-art \n2. \n3. \n4. \n\n****\n1. $448\\times448$.\n2. \n3. NMX()\n\n**Unified Detection**\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/2.png)\n1. $S\\times S$\n2. B5x,t,w,hconfidencex,ybounding boxgrid cellgrid cell01whwhwh01\n3. C\n4. Pascal VOC S=7,B=2,C=20.  $S\\times S\\times (B \\times 5 + C)$  $7\\times 7\\times 30$ \n\nConfidence$Pr(Object) * IOU_{pred}^{turth} $\n\nbounding boxconfidence scoregrid cellobjectconfidence0confidence scoreboxground truthIOUobjectground truthgrid cellgrid cellobjectobjectgrid cell \ngrid cellCgrid cellobject$Pr(Class_{i}|Object)$\n\nbounding boxconfidencescorebounding boxconfidence score\n\nbounding boxconfidence score20\\*(7\\*7\\*2)scorebounding box20200.20NMSbounding boxNMS:bounding boxbounding boxIOUIOU0.500.5scorebounding boxbounding boxbounding boxIOUbounding box20scorescorescore0bounding boxsocre0bounding box\n\n\n****\nGoogLeNet,\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/3.png)\n\n1. ImageNet20224\\*22442448\\*448\n2. Reluleaky Relux<00.1\\*x0\n3. sum-squared errorlocalization errorbounding boxclassificaton errorgrid cellgrid cellconfidence score0localization errorobjectboxconfidence lossloss50.5objectboxconfidence loss1\n4. .\n5. Loss Function![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/4.png)\n\nNMobjecobject4xywh1label7\\*7\\*301\\*305bounding box41confidence610bounding box41confidence20grid cell30 confidenceground truthbounding boxIOUobject0,1confidence01object1object0 loss function\n\n7\\*7\\*30IOUbounding boxconfidencebounding box\n\n**YOLO**\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/5.png)\nyoloFaster RCNN,YOLOFaster RCNN\n\n### YOLO V2 and YOLO 9000\n[](https://arxiv.org/pdf/1612.08242.pdf)\n\n\n\n**YOLO V1**\n1. YOLOYOLOFast R-CNNYOLOYOLO\n2. YOLOv2YOLO\n    * Batch Normalization: map2%\n    * High Resolution Classifier: ImageNet44844810map4%,\n    * Convolutional With Anchor Boxes: YOLO416416448448YOLO324161313map88%.\n    * Dimension Clusters: Anchors k-meansk-meansIOU$d(box,centroid)=1-IOU(box,centroid)$. voccocok=5![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/6.png)\n    * Direct location prediction: RPN(x,y)Sigmoid0~1![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/7.png)55$t_{x}$,$t_{y}$,$t_{w}$,$t_{h}$,$t_{o}$,$(c_{x},c_{y})$,$p_{w}$,$p_{h}$. $Pr(object)\\times IOU(b,object)=\\sigma(t_{o}) $ map 5%\n    * Fine-Grained Features(): ResNet2626512131320481%\n    * Multi-Scale Training:  103306083232![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/8.png)\n\n3. Darknet-19 19555.8VGGYOLOtop-5ImageNet\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/9.png)\n4. Darknet0.140.00050.9ImageNet 1000160over-fitting. 224224448\n5. 10243311VOC552012533512\n6. \n    * YOLOv2\n    * Hierarchical classification()ImageNetWordNetWordTreeWordTreeWordTreesynsets![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/10.png)\n    * YOLO 9000anchors3: COCOImageNet9000WordTree9418ImageNetCOCOImageNet4:1YOLO9000ImageNetCOCOCOCO3\n\n### YOLO V3\n[](https://pjreddie.com/media/files/papers/YOLOv3.pdf)\n\n\n1. YOLO V3V2Softmax lossLogistic loss(logistic)GT.\n2. Anchor bbox priorV25anchorV39anchorIOU.\n3. DetectionV2detectionV33Feature map13\\*132eltwise sum(feature pyramid networks)Feature map26*\\2652\\*52V341652Feature mapV2data13Feature map\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/11.png)\n4.  DarkNet-53: YOLOv2Darknet-193311shortcut connection53![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/12.png)\n5. YOLO V3Pascal Titan X608x60820FPS COCO test-dev  mAP@0.5  57.9%RetinaNet4 YOLO V3 ![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/13.png)\n6. \n    * Anchor box\n    * x,y\n    * focal loss\n    * IOU\n\n\n### SSD: Single Shot MultiBox Detector\n[](https://arxiv.org/pdf/1512.02325.pdf)\n\nSSD\n\n****:\n1. \n2. \n3. \n4. \n\n****:\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/15.png)\nSSD\n* ****\n* ****\n* ****Faster R-CNN[2]![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/16.png)\n*  PS: VGG\n\n****\n1. ****IOU0.5\n2. ****![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/17.png)\n3. ****: ![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/18.png)\n4. ****: 3:1\n5. ****![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/20.png)\n6. **** 16x\n\n****:\nSSDSSD300300512512SSD\n\n### RetinaNet Focal Loss for Dense Object Detection\n[](https://arxiv.org/abs/1708.02002)\n\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/32.png)\n\ncentral issusproposalYOLOPASCAL VOCYOLO V213135845losslossone-stageRetinaNettrade-off\n\n$FL(p_{t}=-(1-p_{t})^{\\gamma}log(p_{t})$\n\n**Focal Loss**\nFocal Loss\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/33.png)\n\n\n\n$CE(p,y)=-log(p_{y})$\n\n\n$CE(p)=-\\alpha_{t}log(p_{t})$\n\n($\\alpha$)\n\n**$FL(p_{t}=-(1-p_{t})^{\\gamma}log(p_{t})$**\n\nPytorch\n\n$L=-\\sum_{i}^{C}onehot\\odot(1-p_{t})^{\\gamma}log(p_{t})$\n\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\n\ndef one_hot(index, classes):\n    size = index.size() + (classes,)\n    view = index.size() + (1,)\n    mask = torch.Tensor(*size).fill_(0)\n    index = index.view(*view)\n    ones = 1.\n    if isinstance(index, Variable):\n        ones = Variable(torch.Tensor(index.size()).fill_(1))\n        mask = Variable(mask, volatile=index.volatile)\n    return mask.scatter_(1, index, ones)\n    \nclass FocalLoss(nn.Module):\n    def __init__(self, gamma=0, eps=1e-7):\n        super(FocalLoss, self).__init__()\n        self.gamma = gamma\n        self.eps = eps\n        \n    def forward(self, input, target):\n        y = one_hot(target, input.size(-1))\n        logit = F.softmax(input)\n        logit = logit.clamp(self.eps, 1. - self.eps)\n        loss = -1 * y * torch.log(logit) # cross entropy\n        loss = loss * (1 - logit) ** self.gamma # focal loss\n        return loss.sum()\n```\n\n****:\n\n1.  0.01Focal Lossimagenetbase net$\\sigma$=0.010.$b=-log(\\frac{(1-\\pi)}{\\pi})$, $\\pi$anchor\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/34.png)\n\nResNetFeature Pyramid NetFPNone-stage.RetinaNet \n* (small FCN attached to each FPN level)\n* (attach another samll FCN to each pyramid level)\n* \n\nAnchors: \nP3P7$32^{2}$$512^{2}$, [1:2,1:1,2:1]. $[2^{0},2^{\\frac{1}{3}},2^{\\frac{1}{2}}]$\n\nIOU:\n\\[0,0.4)\\[0.4,0.5)0.5\n\n## Two Stage\n\n### R-FCN Object Detection via Region-based Fully Convolutional Networks\n\n[](https://link.jianshu.com/?t=https%3A%2F%2Farxiv.org%2Fpdf%2F1605.06409.pdf)\n\nR-FCN  Position-sensitive score map  ROI pooling  Faster R-CNN \n\nPS: \n* ROIResNet-101conv5mAP68.9%ROIconv5Faster R-CNNmAP76.4%\n\n* Faster R-CNNROIproposal300proposalROI300, \n\n****\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/19.png)\n\n* Backbone architecture:ResNet-10110010002048102411\n* $k^{2}(C+1)$Conv: ResNet101W\\*H\\*1024$k^{2}(C+1)$1024\\*1\\*1$k^{2}(C+1)$W\\*Hposition sensitivescore mappredictionk = 3ROI3\\*39\n* ROI pooling: SPPROIfeature map![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/21.png)\n* Vote:k\\*kbinscoresoftmax![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/22.png)\n\n****:\n*  $L(s,t_{x,y,w,h})=L_{cls}(s_{c^{\\star}}+ \\lambda [c^{\\star}>0]L_{reg}(t,t^{\\star})$ \nIOU0.5ROI\n* **OHEM**): easy exampleshard examplesS-OHEM: Stratified Online Hard Example Mining for Object Detection. **S-OHEM** OHEMstratified samplingOHEMS-OHEMA-Fast-RCNN: Hard positive generation via adversary for object detectionOHEMS-OHEMA-Fast-RCNN**GAN**\n* ****:FCNResNet-1013216conv5stride=2stride=1conv5hole algorithm .\n\n****\nM  5\\*5  3\\*3  M ![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/23.png)\n\n 9  9 position-sensitive score maps![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/24.png)\n ROIs  3\\*3  3\\*3  [0][0] ![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/25.png)\n ROIs  ROI position-sensitive ROI-pool\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/26.png)\n ROI \n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/27.png)\n C  C+1 33  (C+1)33  softmax \n\n\n### FPN Feature Pyramid Networks for Object Detection\n[](http://openaccess.thecvf.com/content_cvpr_2017/papers/Lin_Feature_Pyramid_Networks_CVPR_2017_paper.pdf)\n\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/28.png)\n\na. \n\nb. ConvNET\n\nc. \n\nd. FPN\n\nFPN feature map  feature map  element-wise  feature map  feature map \n\n**FPN**:\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/29.png)\n\n1. feature map\n2. bottom-up  top-down  bottom-up Top-down\n3. top-downbottom-upx2bottom-up1x1\n4. \n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/30.png)\n\n\n**RPNFPN**\n1. FPNRPN3x3 conv/ 1x1convs$[P_{2},P_{3},P_{4},P_{5},P_{6}]$$[32^{2},64^{2},128^{2},256^{2},512^{2}]$ {1:2,1:1,2:1}15\n2. RPN\n2. ROIboxROIP5P4\n$k=[k_{0}+log_{2}(\\frac{\\sqrt{wh}}{224}]$ \n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/31.png)\n\n### Light-Head R-CNN: In Defense of Two-Stage Object Detector\n[](https://arxiv.org/abs/1711.07264)\n\ntwo-stage  classification+regression \n* Faster R-CNN: two fully connected layers for RoI recognition\n* R-FCN: produces a large score maps.\n\n detection  two-stage detectorLight-Head R-CNNLight-Head R-CNN  head  R-CNN subnetROI pooling networkROI warping\n\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/35.png)\n\n****\n* **Thin feature map**: ![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/36.png)headfeature mapchannelr-fcnscore map$P\\times P(C+1)$$P\\times P\\times \\alpha$$\\alpha$10score mapchannelr-fcnvoteroi poolingfc\n* **Large separable convolution**: \n\n\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/38.png)![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/39.png)  \n* **Cheap R-CNN**: ![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/37.png) \n\n****\n* **Large**: (1) ResNEt-101; (2) atrous algorithm; (3) RPN chanels 512; (4) Large separable convolution with $C_{mid}=256$\n* **Small**: (1) Xception like; (2) abbandon atrous algorithm; (3) RPN convolution to 256; (4) Large separable convolution with $C_{mid}=64$; (5) Apply PSPooling with alignment techniques as RoI warping(better results involve RoI-align).","slug":"MalongTech-Learning-Object-Detection","published":1,"updated":"2020-04-28T12:38:55.395Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck9jwdng9000bjlrcae01whra","content":"<h1 id=\"Object-detection\"><a href=\"#Object-detection\" class=\"headerlink\" title=\"Object detection\"></a>Object detection</h1><p><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/14.png\" alt=\"image\"></p>\n<table>\n<thead>\n<tr>\n<th>one-stage</th>\n<th>two-stage</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>YOLO V1,V2,V3</td>\n<td>FPN</td>\n</tr>\n<tr>\n<td>SSD</td>\n<td>RFCN</td>\n</tr>\n<tr>\n<td>RetinalNet</td>\n<td>LIghthead</td>\n</tr>\n</tbody>\n</table>\n<a id=\"more\"></a>\n<h2 id=\"One-Stage\"><a href=\"#One-Stage\" class=\"headerlink\" title=\"One Stage\"></a>One Stage</h2><h3 id=\"YOLO-V1\"><a href=\"#YOLO-V1\" class=\"headerlink\" title=\"YOLO V1\"></a>YOLO V1</h3><p><strong> You only look once unified real-time object detection</strong></p>\n<p><a href=\"https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Redmon_You_Only_Look_CVPR_2016_paper.pdf\" target=\"_blank\" rel=\"noopener\">PAPER ADDRESS</a></p>\n<p>YOLOobject detectionbounding box</p>\n<p><strong></strong>: </p>\n<ol>\n<li>YOLOTitan X GPU45 fps, 155 fps</li>\n<li>YOLOsliding windowregion proposalFast R-CNNYOLO</li>\n<li></li>\n</ol>\n<p><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/1.png\" alt=\"image\"></p>\n<p><strong></strong>:</p>\n<ol>\n<li>accuracy  state-of-the-art </li>\n<li></li>\n<li></li>\n<li></li>\n</ol>\n<p><strong></strong></p>\n<ol>\n<li>$448\\times448$.</li>\n<li></li>\n<li>NMX()</li>\n</ol>\n<p><strong>Unified Detection</strong><br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/2.png\" alt=\"image\"></p>\n<ol>\n<li>$S\\times S$</li>\n<li>B5x,t,w,hconfidencex,ybounding boxgrid cellgrid cell01whwhwh01</li>\n<li>C</li>\n<li>Pascal VOC S=7,B=2,C=20.  $S\\times S\\times (B \\times 5 + C)$  $7\\times 7\\times 30$ </li>\n</ol>\n<p>Confidence$Pr(Object) * IOU_{pred}^{turth} $</p>\n<p>bounding boxconfidence scoregrid cellobjectconfidence0confidence scoreboxground truthIOUobjectground truthgrid cellgrid cellobjectobjectgrid cell<br>grid cellCgrid cellobject$Pr(Class_{i}|Object)$</p>\n<p>bounding boxconfidencescorebounding boxconfidence score</p>\n<p>bounding boxconfidence score20*(7*7*2)scorebounding box20200.20NMSbounding boxNMS:bounding boxbounding boxIOUIOU0.500.5scorebounding boxbounding boxbounding boxIOUbounding box20scorescorescore0bounding boxsocre0bounding box</p>\n<p><strong></strong><br>GoogLeNet,<br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/3.png\" alt=\"image\"><br></p>\n<ol>\n<li>ImageNet20224*22442448*448</li>\n<li>Reluleaky Relux&lt;00.1*x0</li>\n<li>sum-squared errorlocalization errorbounding boxclassificaton errorgrid cellgrid cellconfidence score0localization errorobjectboxconfidence lossloss50.5objectboxconfidence loss1</li>\n<li>.</li>\n<li>Loss Function<img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/4.png\" alt=\"image\"></li>\n</ol>\n<p>NMobjecobject4xywh1label7*7*301*305bounding box41confidence610bounding box41confidence20grid cell30 confidenceground truthbounding boxIOUobject0,1confidence01object1object0 loss function</p>\n<p>7*7*30IOUbounding boxconfidencebounding box</p>\n<p><strong>YOLO</strong><br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/5.png\" alt=\"image\"><br>yoloFaster RCNN,YOLOFaster RCNN</p>\n<h3 id=\"YOLO-V2-and-YOLO-9000\"><a href=\"#YOLO-V2-and-YOLO-9000\" class=\"headerlink\" title=\"YOLO V2 and YOLO 9000\"></a>YOLO V2 and YOLO 9000</h3><p><a href=\"https://arxiv.org/pdf/1612.08242.pdf\" target=\"_blank\" rel=\"noopener\"></a></p>\n<p></p>\n<p><strong>YOLO V1</strong></p>\n<ol>\n<li>YOLOYOLOFast R-CNNYOLOYOLO</li>\n<li><p>YOLOv2YOLO</p>\n<ul>\n<li>Batch Normalization: map2%</li>\n<li>High Resolution Classifier: ImageNet44844810map4%,</li>\n<li>Convolutional With Anchor Boxes: YOLO416416448448YOLO324161313map88%.</li>\n<li>Dimension Clusters: Anchors k-meansk-meansIOU$d(box,centroid)=1-IOU(box,centroid)$. voccocok=5<img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/6.png\" alt=\"image\"></li>\n<li>Direct location prediction: RPN(x,y)Sigmoid0~1<img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/7.png\" alt=\"image\">55$t_{x}$,$t_{y}$,$t_{w}$,$t_{h}$,$t_{o}$,$(c_{x},c_{y})$,$p_{w}$,$p_{h}$. $Pr(object)\\times IOU(b,object)=\\sigma(t_{o}) $ map 5%</li>\n<li>Fine-Grained Features(): ResNet2626512131320481%</li>\n<li>Multi-Scale Training:  103306083232<img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/8.png\" alt=\"image\"></li>\n</ul>\n</li>\n<li><p>Darknet-19 19555.8VGGYOLOtop-5ImageNet<br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/9.png\" alt=\"image\"></p>\n</li>\n<li>Darknet0.140.00050.9ImageNet 1000160over-fitting. 224224448</li>\n<li>10243311VOC552012533512</li>\n<li><ul>\n<li>YOLOv2</li>\n<li>Hierarchical classification()ImageNetWordNetWordTreeWordTreeWordTreesynsets<img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/10.png\" alt=\"image\"></li>\n<li>YOLO 9000anchors3: COCOImageNet9000WordTree9418ImageNetCOCOImageNet4:1YOLO9000ImageNetCOCOCOCO3</li>\n</ul>\n</li>\n</ol>\n<h3 id=\"YOLO-V3\"><a href=\"#YOLO-V3\" class=\"headerlink\" title=\"YOLO V3\"></a>YOLO V3</h3><p><a href=\"https://pjreddie.com/media/files/papers/YOLOv3.pdf\" target=\"_blank\" rel=\"noopener\"></a></p>\n<p></p>\n<ol>\n<li>YOLO V3V2Softmax lossLogistic loss(logistic)GT.</li>\n<li>Anchor bbox priorV25anchorV39anchorIOU.</li>\n<li>DetectionV2detectionV33Feature map13*132eltwise sum(feature pyramid networks)Feature map26<em>\\2652\\</em>52V341652Feature mapV2data13Feature map<br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/11.png\" alt=\"image\"></li>\n<li> DarkNet-53: YOLOv2Darknet-193311shortcut connection53<img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/12.png\" alt=\"image\"></li>\n<li>YOLO V3Pascal Titan X608x60820FPS COCO test-dev  <a href=\"mailto:mAP@0.5\" target=\"_blank\" rel=\"noopener\">mAP@0.5</a>  57.9%RetinaNet4  YOLO V3  <img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/13.png\" alt=\"image\"></li>\n<li><ul>\n<li>Anchor box</li>\n<li>x,y</li>\n<li>focal loss</li>\n<li>IOU</li>\n</ul>\n</li>\n</ol>\n<h3 id=\"SSD-Single-Shot-MultiBox-Detector\"><a href=\"#SSD-Single-Shot-MultiBox-Detector\" class=\"headerlink\" title=\"SSD: Single Shot MultiBox Detector\"></a>SSD: Single Shot MultiBox Detector</h3><p><a href=\"https://arxiv.org/pdf/1512.02325.pdf\" target=\"_blank\" rel=\"noopener\"></a></p>\n<p>SSD</p>\n<p><strong></strong>:</p>\n<ol>\n<li></li>\n<li></li>\n<li></li>\n<li></li>\n</ol>\n<p><strong></strong>:<br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/15.png\" alt=\"image\"><br>SSD</p>\n<ul>\n<li><strong></strong></li>\n<li><strong></strong></li>\n<li><strong></strong>Faster R-CNN[2]<img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/16.png\" alt=\"image\"></li>\n<li> PS: VGG</li>\n</ul>\n<p><strong></strong></p>\n<ol>\n<li><strong></strong>IOU0.5</li>\n<li><strong></strong><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/17.png\" alt=\"image\"></li>\n<li><strong></strong>: <img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/18.png\" alt=\"image\"></li>\n<li><strong></strong>: 3:1</li>\n<li><strong></strong><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/20.png\" alt=\"image\"></li>\n<li><strong></strong> 16x</li>\n</ol>\n<p><strong></strong>:<br>SSDSSD300300512512SSD</p>\n<h3 id=\"RetinaNet-Focal-Loss-for-Dense-Object-Detection\"><a href=\"#RetinaNet-Focal-Loss-for-Dense-Object-Detection\" class=\"headerlink\" title=\"RetinaNet Focal Loss for Dense Object Detection\"></a>RetinaNet Focal Loss for Dense Object Detection</h3><p><a href=\"https://arxiv.org/abs/1708.02002\" target=\"_blank\" rel=\"noopener\"></a></p>\n<p><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/32.png\" alt=\"image\"></p>\n<p>central issusproposalYOLOPASCAL VOCYOLO V213135845losslossone-stageRetinaNettrade-off</p>\n<p>$FL(p_{t}=-(1-p_{t})^{\\gamma}log(p_{t})$</p>\n<p><strong>Focal Loss</strong><br>Focal Loss<br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/33.png\" alt=\"image\"></p>\n<p></p>\n<p>$CE(p,y)=-log(p_{y})$</p>\n<p><br>$CE(p)=-\\alpha_{t}log(p_{t})$</p>\n<p>($\\alpha$)</p>\n<p><strong>$FL(p_{t}=-(1-p_{t})^{\\gamma}log(p_{t})$</strong></p>\n<p>Pytorch</p>\n<p>$L=-\\sum_{i}^{C}onehot\\odot(1-p_{t})^{\\gamma}log(p_{t})$</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">import</span> torch.nn <span class=\"keyword\">as</span> nn</span><br><span class=\"line\"><span class=\"keyword\">import</span> torch.nn.functional <span class=\"keyword\">as</span> F</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch.autograd <span class=\"keyword\">import</span> Variable</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">one_hot</span><span class=\"params\">(index, classes)</span>:</span></span><br><span class=\"line\">    size = index.size() + (classes,)</span><br><span class=\"line\">    view = index.size() + (<span class=\"number\">1</span>,)</span><br><span class=\"line\">    mask = torch.Tensor(*size).fill_(<span class=\"number\">0</span>)</span><br><span class=\"line\">    index = index.view(*view)</span><br><span class=\"line\">    ones = <span class=\"number\">1.</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> isinstance(index, Variable):</span><br><span class=\"line\">        ones = Variable(torch.Tensor(index.size()).fill_(<span class=\"number\">1</span>))</span><br><span class=\"line\">        mask = Variable(mask, volatile=index.volatile)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> mask.scatter_(<span class=\"number\">1</span>, index, ones)</span><br><span class=\"line\">    </span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">FocalLoss</span><span class=\"params\">(nn.Module)</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self, gamma=<span class=\"number\">0</span>, eps=<span class=\"number\">1e-7</span>)</span>:</span></span><br><span class=\"line\">        super(FocalLoss, self).__init__()</span><br><span class=\"line\">        self.gamma = gamma</span><br><span class=\"line\">        self.eps = eps</span><br><span class=\"line\">        </span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">forward</span><span class=\"params\">(self, input, target)</span>:</span></span><br><span class=\"line\">        y = one_hot(target, input.size(<span class=\"number\">-1</span>))</span><br><span class=\"line\">        logit = F.softmax(input)</span><br><span class=\"line\">        logit = logit.clamp(self.eps, <span class=\"number\">1.</span> - self.eps)</span><br><span class=\"line\">        loss = <span class=\"number\">-1</span> * y * torch.log(logit) <span class=\"comment\"># cross entropy</span></span><br><span class=\"line\">        loss = loss * (<span class=\"number\">1</span> - logit) ** self.gamma <span class=\"comment\"># focal loss</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> loss.sum()</span><br></pre></td></tr></table></figure>\n<p><strong></strong>:</p>\n<ol>\n<li> 0.01Focal Lossimagenetbase net$\\sigma$=0.010.$b=-log(\\frac{(1-\\pi)}{\\pi})$, $\\pi$anchor<br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/34.png\" alt=\"image\"></li>\n</ol>\n<p>ResNetFeature Pyramid NetFPNone-stage.RetinaNet </p>\n<ul>\n<li>(small FCN attached to each FPN level)</li>\n<li>(attach another samll FCN to each pyramid level)</li>\n<li></li>\n</ul>\n<p>Anchors:<br>P3P7$32^{2}$$512^{2}$, [1:2,1:1,2:1]. $[2^{0},2^{\\frac{1}{3}},2^{\\frac{1}{2}}]$</p>\n<p>IOU:<br>[0,0.4)[0.4,0.5)0.5</p>\n<h2 id=\"Two-Stage\"><a href=\"#Two-Stage\" class=\"headerlink\" title=\"Two Stage\"></a>Two Stage</h2><h3 id=\"R-FCN-Object-Detection-via-Region-based-Fully-Convolutional-Networks\"><a href=\"#R-FCN-Object-Detection-via-Region-based-Fully-Convolutional-Networks\" class=\"headerlink\" title=\"R-FCN Object Detection via Region-based Fully Convolutional Networks\"></a>R-FCN Object Detection via Region-based Fully Convolutional Networks</h3><p><a href=\"https://link.jianshu.com/?t=https%3A%2F%2Farxiv.org%2Fpdf%2F1605.06409.pdf\" target=\"_blank\" rel=\"noopener\"></a></p>\n<p>R-FCN  Position-sensitive score map  ROI pooling  Faster R-CNN </p>\n<p>PS: </p>\n<ul>\n<li><p>ROIResNet-101conv5mAP68.9%ROIconv5Faster R-CNNmAP76.4%</p>\n</li>\n<li><p>Faster R-CNNROIproposal300proposalROI300, </p>\n</li>\n</ul>\n<p><strong></strong><br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/19.png\" alt=\"image\"></p>\n<ul>\n<li>Backbone architecture: ResNet-10110010002048102411</li>\n<li>$k^{2}(C+1)$Conv: ResNet101W*H*1024$k^{2}(C+1)$1024*1*1$k^{2}(C+1)$W*Hposition sensitivescore mappredictionk = 3ROI3*39</li>\n<li>ROI pooling: SPPROIfeature map<img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/21.png\" alt=\"image\"></li>\n<li>Vote:k*kbinscoresoftmax<img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/22.png\" alt=\"image\"></li>\n</ul>\n<p><strong></strong>:</p>\n<ul>\n<li> $L(s,t_{x,y,w,h})=L_{cls}(s_{c^{\\star}}+ \\lambda [c^{\\star}&gt;0]L_{reg}(t,t^{\\star})$<br>IOU0.5ROI</li>\n<li><strong>OHEM</strong>): easy exampleshard examplesS-OHEM: Stratified Online Hard Example Mining for Object Detection. <strong>S-OHEM</strong> OHEMstratified samplingOHEMS-OHEMA-Fast-RCNN: Hard positive generation via adversary for object detectionOHEMS-OHEMA-Fast-RCNN<strong>GAN</strong></li>\n<li><strong></strong>:FCNResNet-1013216conv5stride=2stride=1conv5hole algorithm .</li>\n</ul>\n<p><strong></strong><br>M  5*5  3*3  M <img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/23.png\" alt=\"image\"></p>\n<p> 9  9 position-sensitive score maps<img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/24.png\" alt=\"image\"><br> ROIs  3*3  3*3  [0][0] <img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/25.png\" alt=\"image\"><br> ROIs  ROI position-sensitive ROI-pool<br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/26.png\" alt=\"image\"><br> ROI <br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/27.png\" alt=\"image\"><br> C  C+1 33  (C+1)33  softmax </p>\n<h3 id=\"FPN-Feature-Pyramid-Networks-for-Object-Detection\"><a href=\"#FPN-Feature-Pyramid-Networks-for-Object-Detection\" class=\"headerlink\" title=\"FPN Feature Pyramid Networks for Object Detection\"></a>FPN Feature Pyramid Networks for Object Detection</h3><p><a href=\"http://openaccess.thecvf.com/content_cvpr_2017/papers/Lin_Feature_Pyramid_Networks_CVPR_2017_paper.pdf\" target=\"_blank\" rel=\"noopener\"></a></p>\n<p><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/28.png\" alt=\"image\"></p>\n<p>a. </p>\n<p>b. ConvNET</p>\n<p>c. </p>\n<p>d. FPN</p>\n<p>FPN feature map  feature map  element-wise  feature map  feature map </p>\n<p><strong>FPN</strong>:<br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/29.png\" alt=\"image\"></p>\n<ol>\n<li>feature map</li>\n<li>bottom-up  top-down  bottom-up Top-down</li>\n<li>top-downbottom-upx2bottom-up1x1</li>\n<li><br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/30.png\" alt=\"image\"></li>\n</ol>\n<p><strong>RPNFPN</strong></p>\n<ol>\n<li>FPNRPN3x3 conv/ 1x1convs$[P_{2},P_{3},P_{4},P_{5},P_{6}]$$[32^{2},64^{2},128^{2},256^{2},512^{2}]$ {1:2,1:1,2:1}15</li>\n<li>RPN</li>\n<li>ROIboxROIP5P4<br>$k=[k_{0}+log_{2}(\\frac{\\sqrt{wh}}{224}]$ <br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/31.png\" alt=\"image\"></li>\n</ol>\n<h3 id=\"Light-Head-R-CNN-In-Defense-of-Two-Stage-Object-Detector\"><a href=\"#Light-Head-R-CNN-In-Defense-of-Two-Stage-Object-Detector\" class=\"headerlink\" title=\"Light-Head R-CNN: In Defense of Two-Stage Object Detector\"></a>Light-Head R-CNN: In Defense of Two-Stage Object Detector</h3><p><a href=\"https://arxiv.org/abs/1711.07264\" target=\"_blank\" rel=\"noopener\"></a></p>\n<p>two-stage  classification+regression </p>\n<ul>\n<li>Faster R-CNN: two fully connected layers for RoI recognition</li>\n<li>R-FCN: produces a large score maps.</li>\n</ul>\n<p> detection  two-stage detectorLight-Head R-CNNLight-Head R-CNN  head  R-CNN subnetROI pooling network ROI warping</p>\n<p><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/35.png\" alt=\"image\"></p>\n<p><strong></strong></p>\n<ul>\n<li><strong>Thin feature map</strong>: <img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/36.png\" alt=\"image\">headfeature mapchannelr-fcnscore map$P\\times P(C+1)$$P\\times P\\times \\alpha$$\\alpha$10score mapchannelr-fcnvoteroi poolingfc</li>\n<li><strong>Large separable convolution</strong>: </li>\n</ul>\n<p><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/38.png\" alt=\"image\"><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/39.png\" alt=\"image\">  </p>\n<ul>\n<li><strong>Cheap R-CNN</strong>: <img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/37.png\" alt=\"image\"> </li>\n</ul>\n<p><strong></strong></p>\n<ul>\n<li><strong>Large</strong>: (1) ResNEt-101; (2) atrous algorithm; (3) RPN chanels 512; (4) Large separable convolution with $C_{mid}=256$</li>\n<li><strong>Small</strong>: (1) Xception like; (2) abbandon atrous algorithm; (3) RPN convolution to 256; (4) Large separable convolution with $C_{mid}=64$; (5) Apply PSPooling with alignment techniques as RoI warping(better results involve RoI-align).</li>\n</ul>\n","site":{"data":{}},"excerpt":"<h1 id=\"Object-detection\"><a href=\"#Object-detection\" class=\"headerlink\" title=\"Object detection\"></a>Object detection</h1><p><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/14.png\" alt=\"image\"></p>\n<table>\n<thead>\n<tr>\n<th>one-stage</th>\n<th>two-stage</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>YOLO V1,V2,V3</td>\n<td>FPN</td>\n</tr>\n<tr>\n<td>SSD</td>\n<td>RFCN</td>\n</tr>\n<tr>\n<td>RetinalNet</td>\n<td>LIghthead</td>\n</tr>\n</tbody>\n</table>","more":"<h2 id=\"One-Stage\"><a href=\"#One-Stage\" class=\"headerlink\" title=\"One Stage\"></a>One Stage</h2><h3 id=\"YOLO-V1\"><a href=\"#YOLO-V1\" class=\"headerlink\" title=\"YOLO V1\"></a>YOLO V1</h3><p><strong> You only look once unified real-time object detection</strong></p>\n<p><a href=\"https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Redmon_You_Only_Look_CVPR_2016_paper.pdf\" target=\"_blank\" rel=\"noopener\">PAPER ADDRESS</a></p>\n<p>YOLOobject detectionbounding box</p>\n<p><strong></strong>: </p>\n<ol>\n<li>YOLOTitan X GPU45 fps, 155 fps</li>\n<li>YOLOsliding windowregion proposalFast R-CNNYOLO</li>\n<li></li>\n</ol>\n<p><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/1.png\" alt=\"image\"></p>\n<p><strong></strong>:</p>\n<ol>\n<li>accuracy  state-of-the-art </li>\n<li></li>\n<li></li>\n<li></li>\n</ol>\n<p><strong></strong></p>\n<ol>\n<li>$448\\times448$.</li>\n<li></li>\n<li>NMX()</li>\n</ol>\n<p><strong>Unified Detection</strong><br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/2.png\" alt=\"image\"></p>\n<ol>\n<li>$S\\times S$</li>\n<li>B5x,t,w,hconfidencex,ybounding boxgrid cellgrid cell01whwhwh01</li>\n<li>C</li>\n<li>Pascal VOC S=7,B=2,C=20.  $S\\times S\\times (B \\times 5 + C)$  $7\\times 7\\times 30$ </li>\n</ol>\n<p>Confidence$Pr(Object) * IOU_{pred}^{turth} $</p>\n<p>bounding boxconfidence scoregrid cellobjectconfidence0confidence scoreboxground truthIOUobjectground truthgrid cellgrid cellobjectobjectgrid cell<br>grid cellCgrid cellobject$Pr(Class_{i}|Object)$</p>\n<p>bounding boxconfidencescorebounding boxconfidence score</p>\n<p>bounding boxconfidence score20*(7*7*2)scorebounding box20200.20NMSbounding boxNMS:bounding boxbounding boxIOUIOU0.500.5scorebounding boxbounding boxbounding boxIOUbounding box20scorescorescore0bounding boxsocre0bounding box</p>\n<p><strong></strong><br>GoogLeNet,<br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/3.png\" alt=\"image\"><br></p>\n<ol>\n<li>ImageNet20224*22442448*448</li>\n<li>Reluleaky Relux&lt;00.1*x0</li>\n<li>sum-squared errorlocalization errorbounding boxclassificaton errorgrid cellgrid cellconfidence score0localization errorobjectboxconfidence lossloss50.5objectboxconfidence loss1</li>\n<li>.</li>\n<li>Loss Function<img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/4.png\" alt=\"image\"></li>\n</ol>\n<p>NMobjecobject4xywh1label7*7*301*305bounding box41confidence610bounding box41confidence20grid cell30 confidenceground truthbounding boxIOUobject0,1confidence01object1object0 loss function</p>\n<p>7*7*30IOUbounding boxconfidencebounding box</p>\n<p><strong>YOLO</strong><br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/5.png\" alt=\"image\"><br>yoloFaster RCNN,YOLOFaster RCNN</p>\n<h3 id=\"YOLO-V2-and-YOLO-9000\"><a href=\"#YOLO-V2-and-YOLO-9000\" class=\"headerlink\" title=\"YOLO V2 and YOLO 9000\"></a>YOLO V2 and YOLO 9000</h3><p><a href=\"https://arxiv.org/pdf/1612.08242.pdf\" target=\"_blank\" rel=\"noopener\"></a></p>\n<p></p>\n<p><strong>YOLO V1</strong></p>\n<ol>\n<li>YOLOYOLOFast R-CNNYOLOYOLO</li>\n<li><p>YOLOv2YOLO</p>\n<ul>\n<li>Batch Normalization: map2%</li>\n<li>High Resolution Classifier: ImageNet44844810map4%,</li>\n<li>Convolutional With Anchor Boxes: YOLO416416448448YOLO324161313map88%.</li>\n<li>Dimension Clusters: Anchors k-meansk-meansIOU$d(box,centroid)=1-IOU(box,centroid)$. voccocok=5<img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/6.png\" alt=\"image\"></li>\n<li>Direct location prediction: RPN(x,y)Sigmoid0~1<img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/7.png\" alt=\"image\">55$t_{x}$,$t_{y}$,$t_{w}$,$t_{h}$,$t_{o}$,$(c_{x},c_{y})$,$p_{w}$,$p_{h}$. $Pr(object)\\times IOU(b,object)=\\sigma(t_{o}) $ map 5%</li>\n<li>Fine-Grained Features(): ResNet2626512131320481%</li>\n<li>Multi-Scale Training:  103306083232<img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/8.png\" alt=\"image\"></li>\n</ul>\n</li>\n<li><p>Darknet-19 19555.8VGGYOLOtop-5ImageNet<br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/9.png\" alt=\"image\"></p>\n</li>\n<li>Darknet0.140.00050.9ImageNet 1000160over-fitting. 224224448</li>\n<li>10243311VOC552012533512</li>\n<li><ul>\n<li>YOLOv2</li>\n<li>Hierarchical classification()ImageNetWordNetWordTreeWordTreeWordTreesynsets<img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/10.png\" alt=\"image\"></li>\n<li>YOLO 9000anchors3: COCOImageNet9000WordTree9418ImageNetCOCOImageNet4:1YOLO9000ImageNetCOCOCOCO3</li>\n</ul>\n</li>\n</ol>\n<h3 id=\"YOLO-V3\"><a href=\"#YOLO-V3\" class=\"headerlink\" title=\"YOLO V3\"></a>YOLO V3</h3><p><a href=\"https://pjreddie.com/media/files/papers/YOLOv3.pdf\" target=\"_blank\" rel=\"noopener\"></a></p>\n<p></p>\n<ol>\n<li>YOLO V3V2Softmax lossLogistic loss(logistic)GT.</li>\n<li>Anchor bbox priorV25anchorV39anchorIOU.</li>\n<li>DetectionV2detectionV33Feature map13*132eltwise sum(feature pyramid networks)Feature map26<em>\\2652\\</em>52V341652Feature mapV2data13Feature map<br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/11.png\" alt=\"image\"></li>\n<li> DarkNet-53: YOLOv2Darknet-193311shortcut connection53<img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/12.png\" alt=\"image\"></li>\n<li>YOLO V3Pascal Titan X608x60820FPS COCO test-dev  <a href=\"mailto:mAP@0.5\" target=\"_blank\" rel=\"noopener\">mAP@0.5</a>  57.9%RetinaNet4  YOLO V3  <img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/13.png\" alt=\"image\"></li>\n<li><ul>\n<li>Anchor box</li>\n<li>x,y</li>\n<li>focal loss</li>\n<li>IOU</li>\n</ul>\n</li>\n</ol>\n<h3 id=\"SSD-Single-Shot-MultiBox-Detector\"><a href=\"#SSD-Single-Shot-MultiBox-Detector\" class=\"headerlink\" title=\"SSD: Single Shot MultiBox Detector\"></a>SSD: Single Shot MultiBox Detector</h3><p><a href=\"https://arxiv.org/pdf/1512.02325.pdf\" target=\"_blank\" rel=\"noopener\"></a></p>\n<p>SSD</p>\n<p><strong></strong>:</p>\n<ol>\n<li></li>\n<li></li>\n<li></li>\n<li></li>\n</ol>\n<p><strong></strong>:<br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/15.png\" alt=\"image\"><br>SSD</p>\n<ul>\n<li><strong></strong></li>\n<li><strong></strong></li>\n<li><strong></strong>Faster R-CNN[2]<img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/16.png\" alt=\"image\"></li>\n<li> PS: VGG</li>\n</ul>\n<p><strong></strong></p>\n<ol>\n<li><strong></strong>IOU0.5</li>\n<li><strong></strong><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/17.png\" alt=\"image\"></li>\n<li><strong></strong>: <img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/18.png\" alt=\"image\"></li>\n<li><strong></strong>: 3:1</li>\n<li><strong></strong><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/20.png\" alt=\"image\"></li>\n<li><strong></strong> 16x</li>\n</ol>\n<p><strong></strong>:<br>SSDSSD300300512512SSD</p>\n<h3 id=\"RetinaNet-Focal-Loss-for-Dense-Object-Detection\"><a href=\"#RetinaNet-Focal-Loss-for-Dense-Object-Detection\" class=\"headerlink\" title=\"RetinaNet Focal Loss for Dense Object Detection\"></a>RetinaNet Focal Loss for Dense Object Detection</h3><p><a href=\"https://arxiv.org/abs/1708.02002\" target=\"_blank\" rel=\"noopener\"></a></p>\n<p><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/32.png\" alt=\"image\"></p>\n<p>central issusproposalYOLOPASCAL VOCYOLO V213135845losslossone-stageRetinaNettrade-off</p>\n<p>$FL(p_{t}=-(1-p_{t})^{\\gamma}log(p_{t})$</p>\n<p><strong>Focal Loss</strong><br>Focal Loss<br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/33.png\" alt=\"image\"></p>\n<p></p>\n<p>$CE(p,y)=-log(p_{y})$</p>\n<p><br>$CE(p)=-\\alpha_{t}log(p_{t})$</p>\n<p>($\\alpha$)</p>\n<p><strong>$FL(p_{t}=-(1-p_{t})^{\\gamma}log(p_{t})$</strong></p>\n<p>Pytorch</p>\n<p>$L=-\\sum_{i}^{C}onehot\\odot(1-p_{t})^{\\gamma}log(p_{t})$</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">import</span> torch.nn <span class=\"keyword\">as</span> nn</span><br><span class=\"line\"><span class=\"keyword\">import</span> torch.nn.functional <span class=\"keyword\">as</span> F</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch.autograd <span class=\"keyword\">import</span> Variable</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">one_hot</span><span class=\"params\">(index, classes)</span>:</span></span><br><span class=\"line\">    size = index.size() + (classes,)</span><br><span class=\"line\">    view = index.size() + (<span class=\"number\">1</span>,)</span><br><span class=\"line\">    mask = torch.Tensor(*size).fill_(<span class=\"number\">0</span>)</span><br><span class=\"line\">    index = index.view(*view)</span><br><span class=\"line\">    ones = <span class=\"number\">1.</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> isinstance(index, Variable):</span><br><span class=\"line\">        ones = Variable(torch.Tensor(index.size()).fill_(<span class=\"number\">1</span>))</span><br><span class=\"line\">        mask = Variable(mask, volatile=index.volatile)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> mask.scatter_(<span class=\"number\">1</span>, index, ones)</span><br><span class=\"line\">    </span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">FocalLoss</span><span class=\"params\">(nn.Module)</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self, gamma=<span class=\"number\">0</span>, eps=<span class=\"number\">1e-7</span>)</span>:</span></span><br><span class=\"line\">        super(FocalLoss, self).__init__()</span><br><span class=\"line\">        self.gamma = gamma</span><br><span class=\"line\">        self.eps = eps</span><br><span class=\"line\">        </span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">forward</span><span class=\"params\">(self, input, target)</span>:</span></span><br><span class=\"line\">        y = one_hot(target, input.size(<span class=\"number\">-1</span>))</span><br><span class=\"line\">        logit = F.softmax(input)</span><br><span class=\"line\">        logit = logit.clamp(self.eps, <span class=\"number\">1.</span> - self.eps)</span><br><span class=\"line\">        loss = <span class=\"number\">-1</span> * y * torch.log(logit) <span class=\"comment\"># cross entropy</span></span><br><span class=\"line\">        loss = loss * (<span class=\"number\">1</span> - logit) ** self.gamma <span class=\"comment\"># focal loss</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> loss.sum()</span><br></pre></td></tr></table></figure>\n<p><strong></strong>:</p>\n<ol>\n<li> 0.01Focal Lossimagenetbase net$\\sigma$=0.010.$b=-log(\\frac{(1-\\pi)}{\\pi})$, $\\pi$anchor<br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/34.png\" alt=\"image\"></li>\n</ol>\n<p>ResNetFeature Pyramid NetFPNone-stage.RetinaNet </p>\n<ul>\n<li>(small FCN attached to each FPN level)</li>\n<li>(attach another samll FCN to each pyramid level)</li>\n<li></li>\n</ul>\n<p>Anchors:<br>P3P7$32^{2}$$512^{2}$, [1:2,1:1,2:1]. $[2^{0},2^{\\frac{1}{3}},2^{\\frac{1}{2}}]$</p>\n<p>IOU:<br>[0,0.4)[0.4,0.5)0.5</p>\n<h2 id=\"Two-Stage\"><a href=\"#Two-Stage\" class=\"headerlink\" title=\"Two Stage\"></a>Two Stage</h2><h3 id=\"R-FCN-Object-Detection-via-Region-based-Fully-Convolutional-Networks\"><a href=\"#R-FCN-Object-Detection-via-Region-based-Fully-Convolutional-Networks\" class=\"headerlink\" title=\"R-FCN Object Detection via Region-based Fully Convolutional Networks\"></a>R-FCN Object Detection via Region-based Fully Convolutional Networks</h3><p><a href=\"https://link.jianshu.com/?t=https%3A%2F%2Farxiv.org%2Fpdf%2F1605.06409.pdf\" target=\"_blank\" rel=\"noopener\"></a></p>\n<p>R-FCN  Position-sensitive score map  ROI pooling  Faster R-CNN </p>\n<p>PS: </p>\n<ul>\n<li><p>ROIResNet-101conv5mAP68.9%ROIconv5Faster R-CNNmAP76.4%</p>\n</li>\n<li><p>Faster R-CNNROIproposal300proposalROI300, </p>\n</li>\n</ul>\n<p><strong></strong><br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/19.png\" alt=\"image\"></p>\n<ul>\n<li>Backbone architecture: ResNet-10110010002048102411</li>\n<li>$k^{2}(C+1)$Conv: ResNet101W*H*1024$k^{2}(C+1)$1024*1*1$k^{2}(C+1)$W*Hposition sensitivescore mappredictionk = 3ROI3*39</li>\n<li>ROI pooling: SPPROIfeature map<img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/21.png\" alt=\"image\"></li>\n<li>Vote:k*kbinscoresoftmax<img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/22.png\" alt=\"image\"></li>\n</ul>\n<p><strong></strong>:</p>\n<ul>\n<li> $L(s,t_{x,y,w,h})=L_{cls}(s_{c^{\\star}}+ \\lambda [c^{\\star}&gt;0]L_{reg}(t,t^{\\star})$<br>IOU0.5ROI</li>\n<li><strong>OHEM</strong>): easy exampleshard examplesS-OHEM: Stratified Online Hard Example Mining for Object Detection. <strong>S-OHEM</strong> OHEMstratified samplingOHEMS-OHEMA-Fast-RCNN: Hard positive generation via adversary for object detectionOHEMS-OHEMA-Fast-RCNN<strong>GAN</strong></li>\n<li><strong></strong>:FCNResNet-1013216conv5stride=2stride=1conv5hole algorithm .</li>\n</ul>\n<p><strong></strong><br>M  5*5  3*3  M <img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/23.png\" alt=\"image\"></p>\n<p> 9  9 position-sensitive score maps<img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/24.png\" alt=\"image\"><br> ROIs  3*3  3*3  [0][0] <img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/25.png\" alt=\"image\"><br> ROIs  ROI position-sensitive ROI-pool<br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/26.png\" alt=\"image\"><br> ROI <br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/27.png\" alt=\"image\"><br> C  C+1 33  (C+1)33  softmax </p>\n<h3 id=\"FPN-Feature-Pyramid-Networks-for-Object-Detection\"><a href=\"#FPN-Feature-Pyramid-Networks-for-Object-Detection\" class=\"headerlink\" title=\"FPN Feature Pyramid Networks for Object Detection\"></a>FPN Feature Pyramid Networks for Object Detection</h3><p><a href=\"http://openaccess.thecvf.com/content_cvpr_2017/papers/Lin_Feature_Pyramid_Networks_CVPR_2017_paper.pdf\" target=\"_blank\" rel=\"noopener\"></a></p>\n<p><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/28.png\" alt=\"image\"></p>\n<p>a. </p>\n<p>b. ConvNET</p>\n<p>c. </p>\n<p>d. FPN</p>\n<p>FPN feature map  feature map  element-wise  feature map  feature map </p>\n<p><strong>FPN</strong>:<br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/29.png\" alt=\"image\"></p>\n<ol>\n<li>feature map</li>\n<li>bottom-up  top-down  bottom-up Top-down</li>\n<li>top-downbottom-upx2bottom-up1x1</li>\n<li><br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/30.png\" alt=\"image\"></li>\n</ol>\n<p><strong>RPNFPN</strong></p>\n<ol>\n<li>FPNRPN3x3 conv/ 1x1convs$[P_{2},P_{3},P_{4},P_{5},P_{6}]$$[32^{2},64^{2},128^{2},256^{2},512^{2}]$ {1:2,1:1,2:1}15</li>\n<li>RPN</li>\n<li>ROIboxROIP5P4<br>$k=[k_{0}+log_{2}(\\frac{\\sqrt{wh}}{224}]$ <br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/31.png\" alt=\"image\"></li>\n</ol>\n<h3 id=\"Light-Head-R-CNN-In-Defense-of-Two-Stage-Object-Detector\"><a href=\"#Light-Head-R-CNN-In-Defense-of-Two-Stage-Object-Detector\" class=\"headerlink\" title=\"Light-Head R-CNN: In Defense of Two-Stage Object Detector\"></a>Light-Head R-CNN: In Defense of Two-Stage Object Detector</h3><p><a href=\"https://arxiv.org/abs/1711.07264\" target=\"_blank\" rel=\"noopener\"></a></p>\n<p>two-stage  classification+regression </p>\n<ul>\n<li>Faster R-CNN: two fully connected layers for RoI recognition</li>\n<li>R-FCN: produces a large score maps.</li>\n</ul>\n<p> detection  two-stage detectorLight-Head R-CNNLight-Head R-CNN  head  R-CNN subnetROI pooling network ROI warping</p>\n<p><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/35.png\" alt=\"image\"></p>\n<p><strong></strong></p>\n<ul>\n<li><strong>Thin feature map</strong>: <img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/36.png\" alt=\"image\">headfeature mapchannelr-fcnscore map$P\\times P(C+1)$$P\\times P\\times \\alpha$$\\alpha$10score mapchannelr-fcnvoteroi poolingfc</li>\n<li><strong>Large separable convolution</strong>: </li>\n</ul>\n<p><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/38.png\" alt=\"image\"><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/39.png\" alt=\"image\">  </p>\n<ul>\n<li><strong>Cheap R-CNN</strong>: <img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/37.png\" alt=\"image\"> </li>\n</ul>\n<p><strong></strong></p>\n<ul>\n<li><strong>Large</strong>: (1) ResNEt-101; (2) atrous algorithm; (3) RPN chanels 512; (4) Large separable convolution with $C_{mid}=256$</li>\n<li><strong>Small</strong>: (1) Xception like; (2) abbandon atrous algorithm; (3) RPN convolution to 256; (4) Large separable convolution with $C_{mid}=64$; (5) Apply PSPooling with alignment techniques as RoI warping(better results involve RoI-align).</li>\n</ul>"},{"title":"(Logbook) -- Object Detection System Based on CNN and Capsule Network","date":"2018-05-24T16:00:00.000Z","_content":"\n## Gantt chart\n![image](https://raw.githubusercontent.com/Trouble404/Object-Detection-System-Based-on-CNN-and-Capsule-Network/master/Project%20Plan/gantt%20chart%20of%20project.PNG)\n<!-- more -->\n---\n\n## Check list\n- [x] **1) preparation**\n- [x] ***1.1) Familiarization with develop tools***\n- [x] 1.1.1) Keras\n- [x] 1.1.2) Pythrch\n- [x] ***1.2) Presentation***\n- [x] 1.2.1) Poster conference\n- [x] **2) Create image database**\n- [x] 2.1) Confirmation of detected objects\n- [x] 2.2) Collect and generate the dataset\n- [x] **3) Familiarization with CNN based object detection methods**\n- [x] 3.1) R-CNN\n- [x] 3.2) SPP-net\n- [x] 3.3) Fast R-CNN\n- [x] 3.4) Faster R-CNN\n- [x] **4) Implement object detection system based on one chosen CNN method**\n- [x] 4.1) Pre-processing of images\n- [x] 4.2) Extracting features\n- [x] 4.3) Mode architecture\n- [x] 4.4) Train model and optimization\n- [x] 4.5) Models ensemble\n- [x] **5) Analysis work**\n- [x] 5.1) Evaluation of detection result.\n- [x] **6) Paperwork and bench inspection**\n- [x] 6.1) Logbook\n- [x] 6.2) Write the thesis\n- [x] 6.3) Project video\n- [x] 6.4) Speech and ppt of bench inspection\n- [x] **7) Documents**\n- [x] 7.1) Project Brief\n\n---\n\n## May\n### 28/05/2018\nKeras is a high-level neural networks API, written in Python and capable of running on top of [TensorFlow](https://github.com/tensorflow/tensorflow), CNTK, or Theano.\n\n* **[Keras document](https://keras.io/)**\n\n* **[Keras ](https://keras-cn.readthedocs.io/en/latest/#keraspython)**\n\n---\n#### Installation\n\n* **TensorFlow**\n  [Microsoft Visual Studio 2015](https://www.visualstudio.com/zh-hans/vs/older-downloads/)\n  [CUDA 9.0](http://docs.nvidia.com/cuda/cuda-installation-guide-microsoft-windows/)\n  [cuDNN7](https://developer.nvidia.com/cudnn)\n  [Anaconda](https://www.anaconda.com/download/)\n  \n  * Step 1: Install VS2015\n  * Step 2: Install CUDA 9.0 \n  * Step 3: Install cuDNN7 cudnnbinPATH\n  * Step 4: Install Anaconda PATH,  **Python 3.5**\n  * Step 5: Anaconda ,jupyterbook\n  {% codeblock %}\n  conda create  --name tensorflow python=3.5\n  activate tensorflow\n  conda install nb_conda\n  {% endcodeblock %}\n  * Step 6: Install GPU version TensorFlow.\n  {% codeblock %}\n  pip install -i https://pypi.tuna.tsinghua.edu.cn/simple --ignore-installed --upgrade tensorflow-gpu \n  {% endcodeblock %}\n  \n* **Keras**\n  * Step 1:   Keras GPU \n    {% codeblock %}\n    activate tensorflow\n    pip install keras -U --pre\n    {% endcodeblock %}\n  \n#### Keras**\n* **[NBA with Machine Learning](https://github.com/Trouble404/NBA-with-Machine-Learning)**\n* **[Kaggle- Job salary prediction](https://github.com/Trouble404/kaggle-Job-Salary-Prediction)**\n \n#### TensorFlow CPU \n```python\nimport tensorflow as tf  \nimport os\nimport keras.backend.tensorflow_backend as KTF  \n\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  #GPU\nconfig = tf.ConfigProto()\nconfig.gpu_options.per_process_gpu_memory_fraction = 0.4 #GPUGPU\nsess = tf.Session(config=config)\nKTF.set_session(sess)\n\nwith tf.device('/cpu:0'):\n```\nGPUCPU\n\n#### Jupyter Notebook \n \n```\njupyter notebook --generate-config\n```\n![image](https://raw.githubusercontent.com/Trouble404/Object-Detection-System-Based-on-CNN-and-Capsule-Network/master/Learned%20skills/jupyter%20_setting.PNG)\n\n## June\n### 01/06/2018\n**[PyTorch](https://pytorch.org/about/)** is a python package that provides two high-level features:\n* Tensor computation (like numpy) with strong GPU acceleration\n* Deep Neural Networks built on a tape-based autodiff system\n\n| Package | Description |\n|:----|:----|\n|torch|a Tensor library like NumPy, with strong GPU support|\n|torch.autograd|a tape based automatic differentiation library that supports all differentiable Tensor operations in torch|\n|torch.nn|a neural networks library deeply integrated with autograd designed for maximum flexibility|\n|torch.optim|an optimization package to be used with torch.nn with standard optimization methods such as SGD, RMSProp, LBFGS, Adam etc.|\n|torch.multiprocessing|python multiprocessing, but with magical memory sharing of torch Tensors across processes. Useful for data loading and hogwild training.|\n|torch.utils|DataLoader, Trainer and other utility functions for convenience|\n|torch.legacy(.nn/.optim)|legacy code that has been ported over from torch for backward compatibility reasons|\n\n---\n\n#### Installation\n\n* Step 1: Anaconda ,jupyterbook\n  {% codeblock %}\n  conda create  --name pytorch python=3.5\n  activate pytorch\n  conda install nb_conda\n  {% endcodeblock %}\n* Step 2: Install GPU version PyTorch.\n  {% codeblock %}\n  conda install pytorch cuda90 -c pytorch \n  pip install torchvision\n  {% endcodeblock %}\n\n#### Understanding of PyTorch\n\n* **Tensors**\n  Tensorsnumpyndarrays, TensorGPU\n  ```python\n  from __future__ import print_function\n  import torch\n  x = torch.Tensor(5, 3)  # 5*3\n  x = torch.rand(5, 3)  # \n  x # notebookxx\n  x.size()\n\n  #NOTE: torch.Size tuple, *\n  y = torch.rand(5, 3)\n\n  # \n  x + y # \n  torch.add(x, y) # \n\n  # tensor\n  result = torch.Tensor(5, 3) # \n  torch.add(x, y, out=result) # \n  y.add_(x) # yx\n\n  # tensor'_'\n  # x.copy_(y), x.t_(), x\n\n  #python\n  x[:,1] #x\n  ```\n\n* **Numpy**\n  TorchTensornumpyarrayTorchTensornumpyarray\n  \n  ```python\n# tensornumpy\na = torch.ones(5)\nb = a.numpy()\n\n# numpy,tensor\na.add_(1)\nprint(a)\nprint(b)\n\n# numpyArraytorchTensor\nimport numpy as np\na = np.ones(5)\nb = torch.from_numpy(a)\nnp.add(a, 1, out=a)\nprint(a)\nprint(b)\n\n# CharTensortensorCPUGPU\n# CUDATensorGPU\n# CUDAGPU\nif torch.cuda.is_available():\n    x = x.cuda()\n    y = y.cuda() \n  ```\n* **PyTorchCIFAR10**\n**[code](https://github.com/Trouble404/Object-Detection-System-Based-on-CNN-and-Capsule-Network/blob/master/Learned%20skills/pytorch.ipynb)**\n\n* **MMdnn**\n  MMdnn is a set of tools to help users inter-operate among different deep learning frameworks. E.g. model conversion and visualization. Convert models between Caffe, Keras, MXNet, Tensorflow, CNTK, PyTorch Onnx and CoreML.\n  \n  ![iamge](https://raw.githubusercontent.com/Microsoft/MMdnn/master/docs/supported.jpg)\n  \n  MMdnn\n\n  * DNN\n  * \n  * DNN\n  * \n \n **[Github](https://github.com/Microsoft/MMdnn)**\n \n### 04/06/2018\n#### **Dataset:**\n **[VOC 2012 Dataset](http://host.robots.ox.ac.uk/pascal/VOC/voc2012/index.html)**\n \n#### **Introduce:**\n **Visual Object Classes Challenge 2012 (VOC2012)**\n[PASCAL](http://host.robots.ox.ac.uk/pascal/VOC/)'s full name is Pattern Analysis, Statistical Modelling and Computational Learning.\nVOC's full name is **Visual OBject Classes**.\nThe first competition was held in 2005 and terminated in 2012. I will use the last updated dataset which is [VOC2012](http://host.robots.ox.ac.uk/pascal/VOC/voc2012/index.html) dataset.\n\nThe main aim of this competition is object detection, there are 20 classes objects in the dataset:\n* person\n* bird, cat, cow, dog, horse, sheep\n* aeroplane, bicycle, boat, bus, car, motorbike, train\n* bottle, chair, dining table, potted plant, sofa, tv/monitor\n\n#### **Detection Task**\nReferenced: \n**The PASCAL Visual Object Classes Challenge 2012 (VOC2012) Development Kit**\n**Mark Everingham - John Winn**\nhttp://host.robots.ox.ac.uk/pascal/VOC/voc2012/htmldoc/index.html\n\n**Task:**\nFor each of the twenty classes predict the bounding boxes of each object of that class in a test image (if any). Each bounding box should be output with an associated real-valued confidence of the detection so that a precision/recall curve can be drawn. Participants may choose to tackle all, or any subset of object classes, for example 'cars only' or 'motorbikes and cars'.\n\n**Competitions**:\nTwo competitions are defined according to the choice of training data:\n*  taken from the $VOC_{trainval}$ data provided.\n*  from any source excluding the $VOC_{test}$ data provided.\n\n**Submission of Results**:\nA separate text file of results should be generated for each competition and each class e.g. \\`car'. Each line should be a detection output by the detector in the following format:\n    ```\n    <image identifier> <confidence> <left> <top> <right> <bottom>\n    ```\nwhere (left,top)-(right,bottom) defines the bounding box of the detected object. The top-left pixel in the image has coordinates $(1,1)$. Greater confidence values signify greater confidence that the detection is correct. An example file excerpt is shown below. Note that for the image 2009_000032, multiple objects are detected:\n```\ncomp3_det_test_car.txt:\n    ...\n    2009_000026 0.949297 172.000000 233.000000 191.000000 248.000000\n    2009_000032 0.013737 1.000000 147.000000 114.000000 242.000000\n    2009_000032 0.013737 1.000000 134.000000 94.000000 168.000000\n    2009_000035 0.063948 455.000000 229.000000 491.000000 243.000000\n    ...\n```\n\n**Evaluation**:\nThe detection task will be judged by the precision/recall curve. The principal quantitative measure used will be the average precision (AP). Detections are considered true or false positives based on the area of overlap with ground truth bounding boxes. To be considered a correct detection, the area of overlap $a_o$ between the predicted bounding box $B_p$ and ground truth bounding box $B_{gt}$ must exceed $50\\%$ by the formula: \n<center> $a_o = \\frac{area(B_p \\cap B_{gt})}{area(B_p \\cup B_{gt})}$ </center>\n\n #### **XML**\n xmlgemfield8xml8xmlxml\n ```html\n <?xml version=\"1.0\" encoding=\"utf-8\"?>\n<annotation>\n    <folder>VOC2007</folder>\n    <filename>test100.mp4_3380.jpeg</filename>\n    <size>\n        <width>1280</width>\n        <height>720</height>\n        <depth>3</depth>\n    </size>\n    <object>\n        <name>gemfield</name>\n        <bndbox>\n            <xmin>549</xmin>\n            <xmax>715</xmax>\n            <ymin>257</ymin>\n            <ymax>289</ymax>\n        </bndbox>\n        <truncated>0</truncated>\n        <difficult>0</difficult>\n    </object>\n    <object>\n        <name>civilnet</name>\n        <bndbox>\n            <xmin>842</xmin>\n            <xmax>1009</xmax>\n            <ymin>138</ymin>\n            <ymax>171</ymax>\n        </bndbox>\n        <truncated>0</truncated>\n        <difficult>0</difficult>\n    </object>\n    <segmented>0</segmented>\n</annotation>\n```\n\n2objectgemfieldcivilnet\n\nxml\n* bndbox\n* truncated\n* occluded\n* difficultdifficult\n\n**objectname SSDpy-faster-rcnn**\n\n### 07/06/2018\n#### **Poster conference**\n![iamge](https://raw.githubusercontent.com/Trouble404/Object-Detection-System-Based-on-CNN-and-Capsule-Network/master/Poster/poster.png)\n\n5 People in one group to present their object.\nI present this object to my supervisor in this conference.\n\n### 11/06/2018\n#### **R-CNN**\nPaper: [Rich feature hierarchies for accurate object detection and semantic segmentation](https://arxiv.org/abs/1311.2524)\n\n****\n\n*  (Selective Search)(CNN)\n*      ImageNet ILSVC 20121000 PASCAL VOC 2007   20 CNN   \n\n****\n\n1.  1K~2K Selective Search \n2.   CNN \n3.  SVM \n4.  \n<center>![image](https://raw.githubusercontent.com/Trouble404/Object-Detection-System-Based-on-CNN-and-Capsule-Network/master/Research%20Review/LaTex/1.PNG)</center>\n\n**[Selective Search](https://www.koen.me/research/pub/uijlings-ijcv2013-draft.pdf)**\n1.  (1k~2k )\n2. \n3. \n    \n   * \n   * \n   *   a-b-c-d-e-f-g-hab-cd-ef-gh -> abcd-efgh -> abcdefgh ab-c-d-e-f-g-h ->abcd-e-f-g-h ->abcdef-gh -> abcdefgh\n   * BBOX \n\n### 12/06/2018\n#### **SPP-CNN**\nPaper: [Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition](https://arxiv.org/abs/1406.4729)\n\n****\n\nRCNNCNNRCNN224\\*224CNN\n* region proposalSelective Search2Kregion proposal2KCNN\n* region proposal\n\n\n****\n\n1. selective searchregion proposal\n2. \n   $s \\in S = \\{480,576,688,864,1200\\}$\n   epochmodelmodel1\\*12\\*23\\*36\\*650bins\n3. region proposal224\\*224 \n4. SVMBoundingBox.\n<center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/SPP-NET.jpg)</center>\n\n\n### 13/06/2018\n#### **FAST R-CNN**\nPaper: [Fast R-CNN](https://arxiv.org/abs/1504.08083)\n\n****\n\n* RCNN\n*  \n* : RCNN\n\n\n****\n\n1. convconv\n2. object proposals region of interestRoI\n3. fc\n    KbackgroundsoftmaxRoI\nbbox regressionK4Kbounding-boxtrained end-to-end with a multi-task loss\n<center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/fast-rcnn.png)</center>\n\n### 14~18/06/2018\n#### **FASTER R-CNN**\nI want to use **Faster R-cnn** as the first method to implement object detection system.\n\nPaper: [Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks](https://arxiv.org/abs/1506.01497)\n\nFaster RCNN(feature extraction)proposalbounding box regression(rect refine)classification\n<center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster-rcnn.jpg)</center>\n\n #### \n\n1. Conv layersCNNFaster R-CNNconv+relu+poolingimagefeature mapsfeature mapsRPN\n2. Region Proposal NetworksRPNregion proposalssoftmaxanchorsforegroundbackgroundbounding box regressionanchorsproposals\n3. Roi Poolingfeature mapsproposalsproposal feature maps\n4. Classificationproposal feature mapsproposalbounding box regression\n\n#### \n\n**\\[1. Conv layers\\]**\n   <center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_1.jpg)</center>\n   Conv layersconvpoolingrelupythonVGG16faster_rcnn_test.pt,    Conv layers13conv13relu4poolingConv          layers\n   \n  * conv $kernel\\_size=3$  $pad=1$  $stride=1$ <br>\n  * pooling $kernel\\_size=2$  $pad=0$  $stride=2$\n  \n   Faster RCNN Conv layers $pad=1$ 0                $(M+2)\\times (N+2)$ 3x3 $M\\times N$ Conv layersconv    \n<center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_2.jpg)</center>\n   Conv layerspooling $kernel\\_size=2$  $stride=2$ pooling $M\\times N$  $(M/2) \\times(N/2)$ Conv layersconvrelupooling1/2\n $M\\times N$ Conv layers $(M/16)\\times (N/16)$ Conv layersfeatuure map\n\n**\\[2. Region Proposal Networks(RPN)\\]**\n   OpenCV adaboost+R-CNNSS(Selective      Search)Faster RCNNSSRPNFaster R-CNN    \n   <center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_3.jpg)</center>\n   RPNRPN2softmaxanchorsforeground              backgroundforegroundanchorsbounding box regression          proposalProposalforeground anchorsbounding box regressionproposals    proposalsProposal Layer\n   \n   **2.1 **\n   * +\n   * +\n     <center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_4.jpg)</center>\n     3233     \n     $1\\times1$      \n    \n   **2.2 Anchors**\n   RPNanchorsanchorsrpn/generate_anchors.pydemo    generate_anchors.py\n   [[ -84.  -40.   99.   55.]\n   [-176.  -88.  191.  103.]\n   [-360. -184.  375.  199.]\n   [ -56.  -56.   71.   71.]\n   [-120. -120.  135.  135.]\n   [-248. -248.  263.  263.]\n   [ -36.  -80.   51.   95.]\n   [ -80. -168.   95.  183.]\n   [-168. -344.  183.  359.]]\n\n   4 $(x1,y1,x2,y2)$ 93 $width:height = [1:1, 1:2, 2:1]$ anchors\n   <center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_5.jpg)</center>\n   anchors sizepython demoreshape $800\\times600$anchorsanchors 1:2  $352\\times704$  2:1  $736\\times384$ cover $800\\times600$ \n9anchorsFaster RCNNConv layersfeature maps9anchors2bounding box regression\n  <center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_6.jpg)</center>\n  \n  \n\n* ZF modelConv Layersconv5num_output=256256feature map256-dimensions\n* conv5rpn_conv/3x3num_output=2563x3256-d47\n* conv5 feature mapkanchork=9anhcorforegroundbackground256d featurecls=2k scoresanchor\\[x, y, w, h\\]4reg=4k coordinates\n* anchorsanchors128postive anchors+128negative anchorsanchors5.1\n\n   **2.3 softmaxforegroundbackground**\n   MxNFaster RCNNRPN(M/16)x(N/16) W=M/16  H=N/16 reshapesoftmax1x1\n   <center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_7.jpg)</center>\n   1x1caffe prototxt\n   <center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_19.PNG)</center>\nnum_output=18 $W\\times H \\times 18$ feature maps9anchorsanchorsforegroundbackground $W\\times H\\times (9\\cdot2)$ softmaxforeground anchorsboxforeground anchors\nRPNanchorssoftmaxforeground anchors\n\n   **2.4 bounding box regression**\n Ground Truth(GT)foreground anchorsforeground anchorsGT\n<center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_8.jpg)</center>\n (x, y, w, h) AForeground AnchorsGGTanchor AGG'\n* $anchor A=(A_{x}, A_{y}, A_{w}, A_{h})$  $GT=[G_{x}, G_{y}, G_{w}, G_{h}]$\n* F$F(A_{x}, A_{y}, A_{w}, A_{h})=(G_{x}^{'}, G_{y}^{'}, G_{w}^{'}, G_{h}^{'})$ $(G_{x}^{'}, G_{y}^{'}, G_{w}^{'}, G_{h}^{'}) \\approx (G_{x}, G_{y}, G_{w}, G_{h})$\n<center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_9.jpg)</center>\nF10anchor AG' :\n\n* \n<center>\n$G^{'}_{x} = A_{w} \\cdot d_{x}(A) + A_{x} $\n$G^{'}_{y} = A_{y} \\cdot d_{y}(A) + A_{y} $\n</center>\n* \n<center>\n$G^{'}_{w} = A_{w} \\cdot exp(d_{w}(A)) $\n$G^{'}_{h} = A_{h} \\cdot exp(d_{h}(A)) $\n</center>\n\n4 $d_{x}(A),d_{y}(A),d_{w}(A),d_{h}(A)$ anchor AGT anchors AGT\n\n $d_{x}(A),d_{y}(A),d_{w}(A),d_{h}(A)$ X, W, Y$Y=WX$Xcnn feature mapAGT$(t_{x}, t_{y}, t_{w}, t_{h})$$d_{x}(A),d_{y}(A),d_{w}(A),d_{h}(A)$\n<center>\n$d_{*}(A) = w^{T}_{*} \\cdot \\phi(A)$\n</center>\n\n(A)anchorfeature mapwd(A)\\* xywh$(t_{x}, t_{y}, t_{w}, t_{h})$\n<center>\n$Loss = \\sum^{N}_{i}(t^{i}_{*} - \\hat{w}^{T}_{*} \\cdot \\phi(A^{i}))^{2}$\n</center>\n\n<center>\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_20.jpg)\n</center>\nGT\nFaster RCNNforeground anchorground truth $(t_x, t_y)$  $(t_w, t_h)$ \n<center>\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_21.jpg)\n</center>\nbouding box regressioncnn feature AnchorGT $(t_x, t_y, t_w, t_h)$\nbouding box regressionAnchor $(t_x, t_y, t_w, t_h)$Anchor\n\n   **2.5 proposalsbounding box regression**\nbounding box regressionRPN\n<center>\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_10.jpg)\n</center>\n $num\\_output=36$  $W\\times H\\times 36$ caffe blob \\[1, 36, H, W\\] feature maps9anchorsanchors4$d_{x}(A),d_{y}(A),d_{w}(A),d_{h}(A)$\n\n   **2.6 Proposal Layer**\nProposal Layer $d_{x}(A),d_{y}(A),d_{w}(A),d_{h}(A)$ foreground anchorsproposalRoI Pooling Layer\nim_infoPxQFaster RCNNreshape $M\\times N$ im_info=\\[M, N, scale_factor\\]Conv Layers4pooling $W\\times H=(M/16)\\times(N/16)$ feature_stride=16anchor\n<center>\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_11.jpg)\n</center>\n\nProposal Layer forwardcaffe layer\n1. anchors$[d_{x}(A),d_{y}(A),d_{w}(A),d_{h}(A)]$anchorsbbox regressionanchors\n2. foreground softmax scoresanchorspre_nms_topN(e.g. 6000)anchorsforeground anchors\n3. foreground anchorsroi poolingproposal\n4. width<threshold or height<thresholdforeground anchors\n5. nonmaximum suppression\n6. nmsforeground softmax scoresfg anchorspost_nms_topN(e.g. 300)proposal\n   \n proposal=\\[x1, y1, x2, y2\\] anchorsproposal $M\\times N$ ~   \n   \n**RPN**\n**anchors -> softmaxfg anchors -> bbox regfg anchors -> Proposal Layerproposals**\n\n### 19/06/2018\n####  XML \n xml.etree.ElementTree XML list\n\n* XML\n* \n* XMLPYTHON\n<center>\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_23.PNG)\n</center> \nGithub  jupyter notebook [](https://github.com/Trouble404/Object-Detection-System-Based-on-CNN-and-Capsule-Network/blob/master/Faster-RCNN/parser_voc2012_xml_and_plotting.ipynb) \n\n trainval.txt \n\n\n  17125 \n 11540 \n\n20\n<center>\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_24.PNG)\n</center> \n\n\n### 20/06/2018\n#### BBOXES\n cv2 \n  {% codeblock %}\n  pip install opencv-python\n  {% endcodeblock %}\n OpenCV-python  BGR  RGB\n\nVOC201220 BBOXES\n\n| class | class_mapping | BGR of bbox |\n| :--- | :---- | :---- |\n| Person | 0 | (0, 0, 255) | \n| Aeroplane | 1 | (0, 0, 255) | \n| Tvmonitor | 2 | (0, 128, 0) | \n| Train | 3 | (128, 128, 128) | \n| Boat | 4 | (0, 165, 255) | \n| Dog | 5 | (0, 255, 255) | \n| Chair | 6 | (80, 127, 255) | \n| Bird | 7 | (208, 224, 64) | \n| Bicycle | 8 | (235, 206, 135) | \n| Bottle | 9 | (128, 0, 0) | \n| Sheep | 10 | (140, 180, 210) | \n| Diningtable | 11 | (0, 255, 0) | \n| Horse | 12 | (133, 21, 199) | \n| Motorbike | 13 | (47, 107, 85) | \n| Sofa | 14 | (19, 69, 139) | \n| Cow | 15 | (222, 196, 176) | \n| Car | 16 | (0, 0, 0) | \n| Cat | 17 |  (225, 105, 65) | \n| Bus | 18 | (255, 255, 255) | \n| Pottedplant | 19 | (205, 250, 255) | \n\nshow_image_with_bboxBBOXESXMLlist:\n<center>\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_22.PNG)\n</center>  \nGithub  jupyter notebook [](https://github.com/Trouble404/Object-Detection-System-Based-on-CNN-and-Capsule-Network/blob/master/Faster-RCNN/parser_voc2012_xml_and_plotting.ipynb) \n\nEXAMPLE:\n<center>\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/img_with_bboex_1.PNG)\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/img_with_bboex_2.PNG)\n</center>  \n\n### 21/06/2018\n#### config setting\nset config class:\n                 for image enhancement:\n<center>\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/21_1.PNG)\n</center>  \n\n#### image enhancement\n<center>\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/21_2.PNG)\n</center>  \nAccording to the config of three peremeters, users could augment image with 3 different ways or using them all.\nFor horizontal and vertical flips, 1/3 probability to triggle\nWith 0,90,180,270 rotation, \nThis function could increase the number of datasets.\n\nimage flips and rotation are realized by opencv and replace of height and width\nNew cordinates of bboxes are calculated acccording to different change of image\n\ndetailed in Github, jupyter notebook: [address](https://github.com/Trouble404/Object-Detection-System-Based-on-CNN-and-Capsule-Network/blob/master/Faster-RCNN/config_set_and_image_enhance.ipynb)\n\nOrignal image:\n<center>\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/21_3.PNG)\n</center>  \nhorizontal flip:\n<center>\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/21_4.PNG)\n</center>  \nVertical filp:\n<center>\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/21_5.PNG)\n</center>  \nRandom rotation:\n<center>\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/21_6.PNG)\n</center>  \nHorizontal and then vertical flips:\n<center>\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/21_7.PNG)\n</center>  \n\n### 22/06/2018\n#### Image rezise\nThis function is to rezise input image to a uniform size with same shortest side\n<center>\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/22_2.PNG)\n</center> \n\nAccording to set the value of shortest side, convergent-divergent or augmented another side proportion\n\nTest:\nLeft image is resized image, in this case, the orignal image amplified.\n<center>\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/22_1.png)\n</center> \n\n#### Class Balance\nWhen training the model, if we sent image with no repeating classes, it may help to improve the performance of model. Therefore, this function is to make sure no repeating classes in two closed input image.\n<center>\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/22_3.PNG)\n</center> \n\nTest:\n<center>\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/22_4.PNG)\n</center> \nRandom output 4 iamge with is function, it could find no repeating classes in two closed image. However, it may reduce the number of trainning image because skip some images.\n\n\n### 25~26/06/2018\n#### Region Proposal Networks(RPN)\n<center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_3.jpg)</center>\nRPN2softmaxanchorsforegroundbackgroundforegroundanchorsbounding box regressionproposalProposalforeground anchorsbounding box regressionproposalsproposalsProposal Layer\n\n#### Anchors\n\n<center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_5.jpg)</center>\n4 (x1,y1,x2,y2) 93 width:height = \\[1:1, 1:2, 2:1\\]\n<center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_6.jpg)</center>\nConv layersfeature maps9anchors2bounding box regression.\n\n#### Code\n\n```python\n\"\"\" intersection of two bboxes\n@param ai: left top x,y and right bottom x,y coordinates of bbox 1\n@param bi: left top x,y and right bottom x,y coordinates of bbox 2\n\n@return: area_union: whether contain target classes\n\n\"\"\"\ndef intersection(ai, bi):\n```\n```python\n\"\"\" union of two bboxes\n@param au: left top x,y and right bottom x,y coordinates of bbox 1\n@param bu: left top x,y and right bottom x,y coordinates of bbox 2\n@param area_intersection: intersection area\n\n@return: area_union: whether contain target classes\n\n\"\"\"\ndef union(au, bu, area_intersection):\n```\n\n```python\n\"\"\" calculate ratio of intersection and union\n@param a: left top x,y and right bottom x,y coordinates of bbox 1\n@param b: left top x,y and right bottom x,y coordinates of bbox 2\n\n@return: ratio of intersection and union of two bboxes\n\n\"\"\"\ndef iou(a, b):\n```\n**IOU is used to bounding box regression**\n\n---\n** rpn calculation**\n\n1. Traversal all pre-anchors to calculate IOU with GT bboxes\n2. Set number and proprty of pre-anchors\n3. return specity number of result(Anchors)\n\n```python\n\"\"\" \n\n@param C: configuration\n@param img_data: parsered xml information\n@param width: orignal width of image\n@param hegiht: orignal height of image\n@param resized_width: resized width of image after image processing\n@param resized_heighth: resized height of image after image processing\n@param img_length_calc_function: Keras's image_dim_ordering function\n\n@return: np.copy(y_rpn_cls): whether contain target classes\n@return: np.copy(y_rpn_regr): corrspoding return of gradient\n\n\"\"\"\n\ndef calc_rpn(C, img_data, width, height, resized_width, resized_height, img_length_calc_function):\n```\n\nnum_regions256 \n\n\nInitialise paramters: see [jupyter notebook](https://github.com/Trouble404/Object-Detection-System-Based-on-CNN-and-Capsule-Network/blob/master/Faster-RCNN/rpn_calculation.ipynb)\n\nCalculate the size of map feature:\n```python\n(output_width, output_height) = img_length_calc_function(resized_width, resized_height)\n```\n<br>\nGet the GT box coordinates, and resize to account for image resizing\nafter rezised functon, the coordinates of bboxes need to re-calculation:\n```python\nfor bbox_num, bbox in enumerate(img_data['bboxes']):\n\tgta[bbox_num, 0] = bbox['x1'] * (resized_width / float(width))\n\tgta[bbox_num, 1] = bbox['x2'] * (resized_width / float(width))\n\tgta[bbox_num, 2] = bbox['y1'] * (resized_height / float(height))\n\tgta[bbox_num, 3] = bbox['y2'] * (resized_height / float(height))\n```\ngtax1,x2,y1,y2x1,y1,x2,y2\n<br>\nTraverse all possible group of sizes\nanchor box scales \\[128, 256, 512\\]\nanchor box ratios \\[1:1,1:2,2:1\\]\n```python\nfor anchor_size_idx in range(len(anchor_sizes)):\n\tfor anchor_ratio_idx in range(len(anchor_ratios)):\n\t\tanchor_x = anchor_sizes[anchor_size_idx] * anchor_ratios[anchor_ratio_idx][0]\n\t\tanchor_y = anchor_sizes[anchor_size_idx] * anchor_ratios[anchor_ratio_idx][1]\n```\nTraver one bbox group, all pre boxes generated by anchors\n\noutput_widthoutput_heightwidth and height of map feature\ndownscalemapping ration, defualt 16\nif to delete box out of iamge\n\n```python\nfor ix in range(output_width):\n\tx1_anc = downscale * (ix + 0.5) - anchor_x / 2\n\tx2_anc = downscale * (ix + 0.5) + anchor_x / 2\n\n\tif x1_anc < 0 or x2_anc > resized_width:\n\t\tcontinue\n\n\tfor jy in range(output_height):\n\t\ty1_anc = downscale * (jy + 0.5) - anchor_y / 2\n\t\ty2_anc = downscale * (jy + 0.5) + anchor_y / 2\n\n\t\tif y1_anc < 0 or y2_anc > resized_height:\n\t\t\tcontinue\n```\n<br>\n\n\n\nbboxes bboxIOU\ncurr_ioubbox\n\ntx\nty:tx\ntw:bbox\nth:\n\n```python\nif curr_iou > best_iou_for_bbox[bbox_num] or curr_iou > C.rpn_max_overlap:\n\tcx = (gta[bbox_num, 0] + gta[bbox_num, 1]) / 2.0\n\tcy = (gta[bbox_num, 2] + gta[bbox_num, 3]) / 2.0\n\tcxa = (x1_anc + x2_anc) / 2.0\n\tcya = (y1_anc + y2_anc) / 2.0\n\n\ttx = (cx - cxa) / (x2_anc - x1_anc)\n\tty = (cy - cya) / (y2_anc - y1_anc)\n\ttw = np.log((gta[bbox_num, 1] - gta[bbox_num, 0]) / (x2_anc - x1_anc))\n\tth = np.log((gta[bbox_num, 3] - gta[bbox_num, 2])) / (y2_anc - y1_anc)\n```\nFaster RCNNforeground anchorground truth $(t_x, t_y)$ \n<center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/25_2.jpg)</center>\nbouding box regressioncnn feature AnchorGT $(t_x, t_y, t_w, t_h)$ \nbouding box regressionAnchor $(t_x, t_y, t_w, t_h)$Anchor\n\n<br>\n\n\nbbox\npos\nbest_iou_for_loc\nnum_anchors_for_bbox[bbox_num]bboxpos\nnegneutral\nnegbbox\n\n```python\nif img_data['bboxes'][bbox_num]['class'] != 'bg':\n\n\t# all GT boxes should be mapped to an anchor box, so we keep track of which anchor box was best\n\tif curr_iou > best_iou_for_bbox[bbox_num]:\n\t\tbest_anchor_for_bbox[bbox_num] = [jy, ix, anchor_ratio_idx, anchor_size_idx]\n\t\tbest_iou_for_bbox[bbox_num] = curr_iou\n\t\tbest_x_for_bbox[bbox_num, :] = [x1_anc, x2_anc, y1_anc, y2_anc]\n\t\tbest_dx_for_bbox[bbox_num, :] = [tx, ty, tw, th]\n\n\tif curr_iou > C.rpn_max_overlap:\n\t\tbbox_type = 'pos'\n\t\tnum_anchors_for_bbox[bbox_num] += 1\n\t\tif curr_iou > best_iou_for_loc:\n\t\t\tbest_iou_for_loc = curr_iou\n\t\t\tbest_regr = (tx, ty, tw, th)\n\n\tif C.rpn_min_overlap < curr_iou < C.rpn_max_overlap:\n\n\t\tif bbox_type != 'pos':\n\t\t\tbbox_type = 'neutral'\n```\n<br>\nbbox\n\ny_is_box_validnertual\ny_rpn_overlap\ny_rpn_regr:\n```python\nif bbox_type == 'neg':\n    y_is_box_valid[jy, ix, anchor_ratio_idx + n_anchratios * anchor_size_idx] = 1\n    y_rpn_overlap[jy, ix, anchor_ratio_idx + n_anchratios * anchor_size_idx] = 0\nelif bbox_type == 'neutral':\n    y_is_box_valid[jy, ix, anchor_ratio_idx + n_anchratios * anchor_size_idx] = 0\n    y_rpn_overlap[jy, ix, anchor_ratio_idx + n_anchratios * anchor_size_idx] = 0\nelse:\n    y_is_box_valid[jy, ix, anchor_ratio_idx + n_anchratios * anchor_size_idx] = 1\n    y_rpn_overlap[jy, ix, anchor_ratio_idx + n_anchratios * anchor_size_idx] = 1\n    start = 4 * (anchor_ratio_idx + n_anchratios * anchor_size_idx)\n    y_rpn_regr[jy, ix, start:start+4] = best_regr\n```\n<br>\nbboxposanchorpos\n```python\nfor idx in range(num_anchors_for_bbox.shape[0]):\n\tif num_anchors_for_bbox[idx] == 0:\n\t\t# no box with an IOU greater than zero ...\n\t\tif best_anchor_for_bbox[idx, 0] == -1:\n\t\t\tcontinue\n\t\ty_is_box_valid[best_anchor_for_bbox[idx,0], best_anchor_for_bbox[idx,1], best_anchor_for_bbox[idx,2] + n_anchratios *best_anchor_for_bbox[idx,3]] = 1\n\t\ty_rpn_overlap[best_anchor_for_bbox[idx,0], best_anchor_for_bbox[idx,1], best_anchor_for_bbox[idx,2] + n_anchratios *best_anchor_for_bbox[idx,3]] = 1\n\t\tstart = 4 * (best_anchor_for_bbox[idx,2] + n_anchratios * best_anchor_for_bbox[idx,3])\n\t\ty_rpn_regr[best_anchor_for_bbox[idx,0], best_anchor_for_bbox[idx,1], start:start+4] = best_dx_for_bbox[idx, :]\n```\n<br>\n, Tensorflow batch size,  \n```python\n\ty_rpn_overlap = np.transpose(y_rpn_overlap, (2, 0, 1))\n\ty_rpn_overlap = np.expand_dims(y_rpn_overlap, axis=0)\n\n\ty_is_box_valid = np.transpose(y_is_box_valid, (2, 0, 1))\n\ty_is_box_valid = np.expand_dims(y_is_box_valid, axis=0)\n\n\ty_rpn_regr = np.transpose(y_rpn_regr, (2, 0, 1))\n\ty_rpn_regr = np.expand_dims(y_rpn_regr, axis=0)\n```\n<br>\nnum_regions\nposnum_regions / 2pos\nposnegnum_regionsneg\n 256128positive,128negative  \n```python\npos_locs = np.where(np(y_rpn_overlap[0, :, :, :] =.logical_and= 1, y_is_box_valid[0, :, :, :] == 1))\nneg_locs = np.where(np.logical_and(y_rpn_overlap[0, :, :, :] == 0, y_is_box_valid[0, :, :, :] == 1))\nnum_regions = 256\n\nif len(pos_locs[0]) > num_regions / 2:\n\tval_locs = random.sample(range(len(pos_locs[0])), len(pos_locs[0]) - num_regions / 2)\n\ty_is_box_valid[0, pos_locs[0][val_locs], pos_locs[1][val_locs], pos_locs[2][val_locs]] = 0\n\tnum_pos = num_regions / 2\n\nif len(neg_locs[0]) + num_pos > num_regions:\n\tval_locs = random.sample(range(len(neg_locs[0])), len(neg_locs[0]) - num_pos)\n\t y_is_box_valid[0, neg_locs[0][val_locs], neg_locs[1][val_locs], neg_locs[2][val_locs]] = 0\n```\n\n<br>\n\n### 27/06/2018\n#### project brief\nRe organization of Project plan\n\n#### Anchors Iterative\nIntegration of privous work:\nIn each anchor: config file -> rpn_stride = 16 means generate one anchor in 16 pixels\n[Jupyter Notebook address](https://github.com/Trouble404/Object-Detection-System-Based-on-CNN-and-Capsule-Network/blob/master/Faster-RCNN/anchor_rpn.ipynb)\n\n\nFunction description\n```python\n\"\"\"\n@param all_img_data: Parsered xml file  \n@param class_count: Counting of the number of all classes objects\n@param C: Configuration class\n@param img_length_calc_function: resnet's get_img_output_length() function\n@param backend: Tensorflow in this project\n#param mode: train or val\n\nyield np.copy(x_img), [np.copy(y_rpn_cls), np.copy(y_rpn_regr)], img_data_aug\n@return: np.copy(x_img): image's matrix data\n@return: [np.copy(y_rpn_cls), np.copy(y_rpn_regr)]: calculated rpn class and radient\n@return: img_data_aug: correspoding parsed xml information\n\n\"\"\"\n\ndef get_anchor_gt(all_img_data, class_count, C, img_length_calc_function, backend, mode='train'):\n```\n<br>\n**Traverse all input image based on input xml information**\n\n* Apply class balance function: \n```python\nC.balanced_classes = True\nsample_selector = image_processing.SampleSelector(class_count)\nif C.balanced_classes and sample_selector.skip_sample_for_balanced_class(img_data):\n    continue\n```\n<br>\n\n* Apply image enhance\nif input mode is train, apply image enhance to obtain augmented image xml and matrix, if mode is val, obtain image orignal xml and matrix\n```python\nif mode == 'train':\n    img_data_aug, x_img = image_enhance.augment(img_data, C, augment=True)\nelse:\n    img_data_aug, x_img = image_enhance.augment(img_data, C, augment=False)\n```\nverifacation width and hegiht in xml and matrix\n```python\n(width, height) = (img_data_aug['width'], img_data_aug['height'])\n(rows, cols, _) = x_img.shape\nassert cols == width\nassert rows == height\n```\n<br>\n\n* Apply rezise function\n```python\n(resized_width, resized_height) = image_processing.get_new_img_size(width, height, C.im_size)\nx_img = cv2.resize(x_img, (resized_width, resized_height), interpolation=cv2.INTER_CUBIC)\n```\n<br>\n\n* Apply rpn calculation\n```python\ny_rpn_cls, y_rpn_regr = rpn_calculation.calc_rpn(C, img_data_aug, width, height, resized_width, resized_height, img_length_calc_function)\n```\n<br>\n\n* Zero-center by mean pixel, and preprocess image format\nBGR -> RGB because when apply resnet, it need RGB but in cv2, it use BGR\n```python\nx_img = x_img[:,:, (2, 1, 0)]\n```\n   For using pre-trainning model, needs to mins mean channel in each dim\n```python\nx_img = x_img.astype(np.float32)\nx_img[:, :, 0] -= C.img_channel_mean[0]\nx_img[:, :, 1] -= C.img_channel_mean[1]\nx_img[:, :, 2] -= C.img_channel_mean[2]\nx_img /= C.img_scaling_factor # default to 1,so no change here\n```\n   expand for batch size\n```python\nx_img = np.expand_dims(x_img, axis=0)\n```\n  for using pre-trainning model, need to sclaling the std to match pre trained model\n```python\ny_rpn_regr[:, y_rpn_regr.shape[1]//2:, :, :] *= C.std_scaling # scaling is 4 here\n```\n  in tensorflow, sort as batch size, width, height, deep\n```python\nif backend == 'tf':\n    x_img = np.transpose(x_img, (0, 3, 2, 1))\n    y_rpn_cls = np.transpose(y_rpn_cls, (0, 3, 2, 1))\n\ty_rpn_regr = np.transpose(y_rpn_regr, (0, 3, 2, 1))\t\t\t\t\t\t\t\t\n```\n  generator to iteror, using next() to loop\n```python\nyield np.copy(x_img), [np.copy(y_rpn_cls), np.copy(y_rpn_regr)], img_data_aug  \n```\n<br>\n\n```python\ndata_gen_train = get_anchor_gt(train_imgs, classes_count, C, nn.get_img_output_length, K.image_dim_ordering(), mode='train')\ndata_gen_val = get_anchor_gt(val_imgs, classes_count, C, nn.get_img_output_length,K.image_dim_ordering(), mode='val')\n```\nTest:\n```python\nimg,rpn,img_aug = next(data_gen_train)\n```\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/26_1.PNG)\n\n### 28/06/2018\n#### Resnet50 structure\n: https://arxiv.org/abs/1512.03385\n\n \n**Is learning better networks as easy as stacking more layers?**\n\n \n**vanishing/exploding gradients**//normalized initialization and intermediate normalization layers \n**degradation**accuracyfigure156-layererror20-layererror\n\nHe kaiMinglayersdegradationidentity mapping \n\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/01_2.jpg)\n\nplain layer \n\n1) Our extremely deep residual nets are easy to optimize, but the counterpart plain nets (that simply stack layers) exhibit higher training error when the depth increases; \n2) Our deep residual nets can easily enjoy accuracy gains from greatly increased depth, producing results substantially better than previous networks.\n\n\n### 29/06/2018\n#### Resnet50 image structure\n![iamge](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/1_01.png)\nResNet2blockIdentity BlockdimensionblockConv Blockdimensionfeature vectordimension\n\nCNNimageconvertdepthfeature mapkernelVGG3x3outputchannelIdentity BlockConv BlockIdentity Block.\n\nConv Block:\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/01_3.png)\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/01_4.png)\nshortcut pathconv2D layer1x1 filter sizemain pathdimensionshortcut path.\n\n## July\n### 02/07/2018\n#### Construct resnet by keras\nxF(x)shape\n\nkerasidentity_blockconv_block\n\n```python\nfrom keras.models import Model\nfrom keras.layers import Input,Dense,BatchNormalization,Conv2D,MaxPooling2D,AveragePooling2D,ZeroPadding2D\nfrom keras.layers import add,Flatten\nfrom keras.optimizers import SGD\n```\n\nidentity_block, convshortcut\n```python\ndef Conv2d_BN(x, nb_filter,kernel_size, strides=(1,1), padding='same',name=None):\n    if name is not None:\n        bn_name = name + '_bn'\n        conv_name = name + '_conv'\n    else:\n        bn_name = None\n        conv_name = None\n \n    x = Conv2D(nb_filter,kernel_size,padding=padding,strides=strides,activation='relu',name=conv_name)(x)\n    x = BatchNormalization(axis=3,name=bn_name)(x)\n    return x\n    \ndef Conv_Block(inpt,nb_filter,kernel_size,strides=(1,1), with_conv_shortcut=False):\n    x = Conv2d_BN(inpt,nb_filter=nb_filter[0],kernel_size=(1,1),strides=strides,padding='same')\n    x = Conv2d_BN(x, nb_filter=nb_filter[1], kernel_size=(3,3), padding='same')\n    x = Conv2d_BN(x, nb_filter=nb_filter[2], kernel_size=(1,1), padding='same')\n    if with_conv_shortcut:\n        shortcut = Conv2d_BN(inpt,nb_filter=nb_filter[2],strides=strides,kernel_size=kernel_size)\n        x = add([x,shortcut])\n        return x\n    else:\n        x = add([x,inpt])\n        return x\n```\n\nidentity_blockconv_block\n```python\ninpt = Input(shape=(224,224,3))\nx = ZeroPadding2D((3,3))(inpt)\nx = Conv2d_BN(x,nb_filter=64,kernel_size=(7,7),strides=(2,2),padding='valid')\nx = MaxPooling2D(pool_size=(3,3),strides=(2,2),padding='same')(x)\n \nx = Conv_Block(x,nb_filter=[64,64,256],kernel_size=(3,3),strides=(1,1),with_conv_shortcut=True)\nx = Conv_Block(x,nb_filter=[64,64,256],kernel_size=(3,3))\nx = Conv_Block(x,nb_filter=[64,64,256],kernel_size=(3,3))\n \nx = Conv_Block(x,nb_filter=[128,128,512],kernel_size=(3,3),strides=(2,2),with_conv_shortcut=True)\nx = Conv_Block(x,nb_filter=[128,128,512],kernel_size=(3,3))\nx = Conv_Block(x,nb_filter=[128,128,512],kernel_size=(3,3))\nx = Conv_Block(x,nb_filter=[128,128,512],kernel_size=(3,3))\n \nx = Conv_Block(x,nb_filter=[256,256,1024],kernel_size=(3,3),strides=(2,2),with_conv_shortcut=True)\nx = Conv_Block(x,nb_filter=[256,256,1024],kernel_size=(3,3))\nx = Conv_Block(x,nb_filter=[256,256,1024],kernel_size=(3,3))\nx = Conv_Block(x,nb_filter=[256,256,1024],kernel_size=(3,3))\nx = Conv_Block(x,nb_filter=[256,256,1024],kernel_size=(3,3))\nx = Conv_Block(x,nb_filter=[256,256,1024],kernel_size=(3,3))\n \nx = Conv_Block(x,nb_filter=[512,512,2048],kernel_size=(3,3),strides=(2,2),with_conv_shortcut=True)\nx = Conv_Block(x,nb_filter=[512,512,2048],kernel_size=(3,3))\nx = Conv_Block(x,nb_filter=[512,512,2048],kernel_size=(3,3))\nx = AveragePooling2D(pool_size=(7,7))(x)\nx = Flatten()(x)\nx = Dense(1000,activation='softmax')(x)\n\nmodel = Model(inputs=inpt,outputs=x)\nsgd = SGD(decay=0.0001,momentum=0.9)\nmodel.compile(loss='categorical_crossentropy',optimizer=sgd,metrics=['accuracy'])\nmodel.summary()\n```\n\n[jupyter notebook](https://github.com/Trouble404/Object-Detection-System-Based-on-CNN-and-Capsule-Network/blob/master/Faster-RCNN/Resnet50/resnet50_keras.ipynb)\n\n\n### 03/07/2018\n#### load pre-trained model of resnet50\n\n\n* ResNet50resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n* ResNet50\n* \n* \n* blockblockResNet50 fine-tune\n\n[](https://github.com/fchollet/deep-learning-models/releases)\n\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/01_7.JPG)\n\n\nidentity\n```python\ndef identity_block(X, f, filters, stage, block):\n    # defining name basis\n    conv_name_base = 'res' + str(stage) + block + '_branch'\n    bn_name_base = 'bn' + str(stage) + block + '_branch'\n    \n    # Retrieve Filters\n    F1, F2, F3 = filters\n    \n    # Save the input value. You'll need this later to add back to the main path. \n    X_shortcut = X\n    \n    # First component of main path\n    X = Conv2D(filters = F1, kernel_size = (1, 1), strides = (1,1), padding = 'valid', name = conv_name_base + '2a')(X)\n    X = BatchNormalization(axis = 3, name = bn_name_base + '2a')(X)\n    X = Activation('relu')(X)\n    \n    # Second component of main path (3 lines)\n    X = Conv2D(filters= F2, kernel_size=(f,f),strides=(1,1),padding='same',name=conv_name_base + '2b')(X)\n    X = BatchNormalization(axis=3,name=bn_name_base+'2b')(X)\n    X = Activation('relu')(X)\n \n    # Third component of main path (2 lines)\n    X = Conv2D(filters=F3,kernel_size=(1,1),strides=(1,1),padding='valid',name=conv_name_base+'2c')(X)\n    X = BatchNormalization(axis=3,name=bn_name_base+'2c')(X)\n \n    # Final step: Add shortcut value to main path, and pass it through a RELU activation (2 lines)\n    X = Add()([X, X_shortcut])\n    X = Activation('relu')(X)\n    return X\n```\n\nconv\n```python\ndef convolutional_block(X, f, filters, stage, block, s = 2):\n    \n    # defining name basis\n    conv_name_base = 'res' + str(stage) + block + '_branch'\n    bn_name_base = 'bn' + str(stage) + block + '_branch'\n    \n    # Retrieve Filters\n    F1, F2, F3 = filters\n    \n    # Save the input value\n    X_shortcut = X\n \n    ##### MAIN PATH #####\n    # First component of main path \n    X = Conv2D(F1, (1, 1), strides = (s,s),padding='valid',name = conv_name_base + '2a')(X)\n    X = BatchNormalization(axis = 3, name = bn_name_base + '2a')(X)\n    X = Activation('relu')(X)\n \n    # Second component of main path (3 lines)\n    X = Conv2D(F2,(f,f),strides=(1,1),padding='same',name=conv_name_base+'2b')(X)\n    X = BatchNormalization(axis=3,name=bn_name_base+'2b')(X)\n    X = Activation('relu')(X)\n \n    # Third component of main path (2 lines)\n    X = Conv2D(F3,(1,1),strides=(1,1),padding='valid',name=conv_name_base+'2c')(X)\n    X = BatchNormalization(axis=3,name=bn_name_base+'2c')(X)\n \n    ##### SHORTCUT PATH #### (2 lines)\n    X_shortcut = Conv2D(F3,(1,1),strides=(s,s),padding='valid',name=conv_name_base+'1')(X_shortcut)\n    X_shortcut = BatchNormalization(axis=3,name =bn_name_base+'1')(X_shortcut)\n \n    # Final step: Add shortcut value to main path, and pass it through a RELU activation (2 lines)\n    X = Add()([X,X_shortcut])\n    X = Activation('relu')(X)\n    return X\n```\n\nresnet50\n```python\ndef ResNet50(input_shape = (64, 64, 3), classes = 30):\n    # Define the input as a tensor with shape input_shape\n    X_input = Input(input_shape)\n \n    # Zero-Padding\n    X = ZeroPadding2D((3, 3))(X_input)\n    \n    # Stage 1\n    X = Conv2D(64, (7, 7), strides = (2, 2), name = 'conv1')(X)\n    X = BatchNormalization(axis = 3, name = 'bn_conv1')(X)\n    X = Activation('relu')(X)\n    X = MaxPooling2D((3, 3), strides=(2, 2))(X)\n \n    # Stage 2\n    X = convolutional_block(X, f = 3, filters = [64, 64, 256], stage = 2, block='a', s = 1)\n    X = identity_block(X, 3, [64, 64, 256], stage=2, block='b')\n    X = identity_block(X, 3, [64, 64, 256], stage=2, block='c')\n \n    ### START CODE HERE ###\n \n    # Stage 3 (4 lines)\n    X = convolutional_block(X, f = 3,filters= [128,128,512],stage=3,block='a',s=2)\n    X = identity_block(X,3,[128,128,512],stage=3,block='b')\n    X = identity_block(X,3,[128,128,512],stage=3,block='c')\n    X = identity_block(X,3,[128,128,512],stage=3,block='d')\n \n    # Stage 4 (6 lines)\n    X = convolutional_block(X,f=3,filters=[256,256,1024],stage=4,block='a',s=2)\n    X = identity_block(X,3,[256,256,1024],stage=4,block='b')\n    X = identity_block(X,3,[256,256,1024],stage=4,block='c')\n    X = identity_block(X,3,[256,256,1024],stage=4,block='d')\n    X = identity_block(X,3,[256,256,1024],stage=4,block='e')\n    X = identity_block(X,3,[256,256,1024],stage=4,block='f')\n \n    # Stage 5 (3 lines)\n    X = convolutional_block(X, f = 3,filters= [512,512,2048],stage=5,block='a',s=2)\n    X = identity_block(X,3,[512,512,2048],stage=5,block='b')\n    X = identity_block(X,3,[512,512,2048],stage=5,block='c')\n \n    # AVGPOOL (1 line). Use \"X = AveragePooling2D(...)(X)\"\n    X = AveragePooling2D((2,2),strides=(2,2))(X)\n \n    # output layer\n    X = Flatten()(X)\n    model = Model(inputs = X_input, outputs = X, name='ResNet50')\n \n    return model\n```\n\n\n```python\nbase_model = ResNet50(input_shape=(224,224,3),classes=30) \nbase_model.load_weights('resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5')\n```\n\n\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/01_8.JPG)\n### 04/07/2018\n#### Loading pre-trained model\nkerasmodel.load_weightsmodel.load_weights('my_model_weights.h5', by_name=True) x= Dense(100, weights=oldModel.layers[1].get_weights())(x)\n\n\n```python\ntry:\n    base_model.load_weights('resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5',by_name=True)\n    print(\"load successful\")\nexcept:\n    print(\"load failed\")\n```\n[jupyter notebook](https://github.com/Trouble404/Object-Detection-System-Based-on-CNN-and-Capsule-Network/blob/master/Faster-RCNN/Resnet50/resnet50_pre_load.ipynb)\n\n### 05~06/07/2018\n#### construct faster rcnn net\n**RoiPoolingConv**\n\nROI\nROIRegion of Interest\n1Fast RCNN RoISelective Search\n2Faster RCNNRPNRoIs\nkerasLayer\n```python\nclass RoiPoolingConv(Layer):\n```\n[](http://keras-cn.readthedocs.io/en/latest/layers/writting_layer/)\n\n\n\\*\\*[kwargs](https://www.cnblogs.com/xuyuanyuan123/p/6674645.html)\n[super](https://link.zhihu.com/?target=http%3A//python.jobbole.com/86787/):\n```python\n'''ROI pooling layer for 2D inputs.\n    # Arguments\n        pool_size: int\n            Size of pooling region to use. pool_size = 7 will result in a 7x7 region.\n        num_rois: number of regions of interest to be used\n    '''\n#  \n    def __init__(self, pool_size, num_rois, **kwargs):\n\n        self.dim_ordering = K.image_dim_ordering()\n        # print error when kernel not tensorflow or thoean\n        assert self.dim_ordering in {'tf'}, 'dim_ordering must be in tf'\n\n        self.pool_size = pool_size\n        self.num_rois = num_rois\n\n        super(RoiPoolingConv, self).__init__(**kwargs)\n```\n\n:\n```python\ndef build(self, input_shape):\n        if self.dim_ordering == 'tf':\n            self.nb_channels = input_shape[0][3]\n```\n\n\n```python\ndef compute_output_shape(self, input_shape):\n        if self.dim_ordering == 'tf':\n            return None, self.num_rois, self.pool_size, self.pool_size, self.nb_channels\n```\n\n,, output:\n```python\ndef call(self, x, mask=None):\n\n        assert(len(x) == 2)\n\n        img = x[0]\n        rois = x[1]\n\n        input_shape = K.shape(img)\n\n        outputs = []\n\n        for roi_idx in range(self.num_rois):\n\n            x = rois[0, roi_idx, 0]\n            y = rois[0, roi_idx, 1]\n            w = rois[0, roi_idx, 2]\n            h = rois[0, roi_idx, 3]\n            \n            row_length = w / float(self.pool_size)\n            col_length = h / float(self.pool_size)\n\n            num_pool_regions = self.pool_size\n\n            if self.dim_ordering == 'tf':\n                x = K.cast(x, 'int32')\n                y = K.cast(y, 'int32')\n                w = K.cast(w, 'int32')\n                h = K.cast(h, 'int32')\n\n                # resize porposal of feature map\n                rs = tf.image.resize_images(img[:, y:y+h, x:x+w, :], (self.pool_size, self.pool_size))\n                outputs.append(rs)\n\n        # outputsshape:(?, 7, 7, 512)\n        final_output = K.concatenate(outputs, axis=0)\n        final_output = K.reshape(final_output, (1, self.num_rois, self.pool_size, self.pool_size, self.nb_channels))\n        # shape:(1, 32, 7, 7, 512)\n        final_output = K.permute_dimensions(final_output, (0, 1, 2, 3, 4))\n\n        return final_output\n```\nbatchvectorbatchRoIvectorchannel \\* w \\* hRoI Poolingboxw \\* h.\n\n**TimeDistributed **\nFastRcnnROIpoolingRoiKerasTimeDistributed\n\nRelutensorTimeDistributedconv2DTimeDistributedROIROI\n\nFaster RCNNbboxnum_roisnum_roisTimeDistributedwrap\n\nconv  identity\n```python\ndef conv_block_td(input_tensor, kernel_size, filters, stage, block, input_shape, strides=(2, 2), trainable=True):\n\n    # conv block time distributed\n\n    nb_filter1, nb_filter2, nb_filter3 = filters\n\n    bn_axis = 3\n\n    conv_name_base = 'res' + str(stage) + block + '_branch'\n    bn_name_base = 'bn' + str(stage) + block + '_branch'\n\n    x = TimeDistributed(Convolution2D(nb_filter1, (1, 1), strides=strides, trainable=trainable, kernel_initializer='normal'), input_shape=input_shape, name=conv_name_base + '2a')(input_tensor)\n    x = TimeDistributed(BatchNormalization(axis=bn_axis), name=bn_name_base + '2a')(x)\n    x = Activation('relu')(x)\n\n    x = TimeDistributed(Convolution2D(nb_filter2, (kernel_size, kernel_size), padding='same', trainable=trainable, kernel_initializer='normal'), name=conv_name_base + '2b')(x)\n    x = TimeDistributed(BatchNormalization(axis=bn_axis), name=bn_name_base + '2b')(x)\n    x = Activation('relu')(x)\n\n    x = TimeDistributed(Convolution2D(nb_filter3, (1, 1), kernel_initializer='normal'), name=conv_name_base + '2c', trainable=trainable)(x)\n    x = TimeDistributed(BatchNormalization(axis=bn_axis), name=bn_name_base + '2c')(x)\n\n    shortcut = TimeDistributed(Convolution2D(nb_filter3, (1, 1), strides=strides, trainable=trainable, kernel_initializer='normal'), name=conv_name_base + '1')(input_tensor)\n    shortcut = TimeDistributed(BatchNormalization(axis=bn_axis), name=bn_name_base + '1')(shortcut)\n\n    x = Add()([x, shortcut])\n    x = Activation('relu')(x)\n    return x\n```\n\n```python\ndef identity_block_td(input_tensor, kernel_size, filters, stage, block, trainable=True):\n\n    # identity block time distributed\n\n    nb_filter1, nb_filter2, nb_filter3 = filters\n\n    bn_axis = 3\n\n    conv_name_base = 'res' + str(stage) + block + '_branch'\n    bn_name_base = 'bn' + str(stage) + block + '_branch'\n\n    x = TimeDistributed(Convolution2D(nb_filter1, (1, 1), trainable=trainable, kernel_initializer='normal'), name=conv_name_base + '2a')(input_tensor)\n    x = TimeDistributed(BatchNormalization(axis=bn_axis), name=bn_name_base + '2a')(x)\n    x = Activation('relu')(x)\n\n    x = TimeDistributed(Convolution2D(nb_filter2, (kernel_size, kernel_size), trainable=trainable, kernel_initializer='normal',padding='same'), name=conv_name_base + '2b')(x)\n    x = TimeDistributed(BatchNormalization(axis=bn_axis), name=bn_name_base + '2b')(x)\n    x = Activation('relu')(x)\n\n    x = TimeDistributed(Convolution2D(nb_filter3, (1, 1), trainable=trainable, kernel_initializer='normal'), name=conv_name_base + '2c')(x)\n    x = TimeDistributed(BatchNormalization(axis=bn_axis), name=bn_name_base + '2c')(x)\n\n    x = Add()([x, input_tensor])\n    x = Activation('relu')(x)\n\n    return x\n```\n2DTimeDistributedDense\n\n**resnet50stage**\n```python\ndef classifier_layers(x, input_shape, trainable=False):\n\n    # Stage 5\n    x = conv_block_td(x, 3, [512, 512, 2048], stage=5, block='a', input_shape=input_shape, strides=(2, 2), trainable=trainable)\n    x = identity_block_td(x, 3, [512, 512, 2048], stage=5, block='b', trainable=trainable)\n    x = identity_block_td(x, 3, [512, 512, 2048], stage=5, block='c', trainable=trainable)\n\n    # AVGPOOL\n    x = TimeDistributed(AveragePooling2D((7, 7)), name='avg_pool')(x)\n\n    return x\n```\n\n* RoiPoolingConvshape(1, 32, 7, 7, 512)batch_size,\n* TimeDistributed3D1323232\n* out_classshape:(?, 32, 21); out_regrshape:(?, 32, 80)\n```python\ndef classifier(base_layers, input_rois, num_rois, nb_classes = 21, trainable=False):\n\n    pooling_regions = 14\n    input_shape = (num_rois,14,14,1024)\n\n    out_roi_pool = RoiPoolingConv(pooling_regions, num_rois)([base_layers, input_rois])\n    out = classifier_layers(out_roi_pool, input_shape=input_shape, trainable=True)\n\n    out = TimeDistributed(Flatten())(out)\n\n    out_class = TimeDistributed(Dense(nb_classes, activation='softmax', kernel_initializer='zero'), name='dense_class_{}'.format(nb_classes))(out)\n    # note: no regression target for bg class\n    out_regr = TimeDistributed(Dense(4 * (nb_classes-1), activation='linear', kernel_initializer='zero'), name='dense_regress_{}'.format(nb_classes))(out)\n    return [out_class, out_regr]\n```\n\n**RPN**\n* x_class:sigmoidnum_anchors\n* x_regr\n```python\ndef rpn(base_layers,num_anchors):\n\n    x = Convolution2D(512, (3, 3), padding='same', activation='relu', kernel_initializer='normal', name='rpn_conv1')(base_layers)\n\n    x_class = Convolution2D(num_anchors, (1, 1), activation='sigmoid', kernel_initializer='uniform', name='rpn_out_class')(x)\n    x_regr = Convolution2D(num_anchors * 4, (1, 1), activation='linear', kernel_initializer='zero', name='rpn_out_regress')(x)\n\n    return [x_class, x_regr, base_layers]\n```\n\n**resnet**\n```python\ndef nn_base(input_tensor=None, trainable=False):\n\n    # Determine proper input shape\n\n    input_shape = (None, None, 3)\n\n    if input_tensor is None:\n        img_input = Input(shape=input_shape)\n    else:\n        if not K.is_keras_tensor(input_tensor):\n            img_input = Input(tensor=input_tensor, shape=input_shape)\n        else:\n            img_input = input_tensor\n\n    bn_axis = 3\n\n    # Zero-Padding\n    x = ZeroPadding2D((3, 3))(img_input)\n\n    # Stage 1\n    x = Convolution2D(64, (7, 7), strides=(2, 2), name='conv1', trainable = trainable)(x)\n    x = BatchNormalization(axis=bn_axis, name='bn_conv1')(x)\n    x = Activation('relu')(x)\n    x = MaxPooling2D((3, 3), strides=(2, 2))(x)\n\n    # Stage 2\n    x = conv_block(x, 3, [64, 64, 256], stage=2, block='a', strides=(1, 1), trainable = trainable)\n    x = identity_block(x, 3, [64, 64, 256], stage=2, block='b', trainable = trainable)\n    x = identity_block(x, 3, [64, 64, 256], stage=2, block='c', trainable = trainable)\n\n    # Stage 3\n    x = conv_block(x, 3, [128, 128, 512], stage=3, block='a', trainable = trainable)\n    x = identity_block(x, 3, [128, 128, 512], stage=3, block='b', trainable = trainable)\n    x = identity_block(x, 3, [128, 128, 512], stage=3, block='c', trainable = trainable)\n    x = identity_block(x, 3, [128, 128, 512], stage=3, block='d', trainable = trainable)\n\n    # Stage 4\n    x = conv_block(x, 3, [256, 256, 1024], stage=4, block='a', trainable = trainable)\n    x = identity_block(x, 3, [256, 256, 1024], stage=4, block='b', trainable = trainable)\n    x = identity_block(x, 3, [256, 256, 1024], stage=4, block='c', trainable = trainable)\n    x = identity_block(x, 3, [256, 256, 1024], stage=4, block='d', trainable = trainable)\n    x = identity_block(x, 3, [256, 256, 1024], stage=4, block='e', trainable = trainable)\n    x = identity_block(x, 3, [256, 256, 1024], stage=4, block='f', trainable = trainable)\n\n    return x\n```\n\n****\n```python\n# define the base network (resnet here)\nshared_layers = nn.nn_base(img_input, trainable=True)\n\n# define the RPN, built on the base layers\n# 9 types of anchors\nnum_anchors = len(C.anchor_box_scales) * len(C.anchor_box_ratios)\nrpn = nn.rpn(shared_layers, num_anchors)\n\nclassifier = nn.classifier(shared_layers, roi_input, C.num_rois, nb_classes=len(classes_count), trainable=True)\n\nmodel_rpn = Model(img_input, rpn[:2])\nmodel_classifier = Model([img_input, roi_input], classifier)\n\n# this is a model that holds both the RPN and the classifier, used to load/save weights for the models\nmodel_all = Model([img_input, roi_input], rpn[:2] + classifier)\n```\n\n[jupyter notebook](https://github.com/Trouble404/Object-Detection-System-Based-on-CNN-and-Capsule-Network/blob/master/Faster-RCNN/Resnet50/workflow_model.ipynb)\n\n### 09/07/2018\n#### Loss define\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/6_1.JPG)\n\n(Multi-task Loss Function)Softmax Classification LossBounding Box Regression Loss\n\n$L(\\{p_{i}\\},\\{t_{i}\\}) = \\frac{1}{N_{cls}}\\sum_{i}L_{cls}(p_{i},p_{i}^{\\ast}) + \\lambda\\frac{1}{N_{reg}}\\sum_{i}L_{reg}(t_{i},t_{i}^{\\ast})$\n\n**Softmax Classification**\nRPN(cls)2k = 18conv5-3WH18conv5-39anchorsanchorscore(fg/bg)anchorSoftmaxreshape\n$p_{i}$$p_{i}^{\\ast}$(label)anchor$p_{i}^{\\ast}$1$p_{i}^{\\ast}$0$L_{cls}$(log loss)\n\n**Bounding Box Regression**\nRPN4k = 36$[x,y,w,h]$box(predicted box)$[x,y,w,h]$anchor$[x_{a},y_{a},w_{a},h_{a}]$ground truth$[x^{\\ast},y^{\\ast},w^{\\ast},h^{\\ast}]$anchor{$t$}ground truthanchor{$t^{\\ast}$}\n\n$t_{x}=\\frac{(x-x_{a})}{w_{a}}$ , $t_{y}=\\frac{(y-y_{a})}{h_{a}}$ , $t_{w}=log(\\frac{w}{w_{a}})$ , $t_{h}=log(\\frac{h}{h_{a}})$ (1)\n$t_{x}^{\\ast}=\\frac{(x^{\\ast}-x_{a})}{w_{a}}$ , $t_{y}^{\\ast}=\\frac{(y^{\\ast}-y_{a})}{h_{a}}$ , $t_{w}^{\\ast}=log(\\frac{w^{\\ast}}{w_{a}})$ , $t_{h}^{\\ast}=log(\\frac{h^{\\ast}}{h_{a}})$ (2)\n\n{t}${t^{\\ast}}$${t}$${t^{\\ast}}$${t}$(1)\n\nSmooth L1:\n\n$$ Smooth_{L1}(x) =\\left\\{\n\\begin{aligned}\n0.5x^{2} \\ \\ |x| \\leqslant 1\\\\\n|x| - 0.5 \\ \\ otherwise \n\\end{aligned}\n\\right.\n$$\n\n$L_{reg} = Smooth_{L1}(t-t^{\\ast})$\nSmooth L1L2L1\n![iamge](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/7_1.JPG)\n\n$p_{i}^{\\ast}L_{reg}$anchor$(p_{i}^{\\ast}=1)$anchorbboxground truth(IoU)bboxground truthwork(cls)(reg){p}{t}$N_{cls}$$N_{reg}$\n\n### 10/07/2018\n#### loss code\n  generator to iteror, using next() to loop\n```python\nyield np.copy(x_img), [np.copy(y_rpn_cls), np.copy(y_rpn_regr)], img_data_aug  \n```\nRpn calculation:\n```python\nimg,rpn,img_aug = next(data_gen_train)\n```\n\n ![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/7_2.JPG)\n\ndef \n\n\n\n\n$L$  cls \n$L(\\{p_{i}\\},\\{t_{i}\\}) = \\frac{1}{N_{cls}}\\sum_{i}L_{cls}(p_{i},p_{i}^{\\ast})$\n\n$p_{i}$$p_{i}^{\\ast}$(label)anchor$p_{i}^{\\ast}$1$p_{i}^{\\ast}$0$L_{cls}$(log loss)\n\n  rpn loss cls:\n```python\ndef rpn_loss_cls(num_anchors):\n\tdef rpn_loss_cls_fixed_num(y_true, y_pred):\n            # binary_crossentropy -> logloss\n            # epsilon to increase robustness\n\t\treturn lambda_rpn_class * K.sum(y_true[:, :, :, :num_anchors] * K.binary_crossentropy(y_pred[:, :, :, :], y_true[:, :, :, num_anchors:])) / K.sum(epsilon + y_true[:, :, :, :num_anchors])\n\treturn rpn_loss_cls_fixed_num\n```\n\n$L$  reg \n$L(\\{p_{i}\\},\\{t_{i}\\}) =  \\lambda\\frac{1}{N_{reg}}\\sum_{i}L_{reg}(t_{i},t_{i}^{\\ast})$\nSmooth L1:\n\n$$ Smooth_{L1}(x) =\\left\\{\n\\begin{aligned}\n0.5x^{2} \\ \\ |x| \\leqslant 1\\\\\n|x| - 0.5 \\ \\ otherwise \n\\end{aligned}\n\\right.\n$$\n$L_{reg} = Smooth_{L1}(t-t^{\\ast})$\n\n  rpn loss reg:\n```python\ndef rpn_loss_regr(num_anchors):\n\tdef rpn_loss_regr_fixed_num(y_true, y_pred):\n\n\t\t# difference of ture value and predicted value\n\t\tx = y_true[:, :, :, 4 * num_anchors:] - y_pred\n\t\t# absulote value of difference\n\t\tx_abs = K.abs(x)\n\t\t# if absulote value less than 1, x_bool == 1, else x_bool = 0\n\t\tx_bool = K.cast(K.less_equal(x_abs, 1.0), tf.float32)\n\n\t\treturn lambda_rpn_regr * K.sum(y_true[:, :, :, :4 * num_anchors] * (x_bool * (0.5 * x * x) + (1 - x_bool) * (x_abs - 0.5))) / K.sum(epsilon + y_true[:, :, :, :4 * num_anchors])\n\n\treturn rpn_loss_regr_fixed_num\n```\n\nclasslossclass_loss_clslossK.meanloss\n```python\ndef class_loss_regr(num_classes):\n\tdef class_loss_regr_fixed_num(y_true, y_pred):\n\t\tx = y_true[:, :, 4*num_classes:] - y_pred\n\t\tx_abs = K.abs(x)\n\t\tx_bool = K.cast(K.less_equal(x_abs, 1.0), 'float32')\n\t\treturn lambda_cls_regr * K.sum(y_true[:, :, :4*num_classes] * (x_bool * (0.5 * x * x) + (1 - x_bool) * (x_abs - 0.5))) / K.sum(epsilon + y_true[:, :, :4*num_classes])\n\treturn class_loss_regr_fixed_num\n\n\ndef class_loss_cls(y_true, y_pred):\n\treturn lambda_cls_class * K.mean(categorical_crossentropy(y_true[0, :, :], y_pred[0, :, :]))\n```\n\n### 11/07/2018\n#### Iridis\n#### High Performance Computing (HPC)\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/8_1.jpg)\n[Introduction](https://www.southampton.ac.uk/isolutions/staff/high-performance-computing.page)\n\nIridis 5 specifications\n* #251 in hte world(Based on July 2018 TOPP500 list) with $R_{peak}\\sim1305.6\\ TFlops/s$\n* 464 2.0 GHz nodes with 40 cores per node, 192 GB memeory\n* 10 nodes with 4xGTX1080TI GPUs, 28 cores(hyper-threaded), 128 GB memeory\n* 10 nodes with 2xVolta Tesia GPUs, same as thandard compute\n* 2.2 PB disk with paraller file system (>12GB\\s)\n* 5M Project delivered by OCF/IBM\n\n[MobaXterm](https://mobaxterm.mobatek.net/)\n\n#### create my own conda envieroment\nFllowing instroduction before\n\n#### Slurm command\n\nCommand | Definition\n---- | ---\nsbatch | Submits job scripts into system for execution (queued)\nscancel |  Cancels a job\nscontrol | Used to display Slurm state, several options only available to root\nsinfo | Display state of partitions and nodes\nsqueue | Display state of jobs\nsalloc | Submit a job for execution, or initiate job in real time\n\n** Bash script**\n```bash\n#!/bin/bash\n#SBATCH -J faster_rcnn \n#SBATCH -o train_7.out\n#SBATCH --ntasks=28\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=8\n#SBATCH --time=00:05:00\n#SBATCH --gres=gpu:1\n#SBATCH -p lyceum\n\nmodule load conda\nmodule load cuda\nsource activate project\npython test_frcnn.py\n```\n\n\n### 12~13/07/2018\n#### change plan\n\nfaster r-cnncapsulefaster-rcnnfine turn\n\n1\nResNetIncRes V2ResNeXt  VGG \n\n2RPN\n  RPN  Proposal \n\n3\n   \n\n---\n\n@1ION\nInside outside net: Detecting objects in context with skip pooling and recurrent neural networks\n\n\n1Inside Net\n Inside  ROI  Scale  Feature Map\n Skip-Pooling conv3-4-5-context \n \n\n2Outside Net\n Outside  ROI  Contextual\n RNN  IRNN\n \n\n---\n\n@2 HyperNet\nHypernet: Towards accurate region proposal generation and joint object detection\n Region Proposal \n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/8_2.JPG)\n   \n\n1Hyper Feature Extraction \n Feature  Max Pooling  Deconv\n Feature ConCat  LRN ION  L2 Norm\n\n2Region Proposal Generation\n ConvNet RPN )\n ROI Pooling Conv  FC  Position  ROI Pooling  13\\*13  bin Conv3\\*3\\*4 13\\*13\\*4  Cube FC  256d \n Score+ BBox_Reg  Faster  Location OffSet\n Overlap Greedy NMS  IOU 0.7 Image  1k  Region Top-200  Detetcion\n Edge Box  Deep Box Deep Proposal \n\n3Object Detection\n Fast RCNN\na FC  3*3*63\nb DropOut  0.5  0.25\n Proposal NMS  Box\n\n---\n\n@3 MSCNN\nA Unified Multi-scale Deep Convolutional Neural Network for Fast Object Detection\naScaleScaleFeature\nScaleScaleFeature\n\nb\n\n\ncScale->->Model\n a b Trade-Off\n\ndScale->->->1Model\n\neRCNNProposalCNN\n aPatch\n\nfRPNCNN\n b\n\ng\n cover\n\n---\n\nNMSsoft-nms\nRepulsion loss overlapping  loss\nfaster rcnn\n\n### 16~20/07/2018\n\n\n\n### 23/07/2018\n#### fix boxes location by regrident\nregranchor\n\n```python\n\"\"\" fix boxes with grident\n\n@param X: current cordinates of box\n@param T: coresspoding grident\n\n\n@return: Fixed cordinates of box\n\"\"\"\ndef apply_regr_np(X, T):\n\ttry:\n\t\tx = X[0, :, :]\n\t\ty = X[1, :, :]\n\t\tw = X[2, :, :]\n\t\th = X[3, :, :]\n\n\t\ttx = T[0, :, :]\n\t\tty = T[1, :, :]\n\t\ttw = T[2, :, :]\n\t\tth = T[3, :, :]\n```\n$t_{x}=\\frac{(x-x_{a})}{w_{a}}$ , $t_{y}=\\frac{(y-y_{a})}{h_{a}}$ , $t_{w}=log(\\frac{w}{w_{a}})$ , $t_{h}=log(\\frac{h}{h_{a}})$ (1)\n$t_{x}^{\\ast}=\\frac{(x^{\\ast}-x_{a})}{w_{a}}$ , $t_{y}^{\\ast}=\\frac{(y^{\\ast}-y_{a})}{h_{a}}$ , $t_{w}^{\\ast}=log(\\frac{w^{\\ast}}{w_{a}})$ , $t_{h}^{\\ast}=log(\\frac{h^{\\ast}}{h_{a}})$ (2)\n\n{t}${t^{\\ast}}$${t}$${t^{\\ast}}$${t}$(1)\n\n\n```python\n\t\t# centre cordinate\n\t\tcx = x + w/2.\n\t\tcy = y + h/2.\n\t\t# fixed centre cordinate\n\t\tcx1 = tx * w + cx\n\t\tcy1 = ty * h + cy\n\n\t\t# fixed wdith and height\n\t\tw1 = np.exp(tw.astype(np.float64)) * w\n\t\th1 = np.exp(th.astype(np.float64)) * h\n\n\t\t# fixed left top corner's cordinate\n\t\tx1 = cx1 - w1/2.\n\t\ty1 = cy1 - h1/2.\n\n\t\t# apporximate\n\t\tx1 = np.round(x1)\n\t\ty1 = np.round(y1)\n\t\tw1 = np.round(w1)\n\t\th1 = np.round(h1)\n\t\treturn np.stack([x1, y1, w1, h1])\n\texcept Exception as e:\n\t\tprint(e)\n\t\treturn X\n```\n\n\n#### NMS no max suppression\n\n\n\n```python\n\"\"\" NMS , delete overlapping box\n\n@param boxes: (n,4) box and coresspoding cordinates\n@param probs: (n,) box adn coresspding possibility\n@param overlap_thresh: treshold of delet box overlapping\n@param max_boxes: maximum keeping number of boxes\n\n\n@return: boxes: boxes cordinates(x1,y1,x2,y2)\n@return: probs: coresspoding possibility\n\"\"\"\ndef non_max_suppression_fast(boxes, probs, overlap_thresh=0.9, max_boxes=300):\n```\n\n```python\nif len(boxes) == 0:\n   return []\n\n# grab the coordinates of the bounding boxes\nx1 = boxes[:, 0]\ny1 = boxes[:, 1]\nx2 = boxes[:, 2]\ny2 = boxes[:, 3]\n\nnp.testing.assert_array_less(x1, x2)\nnp.testing.assert_array_less(y1, y2)\n\n# if the bounding boxes integers, convert them to floats --\n# this is important since we'll be doing a bunch of divisions\nif boxes.dtype.kind == \"i\":\n\tboxes = boxes.astype(\"float\")\n```\n\n\n\n\n```python\n# initialize the list of picked indexes\t\npick = []\n\n# calculate the areas\narea = (x2 - x1) * (y2 - y1)\n\n# sort the bounding boxes \nidxs = np.argsort(probs)\n```\npick\n\nprobs\n```python\nwhile len(idxs) > 0:\n# grab the last index in the indexes list and add the\n# index value to the list of picked indexes\nlast = len(idxs) - 1\ni = idxs[last]\npick.append(i)\n```\noverlap_thresh\n\nidxs\noverlap_thresh\nmax_boxes\n```python\n# find the intersection\n\nxx1_int = np.maximum(x1[i], x1[idxs[:last]])\nyy1_int = np.maximum(y1[i], y1[idxs[:last]])\nxx2_int = np.minimum(x2[i], x2[idxs[:last]])\nyy2_int = np.minimum(y2[i], y2[idxs[:last]])\n\nww_int = np.maximum(0, xx2_int - xx1_int)\nhh_int = np.maximum(0, yy2_int - yy1_int)\n\narea_int = ww_int * hh_int\n```\nidxspick\n```python\n# find the union\narea_union = area[i] + area[idxs[:last]] - area_int\n\n# compute the ratio of overlap\noverlap = area_int/(area_union + 1e-6)\n\n# delete all indexes from the index list that have\nidxs = np.delete(idxs, np.concatenate(([last],np.where(overlap > overlap_thresh)[0])))\n```\n\n```python\nif len(pick) >= max_boxes:\n   break\n```\n\\[np.concatest\\]\nmax_boxes\n```python\nboxes = boxes[pick].astype(\"int\")\nprobs = probs[pick]\nreturn boxes, probs\n```\npick\n\n### 24/07/2018\n#### rpn to porposal fixed\nrpn\n\nanchor_sizeanchor_ratio\n\n\n\n\n1regr_layer\\[0, :, :, 4 \\* curr_layer:4 \\* curr_layer + 4]\n2curr_layer\n\nanchorx,y,w,h\n\nregranchor\n\n\n0\nx1,y1,x2,y2\n\nall_boxesn,4all_probsn,\n\n\nnp.where() \nnp.delete(all_boxes, idxs, 0)\n\n9\n```python\n\"\"\" rpn to porposal\n\n@param rpn_layer: porposal's coresspoding possibiliy, whether item exciseted\n@param regr_layer: porposal's coresspoding regrident\n@param C: Configuration\n@param dim_ordering: Dimensional organization\n@param use_regr=True: wether use regurident to fix proposal\n@param max_boxes=300: max boxes after apply this function\n@param overlap_thresh=0.9: threshold of overlapping\n@param C: Configuration\n\n@return: max_boxes proposal with format (x1,y1,x2,y2)\n\"\"\"\n\ndef rpn_to_roi(rpn_layer, regr_layer, C, dim_ordering, use_regr=True, max_boxes=300,overlap_thresh=0.9):\n\t# std_scaling default 4\n\tregr_layer = regr_layer / C.std_scaling\n\n\tanchor_sizes = C.anchor_box_scales\n\tanchor_ratios = C.anchor_box_ratios\n\n\tassert rpn_layer.shape[0] == 1\n\n\t# obtain img's width and height's matrix\n\t(rows, cols) = rpn_layer.shape[1:3]\n\n\tcurr_layer = 0\n\n\tA = np.zeros((4, rpn_layer.shape[1], rpn_layer.shape[2], rpn_layer.shape[3]))\n\n\t# anchor size is [128, 256, 512]\n\tfor anchor_size in anchor_sizes:\n\t\t# anchor ratio is [1,2,1]\n\t\tfor anchor_ratio in anchor_ratios:\n\t\t\t# rpn_stride = 16\n\t\t\t# obatin anchor's weidth and height on feature map\n\t\t\tanchor_x = (anchor_size * anchor_ratio[0])/C.rpn_stride\n\t\t\tanchor_y = (anchor_size * anchor_ratio[1])/C.rpn_stride\n\n\t\t\t# obtain current regrident\n\t\t\t# when one dimentional obtain a value, the new varirant will decrease one dimenttion\n\t\t\tregr = regr_layer[0, :, :, 4 * curr_layer:4 * curr_layer + 4]\n\t\t\t# put depth to first bacause tensorflow as backend\n\t\t\tregr = np.transpose(regr, (2, 0, 1))\n\n\t\t\t# The rows of the output array X are copies of the vector x; columns of the output array Y are copies of the vector y\n\t\t\t# each cordinartes of matrix cls and rows\n\t\t\tX, Y = np.meshgrid(np.arange(cols),np. arange(rows))\n\n\t\t\t# obtain anchors's (x,y,w,h)\n\t\t\tA[0, :, :, curr_layer] = X - anchor_x/2\n\t\t\tA[1, :, :, curr_layer] = Y - anchor_y/2\n\t\t\tA[2, :, :, curr_layer] = anchor_x\n\t\t\tA[3, :, :, curr_layer] = anchor_y\n\n\t\t\t# fix boxes with grident\n\t\t\tif use_regr:\n\t\t\t\t# fixed corinates of box\n\t\t\t\tA[:, :, :, curr_layer] = apply_regr_np(A[:, :, :, curr_layer], regr)\n\n\t\t\t# fix unreasonable cordinates\n\t\t\t# np.maximum(1,[]) will set the value less than 1 in [] to 1\n\t\t\t# box's width and height can't less than 0\n\t\t\tA[2, :, :, curr_layer] = np.maximum(1, A[2, :, :, curr_layer])\n\t\t\tA[3, :, :, curr_layer] = np.maximum(1, A[3, :, :, curr_layer])\n\t\t\t# fixed right bottom cordinates\n\t\t\tA[2, :, :, curr_layer] += A[0, :, :, curr_layer]\n\t\t\tA[3, :, :, curr_layer] += A[1, :, :, curr_layer]\n\n\t\t\t# left top corner cordinates can't out image\n\t\t\tA[0, :, :, curr_layer] = np.maximum(0, A[0, :, :, curr_layer])\n\t\t\tA[1, :, :, curr_layer] = np.maximum(0, A[1, :, :, curr_layer])\n\t\t\t# right bottom corner cordinates can't out img\n\t\t\tA[2, :, :, curr_layer] = np.minimum(cols-1, A[2, :, :, curr_layer])\n\t\t\tA[3, :, :, curr_layer] = np.minimum(rows-1, A[3, :, :, curr_layer])\n\n\t\t\t# next layer\n\t\t\tcurr_layer += 1\n\n\t# obtain (n,4) object and coresspoding cordinate\n\tall_boxes = np.reshape(A.transpose((0, 3, 1,2)), (4, -1)).transpose((1, 0))\n\t# obtain(n,) object and creoespdoing possibility\n\tall_probs = rpn_layer.transpose((0, 3, 1, 2)).reshape((-1))\n\n\t# cordinates of left top and right bottom of box\n\tx1 = all_boxes[:, 0]\n\ty1 = all_boxes[:, 1]\n\tx2 = all_boxes[:, 2]\n\ty2 = all_boxes[:, 3]\n\n\t# find where right cordinate bigger than left cordinate\n\tidxs = np.where((x1 - x2 >= 0) | (y1 - y2 >= 0))\n\t# delete thoese point at 0 dimentional -> all boxes\n\tall_boxes = np.delete(all_boxes, idxs, 0)\n\tall_probs = np.delete(all_probs, idxs, 0)\n\n\t# apply NMS to reduce overlapping boxes\n\tresult = non_max_suppression_fast(all_boxes, all_probs, overlap_thresh=overlap_thresh, max_boxes=max_boxes)[0]\n\n\treturn result\n```\n\n### 25/07/2018\n#### generate classifier's trainning data\nclassifier,\n\n\nbboxes\n\nR, bboxes\n\n:\n\n\n\n\n\n1one-hot\ny_class_num\ncoordslabelsloss\n```python\nclass_num = 2\nclass_label = 10 * [0]\nprint(class_label)\nclass_label[class_num] = 1\nprint(class_label)\n\n[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n[0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n```\n\n\n\n\n```python\n\"\"\" generate classifier training data\n\n@param R: porposal -> boxes\n@param img_data: image data\n@param C: configuration\n@param class_mapping: classes and coresspoding numbers\n\n@return: np.expand_dims(X, axis=0): boxes after filter\n@return: np.expand_dims(Y1, axis=0): boxes coresspoding class\n@return: np.expand_dims(Y2, axis=0): boxes coresspoding regurident\n@return IoUs: IOU\n\n\"\"\"\ndef calc_iou(R, img_data, C, class_mapping):\n\n\t# obtain boxxes information from img data\n\tbboxes = img_data['bboxes']\n\t# obtain width and height of img\n\t(width, height) = (img_data['width'], img_data['height'])\n\t# get image dimensions for resizing\n\t# Fix image's shortest edge to config setting: eg: 600\n\t(resized_width, resized_height) = get_new_img_size(width, height, C.im_size)\n\n\t# record parameters, bboxes cordinates on feature map\n\tgta = np.zeros((len(bboxes), 4))\n\n\t# change bboxes's width and height because the img was rezised\n\tfor bbox_num, bbox in enumerate(bboxes):\n\t\t# get the GT box coordinates, and resize to account for image resizing\n\t\t# /C.rpn_stride mapping to feature map\n\t\tgta[bbox_num, 0] = int(round(bbox['x1'] * (resized_width / float(width))/C.rpn_stride))\n\t\tgta[bbox_num, 1] = int(round(bbox['x2'] * (resized_width / float(width))/C.rpn_stride))\n\t\tgta[bbox_num, 2] = int(round(bbox['y1'] * (resized_height / float(height))/C.rpn_stride))\n\t\tgta[bbox_num, 3] = int(round(bbox['y2'] * (resized_height / float(height))/C.rpn_stride))\n\n\tx_roi = []\n\ty_class_num = []\n\ty_class_regr_coords = []\n\ty_class_regr_label = []\n\tIoUs = [] # for debugging only\n\n\t# for all given proposals -> boxes\n\tfor ix in range(R.shape[0]):\n\t\t# current boxes's cordinates\n\t\t(x1, y1, x2, y2) = R[ix, :]\n\t\tx1 = int(round(x1))\n\t\ty1 = int(round(y1))\n\t\tx2 = int(round(x2))\n\t\ty2 = int(round(y2))\n\n\t\tbest_iou = 0.0\n\t\tbest_bbox = -1\n\t\t# using current proposal to compare with given xml's boxes\n\t\tfor bbox_num in range(len(bboxes)):\n\t\t\t# calculate current iou\n\t\t\tcurr_iou = iou([gta[bbox_num, 0], gta[bbox_num, 2], gta[bbox_num, 1], gta[bbox_num, 3]], [x1, y1, x2, y2])\n\t\t\t# update parameters\n\t\t\tif curr_iou > best_iou:\n\t\t\t\tbest_iou = curr_iou\n\t\t\t\tbest_bbox = bbox_num\n\n\t\t# if iou to small, we don't put it in trainning because it should be backgroud\n\t\tif best_iou < C.classifier_min_overlap:\n\t\t\t\tcontinue\n\t\telse:\n\t\t\t# saveing left top cordinates, width and height\n\t\t\tw = x2 - x1\n\t\t\th = y2 - y1\n\t\t\tx_roi.append([x1, y1, w, h])\n\t\t\t# saving this bbox's iou\n\t\t\tIoUs.append(best_iou)\n\n\t\t\t# hard to classfier -> set it to backgroud\n\t\t\tif C.classifier_min_overlap <= best_iou < C.classifier_max_overlap:\n\t\t\t\t# hard negative example\n\t\t\t\tcls_name = 'bg'\n\n\t\t\t# valid proposal\n\t\t\telif C.classifier_max_overlap <= best_iou:\n\t\t\t\t# coresspoding class name\n\t\t\t\tcls_name = bboxes[best_bbox]['class']\n\n\t\t\t\t# calculate rpn graident with true cordinates given by xml file\n\t\t\t\tcxg = (gta[best_bbox, 0] + gta[best_bbox, 1]) / 2.0\n\t\t\t\tcyg = (gta[best_bbox, 2] + gta[best_bbox, 3]) / 2.0\n\n\t\t\t\tcx = x1 + w / 2.0\n\t\t\t\tcy = y1 + h / 2.0\n\n\t\t\t\ttx = (cxg - cx) / float(w)\n\t\t\t\tty = (cyg - cy) / float(h)\n\t\t\t\ttw = np.log((gta[best_bbox, 1] - gta[best_bbox, 0]) / float(w))\n\t\t\t\tth = np.log((gta[best_bbox, 3] - gta[best_bbox, 2]) / float(h))\n\t\t\telse:\n\t\t\t\tprint('roi = {}'.format(best_iou))\n\t\t\t\traise RuntimeError\n\n\t\t# class name's mapping number\n\t\tclass_num = class_mapping[cls_name]\n\t\t# list of calss label\n\t\tclass_label = len(class_mapping) * [0]\n\t\t# set class_num's coresspoding location to 1\n\t\tclass_label[class_num] = 1\n\t\t# privous is one-hot vector\n\n\t\t# saving the one-hot vector\n\t\ty_class_num.append(copy.deepcopy(class_label))\n\n\t\t# coords used to saving calculated graident\n\t\tcoords = [0] * 4 * (len(class_mapping) - 1)\n\t\t# labels used to decide whether adding to loss calculation\n\t\tlabels = [0] * 4 * (len(class_mapping) - 1)\n\t\tif cls_name != 'bg':\n\t\t\tlabel_pos = 4 * class_num\n\t\t\tsx, sy, sw, sh = C.classifier_regr_std\n\t\t\tcoords[label_pos:4+label_pos] = [sx*tx, sy*ty, sw*tw, sh*th]\n\t\t\tlabels[label_pos:4+label_pos] = [1, 1, 1, 1]\n\t\t\ty_class_regr_coords.append(copy.deepcopy(coords))\n\t\t\ty_class_regr_label.append(copy.deepcopy(labels))\n\t\telse:\n\t\t\ty_class_regr_coords.append(copy.deepcopy(coords))\n\t\t\ty_class_regr_label.append(copy.deepcopy(labels))\n\n\t# no bboxes\n\tif len(x_roi) == 0:\n\t\treturn None, None, None, None\n\n\t# matrix with [x1, y1, w, h]\n\tX = np.array(x_roi)\n\t# boxxes coresspoding class number\n\tY1 = np.array(y_class_num)\n\t# matrix of whether adding to calculation and coresspoding regrident\n\tY2 = np.concatenate([np.array(y_class_regr_label),np.array(y_class_regr_coords)],axis=1)\n\n\t# adding batch size dimention\n\treturn np.expand_dims(X, axis=0), np.expand_dims(Y1, axis=0), np.expand_dims(Y2, axis=0), IoUs\n```\n\n### 26~27/07/2018\n#### model parameters\n\n```python\n# rpn optimizer\noptimizer = Adam(lr=1e-5)\n# classifier optimizer\noptimizer_classifier = Adam(lr=1e-5)\n# defined loss apply, metrics used to print accury\nmodel_rpn.compile(optimizer=optimizer, loss=[losses.rpn_loss_cls(num_anchors), losses.rpn_loss_regr(num_anchors)])\nmodel_classifier.compile(optimizer=optimizer_classifier, loss=[losses.class_loss_cls, losses.class_loss_regr(len(classes_count)-1)], metrics={'dense_class_{}'.format(len(classes_count)): 'accuracy'})\n# for saving weight\nmodel_all.compile(optimizer='sgd', loss='mae')\n\n# traing time of each epochs\nepoch_length = 1000\n# totoal epochs\nnum_epochs = 2000\n#\niter_num = 0\n# losses saving matrix\nlosses = np.zeros((epoch_length, 5))\nrpn_accuracy_rpn_monitor = []\nrpn_accuracy_for_epoch = []\nstart_time = time.time()\n# current total loss\nbest_loss = np.Inf\n# sorted classing mapping\nclass_mapping_inv = {v: k for k, v in class_mapping.items()}\n```\n\n#### Training process\n\n**rpn**\nRPN,XY\n\n**rpnclassifier:**\n\n\n\nY1\\[0, :, -1\\]0batch-1\nneg_samples = neg_samples\\[0\\]\nC.num_roisclassifierC.num_rois11\n\n**classifier:**\nLossaccury\nloss_class\\[3\\]\n```python\nclassiferloss\n[1.4640709, 1.0986123, 0.36545864, 0.15625]\n```\nlosslistnumpy\nepochepochlossepochepoch.\n\n---\n\n```python\n# Training Process\nprint('Starting training')\n\nfor epoch_num in range(num_epochs):\n    #progbar is used to print % of processing\n\tprogbar = generic_utils.Progbar(epoch_length)\n    # print current process\n\tprint('Epoch {}/{}'.format(epoch_num + 1, num_epochs))\n\n\twhile True:\n\t\ttry:\n\n            # verbose True to print RPN situation, if can't generate boxes on positivate object, it will print error\n\t\t\tif len(rpn_accuracy_rpn_monitor) == epoch_length and C.verbose:\n                # postivate boxes / all boxes\n\t\t\t\tmean_overlapping_bboxes = float(sum(rpn_accuracy_rpn_monitor))/len(rpn_accuracy_rpn_monitor)\n\t\t\t\trpn_accuracy_rpn_monitor = []\n\t\t\t\tprint('Average number of overlapping bounding boxes from RPN = {} for {} previous iterations'.format(mean_overlapping_bboxes, epoch_length))\n\t\t\t\tif mean_overlapping_bboxes == 0:\n\t\t\t\t\tprint('RPN is not producing bounding boxes that overlap the ground truth boxes. Check RPN settings or keep training.')\n\n            # obtain img, rpn information and img xml format\n\t\t\tX, Y, img_data = next(data_gen_train)\n\n            # train RPN net, X is img, Y is correspoding class type and graident\n\t\t\tloss_rpn = model_rpn.train_on_batch(X, Y)\n\n            # predict new Y from privious rpn model\n\t\t\tP_rpn = model_rpn.predict_on_batch(X)\n\n            # transform predicted rpn to cordinates of boxes\n\t\t\tR = rpn_to_boxes.rpn_to_roi(P_rpn[0], P_rpn[1], C, K.image_dim_ordering(), use_regr=True, overlap_thresh=0.7, max_boxes=300)\n\t\t\t# note: calc_iou converts from (x1,y1,x2,y2) to (x,y,w,h) format\n            # X2: [x,y,w,h]\n            # Y1: coresspoding class number -> one hot vector\n            # Y2: boxes coresspoding regrident\n\t\t\tX2, Y1, Y2, IouS = rpn_to_classifier.calc_iou(R, img_data, C, class_mapping)\n\n            # no box, stop this epoch\n\t\t\tif X2 is None:\n\t\t\t\trpn_accuracy_rpn_monitor.append(0)\n\t\t\t\trpn_accuracy_for_epoch.append(0)\n\t\t\t\tcontinue\n\n            # if last position of one-hot is 1 -> is background\n\t\t\tneg_samples = np.where(Y1[0, :, -1] == 1)\n            # else is postivate sample\n\t\t\tpos_samples = np.where(Y1[0, :, -1] == 0)\n\n            # obtain backgourd samples's coresspoding rows\n\t\t\tif len(neg_samples) > 0:\n\t\t\t\tneg_samples = neg_samples[0]\n\t\t\telse:\n\t\t\t\tneg_samples = []\n\n            # obtain posivate samples's coresspoding rows\n\t\t\tif len(pos_samples) > 0:\n\t\t\t\tpos_samples = pos_samples[0]\n\t\t\telse:\n\t\t\t\tpos_samples = []\n\t\t\t# saving posivate samples's number\n\t\t\trpn_accuracy_rpn_monitor.append(len(pos_samples))\n\t\t\trpn_accuracy_for_epoch.append((len(pos_samples)))\n\n            # default 4 here\n\t\t\tif C.num_rois > 1:\n                # wehn postivate samples less than 2\n\t\t\t\tif len(pos_samples) < C.num_rois//2:\n                    # chosse all samples\n\t\t\t\t\tselected_pos_samples = pos_samples.tolist()\n\t\t\t\telse:\n                    # random choose 2 samples\n\t\t\t\t\tselected_pos_samples = np.random.choice(pos_samples, C.num_rois//2, replace=False).tolist()\n\t\t\t\ttry:\n                    # random choose num_rois - positave samples naegivate samples\n\t\t\t\t\tselected_neg_samples = np.random.choice(neg_samples, C.num_rois - len(selected_pos_samples), replace=False).tolist()\n\t\t\t\texcept:\n                    # if no enought neg samples, copy priouvs neg sample\n\t\t\t\t\tselected_neg_samples = np.random.choice(neg_samples, C.num_rois - len(selected_pos_samples), replace=True).tolist()\n\n                # samples picked to classifier network\n\t\t\t\tsel_samples = selected_pos_samples + selected_neg_samples\n\t\t\telse:\n\t\t\t\t# in the extreme case where num_rois = 1, we pick a random pos or neg sample\n\t\t\t\tselected_pos_samples = pos_samples.tolist()\n\t\t\t\tselected_neg_samples = neg_samples.tolist()\n\t\t\t\tif np.random.randint(0, 2):\n\t\t\t\t\tsel_samples = random.choice(neg_samples)\n\t\t\t\telse:\n\t\t\t\t\tsel_samples = random.choice(pos_samples)\n\n            # train classifier, img data, selceted samples' cordinates, mapping number of selected samples, coresspoding regreident\n\t\t\tloss_class = model_classifier.train_on_batch([X, X2[:, sel_samples, :]], [Y1[:, sel_samples, :], Y2[:, sel_samples, :]])\n\n            # in Keras, if loss part bigger than 1, it will return sum of part losses, each loss and accury\n            # put each losses and accury into losses\n\t\t\tlosses[iter_num, 0] = loss_rpn[1]\n\t\t\tlosses[iter_num, 1] = loss_rpn[2]\n\n\t\t\tlosses[iter_num, 2] = loss_class[1]\n\t\t\tlosses[iter_num, 3] = loss_class[2]\n\t\t\tlosses[iter_num, 4] = loss_class[3]\n\n            # next iter\n\t\t\titer_num += 1\n\n            # display and update current mean value of losses\n\t\t\tprogbar.update(iter_num, [('rpn_cls', np.mean(losses[:iter_num, 0])), ('rpn_regr', np.mean(losses[:iter_num, 1])),\n\t\t\t\t\t\t\t\t\t  ('detector_cls', np.mean(losses[:iter_num, 2])), ('detector_regr', np.mean(losses[:iter_num, 3]))])\n\n            # reach epoch_length\n\t\t\tif iter_num == epoch_length:\n\t\t\t\tloss_rpn_cls = np.mean(losses[:, 0])\n\t\t\t\tloss_rpn_regr = np.mean(losses[:, 1])\n\t\t\t\tloss_class_cls = np.mean(losses[:, 2])\n\t\t\t\tloss_class_regr = np.mean(losses[:, 3])\n\t\t\t\tclass_acc = np.mean(losses[:, 4])\n\n                # negativate samples / all samples\n\t\t\t\tmean_overlapping_bboxes = float(sum(rpn_accuracy_for_epoch)) / len(rpn_accuracy_for_epoch)\n                # reset\n\t\t\t\trpn_accuracy_for_epoch = []\n\n                # print trainning loss and accrury\n\t\t\t\tif C.verbose:\n\t\t\t\t\tprint('Mean number of bounding boxes from RPN overlapping ground truth boxes: {}'.format(mean_overlapping_bboxes))\n\t\t\t\t\tprint('Classifier accuracy for bounding boxes from RPN: {}'.format(class_acc))\n\t\t\t\t\tprint('Loss RPN classifier: {}'.format(loss_rpn_cls))\n\t\t\t\t\tprint('Loss RPN regression: {}'.format(loss_rpn_regr))\n\t\t\t\t\tprint('Loss Detector classifier: {}'.format(loss_class_cls))\n\t\t\t\t\tprint('Loss Detector regression: {}'.format(loss_class_regr))\n                    # trainng time of one epoch\n\t\t\t\t\tprint('Elapsed time: {}'.format(time.time() - start_time))\n                    \n                # total loss\n\t\t\t\tcurr_loss = loss_rpn_cls + loss_rpn_regr + loss_class_cls + loss_class_regr\n                # reset\n\t\t\t\titer_num = 0\n                # reset time\n\t\t\t\tstart_time = time.time()\n\n                # if obtain smaller total loss, save weight of current model\n\t\t\t\tif curr_loss < best_loss:\n\t\t\t\t\tif C.verbose:\n\t\t\t\t\t\tprint('Total loss decreased from {} to {}, saving weights'.format(best_loss,curr_loss))\n\t\t\t\t\tbest_loss = curr_loss\n\t\t\t\t\tmodel_all.save_weights(C.model_path)\n\n\t\t\t\tbreak\n\n\t\texcept Exception as e:\n\t\t\tprint('Exception: {}'.format(e))\n\t\t\tcontinue\n\nprint('Training complete, exiting.')\n```\n[jupyter notebook](https://github.com/Trouble404/Object-Detection-System-Based-on-CNN-and-Capsule-Network/blob/master/Faster-RCNN/workflow_train.ipynb)\n\n### 30/07/2018\n#### Running at GPU enviorment\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/9_1.JPG)\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/9_2.JPG)\nMeet error in GPU version tensorflow\nNo enough memory.\n\nTry to Running at Irius:\n\nSetting 3 differnet configration:\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/9_3.JPG)\nat Prjoect1 file:\nset epoch_length to number of training img\n```python\nepoch_length = 11540\nnum_epochs = 100\n```\nApply img enhance function\n```python\nC.use_horizontal_flips = True\nC.use_vertical_flips = True\nC.rot_90 = True\n```\n\n---\n\nat Prjoect file:\nset epoch_length to 1000, increase epoch\n```python\nepoch_length = 1000\nnum_epochs = 2000\n```\nApply img enhance and class balance function\n```python\nC.use_horizontal_flips = True\nC.use_vertical_flips = True\nC.rot_90 = True\nC.balanced_classes = True\n```\n---\n\nat Prjoect3 file:\nset epoch_length to 1000, increase epoch\n```python\nepoch_length = 1000\nnum_epochs = 2000\n```\nApply img enhance\n```python\nC.use_horizontal_flips = True\nC.use_vertical_flips = True\nC.rot_90 = True\n```\n\n---\n#### check irdius work\n```bash\nmyqueue\n```\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/9_4.JPG)\n\n```bash\nssh pink59\nnvidia-smi\n```\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/9_5.JPG)\n\n### 31/07/2018\n#### obtain trained model and log file\n Iriuds GPU24\nLogBook.\n\n#### plot rpn and classfier loss\nepochrpn_cls, rpn_regr, detc_cls, detc_regr\nList\n```python\ndef obtain_each_batch(filename):\n    n = 0\n    rpn_cls = []\n    rpn_regr = []\n    detector_cls = []\n    detector_regr = []\n    f = open(filename,'r',buffering=-1)\n    lines = f.readlines()\n    for line in lines:\n        n = n + 1\n        match = re.match(r'.* - rpn_cls: (.*) - rpn_regr: .*', line, re.M|re.I)\n        if match is None:\n            continue\n        else: \n            rpn_cls.append(float(match.group(1)))\n\n        match = re.match(r'.* - rpn_regr: (.*) - detector_cls: .*', line, re.M|re.I)\n        if match is None:\n            continue\n        else: \n            rpn_regr.append(float(match.group(1)))            \n            \n        match = re.match(r'.* - detector_cls: (.*) - detector_regr: .*', line, re.M|re.I)\n        if match is None:\n            continue\n        else: \n            detector_cls.append(float(match.group(1))) \n            \n        match = re.match(r'.* - detector_regr: (.*)\\n', line, re.M|re.I)\n        if match is None:\n            continue\n        else: \n            det_regr = match.group(1)[0:6]\n            detector_regr.append(float(det_regr))\n\n    f.close()\n    print(n)\n    return rpn_cls, rpn_regr, detector_cls, detector_regr  \n```\n\nepochaccury, loss of rpn cls, loss of rpn regr, loss of detc cls, loss of detc regr\nlist\n```python\ndef obtain_batch(filename):\n    n = 0\n    accuracy = []\n    loss_rpn_cls = []\n    loss_rpn_regr = []\n    loss_detc_cls = []\n    loss_detc_regr = []\n    f = open(filename,'r',buffering=-1)\n    lines = f.readlines()\n    \n    for line in lines:\n        n = n + 1\n        if 'Classifier accuracy for bounding boxes from RPN' in line:\n            result = re.findall(r\"\\d+\\.?\\d*\",line)\n            accuracy.append(float(result[0]))\n            \n        if 'Loss RPN classifier' in line:\n            result = re.findall(r\"\\d+\\.?\\d*\",line)\n            loss_rpn_cls.append(float(result[0]))       \n\n        if 'Loss RPN regression' in line:\n            result = re.findall(r\"\\d+\\.?\\d*\",line)\n            loss_rpn_regr.append(float(result[0]))\n            \n        if 'Loss Detector classifier' in line:\n            result = re.findall(r\"\\d+\\.?\\d*\",line)\n            loss_detc_cls.append(float(result[0]))\n            \n        if 'Loss Detector regression' in line:\n            result = re.findall(r\"\\d+\\.?\\d*\",line)\n            loss_detc_regr.append(float(result[0])) \n            \n    f.close()\n    print(n)\n    return accuracy, loss_rpn_cls, loss_rpn_regr, loss_detc_cls, loss_detc_regr\n```\n\n\n#### plot epoch loss and accury\n```python\nfilename = r'F:\\desktop\\\\1000-no_balance\\train1.out'\naa,bb,cc,dd,ee = obtain_batch(filename)\nx_cor = np.arange(0,len(aa),1)\n\nplt.plot(x_cor,aa, c='b', label = \"Accuracy\")\nplt.plot(x_cor,bb, c='c', label = \"Loss RPN classifier\")\nplt.plot(x_cor,cc, c='g', label = \"Loss RPN regression\")\nplt.plot(x_cor,dd, c='k', label = \"Loss Detector classifier\")\nplt.plot(x_cor,ee, c='m', label = \"Loss Detector regression\")\nplt.ylabel(\"Value of Accuracy and Loss\") \nplt.xlabel(\"Number of Epoch\")\nplt.title('Loss and Accuracy for Totoal Epochs')  \nplt.legend()\nplt.ylim(0,2)\n#plt.xlim(0,11540)\nplt.savefig(\"pic1.PNG\", dpi = 600)\nplt.show()\n```\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/pic1.PNG)\n\n```python\nfilename = r'F:\\desktop\\\\1000-no_balance\\train1.out'\na,b,c,d = obtain_each_batch(filename)\nx_cor = np.arange(0,len(a),1)\n\nplt.plot(x_cor,a, c='b', label = \"rpn_cls\")\nplt.plot(x_cor,b, c='c', label = \"rpn_regr\")\nplt.plot(x_cor,c, c='g', label = \"detector_cls\")\nplt.plot(x_cor,d, c='k', label = \"detector_regr\")\nplt.ylabel(\"Value of Loss\") \nplt.xlabel(\"Epoch Length\")\nplt.title('Loss for Lenght of Epoch')  \nplt.legend()\n#plt.ylim(0,2)\nplt.xlim(80787,92327)\nplt.savefig(\"pic2.PNG\", dpi = 600)\nplt.show()\n```\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/pic2.PNG)\n\n[Jupyter notebook](https://github.com/Trouble404/Object-Detection-System-Based-on-CNN-and-Capsule-Network/blob/master/Plot/logbook_plot.ipynb)\n\n## August\n### 01~02/08/2018\n#### test network\ntrain\n\n\n**rpn**\n```python\nshared_layers = nn.nn_base(img_input, trainable=True)\nnum_anchors = len(C.anchor_box_scales)*len(C.anchor_box_ratios)\nrpn_layers = nn.rpn(shared_layers,num_anchors)\n```\n\n**classifier**\n```python\nclassifier = nn.classifier(feature_map_input, roi_input, C.num_rois,nb_classes=len(class_mapping), trainable=True)\n```\n\n****\n```python\nC.model_path = 'gpu_resnet50_weights.h5'\ntry:\n    print('Loading weights from {}'.format(C.model_path))\n    model_rpn.load_weights(C.model_path, by_name=True)\n    model_classifier.load_weights(C.model_path, by_name=True)\nexcept:\n    print('can not load')\n```\n\n****\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/test/test4.jpg)\n\n1. \n    \n    \n    \n    img\n```python\ndef format_img_size(img, C):\n    (height,width,_) = img.shape\n    if width <= height:\n        ratio = C.im_size/width\n    else:\n        ratio = C.im_size/height\n    new_width, new_height = image_processing.get_new_img_size(width,height, C.im_size)\n    img = cv2.resize(img, (new_width, new_height), interpolation=cv2.INTER_CUBIC)\n    return img, ratio\n```\n2. \n     BGRRGBRESNET\n    np.float32\n    1\n    \n    \n```python\ndef format_img_channels(img, C):\n\t\"\"\" formats the image channels based on config \"\"\"\n\timg = img[:, :, (2, 1, 0)]\n\timg = img.astype(np.float32)\n\timg[:, :, 0] -= C.img_channel_mean[0]\n\timg[:, :, 1] -= C.img_channel_mean[1]\n\timg[:, :, 2] -= C.img_channel_mean[2]\n\timg /= C.img_scaling_factor\n\timg = np.transpose(img, (2, 0, 1))\n\timg = np.expand_dims(img, axis=0)\n\treturn img\n```\ntensorflow\n\n****\nY1:anchor\nY2:anchor\nF:\n\n```python\n[Y1, Y2, F] = model_rpn.predict(X)\n```\n\nrpn16anchorrpn\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/pic_anchors.png)\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/25_1.jpg)\n\n**rpn:**\n300(x1,y1,x2,y2)\n```python\n# transform predicted rpn to cordinates of boxes\nR = rpn_to_boxes.rpn_to_roi(Y1, Y2, C, K.image_dim_ordering(), use_regr=True, overlap_thresh=0.7)\n```\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/pic_rois.png)\n\n(x1,y1,x2,y2)  (x,y,w,h)\n```python\nR[:, 2] -= R[:, 0]\nR[:, 3] -= R[:, 1]\n```\n\n****\nC.num_rois\n32300/32, 10\n3232\n\n3232\n```python\n# divided 32 bboxes as one group\nfor jk in range(R.shape[0]//C.num_rois + 1):\n    # pick num_rios(32) bboxes one time, only pick to last bboxes in last group\n    ROIs = np.expand_dims(R[C.num_rois*jk:C.num_rois*(jk+1), :], axis=0)\n    #print(ROIs.shape)\n    \n    # no proposals, out iter\n    if ROIs.shape[1] == 0:\n        break\n\n    # when last time can't obtain num_rios(32) bboxes, adding bboxes with 0 to fill to 32 bboxes\n    if jk == R.shape[0]//C.num_rois:\n        #pad R\n        curr_shape = ROIs.shape\n        target_shape = (curr_shape[0],C.num_rois,curr_shape[2])\n        ROIs_padded = np.zeros(target_shape).astype(ROIs.dtype)\n        ROIs_padded[:, :curr_shape[1], :] = ROIs\n        ROIs_padded[0, curr_shape[1]:, :] = ROIs[0, 0, :]\n        # 10 group with 320 bboxes\n        ROIs = ROIs_padded\n```\n\n\n****\n\n\nP\\_cls\nP\\_regr\nF:rpn\nROIS:\n```python\n[P_cls, P_regr] = model_classifier_only.predict([F, ROIs])\n```\n\n\n\n```python\n    for ii in range(P_cls.shape[1]):\n\n        # if smaller than setting threshold, we think this bbox invalid\n        # and if this bbox's class is background, we don't need to care about it\n        if np.max(P_cls[0, ii, :]) < bbox_threshold or np.argmax(P_cls[0, ii, :]) == (P_cls.shape[2] - 1):\n            continue\n```\n\n\nlist\n```python\n        # obatain max possibility's class name by class mapping\n        cls_name = class_mapping[np.argmax(P_cls[0, ii, :])]\n\n        # saving bboxes and probs\n        if cls_name not in bboxes:\n            bboxes[cls_name] = []\n            probs[cls_name] = []\n```\n\n\n\n```python\n        # obtain current cordinates of proposal\n        (x, y, w, h) = ROIs[0, ii, :]\n        \n        # obtain the position with max possibility\n        cls_num = np.argmax(P_cls[0, ii, :])\n```\n\n![iamge](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/pic_cls1.png)\n\n\n\n\n C.rpn_stride\n```python\n        try:\n            # obtain privous position's bbox's regrient\n            (tx, ty, tw, th) = P_regr[0, ii, 4*cls_num:4*(cls_num+1)]\n            # waiting test\n            tx /= C.classifier_regr_std[0]\n            ty /= C.classifier_regr_std[1]\n            tw /= C.classifier_regr_std[2]\n            th /= C.classifier_regr_std[3]\n            # fix box with regreient\n            x, y, w, h = rpn_to_boxes.apply_regr(x, y, w, h, tx, ty, tw, th)\n        except:\n            pass\n        # cordinates of current's box on real img\n        bboxes[cls_name].append([C.rpn_stride*x, C.rpn_stride*y, C.rpn_stride*(x+w), C.rpn_stride*(y+h)])\n        # coresspoding posbility\n        probs[cls_name].append(np.max(P_cls[0, ii, :]))\n```\n\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/pic_cls2.png)\n\nbboxesbbox\nNo Max Supression\n```python\n# for all classes in current boxes\nfor key in bboxes:\n\n    # bboxes's cordinates\n    bbox = np.array(bboxes[key])\n    # apply NMX to merge some  overlapping boxes\n    new_boxes, new_probs = rpn_to_boxes.non_max_suppression_fast(bbox, np.array(probs[key]), overlap_thresh=0.5)\n```\n\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/pic_cls3.png)\n\n[Jupyter notebook](https://github.com/Trouble404/Object-Detection-System-Based-on-CNN-and-Capsule-Network/blob/master/Faster-RCNN/test.ipynb)\n\n#### result\nSmall img, only 8k\n<div align=\"center\">\n<img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/test/test1.jpg\" height=\"200px\"  ><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/test/test1.png\" height=\"200px\"  >   \n</div>\n\n---\n\nOverlapping img\n<div align=\"center\">\n<img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/test/test2.jpg\" height=\"200px\"  ><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/test/test2.png\" height=\"200px\"  >   \n</div>\n\n---\n\nCrowed People\n<div align=\"center\">\n<img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/test/test3.jpg\" height=\"260px\"  ><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/test/test3.png\" height=\"260px\"  >   \n</div>\n\n---\n\ncow and people\n<div align=\"center\">\n<img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/test/test4.jpg\" height=\"260px\"  ><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/test/test4.png\" height=\"260px\"  >   \n</div>\n\n---\n\ncar and plane\n<div align=\"center\">\n<img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/test/test5.jpg\" height=\"220px\"  ><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/test/test5.png\" height=\"220px\"  >   \n</div>\n\n---\n\nStreet img\n<div align=\"center\">\n<img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/test/test6.jpg\" height=\"270px\"  ><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/test/test6.png\" height=\"270px\"  >   \n</div>\n\n---\n\nLots Dogs\n<div align=\"center\">\n<img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/test/test7.jpg\" height=\"250px\"  ><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/test/test7.png\" height=\"250px\"  >   \n</div>\n\n---\n\nOverlapping car and people\n<div align=\"center\">\n<img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/test/test8.jpg\" height=\"290px\"  ><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/test/test8.png\" height=\"290px\"  >   \n</div>\n\n\n\n\n### 03/08/2018\n#### evaluation\n**mAP**\nmAPPrecisionRecallobject detectionobjectPrecisionRecall/ P-RAPmeanAPmAPmAP[0,1] \n\n**AP**:PrecisionRecallsklearn.metrics.average\\_precision\\_score \n\nresizesklearn.metrics.average_precision_score\n\n---\n\nrpntrain\nfeature map\n```python\nnum_features = 1024\n\ninput_shape_img = (None, None, 3)\ninput_shape_features = (None, None, num_features)\n\nimg_input = Input(shape=input_shape_img)\nroi_input = Input(shape=(C.num_rois, 4))\nfeature_map_input = Input(shape=input_shape_features)\n\nclassifier = nn.classifier(feature_map_input, roi_input, C.num_rois, nb_classes=len(class_mapping), trainable=True)\n```\n\n\n\nVOC\n```python\ntrain_imgs = []\ntest_imgs = []\n\nfor each in all_imgs:\n\tif each['imageset'] == 'trainval':\n\t\ttrain_imgs.append(each)\n\tif each['imageset'] == 'test':\n\t\ttest_imgs.append(each)\n```\n\nxml\n```python\n    for jk in range(new_boxes.shape[0]):\n        (x1, y1, x2, y2) = new_boxes[jk, :]\n        det = {'x1': x1, 'x2': x2, 'y1': y1, 'y2': y2, 'class': key, 'prob': new_probs[jk]}\n        all_dets.append(det)\n```\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/014_1.JPG)\n\n\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/014_2.JPG)\n\nbbox_matchedFALSETrue\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/14_3.JPG)\n\nidx\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/014_3.JPG)\n\niou0.5\n\n```python\n# process each bbox with hightest prob\nfor box_idx in box_idx_sorted_by_prob:\n    \n    # obtain current box's cordinates, class and prob\n    pred_box = pred[box_idx]\n    pred_class = pred_box['class']\n    pred_x1 = pred_box['x1']\n    pred_x2 = pred_box['x2']\n    pred_y1 = pred_box['y1']\n    pred_y2 = pred_box['y2']\n    pred_prob = pred_box['prob']\n    \n    # if not in P list, save current class infomration to it\n    if pred_class not in P:\n        P[pred_class] = []\n        T[pred_class] = []\n        # put porb to P\n    P[pred_class].append(pred_prob)\n    # used to check whether find current object\n    found_match = False\n\n    # compare each real bbox\n    # obtain real box's cordinates, class and prob\n    for gt_box in gt:\n        gt_class = gt_box['class']\n        # bacause the image is rezied, so calculate the real cordinates\n        gt_x1 = gt_box['x1']/fx\n        gt_x2 = gt_box['x2']/fx\n        gt_y1 = gt_box['y1']/fy\n        gt_y2 = gt_box['y2']/fy\n        \n        # obtain box_matched - all false at beginning\n        gt_seen = gt_box['bbox_matched']\n        \n        # ture class != predicted class\n        if gt_class != pred_class:\n            continue\n        # already matched\n        if gt_seen:\n            continue\n        # calculate iou of predicted bbox and real bbox \n        iou = rpn_calculation.iou((pred_x1, pred_y1, pred_x2, pred_y2), (gt_x1, gt_y1, gt_x2, gt_y2))\n        # if iou > 0.5, we will set this prediction correct\n        if iou >= 0.5:\n            found_match = True\n            gt_box['bbox_matched'] = True\n            break\n        else:\n            continue\n    # 1 means this position's bbox correct match with orignal image\n    T[pred_class].append(int(found_match))\n```\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/014_4.JPG)\n\ndiffculttrue10\n```python\n# adding missing object compared to orignal image\nfor gt_box in gt:\n    if not gt_box['bbox_matched'] and not gt_box['difficult']:\n        if gt_box['class'] not in P:\n            P[gt_box['class']] = []\n            T[gt_box['class']] = []\n\n        # T = 1 means there are object, P = 0 means we did't detected that\n        T[gt_box['class']].append(1)\n        P[gt_box['class']].append(0)\n```\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/014_5.JPG)\n\naverage_precision_scoresklearnapmap\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/014_6.JPG)\n\n[jupyter notebook](https://github.com/Trouble404/Object-Detection-System-Based-on-CNN-and-Capsule-Network/blob/master/Faster-RCNN/map.ipynb)\n\n### 06~10/08/2018\n#### adjust\nap\n[jupyter notebook](https://github.com/Trouble404/Object-Detection-System-Based-on-CNN-and-Capsule-Network/blob/master/Faster-RCNN/map_all.ipynb)\n\n##### Project1 all: 9 models:\nALL_2 NO THRESHOLD OTHER WITH THRESHOLD MOST 0.8\n\n| Classes | ALL_1 | ALL_2 | ALL_3 | ALL_4 | ALL_5 | ALL_6 | ALL_7 | ALL_8 | ALL_9 |\n| :---: | :----: | :---: | :----: | :----: | :----: | :----: | :----: | :----: | :----: |\n| motorbike | 0.2305 | 0.2412 | 0.2132 | 0.2220 | 0.2889 | 0.2528 | 0.2204 | 0.2644 | 0.2336 |\n| person | 0.6489 | 0.6735 | 0.7107 | 0.6652 | 0.7120 | 0.7041 | 0.7238 | 0.7003 | 0.7201 |\n| car | 0.1697 | 0.1563 | 0.2032 | 0.2105 | 0.2308 | 0.2221 | 0.2436 | 0.2053 | 0.2080 |\n| aeroplane | 0.7968 | 0.7062 | 0.7941 | 0.6412 | 0.7871 | 0.7331 | 0.7648 | 0.7659 | 0.6902 |\n| bottle | 0.2213 | 0.2428 | 0.2261 | 0.1943 | 0.2570 | 0.2437 | 0.2899 | 0.1442 | 0.2265 |\n| sheep | 0.6162 | 0.5702 | 0.6876 | 0.6295 | 0.6364 | 0.5710 | 0.6536 | 0.6349 | 0.6455 |\n| tvmonitor | 0.1582 | 0.1601 | 0.2231 | 0.1748 | 0.1551 | 0.1603 | 0.1317 | 0.1584 | 0.1678 |\n| boat | 0.3842 | 0.2621 | 0.2261 | 0.1943 | 0.3499 | 0.2437 | 0.2057 | 0.2748 | 0.3509 |\n| chair | 0.2811 | 0.0563| 0.0891 | 0.0621 | 0.1353 | 0.0865 | 0.0907 | 0.0854 | 0.1282 |\n| bicycle | 0.1464 | 0.1224 | 0.1346 | 0.1781 | 0.1406 | 0.1448 | 0.1810 | 0.1071 | 0.1673 |\n| cat | 0.8901 | 0.8565 | 0.9103 | 0.8417 | 0.8289 | 0.8274 | 0.7572 | 0.9143 | 0.8118 |\n| pottedplant | 0.2075 | 0.0926 | 0.1790 | 0.0532 | 0.1517 | 0.1150 | 0.1080 | 0.1022 | 0.0939 |\n| horse | 0.1185 | 0.0588 | 0.0726 | 0.0489 | 0.0696 | 0.0695 | 0.0637 | 0.0651 | 0.0640 |\n| sofa | 0.2797 | 0.2309 | 0.2852 | 0.2966 | 0.3855 | 0.4817 | 0.3659 | 0.3132 | 0.3090 |\n| dog | 0.5359 | 0.5077 | 0.5578 | 0.4413 | 0.4832 | 0.5793 | 0.5687 | 0.4910 | 0.4598 |\n| cow | 0.7582 | 0.6229 | 0.7295 | 0.5420 | 0.5379 | 0.5312 | 0.5147 | 0.5706 | 0.6503 |\n| diningtable | 0.3979 | 0.2734 | 0.3739 | 0.2963 | 0.4715 | 0.4987 | 0.3895 | 0.4983 | 0.4666 |\n| bus | 0.6203 | 0.5572 | 0.6468 | 0.6032 | 0.6320 | 0.6096 | 0.7169 | 0.5938 | 0.5485 |\n| bird | 0.6164 | 0.6662 | 0.5692 | 0.5751 | 0.5407 | 0.4125 | 0.4925 | 0.4347 | 0.5208 |\n| train | 0.8655 | 0.6916 | 0.7141 | 0.7166 | 0.7643 | 0.8107 | 0.7100 | 0.7194 | 0.6263 |\n| **mAP** | **0.4472** | **0.3874** | **0.4341** | **0.3859** | **0.4279** | **0.4141** | **0.4096** | **0.4022** | **0.4045** |\n\n---\n\n##### Project1 epoch_lenght=1000, epoch:1041 : 7 models:\nALL WITH THRESHOLD MOST 0.51\n\n| Classes | ALL_1 | ALL_2 | ALL_3 | ALL_4 | ALL_5 | ALL_6 | ALL_7 |\n| :---: | :----: | :---: | :----: | :----: | :----: | :----: | :----: |\n| motorbike | 0.2433 | 0.2128 | 0.2232 | 0.2262 | 0.2286 | 0.2393 | 0.2279 |\n| person | 0.6560 | 0.6537 | 0.6742 | 0.6952 | 0.6852 | 0.6719 | 0.6636 |\n| car | 0.1562 | 0.1905 | 0.1479 | 0.2024 | 0.2010 | 0.1379 | 0.1583 | \n| aeroplane | 0.7359 | 0.6837 | 0.6729 | 0.6687 | 0.6957 | 0.7339 | 0.6391 |\n| bottle | 0.1913 | 0.1937 | 0.2635 | 0.1843 | 0.2570 | 0.1632 | 0.1863 | \n| sheep | 0.5429 | 0.5579 | 0.6219 | 0.5355 | 0.5881 | 0.5441 | 0.5824 |\n| tvmonitor | 0.1295 | 0.1601 | 0.1368 | 0.1407 | 0.1147 | 0.1349 | 0.1154 | \n| boat | 0.1913 | 0.2880 | 0.2635 | 0.3433 |0.3335 | 0.3422 | 0.3069 | \n| chair | 0.0587 | 0.0657| 0.0342 | 0.0680 | 0.0695 | 0.0752 | 0.0760 |\n| bicycle | 0.1013 | 0.1485 | 0.1225 | 0.1871 | 0.1685 | 0.1037 | 0.1490 | \n| cat | 0.8737 | 0.8557 | 0.8007 | 0.7982 | 0.8045 | 0.8067 | 0.7732 |\n| pottedplant | 0.0694 | 0.1059 | 0.0748 | 0.0878 | 0.0893 | 0.0690 | 0.0865 |\n| horse | 0.0556 | 0.0561 | 0.0581 | 0.0770 | 0.0575 | 0.0539 | 0.0522 |\n| sofa | 0.2177 | 0.2917 | 0.1699 | 0.1940 | 0.3177 | 0.1863 | 0.1857 |\n| dog | 0.6269 | 0.4989 | 0.5015 | 0.5333 | 0.4914 | 0.5572 | 0.4747 |\n| cow | 0.5216 | 0.6229 | 0.5283 | 0.6426 | 0.4358 | 0.4227 | 0.4589 | \n| diningtable | 0.3076 | 0.3889 | 0.3283 | 0.2404 | 0.4219 | 0.4153 | 0.2627 |\n| bus | 0.5865 | 0.5222 | 0.6312 | 0.5853 | 0.5042 | 0.4882 | 0.5576 |\n| bird | 0.5339 | 0.5039 | 0.5150 | 0.5152 | 0.5838 | 0.3890 | 0.4680 |\n| train | 0.4994 | 0.6541 | 0.6702 | 0.6920 | 0.5959 | 0.5893 | 0.6861 |\n| **mAP** | **0.3699** | **0.3765** | **0.3748** | **0.3814** | **0.3786** | **0.3562** | **0.3555** |\n\n\nclassap\n\n**VOC2007**\nVOC2007OpenCVBUG\n\nVOC2012VOC2007Irius\n\n20AP10002Wclass_balance\n\nclass balance\n\nimagenet\n\n\n\n### 13/08/2018\n#### soft-NMS\nBSMBDBMNtaverage precision, AP\n\nM\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/018_1.JPG)\nNMSSoft-NMSNMS**MM**PASCAL VOC  MS-COCOSoft-NMSSoft-NMS\n\n\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/018_2.JPG)\n\n\nNMS\n$$ s_{i}=\\left\\{\n\\begin{aligned}\ns_{i}, \\ \\ \\ \\ iou(M,b_{i}) <  N_{t} \\\\\n0, \\ \\ \\ \\ iou(M,b_{i}) \\geq  N_{t} \n\\end{aligned}\n\\right.\n$$\n\nSOFT NMS\n$$ s_{i}=\\left\\{\n\\begin{aligned}\ns_{i}, \\ \\ \\ \\ iou(M,b_{i}) <  N_{t} \\\\\n1-iou(M,b_{i}), \\ \\ \\ \\ iou(M,b_{i}) \\geq  N_{t} \n\\end{aligned}\n\\right.\n$$\n\nMNtMM\n\nNtsoft-NMS\n\nGaussian penalty:\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/018_3.JPG)\n\n\n```python\n\"\"\" NMS , delete overlapping box\n\n@param boxes: (n,4) box and coresspoding cordinates\n@param probs: (n,) box adn coresspding possibility\n@param overlap_thresh: treshold of delet box overlapping\n@param max_boxes: maximum keeping number of boxes\n@param method: 1 for linear soft NMS, 2 for gaussian soft NMS\n@param sigma: parameter of gaussian soft NMS\nprob_thresh: threshold of probs after soft NMS\n\n\n@return: boxes: boxes cordinates(x1,y1,x2,y2)\n@return: probs: coresspoding possibility\n\"\"\"\ndef soft_nms(boxes, probs, overlap_thresh=0.9, max_boxes=300, method = 1, sigma=0.5, prob_thresh=0.49):\n    # number of input boxes\n    N = boxes.shape[0]\n    # if the bounding boxes integers, convert them to floats --\n    # this is important since we'll be doing a bunch of divisions\n    if boxes.dtype.kind == \"i\":\n        boxes = boxes.astype(\"float\")\n\n    # iterate all boxes\n    for i in range(N):\n        \n        # obtain current boxes' cordinates and probs\n        maxscore = probs[i]\n        maxpos = i\n\n        tx1 = boxes[i,0]\n        ty1 = boxes[i,1]\n        tx2 = boxes[i,2]\n        ty2 = boxes[i,3]\n        ts = probs[i]\n\n        pos = i + 1\n        # get max box\n        while pos < N:\n            if maxscore < probs[pos]:\n                maxscore = probs[pos]\n                maxpos = pos\n            pos = pos + 1\n\n        # add max box as a detection \n        boxes[i,0] = boxes[maxpos,0]\n        boxes[i,1] = boxes[maxpos,1]\n        boxes[i,2] = boxes[maxpos,2]\n        boxes[i,3] = boxes[maxpos,3]\n        probs[i] = probs[maxpos]\n\n        # swap ith box with position of max box\n        boxes[maxpos,0] = tx1\n        boxes[maxpos,1] = ty1\n        boxes[maxpos,2] = tx2\n        boxes[maxpos,3] = ty2\n        probs[maxpos] = ts\n\n        # cordinates of max box\n        tx1 = boxes[i,0]\n        ty1 = boxes[i,1]\n        tx2 = boxes[i,2]\n        ty2 = boxes[i,3]\n        ts = probs[i]\n\n        pos = i + 1\n        # NMS iterations, note that N changes if detection boxes fall below threshold\n        while pos < N:\n            x1 = boxes[pos, 0]\n            y1 = boxes[pos, 1]\n            x2 = boxes[pos, 2]\n            y2 = boxes[pos, 3]\n            s = probs[pos]\n            \n            # calculate the areas, +1 for robatness\n            area = (x2 - x1 + 1) * (y2 - y1 + 1)\n            iw = (min(tx2, x2) - max(tx1, x1) + 1)\n            # # confirm left top cordinates less than top right\n            if iw > 0:\n                ih = (min(ty2, y2) - max(ty1, y1) + 1)\n                # confirm left top cordinates less than top right\n                if ih > 0:\n                    # find the union\n                    ua = float((tx2 - tx1 + 1) * (ty2 - ty1 + 1) + area - iw * ih)\n                    #iou between max box and detection box\n                    ov = iw * ih / ua\n\n                    if method == 1: # linear\n                        if ov > overlap_thresh: \n                            weight = 1 - ov\n                        else:\n                            weight = 1\n                    elif method == 2: # gaussian\n                        weight = np.exp(-(ov * ov)/sigma)\n                    else: # original NMS\n                        if ov > overlap_thresh: \n                            weight = 0\n                        else:\n                            weight = 1\n\n                    # obtain adjusted probs\n                    probs[pos] = weight*probs[pos]\n\n   \n                    # if box score falls below threshold, discard the box by swapping with last box\n                    # update N\n                    if probs[pos] < prob_thresh:\n                        boxes[pos,0] = boxes[N-1, 0]\n                        boxes[pos,1] = boxes[N-1, 1]\n                        boxes[pos,2] = boxes[N-1, 2]\n                        boxes[pos,3] = boxes[N-1, 3]\n                        probs[pos] = probs[N-1]\n                        N = N - 1\n                        pos = pos - 1\n\n            pos = pos + 1\n    # keep is the idx of current keeping objects, front ith objectes\n    keep = [i for i in range(N)]\n    return boxes[keep], probs[keep]\n```\n\n\n### 14/08/2018\n#### OVERLAPPING OBJECT DETECTION\n\n\n### 15/08/2018\n\n\n### 16/08/2018\n\n\n### 17/08/2018\n\n\n## September\n\n\n\n","source":"_posts/LogBook.md","raw":"---\ntitle: (Logbook) -- Object Detection System Based on CNN and Capsule Network\ndate: 2018-05-25 00:00:00\ntags: [Deep Learning, Object Detection]\ncategories: Msc Project\n---\n\n## Gantt chart\n![image](https://raw.githubusercontent.com/Trouble404/Object-Detection-System-Based-on-CNN-and-Capsule-Network/master/Project%20Plan/gantt%20chart%20of%20project.PNG)\n<!-- more -->\n---\n\n## Check list\n- [x] **1) preparation**\n- [x] ***1.1) Familiarization with develop tools***\n- [x] 1.1.1) Keras\n- [x] 1.1.2) Pythrch\n- [x] ***1.2) Presentation***\n- [x] 1.2.1) Poster conference\n- [x] **2) Create image database**\n- [x] 2.1) Confirmation of detected objects\n- [x] 2.2) Collect and generate the dataset\n- [x] **3) Familiarization with CNN based object detection methods**\n- [x] 3.1) R-CNN\n- [x] 3.2) SPP-net\n- [x] 3.3) Fast R-CNN\n- [x] 3.4) Faster R-CNN\n- [x] **4) Implement object detection system based on one chosen CNN method**\n- [x] 4.1) Pre-processing of images\n- [x] 4.2) Extracting features\n- [x] 4.3) Mode architecture\n- [x] 4.4) Train model and optimization\n- [x] 4.5) Models ensemble\n- [x] **5) Analysis work**\n- [x] 5.1) Evaluation of detection result.\n- [x] **6) Paperwork and bench inspection**\n- [x] 6.1) Logbook\n- [x] 6.2) Write the thesis\n- [x] 6.3) Project video\n- [x] 6.4) Speech and ppt of bench inspection\n- [x] **7) Documents**\n- [x] 7.1) Project Brief\n\n---\n\n## May\n### 28/05/2018\nKeras is a high-level neural networks API, written in Python and capable of running on top of [TensorFlow](https://github.com/tensorflow/tensorflow), CNTK, or Theano.\n\n* **[Keras document](https://keras.io/)**\n\n* **[Keras ](https://keras-cn.readthedocs.io/en/latest/#keraspython)**\n\n---\n#### Installation\n\n* **TensorFlow**\n  [Microsoft Visual Studio 2015](https://www.visualstudio.com/zh-hans/vs/older-downloads/)\n  [CUDA 9.0](http://docs.nvidia.com/cuda/cuda-installation-guide-microsoft-windows/)\n  [cuDNN7](https://developer.nvidia.com/cudnn)\n  [Anaconda](https://www.anaconda.com/download/)\n  \n  * Step 1: Install VS2015\n  * Step 2: Install CUDA 9.0 \n  * Step 3: Install cuDNN7 cudnnbinPATH\n  * Step 4: Install Anaconda PATH,  **Python 3.5**\n  * Step 5: Anaconda ,jupyterbook\n  {% codeblock %}\n  conda create  --name tensorflow python=3.5\n  activate tensorflow\n  conda install nb_conda\n  {% endcodeblock %}\n  * Step 6: Install GPU version TensorFlow.\n  {% codeblock %}\n  pip install -i https://pypi.tuna.tsinghua.edu.cn/simple --ignore-installed --upgrade tensorflow-gpu \n  {% endcodeblock %}\n  \n* **Keras**\n  * Step 1:   Keras GPU \n    {% codeblock %}\n    activate tensorflow\n    pip install keras -U --pre\n    {% endcodeblock %}\n  \n#### Keras**\n* **[NBA with Machine Learning](https://github.com/Trouble404/NBA-with-Machine-Learning)**\n* **[Kaggle- Job salary prediction](https://github.com/Trouble404/kaggle-Job-Salary-Prediction)**\n \n#### TensorFlow CPU \n```python\nimport tensorflow as tf  \nimport os\nimport keras.backend.tensorflow_backend as KTF  \n\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  #GPU\nconfig = tf.ConfigProto()\nconfig.gpu_options.per_process_gpu_memory_fraction = 0.4 #GPUGPU\nsess = tf.Session(config=config)\nKTF.set_session(sess)\n\nwith tf.device('/cpu:0'):\n```\nGPUCPU\n\n#### Jupyter Notebook \n \n```\njupyter notebook --generate-config\n```\n![image](https://raw.githubusercontent.com/Trouble404/Object-Detection-System-Based-on-CNN-and-Capsule-Network/master/Learned%20skills/jupyter%20_setting.PNG)\n\n## June\n### 01/06/2018\n**[PyTorch](https://pytorch.org/about/)** is a python package that provides two high-level features:\n* Tensor computation (like numpy) with strong GPU acceleration\n* Deep Neural Networks built on a tape-based autodiff system\n\n| Package | Description |\n|:----|:----|\n|torch|a Tensor library like NumPy, with strong GPU support|\n|torch.autograd|a tape based automatic differentiation library that supports all differentiable Tensor operations in torch|\n|torch.nn|a neural networks library deeply integrated with autograd designed for maximum flexibility|\n|torch.optim|an optimization package to be used with torch.nn with standard optimization methods such as SGD, RMSProp, LBFGS, Adam etc.|\n|torch.multiprocessing|python multiprocessing, but with magical memory sharing of torch Tensors across processes. Useful for data loading and hogwild training.|\n|torch.utils|DataLoader, Trainer and other utility functions for convenience|\n|torch.legacy(.nn/.optim)|legacy code that has been ported over from torch for backward compatibility reasons|\n\n---\n\n#### Installation\n\n* Step 1: Anaconda ,jupyterbook\n  {% codeblock %}\n  conda create  --name pytorch python=3.5\n  activate pytorch\n  conda install nb_conda\n  {% endcodeblock %}\n* Step 2: Install GPU version PyTorch.\n  {% codeblock %}\n  conda install pytorch cuda90 -c pytorch \n  pip install torchvision\n  {% endcodeblock %}\n\n#### Understanding of PyTorch\n\n* **Tensors**\n  Tensorsnumpyndarrays, TensorGPU\n  ```python\n  from __future__ import print_function\n  import torch\n  x = torch.Tensor(5, 3)  # 5*3\n  x = torch.rand(5, 3)  # \n  x # notebookxx\n  x.size()\n\n  #NOTE: torch.Size tuple, *\n  y = torch.rand(5, 3)\n\n  # \n  x + y # \n  torch.add(x, y) # \n\n  # tensor\n  result = torch.Tensor(5, 3) # \n  torch.add(x, y, out=result) # \n  y.add_(x) # yx\n\n  # tensor'_'\n  # x.copy_(y), x.t_(), x\n\n  #python\n  x[:,1] #x\n  ```\n\n* **Numpy**\n  TorchTensornumpyarrayTorchTensornumpyarray\n  \n  ```python\n# tensornumpy\na = torch.ones(5)\nb = a.numpy()\n\n# numpy,tensor\na.add_(1)\nprint(a)\nprint(b)\n\n# numpyArraytorchTensor\nimport numpy as np\na = np.ones(5)\nb = torch.from_numpy(a)\nnp.add(a, 1, out=a)\nprint(a)\nprint(b)\n\n# CharTensortensorCPUGPU\n# CUDATensorGPU\n# CUDAGPU\nif torch.cuda.is_available():\n    x = x.cuda()\n    y = y.cuda() \n  ```\n* **PyTorchCIFAR10**\n**[code](https://github.com/Trouble404/Object-Detection-System-Based-on-CNN-and-Capsule-Network/blob/master/Learned%20skills/pytorch.ipynb)**\n\n* **MMdnn**\n  MMdnn is a set of tools to help users inter-operate among different deep learning frameworks. E.g. model conversion and visualization. Convert models between Caffe, Keras, MXNet, Tensorflow, CNTK, PyTorch Onnx and CoreML.\n  \n  ![iamge](https://raw.githubusercontent.com/Microsoft/MMdnn/master/docs/supported.jpg)\n  \n  MMdnn\n\n  * DNN\n  * \n  * DNN\n  * \n \n **[Github](https://github.com/Microsoft/MMdnn)**\n \n### 04/06/2018\n#### **Dataset:**\n **[VOC 2012 Dataset](http://host.robots.ox.ac.uk/pascal/VOC/voc2012/index.html)**\n \n#### **Introduce:**\n **Visual Object Classes Challenge 2012 (VOC2012)**\n[PASCAL](http://host.robots.ox.ac.uk/pascal/VOC/)'s full name is Pattern Analysis, Statistical Modelling and Computational Learning.\nVOC's full name is **Visual OBject Classes**.\nThe first competition was held in 2005 and terminated in 2012. I will use the last updated dataset which is [VOC2012](http://host.robots.ox.ac.uk/pascal/VOC/voc2012/index.html) dataset.\n\nThe main aim of this competition is object detection, there are 20 classes objects in the dataset:\n* person\n* bird, cat, cow, dog, horse, sheep\n* aeroplane, bicycle, boat, bus, car, motorbike, train\n* bottle, chair, dining table, potted plant, sofa, tv/monitor\n\n#### **Detection Task**\nReferenced: \n**The PASCAL Visual Object Classes Challenge 2012 (VOC2012) Development Kit**\n**Mark Everingham - John Winn**\nhttp://host.robots.ox.ac.uk/pascal/VOC/voc2012/htmldoc/index.html\n\n**Task:**\nFor each of the twenty classes predict the bounding boxes of each object of that class in a test image (if any). Each bounding box should be output with an associated real-valued confidence of the detection so that a precision/recall curve can be drawn. Participants may choose to tackle all, or any subset of object classes, for example 'cars only' or 'motorbikes and cars'.\n\n**Competitions**:\nTwo competitions are defined according to the choice of training data:\n*  taken from the $VOC_{trainval}$ data provided.\n*  from any source excluding the $VOC_{test}$ data provided.\n\n**Submission of Results**:\nA separate text file of results should be generated for each competition and each class e.g. \\`car'. Each line should be a detection output by the detector in the following format:\n    ```\n    <image identifier> <confidence> <left> <top> <right> <bottom>\n    ```\nwhere (left,top)-(right,bottom) defines the bounding box of the detected object. The top-left pixel in the image has coordinates $(1,1)$. Greater confidence values signify greater confidence that the detection is correct. An example file excerpt is shown below. Note that for the image 2009_000032, multiple objects are detected:\n```\ncomp3_det_test_car.txt:\n    ...\n    2009_000026 0.949297 172.000000 233.000000 191.000000 248.000000\n    2009_000032 0.013737 1.000000 147.000000 114.000000 242.000000\n    2009_000032 0.013737 1.000000 134.000000 94.000000 168.000000\n    2009_000035 0.063948 455.000000 229.000000 491.000000 243.000000\n    ...\n```\n\n**Evaluation**:\nThe detection task will be judged by the precision/recall curve. The principal quantitative measure used will be the average precision (AP). Detections are considered true or false positives based on the area of overlap with ground truth bounding boxes. To be considered a correct detection, the area of overlap $a_o$ between the predicted bounding box $B_p$ and ground truth bounding box $B_{gt}$ must exceed $50\\%$ by the formula: \n<center> $a_o = \\frac{area(B_p \\cap B_{gt})}{area(B_p \\cup B_{gt})}$ </center>\n\n #### **XML**\n xmlgemfield8xml8xmlxml\n ```html\n <?xml version=\"1.0\" encoding=\"utf-8\"?>\n<annotation>\n    <folder>VOC2007</folder>\n    <filename>test100.mp4_3380.jpeg</filename>\n    <size>\n        <width>1280</width>\n        <height>720</height>\n        <depth>3</depth>\n    </size>\n    <object>\n        <name>gemfield</name>\n        <bndbox>\n            <xmin>549</xmin>\n            <xmax>715</xmax>\n            <ymin>257</ymin>\n            <ymax>289</ymax>\n        </bndbox>\n        <truncated>0</truncated>\n        <difficult>0</difficult>\n    </object>\n    <object>\n        <name>civilnet</name>\n        <bndbox>\n            <xmin>842</xmin>\n            <xmax>1009</xmax>\n            <ymin>138</ymin>\n            <ymax>171</ymax>\n        </bndbox>\n        <truncated>0</truncated>\n        <difficult>0</difficult>\n    </object>\n    <segmented>0</segmented>\n</annotation>\n```\n\n2objectgemfieldcivilnet\n\nxml\n* bndbox\n* truncated\n* occluded\n* difficultdifficult\n\n**objectname SSDpy-faster-rcnn**\n\n### 07/06/2018\n#### **Poster conference**\n![iamge](https://raw.githubusercontent.com/Trouble404/Object-Detection-System-Based-on-CNN-and-Capsule-Network/master/Poster/poster.png)\n\n5 People in one group to present their object.\nI present this object to my supervisor in this conference.\n\n### 11/06/2018\n#### **R-CNN**\nPaper: [Rich feature hierarchies for accurate object detection and semantic segmentation](https://arxiv.org/abs/1311.2524)\n\n****\n\n*  (Selective Search)(CNN)\n*      ImageNet ILSVC 20121000 PASCAL VOC 2007   20 CNN   \n\n****\n\n1.  1K~2K Selective Search \n2.   CNN \n3.  SVM \n4.  \n<center>![image](https://raw.githubusercontent.com/Trouble404/Object-Detection-System-Based-on-CNN-and-Capsule-Network/master/Research%20Review/LaTex/1.PNG)</center>\n\n**[Selective Search](https://www.koen.me/research/pub/uijlings-ijcv2013-draft.pdf)**\n1.  (1k~2k )\n2. \n3. \n    \n   * \n   * \n   *   a-b-c-d-e-f-g-hab-cd-ef-gh -> abcd-efgh -> abcdefgh ab-c-d-e-f-g-h ->abcd-e-f-g-h ->abcdef-gh -> abcdefgh\n   * BBOX \n\n### 12/06/2018\n#### **SPP-CNN**\nPaper: [Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition](https://arxiv.org/abs/1406.4729)\n\n****\n\nRCNNCNNRCNN224\\*224CNN\n* region proposalSelective Search2Kregion proposal2KCNN\n* region proposal\n\n\n****\n\n1. selective searchregion proposal\n2. \n   $s \\in S = \\{480,576,688,864,1200\\}$\n   epochmodelmodel1\\*12\\*23\\*36\\*650bins\n3. region proposal224\\*224 \n4. SVMBoundingBox.\n<center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/SPP-NET.jpg)</center>\n\n\n### 13/06/2018\n#### **FAST R-CNN**\nPaper: [Fast R-CNN](https://arxiv.org/abs/1504.08083)\n\n****\n\n* RCNN\n*  \n* : RCNN\n\n\n****\n\n1. convconv\n2. object proposals region of interestRoI\n3. fc\n    KbackgroundsoftmaxRoI\nbbox regressionK4Kbounding-boxtrained end-to-end with a multi-task loss\n<center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/fast-rcnn.png)</center>\n\n### 14~18/06/2018\n#### **FASTER R-CNN**\nI want to use **Faster R-cnn** as the first method to implement object detection system.\n\nPaper: [Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks](https://arxiv.org/abs/1506.01497)\n\nFaster RCNN(feature extraction)proposalbounding box regression(rect refine)classification\n<center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster-rcnn.jpg)</center>\n\n #### \n\n1. Conv layersCNNFaster R-CNNconv+relu+poolingimagefeature mapsfeature mapsRPN\n2. Region Proposal NetworksRPNregion proposalssoftmaxanchorsforegroundbackgroundbounding box regressionanchorsproposals\n3. Roi Poolingfeature mapsproposalsproposal feature maps\n4. Classificationproposal feature mapsproposalbounding box regression\n\n#### \n\n**\\[1. Conv layers\\]**\n   <center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_1.jpg)</center>\n   Conv layersconvpoolingrelupythonVGG16faster_rcnn_test.pt,    Conv layers13conv13relu4poolingConv          layers\n   \n  * conv $kernel\\_size=3$  $pad=1$  $stride=1$ <br>\n  * pooling $kernel\\_size=2$  $pad=0$  $stride=2$\n  \n   Faster RCNN Conv layers $pad=1$ 0                $(M+2)\\times (N+2)$ 3x3 $M\\times N$ Conv layersconv    \n<center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_2.jpg)</center>\n   Conv layerspooling $kernel\\_size=2$  $stride=2$ pooling $M\\times N$  $(M/2) \\times(N/2)$ Conv layersconvrelupooling1/2\n $M\\times N$ Conv layers $(M/16)\\times (N/16)$ Conv layersfeatuure map\n\n**\\[2. Region Proposal Networks(RPN)\\]**\n   OpenCV adaboost+R-CNNSS(Selective      Search)Faster RCNNSSRPNFaster R-CNN    \n   <center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_3.jpg)</center>\n   RPNRPN2softmaxanchorsforeground              backgroundforegroundanchorsbounding box regression          proposalProposalforeground anchorsbounding box regressionproposals    proposalsProposal Layer\n   \n   **2.1 **\n   * +\n   * +\n     <center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_4.jpg)</center>\n     3233     \n     $1\\times1$      \n    \n   **2.2 Anchors**\n   RPNanchorsanchorsrpn/generate_anchors.pydemo    generate_anchors.py\n   [[ -84.  -40.   99.   55.]\n   [-176.  -88.  191.  103.]\n   [-360. -184.  375.  199.]\n   [ -56.  -56.   71.   71.]\n   [-120. -120.  135.  135.]\n   [-248. -248.  263.  263.]\n   [ -36.  -80.   51.   95.]\n   [ -80. -168.   95.  183.]\n   [-168. -344.  183.  359.]]\n\n   4 $(x1,y1,x2,y2)$ 93 $width:height = [1:1, 1:2, 2:1]$ anchors\n   <center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_5.jpg)</center>\n   anchors sizepython demoreshape $800\\times600$anchorsanchors 1:2  $352\\times704$  2:1  $736\\times384$ cover $800\\times600$ \n9anchorsFaster RCNNConv layersfeature maps9anchors2bounding box regression\n  <center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_6.jpg)</center>\n  \n  \n\n* ZF modelConv Layersconv5num_output=256256feature map256-dimensions\n* conv5rpn_conv/3x3num_output=2563x3256-d47\n* conv5 feature mapkanchork=9anhcorforegroundbackground256d featurecls=2k scoresanchor\\[x, y, w, h\\]4reg=4k coordinates\n* anchorsanchors128postive anchors+128negative anchorsanchors5.1\n\n   **2.3 softmaxforegroundbackground**\n   MxNFaster RCNNRPN(M/16)x(N/16) W=M/16  H=N/16 reshapesoftmax1x1\n   <center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_7.jpg)</center>\n   1x1caffe prototxt\n   <center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_19.PNG)</center>\nnum_output=18 $W\\times H \\times 18$ feature maps9anchorsanchorsforegroundbackground $W\\times H\\times (9\\cdot2)$ softmaxforeground anchorsboxforeground anchors\nRPNanchorssoftmaxforeground anchors\n\n   **2.4 bounding box regression**\n Ground Truth(GT)foreground anchorsforeground anchorsGT\n<center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_8.jpg)</center>\n (x, y, w, h) AForeground AnchorsGGTanchor AGG'\n* $anchor A=(A_{x}, A_{y}, A_{w}, A_{h})$  $GT=[G_{x}, G_{y}, G_{w}, G_{h}]$\n* F$F(A_{x}, A_{y}, A_{w}, A_{h})=(G_{x}^{'}, G_{y}^{'}, G_{w}^{'}, G_{h}^{'})$ $(G_{x}^{'}, G_{y}^{'}, G_{w}^{'}, G_{h}^{'}) \\approx (G_{x}, G_{y}, G_{w}, G_{h})$\n<center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_9.jpg)</center>\nF10anchor AG' :\n\n* \n<center>\n$G^{'}_{x} = A_{w} \\cdot d_{x}(A) + A_{x} $\n$G^{'}_{y} = A_{y} \\cdot d_{y}(A) + A_{y} $\n</center>\n* \n<center>\n$G^{'}_{w} = A_{w} \\cdot exp(d_{w}(A)) $\n$G^{'}_{h} = A_{h} \\cdot exp(d_{h}(A)) $\n</center>\n\n4 $d_{x}(A),d_{y}(A),d_{w}(A),d_{h}(A)$ anchor AGT anchors AGT\n\n $d_{x}(A),d_{y}(A),d_{w}(A),d_{h}(A)$ X, W, Y$Y=WX$Xcnn feature mapAGT$(t_{x}, t_{y}, t_{w}, t_{h})$$d_{x}(A),d_{y}(A),d_{w}(A),d_{h}(A)$\n<center>\n$d_{*}(A) = w^{T}_{*} \\cdot \\phi(A)$\n</center>\n\n(A)anchorfeature mapwd(A)\\* xywh$(t_{x}, t_{y}, t_{w}, t_{h})$\n<center>\n$Loss = \\sum^{N}_{i}(t^{i}_{*} - \\hat{w}^{T}_{*} \\cdot \\phi(A^{i}))^{2}$\n</center>\n\n<center>\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_20.jpg)\n</center>\nGT\nFaster RCNNforeground anchorground truth $(t_x, t_y)$  $(t_w, t_h)$ \n<center>\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_21.jpg)\n</center>\nbouding box regressioncnn feature AnchorGT $(t_x, t_y, t_w, t_h)$\nbouding box regressionAnchor $(t_x, t_y, t_w, t_h)$Anchor\n\n   **2.5 proposalsbounding box regression**\nbounding box regressionRPN\n<center>\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_10.jpg)\n</center>\n $num\\_output=36$  $W\\times H\\times 36$ caffe blob \\[1, 36, H, W\\] feature maps9anchorsanchors4$d_{x}(A),d_{y}(A),d_{w}(A),d_{h}(A)$\n\n   **2.6 Proposal Layer**\nProposal Layer $d_{x}(A),d_{y}(A),d_{w}(A),d_{h}(A)$ foreground anchorsproposalRoI Pooling Layer\nim_infoPxQFaster RCNNreshape $M\\times N$ im_info=\\[M, N, scale_factor\\]Conv Layers4pooling $W\\times H=(M/16)\\times(N/16)$ feature_stride=16anchor\n<center>\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_11.jpg)\n</center>\n\nProposal Layer forwardcaffe layer\n1. anchors$[d_{x}(A),d_{y}(A),d_{w}(A),d_{h}(A)]$anchorsbbox regressionanchors\n2. foreground softmax scoresanchorspre_nms_topN(e.g. 6000)anchorsforeground anchors\n3. foreground anchorsroi poolingproposal\n4. width<threshold or height<thresholdforeground anchors\n5. nonmaximum suppression\n6. nmsforeground softmax scoresfg anchorspost_nms_topN(e.g. 300)proposal\n   \n proposal=\\[x1, y1, x2, y2\\] anchorsproposal $M\\times N$ ~   \n   \n**RPN**\n**anchors -> softmaxfg anchors -> bbox regfg anchors -> Proposal Layerproposals**\n\n### 19/06/2018\n####  XML \n xml.etree.ElementTree XML list\n\n* XML\n* \n* XMLPYTHON\n<center>\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_23.PNG)\n</center> \nGithub  jupyter notebook [](https://github.com/Trouble404/Object-Detection-System-Based-on-CNN-and-Capsule-Network/blob/master/Faster-RCNN/parser_voc2012_xml_and_plotting.ipynb) \n\n trainval.txt \n\n\n  17125 \n 11540 \n\n20\n<center>\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_24.PNG)\n</center> \n\n\n### 20/06/2018\n#### BBOXES\n cv2 \n  {% codeblock %}\n  pip install opencv-python\n  {% endcodeblock %}\n OpenCV-python  BGR  RGB\n\nVOC201220 BBOXES\n\n| class | class_mapping | BGR of bbox |\n| :--- | :---- | :---- |\n| Person | 0 | (0, 0, 255) | \n| Aeroplane | 1 | (0, 0, 255) | \n| Tvmonitor | 2 | (0, 128, 0) | \n| Train | 3 | (128, 128, 128) | \n| Boat | 4 | (0, 165, 255) | \n| Dog | 5 | (0, 255, 255) | \n| Chair | 6 | (80, 127, 255) | \n| Bird | 7 | (208, 224, 64) | \n| Bicycle | 8 | (235, 206, 135) | \n| Bottle | 9 | (128, 0, 0) | \n| Sheep | 10 | (140, 180, 210) | \n| Diningtable | 11 | (0, 255, 0) | \n| Horse | 12 | (133, 21, 199) | \n| Motorbike | 13 | (47, 107, 85) | \n| Sofa | 14 | (19, 69, 139) | \n| Cow | 15 | (222, 196, 176) | \n| Car | 16 | (0, 0, 0) | \n| Cat | 17 |  (225, 105, 65) | \n| Bus | 18 | (255, 255, 255) | \n| Pottedplant | 19 | (205, 250, 255) | \n\nshow_image_with_bboxBBOXESXMLlist:\n<center>\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_22.PNG)\n</center>  \nGithub  jupyter notebook [](https://github.com/Trouble404/Object-Detection-System-Based-on-CNN-and-Capsule-Network/blob/master/Faster-RCNN/parser_voc2012_xml_and_plotting.ipynb) \n\nEXAMPLE:\n<center>\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/img_with_bboex_1.PNG)\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/img_with_bboex_2.PNG)\n</center>  \n\n### 21/06/2018\n#### config setting\nset config class:\n                 for image enhancement:\n<center>\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/21_1.PNG)\n</center>  \n\n#### image enhancement\n<center>\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/21_2.PNG)\n</center>  \nAccording to the config of three peremeters, users could augment image with 3 different ways or using them all.\nFor horizontal and vertical flips, 1/3 probability to triggle\nWith 0,90,180,270 rotation, \nThis function could increase the number of datasets.\n\nimage flips and rotation are realized by opencv and replace of height and width\nNew cordinates of bboxes are calculated acccording to different change of image\n\ndetailed in Github, jupyter notebook: [address](https://github.com/Trouble404/Object-Detection-System-Based-on-CNN-and-Capsule-Network/blob/master/Faster-RCNN/config_set_and_image_enhance.ipynb)\n\nOrignal image:\n<center>\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/21_3.PNG)\n</center>  \nhorizontal flip:\n<center>\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/21_4.PNG)\n</center>  \nVertical filp:\n<center>\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/21_5.PNG)\n</center>  \nRandom rotation:\n<center>\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/21_6.PNG)\n</center>  \nHorizontal and then vertical flips:\n<center>\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/21_7.PNG)\n</center>  \n\n### 22/06/2018\n#### Image rezise\nThis function is to rezise input image to a uniform size with same shortest side\n<center>\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/22_2.PNG)\n</center> \n\nAccording to set the value of shortest side, convergent-divergent or augmented another side proportion\n\nTest:\nLeft image is resized image, in this case, the orignal image amplified.\n<center>\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/22_1.png)\n</center> \n\n#### Class Balance\nWhen training the model, if we sent image with no repeating classes, it may help to improve the performance of model. Therefore, this function is to make sure no repeating classes in two closed input image.\n<center>\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/22_3.PNG)\n</center> \n\nTest:\n<center>\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/22_4.PNG)\n</center> \nRandom output 4 iamge with is function, it could find no repeating classes in two closed image. However, it may reduce the number of trainning image because skip some images.\n\n\n### 25~26/06/2018\n#### Region Proposal Networks(RPN)\n<center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_3.jpg)</center>\nRPN2softmaxanchorsforegroundbackgroundforegroundanchorsbounding box regressionproposalProposalforeground anchorsbounding box regressionproposalsproposalsProposal Layer\n\n#### Anchors\n\n<center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_5.jpg)</center>\n4 (x1,y1,x2,y2) 93 width:height = \\[1:1, 1:2, 2:1\\]\n<center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_6.jpg)</center>\nConv layersfeature maps9anchors2bounding box regression.\n\n#### Code\n\n```python\n\"\"\" intersection of two bboxes\n@param ai: left top x,y and right bottom x,y coordinates of bbox 1\n@param bi: left top x,y and right bottom x,y coordinates of bbox 2\n\n@return: area_union: whether contain target classes\n\n\"\"\"\ndef intersection(ai, bi):\n```\n```python\n\"\"\" union of two bboxes\n@param au: left top x,y and right bottom x,y coordinates of bbox 1\n@param bu: left top x,y and right bottom x,y coordinates of bbox 2\n@param area_intersection: intersection area\n\n@return: area_union: whether contain target classes\n\n\"\"\"\ndef union(au, bu, area_intersection):\n```\n\n```python\n\"\"\" calculate ratio of intersection and union\n@param a: left top x,y and right bottom x,y coordinates of bbox 1\n@param b: left top x,y and right bottom x,y coordinates of bbox 2\n\n@return: ratio of intersection and union of two bboxes\n\n\"\"\"\ndef iou(a, b):\n```\n**IOU is used to bounding box regression**\n\n---\n** rpn calculation**\n\n1. Traversal all pre-anchors to calculate IOU with GT bboxes\n2. Set number and proprty of pre-anchors\n3. return specity number of result(Anchors)\n\n```python\n\"\"\" \n\n@param C: configuration\n@param img_data: parsered xml information\n@param width: orignal width of image\n@param hegiht: orignal height of image\n@param resized_width: resized width of image after image processing\n@param resized_heighth: resized height of image after image processing\n@param img_length_calc_function: Keras's image_dim_ordering function\n\n@return: np.copy(y_rpn_cls): whether contain target classes\n@return: np.copy(y_rpn_regr): corrspoding return of gradient\n\n\"\"\"\n\ndef calc_rpn(C, img_data, width, height, resized_width, resized_height, img_length_calc_function):\n```\n\nnum_regions256 \n\n\nInitialise paramters: see [jupyter notebook](https://github.com/Trouble404/Object-Detection-System-Based-on-CNN-and-Capsule-Network/blob/master/Faster-RCNN/rpn_calculation.ipynb)\n\nCalculate the size of map feature:\n```python\n(output_width, output_height) = img_length_calc_function(resized_width, resized_height)\n```\n<br>\nGet the GT box coordinates, and resize to account for image resizing\nafter rezised functon, the coordinates of bboxes need to re-calculation:\n```python\nfor bbox_num, bbox in enumerate(img_data['bboxes']):\n\tgta[bbox_num, 0] = bbox['x1'] * (resized_width / float(width))\n\tgta[bbox_num, 1] = bbox['x2'] * (resized_width / float(width))\n\tgta[bbox_num, 2] = bbox['y1'] * (resized_height / float(height))\n\tgta[bbox_num, 3] = bbox['y2'] * (resized_height / float(height))\n```\ngtax1,x2,y1,y2x1,y1,x2,y2\n<br>\nTraverse all possible group of sizes\nanchor box scales \\[128, 256, 512\\]\nanchor box ratios \\[1:1,1:2,2:1\\]\n```python\nfor anchor_size_idx in range(len(anchor_sizes)):\n\tfor anchor_ratio_idx in range(len(anchor_ratios)):\n\t\tanchor_x = anchor_sizes[anchor_size_idx] * anchor_ratios[anchor_ratio_idx][0]\n\t\tanchor_y = anchor_sizes[anchor_size_idx] * anchor_ratios[anchor_ratio_idx][1]\n```\nTraver one bbox group, all pre boxes generated by anchors\n\noutput_widthoutput_heightwidth and height of map feature\ndownscalemapping ration, defualt 16\nif to delete box out of iamge\n\n```python\nfor ix in range(output_width):\n\tx1_anc = downscale * (ix + 0.5) - anchor_x / 2\n\tx2_anc = downscale * (ix + 0.5) + anchor_x / 2\n\n\tif x1_anc < 0 or x2_anc > resized_width:\n\t\tcontinue\n\n\tfor jy in range(output_height):\n\t\ty1_anc = downscale * (jy + 0.5) - anchor_y / 2\n\t\ty2_anc = downscale * (jy + 0.5) + anchor_y / 2\n\n\t\tif y1_anc < 0 or y2_anc > resized_height:\n\t\t\tcontinue\n```\n<br>\n\n\n\nbboxes bboxIOU\ncurr_ioubbox\n\ntx\nty:tx\ntw:bbox\nth:\n\n```python\nif curr_iou > best_iou_for_bbox[bbox_num] or curr_iou > C.rpn_max_overlap:\n\tcx = (gta[bbox_num, 0] + gta[bbox_num, 1]) / 2.0\n\tcy = (gta[bbox_num, 2] + gta[bbox_num, 3]) / 2.0\n\tcxa = (x1_anc + x2_anc) / 2.0\n\tcya = (y1_anc + y2_anc) / 2.0\n\n\ttx = (cx - cxa) / (x2_anc - x1_anc)\n\tty = (cy - cya) / (y2_anc - y1_anc)\n\ttw = np.log((gta[bbox_num, 1] - gta[bbox_num, 0]) / (x2_anc - x1_anc))\n\tth = np.log((gta[bbox_num, 3] - gta[bbox_num, 2])) / (y2_anc - y1_anc)\n```\nFaster RCNNforeground anchorground truth $(t_x, t_y)$ \n<center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/25_2.jpg)</center>\nbouding box regressioncnn feature AnchorGT $(t_x, t_y, t_w, t_h)$ \nbouding box regressionAnchor $(t_x, t_y, t_w, t_h)$Anchor\n\n<br>\n\n\nbbox\npos\nbest_iou_for_loc\nnum_anchors_for_bbox[bbox_num]bboxpos\nnegneutral\nnegbbox\n\n```python\nif img_data['bboxes'][bbox_num]['class'] != 'bg':\n\n\t# all GT boxes should be mapped to an anchor box, so we keep track of which anchor box was best\n\tif curr_iou > best_iou_for_bbox[bbox_num]:\n\t\tbest_anchor_for_bbox[bbox_num] = [jy, ix, anchor_ratio_idx, anchor_size_idx]\n\t\tbest_iou_for_bbox[bbox_num] = curr_iou\n\t\tbest_x_for_bbox[bbox_num, :] = [x1_anc, x2_anc, y1_anc, y2_anc]\n\t\tbest_dx_for_bbox[bbox_num, :] = [tx, ty, tw, th]\n\n\tif curr_iou > C.rpn_max_overlap:\n\t\tbbox_type = 'pos'\n\t\tnum_anchors_for_bbox[bbox_num] += 1\n\t\tif curr_iou > best_iou_for_loc:\n\t\t\tbest_iou_for_loc = curr_iou\n\t\t\tbest_regr = (tx, ty, tw, th)\n\n\tif C.rpn_min_overlap < curr_iou < C.rpn_max_overlap:\n\n\t\tif bbox_type != 'pos':\n\t\t\tbbox_type = 'neutral'\n```\n<br>\nbbox\n\ny_is_box_validnertual\ny_rpn_overlap\ny_rpn_regr:\n```python\nif bbox_type == 'neg':\n    y_is_box_valid[jy, ix, anchor_ratio_idx + n_anchratios * anchor_size_idx] = 1\n    y_rpn_overlap[jy, ix, anchor_ratio_idx + n_anchratios * anchor_size_idx] = 0\nelif bbox_type == 'neutral':\n    y_is_box_valid[jy, ix, anchor_ratio_idx + n_anchratios * anchor_size_idx] = 0\n    y_rpn_overlap[jy, ix, anchor_ratio_idx + n_anchratios * anchor_size_idx] = 0\nelse:\n    y_is_box_valid[jy, ix, anchor_ratio_idx + n_anchratios * anchor_size_idx] = 1\n    y_rpn_overlap[jy, ix, anchor_ratio_idx + n_anchratios * anchor_size_idx] = 1\n    start = 4 * (anchor_ratio_idx + n_anchratios * anchor_size_idx)\n    y_rpn_regr[jy, ix, start:start+4] = best_regr\n```\n<br>\nbboxposanchorpos\n```python\nfor idx in range(num_anchors_for_bbox.shape[0]):\n\tif num_anchors_for_bbox[idx] == 0:\n\t\t# no box with an IOU greater than zero ...\n\t\tif best_anchor_for_bbox[idx, 0] == -1:\n\t\t\tcontinue\n\t\ty_is_box_valid[best_anchor_for_bbox[idx,0], best_anchor_for_bbox[idx,1], best_anchor_for_bbox[idx,2] + n_anchratios *best_anchor_for_bbox[idx,3]] = 1\n\t\ty_rpn_overlap[best_anchor_for_bbox[idx,0], best_anchor_for_bbox[idx,1], best_anchor_for_bbox[idx,2] + n_anchratios *best_anchor_for_bbox[idx,3]] = 1\n\t\tstart = 4 * (best_anchor_for_bbox[idx,2] + n_anchratios * best_anchor_for_bbox[idx,3])\n\t\ty_rpn_regr[best_anchor_for_bbox[idx,0], best_anchor_for_bbox[idx,1], start:start+4] = best_dx_for_bbox[idx, :]\n```\n<br>\n, Tensorflow batch size,  \n```python\n\ty_rpn_overlap = np.transpose(y_rpn_overlap, (2, 0, 1))\n\ty_rpn_overlap = np.expand_dims(y_rpn_overlap, axis=0)\n\n\ty_is_box_valid = np.transpose(y_is_box_valid, (2, 0, 1))\n\ty_is_box_valid = np.expand_dims(y_is_box_valid, axis=0)\n\n\ty_rpn_regr = np.transpose(y_rpn_regr, (2, 0, 1))\n\ty_rpn_regr = np.expand_dims(y_rpn_regr, axis=0)\n```\n<br>\nnum_regions\nposnum_regions / 2pos\nposnegnum_regionsneg\n 256128positive,128negative  \n```python\npos_locs = np.where(np(y_rpn_overlap[0, :, :, :] =.logical_and= 1, y_is_box_valid[0, :, :, :] == 1))\nneg_locs = np.where(np.logical_and(y_rpn_overlap[0, :, :, :] == 0, y_is_box_valid[0, :, :, :] == 1))\nnum_regions = 256\n\nif len(pos_locs[0]) > num_regions / 2:\n\tval_locs = random.sample(range(len(pos_locs[0])), len(pos_locs[0]) - num_regions / 2)\n\ty_is_box_valid[0, pos_locs[0][val_locs], pos_locs[1][val_locs], pos_locs[2][val_locs]] = 0\n\tnum_pos = num_regions / 2\n\nif len(neg_locs[0]) + num_pos > num_regions:\n\tval_locs = random.sample(range(len(neg_locs[0])), len(neg_locs[0]) - num_pos)\n\t y_is_box_valid[0, neg_locs[0][val_locs], neg_locs[1][val_locs], neg_locs[2][val_locs]] = 0\n```\n\n<br>\n\n### 27/06/2018\n#### project brief\nRe organization of Project plan\n\n#### Anchors Iterative\nIntegration of privous work:\nIn each anchor: config file -> rpn_stride = 16 means generate one anchor in 16 pixels\n[Jupyter Notebook address](https://github.com/Trouble404/Object-Detection-System-Based-on-CNN-and-Capsule-Network/blob/master/Faster-RCNN/anchor_rpn.ipynb)\n\n\nFunction description\n```python\n\"\"\"\n@param all_img_data: Parsered xml file  \n@param class_count: Counting of the number of all classes objects\n@param C: Configuration class\n@param img_length_calc_function: resnet's get_img_output_length() function\n@param backend: Tensorflow in this project\n#param mode: train or val\n\nyield np.copy(x_img), [np.copy(y_rpn_cls), np.copy(y_rpn_regr)], img_data_aug\n@return: np.copy(x_img): image's matrix data\n@return: [np.copy(y_rpn_cls), np.copy(y_rpn_regr)]: calculated rpn class and radient\n@return: img_data_aug: correspoding parsed xml information\n\n\"\"\"\n\ndef get_anchor_gt(all_img_data, class_count, C, img_length_calc_function, backend, mode='train'):\n```\n<br>\n**Traverse all input image based on input xml information**\n\n* Apply class balance function: \n```python\nC.balanced_classes = True\nsample_selector = image_processing.SampleSelector(class_count)\nif C.balanced_classes and sample_selector.skip_sample_for_balanced_class(img_data):\n    continue\n```\n<br>\n\n* Apply image enhance\nif input mode is train, apply image enhance to obtain augmented image xml and matrix, if mode is val, obtain image orignal xml and matrix\n```python\nif mode == 'train':\n    img_data_aug, x_img = image_enhance.augment(img_data, C, augment=True)\nelse:\n    img_data_aug, x_img = image_enhance.augment(img_data, C, augment=False)\n```\nverifacation width and hegiht in xml and matrix\n```python\n(width, height) = (img_data_aug['width'], img_data_aug['height'])\n(rows, cols, _) = x_img.shape\nassert cols == width\nassert rows == height\n```\n<br>\n\n* Apply rezise function\n```python\n(resized_width, resized_height) = image_processing.get_new_img_size(width, height, C.im_size)\nx_img = cv2.resize(x_img, (resized_width, resized_height), interpolation=cv2.INTER_CUBIC)\n```\n<br>\n\n* Apply rpn calculation\n```python\ny_rpn_cls, y_rpn_regr = rpn_calculation.calc_rpn(C, img_data_aug, width, height, resized_width, resized_height, img_length_calc_function)\n```\n<br>\n\n* Zero-center by mean pixel, and preprocess image format\nBGR -> RGB because when apply resnet, it need RGB but in cv2, it use BGR\n```python\nx_img = x_img[:,:, (2, 1, 0)]\n```\n   For using pre-trainning model, needs to mins mean channel in each dim\n```python\nx_img = x_img.astype(np.float32)\nx_img[:, :, 0] -= C.img_channel_mean[0]\nx_img[:, :, 1] -= C.img_channel_mean[1]\nx_img[:, :, 2] -= C.img_channel_mean[2]\nx_img /= C.img_scaling_factor # default to 1,so no change here\n```\n   expand for batch size\n```python\nx_img = np.expand_dims(x_img, axis=0)\n```\n  for using pre-trainning model, need to sclaling the std to match pre trained model\n```python\ny_rpn_regr[:, y_rpn_regr.shape[1]//2:, :, :] *= C.std_scaling # scaling is 4 here\n```\n  in tensorflow, sort as batch size, width, height, deep\n```python\nif backend == 'tf':\n    x_img = np.transpose(x_img, (0, 3, 2, 1))\n    y_rpn_cls = np.transpose(y_rpn_cls, (0, 3, 2, 1))\n\ty_rpn_regr = np.transpose(y_rpn_regr, (0, 3, 2, 1))\t\t\t\t\t\t\t\t\n```\n  generator to iteror, using next() to loop\n```python\nyield np.copy(x_img), [np.copy(y_rpn_cls), np.copy(y_rpn_regr)], img_data_aug  \n```\n<br>\n\n```python\ndata_gen_train = get_anchor_gt(train_imgs, classes_count, C, nn.get_img_output_length, K.image_dim_ordering(), mode='train')\ndata_gen_val = get_anchor_gt(val_imgs, classes_count, C, nn.get_img_output_length,K.image_dim_ordering(), mode='val')\n```\nTest:\n```python\nimg,rpn,img_aug = next(data_gen_train)\n```\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/26_1.PNG)\n\n### 28/06/2018\n#### Resnet50 structure\n: https://arxiv.org/abs/1512.03385\n\n \n**Is learning better networks as easy as stacking more layers?**\n\n \n**vanishing/exploding gradients**//normalized initialization and intermediate normalization layers \n**degradation**accuracyfigure156-layererror20-layererror\n\nHe kaiMinglayersdegradationidentity mapping \n\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/01_2.jpg)\n\nplain layer \n\n1) Our extremely deep residual nets are easy to optimize, but the counterpart plain nets (that simply stack layers) exhibit higher training error when the depth increases; \n2) Our deep residual nets can easily enjoy accuracy gains from greatly increased depth, producing results substantially better than previous networks.\n\n\n### 29/06/2018\n#### Resnet50 image structure\n![iamge](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/1_01.png)\nResNet2blockIdentity BlockdimensionblockConv Blockdimensionfeature vectordimension\n\nCNNimageconvertdepthfeature mapkernelVGG3x3outputchannelIdentity BlockConv BlockIdentity Block.\n\nConv Block:\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/01_3.png)\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/01_4.png)\nshortcut pathconv2D layer1x1 filter sizemain pathdimensionshortcut path.\n\n## July\n### 02/07/2018\n#### Construct resnet by keras\nxF(x)shape\n\nkerasidentity_blockconv_block\n\n```python\nfrom keras.models import Model\nfrom keras.layers import Input,Dense,BatchNormalization,Conv2D,MaxPooling2D,AveragePooling2D,ZeroPadding2D\nfrom keras.layers import add,Flatten\nfrom keras.optimizers import SGD\n```\n\nidentity_block, convshortcut\n```python\ndef Conv2d_BN(x, nb_filter,kernel_size, strides=(1,1), padding='same',name=None):\n    if name is not None:\n        bn_name = name + '_bn'\n        conv_name = name + '_conv'\n    else:\n        bn_name = None\n        conv_name = None\n \n    x = Conv2D(nb_filter,kernel_size,padding=padding,strides=strides,activation='relu',name=conv_name)(x)\n    x = BatchNormalization(axis=3,name=bn_name)(x)\n    return x\n    \ndef Conv_Block(inpt,nb_filter,kernel_size,strides=(1,1), with_conv_shortcut=False):\n    x = Conv2d_BN(inpt,nb_filter=nb_filter[0],kernel_size=(1,1),strides=strides,padding='same')\n    x = Conv2d_BN(x, nb_filter=nb_filter[1], kernel_size=(3,3), padding='same')\n    x = Conv2d_BN(x, nb_filter=nb_filter[2], kernel_size=(1,1), padding='same')\n    if with_conv_shortcut:\n        shortcut = Conv2d_BN(inpt,nb_filter=nb_filter[2],strides=strides,kernel_size=kernel_size)\n        x = add([x,shortcut])\n        return x\n    else:\n        x = add([x,inpt])\n        return x\n```\n\nidentity_blockconv_block\n```python\ninpt = Input(shape=(224,224,3))\nx = ZeroPadding2D((3,3))(inpt)\nx = Conv2d_BN(x,nb_filter=64,kernel_size=(7,7),strides=(2,2),padding='valid')\nx = MaxPooling2D(pool_size=(3,3),strides=(2,2),padding='same')(x)\n \nx = Conv_Block(x,nb_filter=[64,64,256],kernel_size=(3,3),strides=(1,1),with_conv_shortcut=True)\nx = Conv_Block(x,nb_filter=[64,64,256],kernel_size=(3,3))\nx = Conv_Block(x,nb_filter=[64,64,256],kernel_size=(3,3))\n \nx = Conv_Block(x,nb_filter=[128,128,512],kernel_size=(3,3),strides=(2,2),with_conv_shortcut=True)\nx = Conv_Block(x,nb_filter=[128,128,512],kernel_size=(3,3))\nx = Conv_Block(x,nb_filter=[128,128,512],kernel_size=(3,3))\nx = Conv_Block(x,nb_filter=[128,128,512],kernel_size=(3,3))\n \nx = Conv_Block(x,nb_filter=[256,256,1024],kernel_size=(3,3),strides=(2,2),with_conv_shortcut=True)\nx = Conv_Block(x,nb_filter=[256,256,1024],kernel_size=(3,3))\nx = Conv_Block(x,nb_filter=[256,256,1024],kernel_size=(3,3))\nx = Conv_Block(x,nb_filter=[256,256,1024],kernel_size=(3,3))\nx = Conv_Block(x,nb_filter=[256,256,1024],kernel_size=(3,3))\nx = Conv_Block(x,nb_filter=[256,256,1024],kernel_size=(3,3))\n \nx = Conv_Block(x,nb_filter=[512,512,2048],kernel_size=(3,3),strides=(2,2),with_conv_shortcut=True)\nx = Conv_Block(x,nb_filter=[512,512,2048],kernel_size=(3,3))\nx = Conv_Block(x,nb_filter=[512,512,2048],kernel_size=(3,3))\nx = AveragePooling2D(pool_size=(7,7))(x)\nx = Flatten()(x)\nx = Dense(1000,activation='softmax')(x)\n\nmodel = Model(inputs=inpt,outputs=x)\nsgd = SGD(decay=0.0001,momentum=0.9)\nmodel.compile(loss='categorical_crossentropy',optimizer=sgd,metrics=['accuracy'])\nmodel.summary()\n```\n\n[jupyter notebook](https://github.com/Trouble404/Object-Detection-System-Based-on-CNN-and-Capsule-Network/blob/master/Faster-RCNN/Resnet50/resnet50_keras.ipynb)\n\n\n### 03/07/2018\n#### load pre-trained model of resnet50\n\n\n* ResNet50resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n* ResNet50\n* \n* \n* blockblockResNet50 fine-tune\n\n[](https://github.com/fchollet/deep-learning-models/releases)\n\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/01_7.JPG)\n\n\nidentity\n```python\ndef identity_block(X, f, filters, stage, block):\n    # defining name basis\n    conv_name_base = 'res' + str(stage) + block + '_branch'\n    bn_name_base = 'bn' + str(stage) + block + '_branch'\n    \n    # Retrieve Filters\n    F1, F2, F3 = filters\n    \n    # Save the input value. You'll need this later to add back to the main path. \n    X_shortcut = X\n    \n    # First component of main path\n    X = Conv2D(filters = F1, kernel_size = (1, 1), strides = (1,1), padding = 'valid', name = conv_name_base + '2a')(X)\n    X = BatchNormalization(axis = 3, name = bn_name_base + '2a')(X)\n    X = Activation('relu')(X)\n    \n    # Second component of main path (3 lines)\n    X = Conv2D(filters= F2, kernel_size=(f,f),strides=(1,1),padding='same',name=conv_name_base + '2b')(X)\n    X = BatchNormalization(axis=3,name=bn_name_base+'2b')(X)\n    X = Activation('relu')(X)\n \n    # Third component of main path (2 lines)\n    X = Conv2D(filters=F3,kernel_size=(1,1),strides=(1,1),padding='valid',name=conv_name_base+'2c')(X)\n    X = BatchNormalization(axis=3,name=bn_name_base+'2c')(X)\n \n    # Final step: Add shortcut value to main path, and pass it through a RELU activation (2 lines)\n    X = Add()([X, X_shortcut])\n    X = Activation('relu')(X)\n    return X\n```\n\nconv\n```python\ndef convolutional_block(X, f, filters, stage, block, s = 2):\n    \n    # defining name basis\n    conv_name_base = 'res' + str(stage) + block + '_branch'\n    bn_name_base = 'bn' + str(stage) + block + '_branch'\n    \n    # Retrieve Filters\n    F1, F2, F3 = filters\n    \n    # Save the input value\n    X_shortcut = X\n \n    ##### MAIN PATH #####\n    # First component of main path \n    X = Conv2D(F1, (1, 1), strides = (s,s),padding='valid',name = conv_name_base + '2a')(X)\n    X = BatchNormalization(axis = 3, name = bn_name_base + '2a')(X)\n    X = Activation('relu')(X)\n \n    # Second component of main path (3 lines)\n    X = Conv2D(F2,(f,f),strides=(1,1),padding='same',name=conv_name_base+'2b')(X)\n    X = BatchNormalization(axis=3,name=bn_name_base+'2b')(X)\n    X = Activation('relu')(X)\n \n    # Third component of main path (2 lines)\n    X = Conv2D(F3,(1,1),strides=(1,1),padding='valid',name=conv_name_base+'2c')(X)\n    X = BatchNormalization(axis=3,name=bn_name_base+'2c')(X)\n \n    ##### SHORTCUT PATH #### (2 lines)\n    X_shortcut = Conv2D(F3,(1,1),strides=(s,s),padding='valid',name=conv_name_base+'1')(X_shortcut)\n    X_shortcut = BatchNormalization(axis=3,name =bn_name_base+'1')(X_shortcut)\n \n    # Final step: Add shortcut value to main path, and pass it through a RELU activation (2 lines)\n    X = Add()([X,X_shortcut])\n    X = Activation('relu')(X)\n    return X\n```\n\nresnet50\n```python\ndef ResNet50(input_shape = (64, 64, 3), classes = 30):\n    # Define the input as a tensor with shape input_shape\n    X_input = Input(input_shape)\n \n    # Zero-Padding\n    X = ZeroPadding2D((3, 3))(X_input)\n    \n    # Stage 1\n    X = Conv2D(64, (7, 7), strides = (2, 2), name = 'conv1')(X)\n    X = BatchNormalization(axis = 3, name = 'bn_conv1')(X)\n    X = Activation('relu')(X)\n    X = MaxPooling2D((3, 3), strides=(2, 2))(X)\n \n    # Stage 2\n    X = convolutional_block(X, f = 3, filters = [64, 64, 256], stage = 2, block='a', s = 1)\n    X = identity_block(X, 3, [64, 64, 256], stage=2, block='b')\n    X = identity_block(X, 3, [64, 64, 256], stage=2, block='c')\n \n    ### START CODE HERE ###\n \n    # Stage 3 (4 lines)\n    X = convolutional_block(X, f = 3,filters= [128,128,512],stage=3,block='a',s=2)\n    X = identity_block(X,3,[128,128,512],stage=3,block='b')\n    X = identity_block(X,3,[128,128,512],stage=3,block='c')\n    X = identity_block(X,3,[128,128,512],stage=3,block='d')\n \n    # Stage 4 (6 lines)\n    X = convolutional_block(X,f=3,filters=[256,256,1024],stage=4,block='a',s=2)\n    X = identity_block(X,3,[256,256,1024],stage=4,block='b')\n    X = identity_block(X,3,[256,256,1024],stage=4,block='c')\n    X = identity_block(X,3,[256,256,1024],stage=4,block='d')\n    X = identity_block(X,3,[256,256,1024],stage=4,block='e')\n    X = identity_block(X,3,[256,256,1024],stage=4,block='f')\n \n    # Stage 5 (3 lines)\n    X = convolutional_block(X, f = 3,filters= [512,512,2048],stage=5,block='a',s=2)\n    X = identity_block(X,3,[512,512,2048],stage=5,block='b')\n    X = identity_block(X,3,[512,512,2048],stage=5,block='c')\n \n    # AVGPOOL (1 line). Use \"X = AveragePooling2D(...)(X)\"\n    X = AveragePooling2D((2,2),strides=(2,2))(X)\n \n    # output layer\n    X = Flatten()(X)\n    model = Model(inputs = X_input, outputs = X, name='ResNet50')\n \n    return model\n```\n\n\n```python\nbase_model = ResNet50(input_shape=(224,224,3),classes=30) \nbase_model.load_weights('resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5')\n```\n\n\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/01_8.JPG)\n### 04/07/2018\n#### Loading pre-trained model\nkerasmodel.load_weightsmodel.load_weights('my_model_weights.h5', by_name=True) x= Dense(100, weights=oldModel.layers[1].get_weights())(x)\n\n\n```python\ntry:\n    base_model.load_weights('resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5',by_name=True)\n    print(\"load successful\")\nexcept:\n    print(\"load failed\")\n```\n[jupyter notebook](https://github.com/Trouble404/Object-Detection-System-Based-on-CNN-and-Capsule-Network/blob/master/Faster-RCNN/Resnet50/resnet50_pre_load.ipynb)\n\n### 05~06/07/2018\n#### construct faster rcnn net\n**RoiPoolingConv**\n\nROI\nROIRegion of Interest\n1Fast RCNN RoISelective Search\n2Faster RCNNRPNRoIs\nkerasLayer\n```python\nclass RoiPoolingConv(Layer):\n```\n[](http://keras-cn.readthedocs.io/en/latest/layers/writting_layer/)\n\n\n\\*\\*[kwargs](https://www.cnblogs.com/xuyuanyuan123/p/6674645.html)\n[super](https://link.zhihu.com/?target=http%3A//python.jobbole.com/86787/):\n```python\n'''ROI pooling layer for 2D inputs.\n    # Arguments\n        pool_size: int\n            Size of pooling region to use. pool_size = 7 will result in a 7x7 region.\n        num_rois: number of regions of interest to be used\n    '''\n#  \n    def __init__(self, pool_size, num_rois, **kwargs):\n\n        self.dim_ordering = K.image_dim_ordering()\n        # print error when kernel not tensorflow or thoean\n        assert self.dim_ordering in {'tf'}, 'dim_ordering must be in tf'\n\n        self.pool_size = pool_size\n        self.num_rois = num_rois\n\n        super(RoiPoolingConv, self).__init__(**kwargs)\n```\n\n:\n```python\ndef build(self, input_shape):\n        if self.dim_ordering == 'tf':\n            self.nb_channels = input_shape[0][3]\n```\n\n\n```python\ndef compute_output_shape(self, input_shape):\n        if self.dim_ordering == 'tf':\n            return None, self.num_rois, self.pool_size, self.pool_size, self.nb_channels\n```\n\n,, output:\n```python\ndef call(self, x, mask=None):\n\n        assert(len(x) == 2)\n\n        img = x[0]\n        rois = x[1]\n\n        input_shape = K.shape(img)\n\n        outputs = []\n\n        for roi_idx in range(self.num_rois):\n\n            x = rois[0, roi_idx, 0]\n            y = rois[0, roi_idx, 1]\n            w = rois[0, roi_idx, 2]\n            h = rois[0, roi_idx, 3]\n            \n            row_length = w / float(self.pool_size)\n            col_length = h / float(self.pool_size)\n\n            num_pool_regions = self.pool_size\n\n            if self.dim_ordering == 'tf':\n                x = K.cast(x, 'int32')\n                y = K.cast(y, 'int32')\n                w = K.cast(w, 'int32')\n                h = K.cast(h, 'int32')\n\n                # resize porposal of feature map\n                rs = tf.image.resize_images(img[:, y:y+h, x:x+w, :], (self.pool_size, self.pool_size))\n                outputs.append(rs)\n\n        # outputsshape:(?, 7, 7, 512)\n        final_output = K.concatenate(outputs, axis=0)\n        final_output = K.reshape(final_output, (1, self.num_rois, self.pool_size, self.pool_size, self.nb_channels))\n        # shape:(1, 32, 7, 7, 512)\n        final_output = K.permute_dimensions(final_output, (0, 1, 2, 3, 4))\n\n        return final_output\n```\nbatchvectorbatchRoIvectorchannel \\* w \\* hRoI Poolingboxw \\* h.\n\n**TimeDistributed **\nFastRcnnROIpoolingRoiKerasTimeDistributed\n\nRelutensorTimeDistributedconv2DTimeDistributedROIROI\n\nFaster RCNNbboxnum_roisnum_roisTimeDistributedwrap\n\nconv  identity\n```python\ndef conv_block_td(input_tensor, kernel_size, filters, stage, block, input_shape, strides=(2, 2), trainable=True):\n\n    # conv block time distributed\n\n    nb_filter1, nb_filter2, nb_filter3 = filters\n\n    bn_axis = 3\n\n    conv_name_base = 'res' + str(stage) + block + '_branch'\n    bn_name_base = 'bn' + str(stage) + block + '_branch'\n\n    x = TimeDistributed(Convolution2D(nb_filter1, (1, 1), strides=strides, trainable=trainable, kernel_initializer='normal'), input_shape=input_shape, name=conv_name_base + '2a')(input_tensor)\n    x = TimeDistributed(BatchNormalization(axis=bn_axis), name=bn_name_base + '2a')(x)\n    x = Activation('relu')(x)\n\n    x = TimeDistributed(Convolution2D(nb_filter2, (kernel_size, kernel_size), padding='same', trainable=trainable, kernel_initializer='normal'), name=conv_name_base + '2b')(x)\n    x = TimeDistributed(BatchNormalization(axis=bn_axis), name=bn_name_base + '2b')(x)\n    x = Activation('relu')(x)\n\n    x = TimeDistributed(Convolution2D(nb_filter3, (1, 1), kernel_initializer='normal'), name=conv_name_base + '2c', trainable=trainable)(x)\n    x = TimeDistributed(BatchNormalization(axis=bn_axis), name=bn_name_base + '2c')(x)\n\n    shortcut = TimeDistributed(Convolution2D(nb_filter3, (1, 1), strides=strides, trainable=trainable, kernel_initializer='normal'), name=conv_name_base + '1')(input_tensor)\n    shortcut = TimeDistributed(BatchNormalization(axis=bn_axis), name=bn_name_base + '1')(shortcut)\n\n    x = Add()([x, shortcut])\n    x = Activation('relu')(x)\n    return x\n```\n\n```python\ndef identity_block_td(input_tensor, kernel_size, filters, stage, block, trainable=True):\n\n    # identity block time distributed\n\n    nb_filter1, nb_filter2, nb_filter3 = filters\n\n    bn_axis = 3\n\n    conv_name_base = 'res' + str(stage) + block + '_branch'\n    bn_name_base = 'bn' + str(stage) + block + '_branch'\n\n    x = TimeDistributed(Convolution2D(nb_filter1, (1, 1), trainable=trainable, kernel_initializer='normal'), name=conv_name_base + '2a')(input_tensor)\n    x = TimeDistributed(BatchNormalization(axis=bn_axis), name=bn_name_base + '2a')(x)\n    x = Activation('relu')(x)\n\n    x = TimeDistributed(Convolution2D(nb_filter2, (kernel_size, kernel_size), trainable=trainable, kernel_initializer='normal',padding='same'), name=conv_name_base + '2b')(x)\n    x = TimeDistributed(BatchNormalization(axis=bn_axis), name=bn_name_base + '2b')(x)\n    x = Activation('relu')(x)\n\n    x = TimeDistributed(Convolution2D(nb_filter3, (1, 1), trainable=trainable, kernel_initializer='normal'), name=conv_name_base + '2c')(x)\n    x = TimeDistributed(BatchNormalization(axis=bn_axis), name=bn_name_base + '2c')(x)\n\n    x = Add()([x, input_tensor])\n    x = Activation('relu')(x)\n\n    return x\n```\n2DTimeDistributedDense\n\n**resnet50stage**\n```python\ndef classifier_layers(x, input_shape, trainable=False):\n\n    # Stage 5\n    x = conv_block_td(x, 3, [512, 512, 2048], stage=5, block='a', input_shape=input_shape, strides=(2, 2), trainable=trainable)\n    x = identity_block_td(x, 3, [512, 512, 2048], stage=5, block='b', trainable=trainable)\n    x = identity_block_td(x, 3, [512, 512, 2048], stage=5, block='c', trainable=trainable)\n\n    # AVGPOOL\n    x = TimeDistributed(AveragePooling2D((7, 7)), name='avg_pool')(x)\n\n    return x\n```\n\n* RoiPoolingConvshape(1, 32, 7, 7, 512)batch_size,\n* TimeDistributed3D1323232\n* out_classshape:(?, 32, 21); out_regrshape:(?, 32, 80)\n```python\ndef classifier(base_layers, input_rois, num_rois, nb_classes = 21, trainable=False):\n\n    pooling_regions = 14\n    input_shape = (num_rois,14,14,1024)\n\n    out_roi_pool = RoiPoolingConv(pooling_regions, num_rois)([base_layers, input_rois])\n    out = classifier_layers(out_roi_pool, input_shape=input_shape, trainable=True)\n\n    out = TimeDistributed(Flatten())(out)\n\n    out_class = TimeDistributed(Dense(nb_classes, activation='softmax', kernel_initializer='zero'), name='dense_class_{}'.format(nb_classes))(out)\n    # note: no regression target for bg class\n    out_regr = TimeDistributed(Dense(4 * (nb_classes-1), activation='linear', kernel_initializer='zero'), name='dense_regress_{}'.format(nb_classes))(out)\n    return [out_class, out_regr]\n```\n\n**RPN**\n* x_class:sigmoidnum_anchors\n* x_regr\n```python\ndef rpn(base_layers,num_anchors):\n\n    x = Convolution2D(512, (3, 3), padding='same', activation='relu', kernel_initializer='normal', name='rpn_conv1')(base_layers)\n\n    x_class = Convolution2D(num_anchors, (1, 1), activation='sigmoid', kernel_initializer='uniform', name='rpn_out_class')(x)\n    x_regr = Convolution2D(num_anchors * 4, (1, 1), activation='linear', kernel_initializer='zero', name='rpn_out_regress')(x)\n\n    return [x_class, x_regr, base_layers]\n```\n\n**resnet**\n```python\ndef nn_base(input_tensor=None, trainable=False):\n\n    # Determine proper input shape\n\n    input_shape = (None, None, 3)\n\n    if input_tensor is None:\n        img_input = Input(shape=input_shape)\n    else:\n        if not K.is_keras_tensor(input_tensor):\n            img_input = Input(tensor=input_tensor, shape=input_shape)\n        else:\n            img_input = input_tensor\n\n    bn_axis = 3\n\n    # Zero-Padding\n    x = ZeroPadding2D((3, 3))(img_input)\n\n    # Stage 1\n    x = Convolution2D(64, (7, 7), strides=(2, 2), name='conv1', trainable = trainable)(x)\n    x = BatchNormalization(axis=bn_axis, name='bn_conv1')(x)\n    x = Activation('relu')(x)\n    x = MaxPooling2D((3, 3), strides=(2, 2))(x)\n\n    # Stage 2\n    x = conv_block(x, 3, [64, 64, 256], stage=2, block='a', strides=(1, 1), trainable = trainable)\n    x = identity_block(x, 3, [64, 64, 256], stage=2, block='b', trainable = trainable)\n    x = identity_block(x, 3, [64, 64, 256], stage=2, block='c', trainable = trainable)\n\n    # Stage 3\n    x = conv_block(x, 3, [128, 128, 512], stage=3, block='a', trainable = trainable)\n    x = identity_block(x, 3, [128, 128, 512], stage=3, block='b', trainable = trainable)\n    x = identity_block(x, 3, [128, 128, 512], stage=3, block='c', trainable = trainable)\n    x = identity_block(x, 3, [128, 128, 512], stage=3, block='d', trainable = trainable)\n\n    # Stage 4\n    x = conv_block(x, 3, [256, 256, 1024], stage=4, block='a', trainable = trainable)\n    x = identity_block(x, 3, [256, 256, 1024], stage=4, block='b', trainable = trainable)\n    x = identity_block(x, 3, [256, 256, 1024], stage=4, block='c', trainable = trainable)\n    x = identity_block(x, 3, [256, 256, 1024], stage=4, block='d', trainable = trainable)\n    x = identity_block(x, 3, [256, 256, 1024], stage=4, block='e', trainable = trainable)\n    x = identity_block(x, 3, [256, 256, 1024], stage=4, block='f', trainable = trainable)\n\n    return x\n```\n\n****\n```python\n# define the base network (resnet here)\nshared_layers = nn.nn_base(img_input, trainable=True)\n\n# define the RPN, built on the base layers\n# 9 types of anchors\nnum_anchors = len(C.anchor_box_scales) * len(C.anchor_box_ratios)\nrpn = nn.rpn(shared_layers, num_anchors)\n\nclassifier = nn.classifier(shared_layers, roi_input, C.num_rois, nb_classes=len(classes_count), trainable=True)\n\nmodel_rpn = Model(img_input, rpn[:2])\nmodel_classifier = Model([img_input, roi_input], classifier)\n\n# this is a model that holds both the RPN and the classifier, used to load/save weights for the models\nmodel_all = Model([img_input, roi_input], rpn[:2] + classifier)\n```\n\n[jupyter notebook](https://github.com/Trouble404/Object-Detection-System-Based-on-CNN-and-Capsule-Network/blob/master/Faster-RCNN/Resnet50/workflow_model.ipynb)\n\n### 09/07/2018\n#### Loss define\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/6_1.JPG)\n\n(Multi-task Loss Function)Softmax Classification LossBounding Box Regression Loss\n\n$L(\\{p_{i}\\},\\{t_{i}\\}) = \\frac{1}{N_{cls}}\\sum_{i}L_{cls}(p_{i},p_{i}^{\\ast}) + \\lambda\\frac{1}{N_{reg}}\\sum_{i}L_{reg}(t_{i},t_{i}^{\\ast})$\n\n**Softmax Classification**\nRPN(cls)2k = 18conv5-3WH18conv5-39anchorsanchorscore(fg/bg)anchorSoftmaxreshape\n$p_{i}$$p_{i}^{\\ast}$(label)anchor$p_{i}^{\\ast}$1$p_{i}^{\\ast}$0$L_{cls}$(log loss)\n\n**Bounding Box Regression**\nRPN4k = 36$[x,y,w,h]$box(predicted box)$[x,y,w,h]$anchor$[x_{a},y_{a},w_{a},h_{a}]$ground truth$[x^{\\ast},y^{\\ast},w^{\\ast},h^{\\ast}]$anchor{$t$}ground truthanchor{$t^{\\ast}$}\n\n$t_{x}=\\frac{(x-x_{a})}{w_{a}}$ , $t_{y}=\\frac{(y-y_{a})}{h_{a}}$ , $t_{w}=log(\\frac{w}{w_{a}})$ , $t_{h}=log(\\frac{h}{h_{a}})$ (1)\n$t_{x}^{\\ast}=\\frac{(x^{\\ast}-x_{a})}{w_{a}}$ , $t_{y}^{\\ast}=\\frac{(y^{\\ast}-y_{a})}{h_{a}}$ , $t_{w}^{\\ast}=log(\\frac{w^{\\ast}}{w_{a}})$ , $t_{h}^{\\ast}=log(\\frac{h^{\\ast}}{h_{a}})$ (2)\n\n{t}${t^{\\ast}}$${t}$${t^{\\ast}}$${t}$(1)\n\nSmooth L1:\n\n$$ Smooth_{L1}(x) =\\left\\{\n\\begin{aligned}\n0.5x^{2} \\ \\ |x| \\leqslant 1\\\\\n|x| - 0.5 \\ \\ otherwise \n\\end{aligned}\n\\right.\n$$\n\n$L_{reg} = Smooth_{L1}(t-t^{\\ast})$\nSmooth L1L2L1\n![iamge](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/7_1.JPG)\n\n$p_{i}^{\\ast}L_{reg}$anchor$(p_{i}^{\\ast}=1)$anchorbboxground truth(IoU)bboxground truthwork(cls)(reg){p}{t}$N_{cls}$$N_{reg}$\n\n### 10/07/2018\n#### loss code\n  generator to iteror, using next() to loop\n```python\nyield np.copy(x_img), [np.copy(y_rpn_cls), np.copy(y_rpn_regr)], img_data_aug  \n```\nRpn calculation:\n```python\nimg,rpn,img_aug = next(data_gen_train)\n```\n\n ![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/7_2.JPG)\n\ndef \n\n\n\n\n$L$  cls \n$L(\\{p_{i}\\},\\{t_{i}\\}) = \\frac{1}{N_{cls}}\\sum_{i}L_{cls}(p_{i},p_{i}^{\\ast})$\n\n$p_{i}$$p_{i}^{\\ast}$(label)anchor$p_{i}^{\\ast}$1$p_{i}^{\\ast}$0$L_{cls}$(log loss)\n\n  rpn loss cls:\n```python\ndef rpn_loss_cls(num_anchors):\n\tdef rpn_loss_cls_fixed_num(y_true, y_pred):\n            # binary_crossentropy -> logloss\n            # epsilon to increase robustness\n\t\treturn lambda_rpn_class * K.sum(y_true[:, :, :, :num_anchors] * K.binary_crossentropy(y_pred[:, :, :, :], y_true[:, :, :, num_anchors:])) / K.sum(epsilon + y_true[:, :, :, :num_anchors])\n\treturn rpn_loss_cls_fixed_num\n```\n\n$L$  reg \n$L(\\{p_{i}\\},\\{t_{i}\\}) =  \\lambda\\frac{1}{N_{reg}}\\sum_{i}L_{reg}(t_{i},t_{i}^{\\ast})$\nSmooth L1:\n\n$$ Smooth_{L1}(x) =\\left\\{\n\\begin{aligned}\n0.5x^{2} \\ \\ |x| \\leqslant 1\\\\\n|x| - 0.5 \\ \\ otherwise \n\\end{aligned}\n\\right.\n$$\n$L_{reg} = Smooth_{L1}(t-t^{\\ast})$\n\n  rpn loss reg:\n```python\ndef rpn_loss_regr(num_anchors):\n\tdef rpn_loss_regr_fixed_num(y_true, y_pred):\n\n\t\t# difference of ture value and predicted value\n\t\tx = y_true[:, :, :, 4 * num_anchors:] - y_pred\n\t\t# absulote value of difference\n\t\tx_abs = K.abs(x)\n\t\t# if absulote value less than 1, x_bool == 1, else x_bool = 0\n\t\tx_bool = K.cast(K.less_equal(x_abs, 1.0), tf.float32)\n\n\t\treturn lambda_rpn_regr * K.sum(y_true[:, :, :, :4 * num_anchors] * (x_bool * (0.5 * x * x) + (1 - x_bool) * (x_abs - 0.5))) / K.sum(epsilon + y_true[:, :, :, :4 * num_anchors])\n\n\treturn rpn_loss_regr_fixed_num\n```\n\nclasslossclass_loss_clslossK.meanloss\n```python\ndef class_loss_regr(num_classes):\n\tdef class_loss_regr_fixed_num(y_true, y_pred):\n\t\tx = y_true[:, :, 4*num_classes:] - y_pred\n\t\tx_abs = K.abs(x)\n\t\tx_bool = K.cast(K.less_equal(x_abs, 1.0), 'float32')\n\t\treturn lambda_cls_regr * K.sum(y_true[:, :, :4*num_classes] * (x_bool * (0.5 * x * x) + (1 - x_bool) * (x_abs - 0.5))) / K.sum(epsilon + y_true[:, :, :4*num_classes])\n\treturn class_loss_regr_fixed_num\n\n\ndef class_loss_cls(y_true, y_pred):\n\treturn lambda_cls_class * K.mean(categorical_crossentropy(y_true[0, :, :], y_pred[0, :, :]))\n```\n\n### 11/07/2018\n#### Iridis\n#### High Performance Computing (HPC)\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/8_1.jpg)\n[Introduction](https://www.southampton.ac.uk/isolutions/staff/high-performance-computing.page)\n\nIridis 5 specifications\n* #251 in hte world(Based on July 2018 TOPP500 list) with $R_{peak}\\sim1305.6\\ TFlops/s$\n* 464 2.0 GHz nodes with 40 cores per node, 192 GB memeory\n* 10 nodes with 4xGTX1080TI GPUs, 28 cores(hyper-threaded), 128 GB memeory\n* 10 nodes with 2xVolta Tesia GPUs, same as thandard compute\n* 2.2 PB disk with paraller file system (>12GB\\s)\n* 5M Project delivered by OCF/IBM\n\n[MobaXterm](https://mobaxterm.mobatek.net/)\n\n#### create my own conda envieroment\nFllowing instroduction before\n\n#### Slurm command\n\nCommand | Definition\n---- | ---\nsbatch | Submits job scripts into system for execution (queued)\nscancel |  Cancels a job\nscontrol | Used to display Slurm state, several options only available to root\nsinfo | Display state of partitions and nodes\nsqueue | Display state of jobs\nsalloc | Submit a job for execution, or initiate job in real time\n\n** Bash script**\n```bash\n#!/bin/bash\n#SBATCH -J faster_rcnn \n#SBATCH -o train_7.out\n#SBATCH --ntasks=28\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=8\n#SBATCH --time=00:05:00\n#SBATCH --gres=gpu:1\n#SBATCH -p lyceum\n\nmodule load conda\nmodule load cuda\nsource activate project\npython test_frcnn.py\n```\n\n\n### 12~13/07/2018\n#### change plan\n\nfaster r-cnncapsulefaster-rcnnfine turn\n\n1\nResNetIncRes V2ResNeXt  VGG \n\n2RPN\n  RPN  Proposal \n\n3\n   \n\n---\n\n@1ION\nInside outside net: Detecting objects in context with skip pooling and recurrent neural networks\n\n\n1Inside Net\n Inside  ROI  Scale  Feature Map\n Skip-Pooling conv3-4-5-context \n \n\n2Outside Net\n Outside  ROI  Contextual\n RNN  IRNN\n \n\n---\n\n@2 HyperNet\nHypernet: Towards accurate region proposal generation and joint object detection\n Region Proposal \n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/8_2.JPG)\n   \n\n1Hyper Feature Extraction \n Feature  Max Pooling  Deconv\n Feature ConCat  LRN ION  L2 Norm\n\n2Region Proposal Generation\n ConvNet RPN )\n ROI Pooling Conv  FC  Position  ROI Pooling  13\\*13  bin Conv3\\*3\\*4 13\\*13\\*4  Cube FC  256d \n Score+ BBox_Reg  Faster  Location OffSet\n Overlap Greedy NMS  IOU 0.7 Image  1k  Region Top-200  Detetcion\n Edge Box  Deep Box Deep Proposal \n\n3Object Detection\n Fast RCNN\na FC  3*3*63\nb DropOut  0.5  0.25\n Proposal NMS  Box\n\n---\n\n@3 MSCNN\nA Unified Multi-scale Deep Convolutional Neural Network for Fast Object Detection\naScaleScaleFeature\nScaleScaleFeature\n\nb\n\n\ncScale->->Model\n a b Trade-Off\n\ndScale->->->1Model\n\neRCNNProposalCNN\n aPatch\n\nfRPNCNN\n b\n\ng\n cover\n\n---\n\nNMSsoft-nms\nRepulsion loss overlapping  loss\nfaster rcnn\n\n### 16~20/07/2018\n\n\n\n### 23/07/2018\n#### fix boxes location by regrident\nregranchor\n\n```python\n\"\"\" fix boxes with grident\n\n@param X: current cordinates of box\n@param T: coresspoding grident\n\n\n@return: Fixed cordinates of box\n\"\"\"\ndef apply_regr_np(X, T):\n\ttry:\n\t\tx = X[0, :, :]\n\t\ty = X[1, :, :]\n\t\tw = X[2, :, :]\n\t\th = X[3, :, :]\n\n\t\ttx = T[0, :, :]\n\t\tty = T[1, :, :]\n\t\ttw = T[2, :, :]\n\t\tth = T[3, :, :]\n```\n$t_{x}=\\frac{(x-x_{a})}{w_{a}}$ , $t_{y}=\\frac{(y-y_{a})}{h_{a}}$ , $t_{w}=log(\\frac{w}{w_{a}})$ , $t_{h}=log(\\frac{h}{h_{a}})$ (1)\n$t_{x}^{\\ast}=\\frac{(x^{\\ast}-x_{a})}{w_{a}}$ , $t_{y}^{\\ast}=\\frac{(y^{\\ast}-y_{a})}{h_{a}}$ , $t_{w}^{\\ast}=log(\\frac{w^{\\ast}}{w_{a}})$ , $t_{h}^{\\ast}=log(\\frac{h^{\\ast}}{h_{a}})$ (2)\n\n{t}${t^{\\ast}}$${t}$${t^{\\ast}}$${t}$(1)\n\n\n```python\n\t\t# centre cordinate\n\t\tcx = x + w/2.\n\t\tcy = y + h/2.\n\t\t# fixed centre cordinate\n\t\tcx1 = tx * w + cx\n\t\tcy1 = ty * h + cy\n\n\t\t# fixed wdith and height\n\t\tw1 = np.exp(tw.astype(np.float64)) * w\n\t\th1 = np.exp(th.astype(np.float64)) * h\n\n\t\t# fixed left top corner's cordinate\n\t\tx1 = cx1 - w1/2.\n\t\ty1 = cy1 - h1/2.\n\n\t\t# apporximate\n\t\tx1 = np.round(x1)\n\t\ty1 = np.round(y1)\n\t\tw1 = np.round(w1)\n\t\th1 = np.round(h1)\n\t\treturn np.stack([x1, y1, w1, h1])\n\texcept Exception as e:\n\t\tprint(e)\n\t\treturn X\n```\n\n\n#### NMS no max suppression\n\n\n\n```python\n\"\"\" NMS , delete overlapping box\n\n@param boxes: (n,4) box and coresspoding cordinates\n@param probs: (n,) box adn coresspding possibility\n@param overlap_thresh: treshold of delet box overlapping\n@param max_boxes: maximum keeping number of boxes\n\n\n@return: boxes: boxes cordinates(x1,y1,x2,y2)\n@return: probs: coresspoding possibility\n\"\"\"\ndef non_max_suppression_fast(boxes, probs, overlap_thresh=0.9, max_boxes=300):\n```\n\n```python\nif len(boxes) == 0:\n   return []\n\n# grab the coordinates of the bounding boxes\nx1 = boxes[:, 0]\ny1 = boxes[:, 1]\nx2 = boxes[:, 2]\ny2 = boxes[:, 3]\n\nnp.testing.assert_array_less(x1, x2)\nnp.testing.assert_array_less(y1, y2)\n\n# if the bounding boxes integers, convert them to floats --\n# this is important since we'll be doing a bunch of divisions\nif boxes.dtype.kind == \"i\":\n\tboxes = boxes.astype(\"float\")\n```\n\n\n\n\n```python\n# initialize the list of picked indexes\t\npick = []\n\n# calculate the areas\narea = (x2 - x1) * (y2 - y1)\n\n# sort the bounding boxes \nidxs = np.argsort(probs)\n```\npick\n\nprobs\n```python\nwhile len(idxs) > 0:\n# grab the last index in the indexes list and add the\n# index value to the list of picked indexes\nlast = len(idxs) - 1\ni = idxs[last]\npick.append(i)\n```\noverlap_thresh\n\nidxs\noverlap_thresh\nmax_boxes\n```python\n# find the intersection\n\nxx1_int = np.maximum(x1[i], x1[idxs[:last]])\nyy1_int = np.maximum(y1[i], y1[idxs[:last]])\nxx2_int = np.minimum(x2[i], x2[idxs[:last]])\nyy2_int = np.minimum(y2[i], y2[idxs[:last]])\n\nww_int = np.maximum(0, xx2_int - xx1_int)\nhh_int = np.maximum(0, yy2_int - yy1_int)\n\narea_int = ww_int * hh_int\n```\nidxspick\n```python\n# find the union\narea_union = area[i] + area[idxs[:last]] - area_int\n\n# compute the ratio of overlap\noverlap = area_int/(area_union + 1e-6)\n\n# delete all indexes from the index list that have\nidxs = np.delete(idxs, np.concatenate(([last],np.where(overlap > overlap_thresh)[0])))\n```\n\n```python\nif len(pick) >= max_boxes:\n   break\n```\n\\[np.concatest\\]\nmax_boxes\n```python\nboxes = boxes[pick].astype(\"int\")\nprobs = probs[pick]\nreturn boxes, probs\n```\npick\n\n### 24/07/2018\n#### rpn to porposal fixed\nrpn\n\nanchor_sizeanchor_ratio\n\n\n\n\n1regr_layer\\[0, :, :, 4 \\* curr_layer:4 \\* curr_layer + 4]\n2curr_layer\n\nanchorx,y,w,h\n\nregranchor\n\n\n0\nx1,y1,x2,y2\n\nall_boxesn,4all_probsn,\n\n\nnp.where() \nnp.delete(all_boxes, idxs, 0)\n\n9\n```python\n\"\"\" rpn to porposal\n\n@param rpn_layer: porposal's coresspoding possibiliy, whether item exciseted\n@param regr_layer: porposal's coresspoding regrident\n@param C: Configuration\n@param dim_ordering: Dimensional organization\n@param use_regr=True: wether use regurident to fix proposal\n@param max_boxes=300: max boxes after apply this function\n@param overlap_thresh=0.9: threshold of overlapping\n@param C: Configuration\n\n@return: max_boxes proposal with format (x1,y1,x2,y2)\n\"\"\"\n\ndef rpn_to_roi(rpn_layer, regr_layer, C, dim_ordering, use_regr=True, max_boxes=300,overlap_thresh=0.9):\n\t# std_scaling default 4\n\tregr_layer = regr_layer / C.std_scaling\n\n\tanchor_sizes = C.anchor_box_scales\n\tanchor_ratios = C.anchor_box_ratios\n\n\tassert rpn_layer.shape[0] == 1\n\n\t# obtain img's width and height's matrix\n\t(rows, cols) = rpn_layer.shape[1:3]\n\n\tcurr_layer = 0\n\n\tA = np.zeros((4, rpn_layer.shape[1], rpn_layer.shape[2], rpn_layer.shape[3]))\n\n\t# anchor size is [128, 256, 512]\n\tfor anchor_size in anchor_sizes:\n\t\t# anchor ratio is [1,2,1]\n\t\tfor anchor_ratio in anchor_ratios:\n\t\t\t# rpn_stride = 16\n\t\t\t# obatin anchor's weidth and height on feature map\n\t\t\tanchor_x = (anchor_size * anchor_ratio[0])/C.rpn_stride\n\t\t\tanchor_y = (anchor_size * anchor_ratio[1])/C.rpn_stride\n\n\t\t\t# obtain current regrident\n\t\t\t# when one dimentional obtain a value, the new varirant will decrease one dimenttion\n\t\t\tregr = regr_layer[0, :, :, 4 * curr_layer:4 * curr_layer + 4]\n\t\t\t# put depth to first bacause tensorflow as backend\n\t\t\tregr = np.transpose(regr, (2, 0, 1))\n\n\t\t\t# The rows of the output array X are copies of the vector x; columns of the output array Y are copies of the vector y\n\t\t\t# each cordinartes of matrix cls and rows\n\t\t\tX, Y = np.meshgrid(np.arange(cols),np. arange(rows))\n\n\t\t\t# obtain anchors's (x,y,w,h)\n\t\t\tA[0, :, :, curr_layer] = X - anchor_x/2\n\t\t\tA[1, :, :, curr_layer] = Y - anchor_y/2\n\t\t\tA[2, :, :, curr_layer] = anchor_x\n\t\t\tA[3, :, :, curr_layer] = anchor_y\n\n\t\t\t# fix boxes with grident\n\t\t\tif use_regr:\n\t\t\t\t# fixed corinates of box\n\t\t\t\tA[:, :, :, curr_layer] = apply_regr_np(A[:, :, :, curr_layer], regr)\n\n\t\t\t# fix unreasonable cordinates\n\t\t\t# np.maximum(1,[]) will set the value less than 1 in [] to 1\n\t\t\t# box's width and height can't less than 0\n\t\t\tA[2, :, :, curr_layer] = np.maximum(1, A[2, :, :, curr_layer])\n\t\t\tA[3, :, :, curr_layer] = np.maximum(1, A[3, :, :, curr_layer])\n\t\t\t# fixed right bottom cordinates\n\t\t\tA[2, :, :, curr_layer] += A[0, :, :, curr_layer]\n\t\t\tA[3, :, :, curr_layer] += A[1, :, :, curr_layer]\n\n\t\t\t# left top corner cordinates can't out image\n\t\t\tA[0, :, :, curr_layer] = np.maximum(0, A[0, :, :, curr_layer])\n\t\t\tA[1, :, :, curr_layer] = np.maximum(0, A[1, :, :, curr_layer])\n\t\t\t# right bottom corner cordinates can't out img\n\t\t\tA[2, :, :, curr_layer] = np.minimum(cols-1, A[2, :, :, curr_layer])\n\t\t\tA[3, :, :, curr_layer] = np.minimum(rows-1, A[3, :, :, curr_layer])\n\n\t\t\t# next layer\n\t\t\tcurr_layer += 1\n\n\t# obtain (n,4) object and coresspoding cordinate\n\tall_boxes = np.reshape(A.transpose((0, 3, 1,2)), (4, -1)).transpose((1, 0))\n\t# obtain(n,) object and creoespdoing possibility\n\tall_probs = rpn_layer.transpose((0, 3, 1, 2)).reshape((-1))\n\n\t# cordinates of left top and right bottom of box\n\tx1 = all_boxes[:, 0]\n\ty1 = all_boxes[:, 1]\n\tx2 = all_boxes[:, 2]\n\ty2 = all_boxes[:, 3]\n\n\t# find where right cordinate bigger than left cordinate\n\tidxs = np.where((x1 - x2 >= 0) | (y1 - y2 >= 0))\n\t# delete thoese point at 0 dimentional -> all boxes\n\tall_boxes = np.delete(all_boxes, idxs, 0)\n\tall_probs = np.delete(all_probs, idxs, 0)\n\n\t# apply NMS to reduce overlapping boxes\n\tresult = non_max_suppression_fast(all_boxes, all_probs, overlap_thresh=overlap_thresh, max_boxes=max_boxes)[0]\n\n\treturn result\n```\n\n### 25/07/2018\n#### generate classifier's trainning data\nclassifier,\n\n\nbboxes\n\nR, bboxes\n\n:\n\n\n\n\n\n1one-hot\ny_class_num\ncoordslabelsloss\n```python\nclass_num = 2\nclass_label = 10 * [0]\nprint(class_label)\nclass_label[class_num] = 1\nprint(class_label)\n\n[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n[0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n```\n\n\n\n\n```python\n\"\"\" generate classifier training data\n\n@param R: porposal -> boxes\n@param img_data: image data\n@param C: configuration\n@param class_mapping: classes and coresspoding numbers\n\n@return: np.expand_dims(X, axis=0): boxes after filter\n@return: np.expand_dims(Y1, axis=0): boxes coresspoding class\n@return: np.expand_dims(Y2, axis=0): boxes coresspoding regurident\n@return IoUs: IOU\n\n\"\"\"\ndef calc_iou(R, img_data, C, class_mapping):\n\n\t# obtain boxxes information from img data\n\tbboxes = img_data['bboxes']\n\t# obtain width and height of img\n\t(width, height) = (img_data['width'], img_data['height'])\n\t# get image dimensions for resizing\n\t# Fix image's shortest edge to config setting: eg: 600\n\t(resized_width, resized_height) = get_new_img_size(width, height, C.im_size)\n\n\t# record parameters, bboxes cordinates on feature map\n\tgta = np.zeros((len(bboxes), 4))\n\n\t# change bboxes's width and height because the img was rezised\n\tfor bbox_num, bbox in enumerate(bboxes):\n\t\t# get the GT box coordinates, and resize to account for image resizing\n\t\t# /C.rpn_stride mapping to feature map\n\t\tgta[bbox_num, 0] = int(round(bbox['x1'] * (resized_width / float(width))/C.rpn_stride))\n\t\tgta[bbox_num, 1] = int(round(bbox['x2'] * (resized_width / float(width))/C.rpn_stride))\n\t\tgta[bbox_num, 2] = int(round(bbox['y1'] * (resized_height / float(height))/C.rpn_stride))\n\t\tgta[bbox_num, 3] = int(round(bbox['y2'] * (resized_height / float(height))/C.rpn_stride))\n\n\tx_roi = []\n\ty_class_num = []\n\ty_class_regr_coords = []\n\ty_class_regr_label = []\n\tIoUs = [] # for debugging only\n\n\t# for all given proposals -> boxes\n\tfor ix in range(R.shape[0]):\n\t\t# current boxes's cordinates\n\t\t(x1, y1, x2, y2) = R[ix, :]\n\t\tx1 = int(round(x1))\n\t\ty1 = int(round(y1))\n\t\tx2 = int(round(x2))\n\t\ty2 = int(round(y2))\n\n\t\tbest_iou = 0.0\n\t\tbest_bbox = -1\n\t\t# using current proposal to compare with given xml's boxes\n\t\tfor bbox_num in range(len(bboxes)):\n\t\t\t# calculate current iou\n\t\t\tcurr_iou = iou([gta[bbox_num, 0], gta[bbox_num, 2], gta[bbox_num, 1], gta[bbox_num, 3]], [x1, y1, x2, y2])\n\t\t\t# update parameters\n\t\t\tif curr_iou > best_iou:\n\t\t\t\tbest_iou = curr_iou\n\t\t\t\tbest_bbox = bbox_num\n\n\t\t# if iou to small, we don't put it in trainning because it should be backgroud\n\t\tif best_iou < C.classifier_min_overlap:\n\t\t\t\tcontinue\n\t\telse:\n\t\t\t# saveing left top cordinates, width and height\n\t\t\tw = x2 - x1\n\t\t\th = y2 - y1\n\t\t\tx_roi.append([x1, y1, w, h])\n\t\t\t# saving this bbox's iou\n\t\t\tIoUs.append(best_iou)\n\n\t\t\t# hard to classfier -> set it to backgroud\n\t\t\tif C.classifier_min_overlap <= best_iou < C.classifier_max_overlap:\n\t\t\t\t# hard negative example\n\t\t\t\tcls_name = 'bg'\n\n\t\t\t# valid proposal\n\t\t\telif C.classifier_max_overlap <= best_iou:\n\t\t\t\t# coresspoding class name\n\t\t\t\tcls_name = bboxes[best_bbox]['class']\n\n\t\t\t\t# calculate rpn graident with true cordinates given by xml file\n\t\t\t\tcxg = (gta[best_bbox, 0] + gta[best_bbox, 1]) / 2.0\n\t\t\t\tcyg = (gta[best_bbox, 2] + gta[best_bbox, 3]) / 2.0\n\n\t\t\t\tcx = x1 + w / 2.0\n\t\t\t\tcy = y1 + h / 2.0\n\n\t\t\t\ttx = (cxg - cx) / float(w)\n\t\t\t\tty = (cyg - cy) / float(h)\n\t\t\t\ttw = np.log((gta[best_bbox, 1] - gta[best_bbox, 0]) / float(w))\n\t\t\t\tth = np.log((gta[best_bbox, 3] - gta[best_bbox, 2]) / float(h))\n\t\t\telse:\n\t\t\t\tprint('roi = {}'.format(best_iou))\n\t\t\t\traise RuntimeError\n\n\t\t# class name's mapping number\n\t\tclass_num = class_mapping[cls_name]\n\t\t# list of calss label\n\t\tclass_label = len(class_mapping) * [0]\n\t\t# set class_num's coresspoding location to 1\n\t\tclass_label[class_num] = 1\n\t\t# privous is one-hot vector\n\n\t\t# saving the one-hot vector\n\t\ty_class_num.append(copy.deepcopy(class_label))\n\n\t\t# coords used to saving calculated graident\n\t\tcoords = [0] * 4 * (len(class_mapping) - 1)\n\t\t# labels used to decide whether adding to loss calculation\n\t\tlabels = [0] * 4 * (len(class_mapping) - 1)\n\t\tif cls_name != 'bg':\n\t\t\tlabel_pos = 4 * class_num\n\t\t\tsx, sy, sw, sh = C.classifier_regr_std\n\t\t\tcoords[label_pos:4+label_pos] = [sx*tx, sy*ty, sw*tw, sh*th]\n\t\t\tlabels[label_pos:4+label_pos] = [1, 1, 1, 1]\n\t\t\ty_class_regr_coords.append(copy.deepcopy(coords))\n\t\t\ty_class_regr_label.append(copy.deepcopy(labels))\n\t\telse:\n\t\t\ty_class_regr_coords.append(copy.deepcopy(coords))\n\t\t\ty_class_regr_label.append(copy.deepcopy(labels))\n\n\t# no bboxes\n\tif len(x_roi) == 0:\n\t\treturn None, None, None, None\n\n\t# matrix with [x1, y1, w, h]\n\tX = np.array(x_roi)\n\t# boxxes coresspoding class number\n\tY1 = np.array(y_class_num)\n\t# matrix of whether adding to calculation and coresspoding regrident\n\tY2 = np.concatenate([np.array(y_class_regr_label),np.array(y_class_regr_coords)],axis=1)\n\n\t# adding batch size dimention\n\treturn np.expand_dims(X, axis=0), np.expand_dims(Y1, axis=0), np.expand_dims(Y2, axis=0), IoUs\n```\n\n### 26~27/07/2018\n#### model parameters\n\n```python\n# rpn optimizer\noptimizer = Adam(lr=1e-5)\n# classifier optimizer\noptimizer_classifier = Adam(lr=1e-5)\n# defined loss apply, metrics used to print accury\nmodel_rpn.compile(optimizer=optimizer, loss=[losses.rpn_loss_cls(num_anchors), losses.rpn_loss_regr(num_anchors)])\nmodel_classifier.compile(optimizer=optimizer_classifier, loss=[losses.class_loss_cls, losses.class_loss_regr(len(classes_count)-1)], metrics={'dense_class_{}'.format(len(classes_count)): 'accuracy'})\n# for saving weight\nmodel_all.compile(optimizer='sgd', loss='mae')\n\n# traing time of each epochs\nepoch_length = 1000\n# totoal epochs\nnum_epochs = 2000\n#\niter_num = 0\n# losses saving matrix\nlosses = np.zeros((epoch_length, 5))\nrpn_accuracy_rpn_monitor = []\nrpn_accuracy_for_epoch = []\nstart_time = time.time()\n# current total loss\nbest_loss = np.Inf\n# sorted classing mapping\nclass_mapping_inv = {v: k for k, v in class_mapping.items()}\n```\n\n#### Training process\n\n**rpn**\nRPN,XY\n\n**rpnclassifier:**\n\n\n\nY1\\[0, :, -1\\]0batch-1\nneg_samples = neg_samples\\[0\\]\nC.num_roisclassifierC.num_rois11\n\n**classifier:**\nLossaccury\nloss_class\\[3\\]\n```python\nclassiferloss\n[1.4640709, 1.0986123, 0.36545864, 0.15625]\n```\nlosslistnumpy\nepochepochlossepochepoch.\n\n---\n\n```python\n# Training Process\nprint('Starting training')\n\nfor epoch_num in range(num_epochs):\n    #progbar is used to print % of processing\n\tprogbar = generic_utils.Progbar(epoch_length)\n    # print current process\n\tprint('Epoch {}/{}'.format(epoch_num + 1, num_epochs))\n\n\twhile True:\n\t\ttry:\n\n            # verbose True to print RPN situation, if can't generate boxes on positivate object, it will print error\n\t\t\tif len(rpn_accuracy_rpn_monitor) == epoch_length and C.verbose:\n                # postivate boxes / all boxes\n\t\t\t\tmean_overlapping_bboxes = float(sum(rpn_accuracy_rpn_monitor))/len(rpn_accuracy_rpn_monitor)\n\t\t\t\trpn_accuracy_rpn_monitor = []\n\t\t\t\tprint('Average number of overlapping bounding boxes from RPN = {} for {} previous iterations'.format(mean_overlapping_bboxes, epoch_length))\n\t\t\t\tif mean_overlapping_bboxes == 0:\n\t\t\t\t\tprint('RPN is not producing bounding boxes that overlap the ground truth boxes. Check RPN settings or keep training.')\n\n            # obtain img, rpn information and img xml format\n\t\t\tX, Y, img_data = next(data_gen_train)\n\n            # train RPN net, X is img, Y is correspoding class type and graident\n\t\t\tloss_rpn = model_rpn.train_on_batch(X, Y)\n\n            # predict new Y from privious rpn model\n\t\t\tP_rpn = model_rpn.predict_on_batch(X)\n\n            # transform predicted rpn to cordinates of boxes\n\t\t\tR = rpn_to_boxes.rpn_to_roi(P_rpn[0], P_rpn[1], C, K.image_dim_ordering(), use_regr=True, overlap_thresh=0.7, max_boxes=300)\n\t\t\t# note: calc_iou converts from (x1,y1,x2,y2) to (x,y,w,h) format\n            # X2: [x,y,w,h]\n            # Y1: coresspoding class number -> one hot vector\n            # Y2: boxes coresspoding regrident\n\t\t\tX2, Y1, Y2, IouS = rpn_to_classifier.calc_iou(R, img_data, C, class_mapping)\n\n            # no box, stop this epoch\n\t\t\tif X2 is None:\n\t\t\t\trpn_accuracy_rpn_monitor.append(0)\n\t\t\t\trpn_accuracy_for_epoch.append(0)\n\t\t\t\tcontinue\n\n            # if last position of one-hot is 1 -> is background\n\t\t\tneg_samples = np.where(Y1[0, :, -1] == 1)\n            # else is postivate sample\n\t\t\tpos_samples = np.where(Y1[0, :, -1] == 0)\n\n            # obtain backgourd samples's coresspoding rows\n\t\t\tif len(neg_samples) > 0:\n\t\t\t\tneg_samples = neg_samples[0]\n\t\t\telse:\n\t\t\t\tneg_samples = []\n\n            # obtain posivate samples's coresspoding rows\n\t\t\tif len(pos_samples) > 0:\n\t\t\t\tpos_samples = pos_samples[0]\n\t\t\telse:\n\t\t\t\tpos_samples = []\n\t\t\t# saving posivate samples's number\n\t\t\trpn_accuracy_rpn_monitor.append(len(pos_samples))\n\t\t\trpn_accuracy_for_epoch.append((len(pos_samples)))\n\n            # default 4 here\n\t\t\tif C.num_rois > 1:\n                # wehn postivate samples less than 2\n\t\t\t\tif len(pos_samples) < C.num_rois//2:\n                    # chosse all samples\n\t\t\t\t\tselected_pos_samples = pos_samples.tolist()\n\t\t\t\telse:\n                    # random choose 2 samples\n\t\t\t\t\tselected_pos_samples = np.random.choice(pos_samples, C.num_rois//2, replace=False).tolist()\n\t\t\t\ttry:\n                    # random choose num_rois - positave samples naegivate samples\n\t\t\t\t\tselected_neg_samples = np.random.choice(neg_samples, C.num_rois - len(selected_pos_samples), replace=False).tolist()\n\t\t\t\texcept:\n                    # if no enought neg samples, copy priouvs neg sample\n\t\t\t\t\tselected_neg_samples = np.random.choice(neg_samples, C.num_rois - len(selected_pos_samples), replace=True).tolist()\n\n                # samples picked to classifier network\n\t\t\t\tsel_samples = selected_pos_samples + selected_neg_samples\n\t\t\telse:\n\t\t\t\t# in the extreme case where num_rois = 1, we pick a random pos or neg sample\n\t\t\t\tselected_pos_samples = pos_samples.tolist()\n\t\t\t\tselected_neg_samples = neg_samples.tolist()\n\t\t\t\tif np.random.randint(0, 2):\n\t\t\t\t\tsel_samples = random.choice(neg_samples)\n\t\t\t\telse:\n\t\t\t\t\tsel_samples = random.choice(pos_samples)\n\n            # train classifier, img data, selceted samples' cordinates, mapping number of selected samples, coresspoding regreident\n\t\t\tloss_class = model_classifier.train_on_batch([X, X2[:, sel_samples, :]], [Y1[:, sel_samples, :], Y2[:, sel_samples, :]])\n\n            # in Keras, if loss part bigger than 1, it will return sum of part losses, each loss and accury\n            # put each losses and accury into losses\n\t\t\tlosses[iter_num, 0] = loss_rpn[1]\n\t\t\tlosses[iter_num, 1] = loss_rpn[2]\n\n\t\t\tlosses[iter_num, 2] = loss_class[1]\n\t\t\tlosses[iter_num, 3] = loss_class[2]\n\t\t\tlosses[iter_num, 4] = loss_class[3]\n\n            # next iter\n\t\t\titer_num += 1\n\n            # display and update current mean value of losses\n\t\t\tprogbar.update(iter_num, [('rpn_cls', np.mean(losses[:iter_num, 0])), ('rpn_regr', np.mean(losses[:iter_num, 1])),\n\t\t\t\t\t\t\t\t\t  ('detector_cls', np.mean(losses[:iter_num, 2])), ('detector_regr', np.mean(losses[:iter_num, 3]))])\n\n            # reach epoch_length\n\t\t\tif iter_num == epoch_length:\n\t\t\t\tloss_rpn_cls = np.mean(losses[:, 0])\n\t\t\t\tloss_rpn_regr = np.mean(losses[:, 1])\n\t\t\t\tloss_class_cls = np.mean(losses[:, 2])\n\t\t\t\tloss_class_regr = np.mean(losses[:, 3])\n\t\t\t\tclass_acc = np.mean(losses[:, 4])\n\n                # negativate samples / all samples\n\t\t\t\tmean_overlapping_bboxes = float(sum(rpn_accuracy_for_epoch)) / len(rpn_accuracy_for_epoch)\n                # reset\n\t\t\t\trpn_accuracy_for_epoch = []\n\n                # print trainning loss and accrury\n\t\t\t\tif C.verbose:\n\t\t\t\t\tprint('Mean number of bounding boxes from RPN overlapping ground truth boxes: {}'.format(mean_overlapping_bboxes))\n\t\t\t\t\tprint('Classifier accuracy for bounding boxes from RPN: {}'.format(class_acc))\n\t\t\t\t\tprint('Loss RPN classifier: {}'.format(loss_rpn_cls))\n\t\t\t\t\tprint('Loss RPN regression: {}'.format(loss_rpn_regr))\n\t\t\t\t\tprint('Loss Detector classifier: {}'.format(loss_class_cls))\n\t\t\t\t\tprint('Loss Detector regression: {}'.format(loss_class_regr))\n                    # trainng time of one epoch\n\t\t\t\t\tprint('Elapsed time: {}'.format(time.time() - start_time))\n                    \n                # total loss\n\t\t\t\tcurr_loss = loss_rpn_cls + loss_rpn_regr + loss_class_cls + loss_class_regr\n                # reset\n\t\t\t\titer_num = 0\n                # reset time\n\t\t\t\tstart_time = time.time()\n\n                # if obtain smaller total loss, save weight of current model\n\t\t\t\tif curr_loss < best_loss:\n\t\t\t\t\tif C.verbose:\n\t\t\t\t\t\tprint('Total loss decreased from {} to {}, saving weights'.format(best_loss,curr_loss))\n\t\t\t\t\tbest_loss = curr_loss\n\t\t\t\t\tmodel_all.save_weights(C.model_path)\n\n\t\t\t\tbreak\n\n\t\texcept Exception as e:\n\t\t\tprint('Exception: {}'.format(e))\n\t\t\tcontinue\n\nprint('Training complete, exiting.')\n```\n[jupyter notebook](https://github.com/Trouble404/Object-Detection-System-Based-on-CNN-and-Capsule-Network/blob/master/Faster-RCNN/workflow_train.ipynb)\n\n### 30/07/2018\n#### Running at GPU enviorment\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/9_1.JPG)\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/9_2.JPG)\nMeet error in GPU version tensorflow\nNo enough memory.\n\nTry to Running at Irius:\n\nSetting 3 differnet configration:\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/9_3.JPG)\nat Prjoect1 file:\nset epoch_length to number of training img\n```python\nepoch_length = 11540\nnum_epochs = 100\n```\nApply img enhance function\n```python\nC.use_horizontal_flips = True\nC.use_vertical_flips = True\nC.rot_90 = True\n```\n\n---\n\nat Prjoect file:\nset epoch_length to 1000, increase epoch\n```python\nepoch_length = 1000\nnum_epochs = 2000\n```\nApply img enhance and class balance function\n```python\nC.use_horizontal_flips = True\nC.use_vertical_flips = True\nC.rot_90 = True\nC.balanced_classes = True\n```\n---\n\nat Prjoect3 file:\nset epoch_length to 1000, increase epoch\n```python\nepoch_length = 1000\nnum_epochs = 2000\n```\nApply img enhance\n```python\nC.use_horizontal_flips = True\nC.use_vertical_flips = True\nC.rot_90 = True\n```\n\n---\n#### check irdius work\n```bash\nmyqueue\n```\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/9_4.JPG)\n\n```bash\nssh pink59\nnvidia-smi\n```\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/9_5.JPG)\n\n### 31/07/2018\n#### obtain trained model and log file\n Iriuds GPU24\nLogBook.\n\n#### plot rpn and classfier loss\nepochrpn_cls, rpn_regr, detc_cls, detc_regr\nList\n```python\ndef obtain_each_batch(filename):\n    n = 0\n    rpn_cls = []\n    rpn_regr = []\n    detector_cls = []\n    detector_regr = []\n    f = open(filename,'r',buffering=-1)\n    lines = f.readlines()\n    for line in lines:\n        n = n + 1\n        match = re.match(r'.* - rpn_cls: (.*) - rpn_regr: .*', line, re.M|re.I)\n        if match is None:\n            continue\n        else: \n            rpn_cls.append(float(match.group(1)))\n\n        match = re.match(r'.* - rpn_regr: (.*) - detector_cls: .*', line, re.M|re.I)\n        if match is None:\n            continue\n        else: \n            rpn_regr.append(float(match.group(1)))            \n            \n        match = re.match(r'.* - detector_cls: (.*) - detector_regr: .*', line, re.M|re.I)\n        if match is None:\n            continue\n        else: \n            detector_cls.append(float(match.group(1))) \n            \n        match = re.match(r'.* - detector_regr: (.*)\\n', line, re.M|re.I)\n        if match is None:\n            continue\n        else: \n            det_regr = match.group(1)[0:6]\n            detector_regr.append(float(det_regr))\n\n    f.close()\n    print(n)\n    return rpn_cls, rpn_regr, detector_cls, detector_regr  \n```\n\nepochaccury, loss of rpn cls, loss of rpn regr, loss of detc cls, loss of detc regr\nlist\n```python\ndef obtain_batch(filename):\n    n = 0\n    accuracy = []\n    loss_rpn_cls = []\n    loss_rpn_regr = []\n    loss_detc_cls = []\n    loss_detc_regr = []\n    f = open(filename,'r',buffering=-1)\n    lines = f.readlines()\n    \n    for line in lines:\n        n = n + 1\n        if 'Classifier accuracy for bounding boxes from RPN' in line:\n            result = re.findall(r\"\\d+\\.?\\d*\",line)\n            accuracy.append(float(result[0]))\n            \n        if 'Loss RPN classifier' in line:\n            result = re.findall(r\"\\d+\\.?\\d*\",line)\n            loss_rpn_cls.append(float(result[0]))       \n\n        if 'Loss RPN regression' in line:\n            result = re.findall(r\"\\d+\\.?\\d*\",line)\n            loss_rpn_regr.append(float(result[0]))\n            \n        if 'Loss Detector classifier' in line:\n            result = re.findall(r\"\\d+\\.?\\d*\",line)\n            loss_detc_cls.append(float(result[0]))\n            \n        if 'Loss Detector regression' in line:\n            result = re.findall(r\"\\d+\\.?\\d*\",line)\n            loss_detc_regr.append(float(result[0])) \n            \n    f.close()\n    print(n)\n    return accuracy, loss_rpn_cls, loss_rpn_regr, loss_detc_cls, loss_detc_regr\n```\n\n\n#### plot epoch loss and accury\n```python\nfilename = r'F:\\desktop\\\\1000-no_balance\\train1.out'\naa,bb,cc,dd,ee = obtain_batch(filename)\nx_cor = np.arange(0,len(aa),1)\n\nplt.plot(x_cor,aa, c='b', label = \"Accuracy\")\nplt.plot(x_cor,bb, c='c', label = \"Loss RPN classifier\")\nplt.plot(x_cor,cc, c='g', label = \"Loss RPN regression\")\nplt.plot(x_cor,dd, c='k', label = \"Loss Detector classifier\")\nplt.plot(x_cor,ee, c='m', label = \"Loss Detector regression\")\nplt.ylabel(\"Value of Accuracy and Loss\") \nplt.xlabel(\"Number of Epoch\")\nplt.title('Loss and Accuracy for Totoal Epochs')  \nplt.legend()\nplt.ylim(0,2)\n#plt.xlim(0,11540)\nplt.savefig(\"pic1.PNG\", dpi = 600)\nplt.show()\n```\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/pic1.PNG)\n\n```python\nfilename = r'F:\\desktop\\\\1000-no_balance\\train1.out'\na,b,c,d = obtain_each_batch(filename)\nx_cor = np.arange(0,len(a),1)\n\nplt.plot(x_cor,a, c='b', label = \"rpn_cls\")\nplt.plot(x_cor,b, c='c', label = \"rpn_regr\")\nplt.plot(x_cor,c, c='g', label = \"detector_cls\")\nplt.plot(x_cor,d, c='k', label = \"detector_regr\")\nplt.ylabel(\"Value of Loss\") \nplt.xlabel(\"Epoch Length\")\nplt.title('Loss for Lenght of Epoch')  \nplt.legend()\n#plt.ylim(0,2)\nplt.xlim(80787,92327)\nplt.savefig(\"pic2.PNG\", dpi = 600)\nplt.show()\n```\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/pic2.PNG)\n\n[Jupyter notebook](https://github.com/Trouble404/Object-Detection-System-Based-on-CNN-and-Capsule-Network/blob/master/Plot/logbook_plot.ipynb)\n\n## August\n### 01~02/08/2018\n#### test network\ntrain\n\n\n**rpn**\n```python\nshared_layers = nn.nn_base(img_input, trainable=True)\nnum_anchors = len(C.anchor_box_scales)*len(C.anchor_box_ratios)\nrpn_layers = nn.rpn(shared_layers,num_anchors)\n```\n\n**classifier**\n```python\nclassifier = nn.classifier(feature_map_input, roi_input, C.num_rois,nb_classes=len(class_mapping), trainable=True)\n```\n\n****\n```python\nC.model_path = 'gpu_resnet50_weights.h5'\ntry:\n    print('Loading weights from {}'.format(C.model_path))\n    model_rpn.load_weights(C.model_path, by_name=True)\n    model_classifier.load_weights(C.model_path, by_name=True)\nexcept:\n    print('can not load')\n```\n\n****\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/test/test4.jpg)\n\n1. \n    \n    \n    \n    img\n```python\ndef format_img_size(img, C):\n    (height,width,_) = img.shape\n    if width <= height:\n        ratio = C.im_size/width\n    else:\n        ratio = C.im_size/height\n    new_width, new_height = image_processing.get_new_img_size(width,height, C.im_size)\n    img = cv2.resize(img, (new_width, new_height), interpolation=cv2.INTER_CUBIC)\n    return img, ratio\n```\n2. \n     BGRRGBRESNET\n    np.float32\n    1\n    \n    \n```python\ndef format_img_channels(img, C):\n\t\"\"\" formats the image channels based on config \"\"\"\n\timg = img[:, :, (2, 1, 0)]\n\timg = img.astype(np.float32)\n\timg[:, :, 0] -= C.img_channel_mean[0]\n\timg[:, :, 1] -= C.img_channel_mean[1]\n\timg[:, :, 2] -= C.img_channel_mean[2]\n\timg /= C.img_scaling_factor\n\timg = np.transpose(img, (2, 0, 1))\n\timg = np.expand_dims(img, axis=0)\n\treturn img\n```\ntensorflow\n\n****\nY1:anchor\nY2:anchor\nF:\n\n```python\n[Y1, Y2, F] = model_rpn.predict(X)\n```\n\nrpn16anchorrpn\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/pic_anchors.png)\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/25_1.jpg)\n\n**rpn:**\n300(x1,y1,x2,y2)\n```python\n# transform predicted rpn to cordinates of boxes\nR = rpn_to_boxes.rpn_to_roi(Y1, Y2, C, K.image_dim_ordering(), use_regr=True, overlap_thresh=0.7)\n```\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/pic_rois.png)\n\n(x1,y1,x2,y2)  (x,y,w,h)\n```python\nR[:, 2] -= R[:, 0]\nR[:, 3] -= R[:, 1]\n```\n\n****\nC.num_rois\n32300/32, 10\n3232\n\n3232\n```python\n# divided 32 bboxes as one group\nfor jk in range(R.shape[0]//C.num_rois + 1):\n    # pick num_rios(32) bboxes one time, only pick to last bboxes in last group\n    ROIs = np.expand_dims(R[C.num_rois*jk:C.num_rois*(jk+1), :], axis=0)\n    #print(ROIs.shape)\n    \n    # no proposals, out iter\n    if ROIs.shape[1] == 0:\n        break\n\n    # when last time can't obtain num_rios(32) bboxes, adding bboxes with 0 to fill to 32 bboxes\n    if jk == R.shape[0]//C.num_rois:\n        #pad R\n        curr_shape = ROIs.shape\n        target_shape = (curr_shape[0],C.num_rois,curr_shape[2])\n        ROIs_padded = np.zeros(target_shape).astype(ROIs.dtype)\n        ROIs_padded[:, :curr_shape[1], :] = ROIs\n        ROIs_padded[0, curr_shape[1]:, :] = ROIs[0, 0, :]\n        # 10 group with 320 bboxes\n        ROIs = ROIs_padded\n```\n\n\n****\n\n\nP\\_cls\nP\\_regr\nF:rpn\nROIS:\n```python\n[P_cls, P_regr] = model_classifier_only.predict([F, ROIs])\n```\n\n\n\n```python\n    for ii in range(P_cls.shape[1]):\n\n        # if smaller than setting threshold, we think this bbox invalid\n        # and if this bbox's class is background, we don't need to care about it\n        if np.max(P_cls[0, ii, :]) < bbox_threshold or np.argmax(P_cls[0, ii, :]) == (P_cls.shape[2] - 1):\n            continue\n```\n\n\nlist\n```python\n        # obatain max possibility's class name by class mapping\n        cls_name = class_mapping[np.argmax(P_cls[0, ii, :])]\n\n        # saving bboxes and probs\n        if cls_name not in bboxes:\n            bboxes[cls_name] = []\n            probs[cls_name] = []\n```\n\n\n\n```python\n        # obtain current cordinates of proposal\n        (x, y, w, h) = ROIs[0, ii, :]\n        \n        # obtain the position with max possibility\n        cls_num = np.argmax(P_cls[0, ii, :])\n```\n\n![iamge](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/pic_cls1.png)\n\n\n\n\n C.rpn_stride\n```python\n        try:\n            # obtain privous position's bbox's regrient\n            (tx, ty, tw, th) = P_regr[0, ii, 4*cls_num:4*(cls_num+1)]\n            # waiting test\n            tx /= C.classifier_regr_std[0]\n            ty /= C.classifier_regr_std[1]\n            tw /= C.classifier_regr_std[2]\n            th /= C.classifier_regr_std[3]\n            # fix box with regreient\n            x, y, w, h = rpn_to_boxes.apply_regr(x, y, w, h, tx, ty, tw, th)\n        except:\n            pass\n        # cordinates of current's box on real img\n        bboxes[cls_name].append([C.rpn_stride*x, C.rpn_stride*y, C.rpn_stride*(x+w), C.rpn_stride*(y+h)])\n        # coresspoding posbility\n        probs[cls_name].append(np.max(P_cls[0, ii, :]))\n```\n\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/pic_cls2.png)\n\nbboxesbbox\nNo Max Supression\n```python\n# for all classes in current boxes\nfor key in bboxes:\n\n    # bboxes's cordinates\n    bbox = np.array(bboxes[key])\n    # apply NMX to merge some  overlapping boxes\n    new_boxes, new_probs = rpn_to_boxes.non_max_suppression_fast(bbox, np.array(probs[key]), overlap_thresh=0.5)\n```\n\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/pic_cls3.png)\n\n[Jupyter notebook](https://github.com/Trouble404/Object-Detection-System-Based-on-CNN-and-Capsule-Network/blob/master/Faster-RCNN/test.ipynb)\n\n#### result\nSmall img, only 8k\n<div align=\"center\">\n<img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/test/test1.jpg\" height=\"200px\"  ><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/test/test1.png\" height=\"200px\"  >   \n</div>\n\n---\n\nOverlapping img\n<div align=\"center\">\n<img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/test/test2.jpg\" height=\"200px\"  ><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/test/test2.png\" height=\"200px\"  >   \n</div>\n\n---\n\nCrowed People\n<div align=\"center\">\n<img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/test/test3.jpg\" height=\"260px\"  ><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/test/test3.png\" height=\"260px\"  >   \n</div>\n\n---\n\ncow and people\n<div align=\"center\">\n<img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/test/test4.jpg\" height=\"260px\"  ><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/test/test4.png\" height=\"260px\"  >   \n</div>\n\n---\n\ncar and plane\n<div align=\"center\">\n<img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/test/test5.jpg\" height=\"220px\"  ><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/test/test5.png\" height=\"220px\"  >   \n</div>\n\n---\n\nStreet img\n<div align=\"center\">\n<img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/test/test6.jpg\" height=\"270px\"  ><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/test/test6.png\" height=\"270px\"  >   \n</div>\n\n---\n\nLots Dogs\n<div align=\"center\">\n<img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/test/test7.jpg\" height=\"250px\"  ><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/test/test7.png\" height=\"250px\"  >   \n</div>\n\n---\n\nOverlapping car and people\n<div align=\"center\">\n<img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/test/test8.jpg\" height=\"290px\"  ><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/test/test8.png\" height=\"290px\"  >   \n</div>\n\n\n\n\n### 03/08/2018\n#### evaluation\n**mAP**\nmAPPrecisionRecallobject detectionobjectPrecisionRecall/ P-RAPmeanAPmAPmAP[0,1] \n\n**AP**:PrecisionRecallsklearn.metrics.average\\_precision\\_score \n\nresizesklearn.metrics.average_precision_score\n\n---\n\nrpntrain\nfeature map\n```python\nnum_features = 1024\n\ninput_shape_img = (None, None, 3)\ninput_shape_features = (None, None, num_features)\n\nimg_input = Input(shape=input_shape_img)\nroi_input = Input(shape=(C.num_rois, 4))\nfeature_map_input = Input(shape=input_shape_features)\n\nclassifier = nn.classifier(feature_map_input, roi_input, C.num_rois, nb_classes=len(class_mapping), trainable=True)\n```\n\n\n\nVOC\n```python\ntrain_imgs = []\ntest_imgs = []\n\nfor each in all_imgs:\n\tif each['imageset'] == 'trainval':\n\t\ttrain_imgs.append(each)\n\tif each['imageset'] == 'test':\n\t\ttest_imgs.append(each)\n```\n\nxml\n```python\n    for jk in range(new_boxes.shape[0]):\n        (x1, y1, x2, y2) = new_boxes[jk, :]\n        det = {'x1': x1, 'x2': x2, 'y1': y1, 'y2': y2, 'class': key, 'prob': new_probs[jk]}\n        all_dets.append(det)\n```\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/014_1.JPG)\n\n\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/014_2.JPG)\n\nbbox_matchedFALSETrue\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/14_3.JPG)\n\nidx\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/014_3.JPG)\n\niou0.5\n\n```python\n# process each bbox with hightest prob\nfor box_idx in box_idx_sorted_by_prob:\n    \n    # obtain current box's cordinates, class and prob\n    pred_box = pred[box_idx]\n    pred_class = pred_box['class']\n    pred_x1 = pred_box['x1']\n    pred_x2 = pred_box['x2']\n    pred_y1 = pred_box['y1']\n    pred_y2 = pred_box['y2']\n    pred_prob = pred_box['prob']\n    \n    # if not in P list, save current class infomration to it\n    if pred_class not in P:\n        P[pred_class] = []\n        T[pred_class] = []\n        # put porb to P\n    P[pred_class].append(pred_prob)\n    # used to check whether find current object\n    found_match = False\n\n    # compare each real bbox\n    # obtain real box's cordinates, class and prob\n    for gt_box in gt:\n        gt_class = gt_box['class']\n        # bacause the image is rezied, so calculate the real cordinates\n        gt_x1 = gt_box['x1']/fx\n        gt_x2 = gt_box['x2']/fx\n        gt_y1 = gt_box['y1']/fy\n        gt_y2 = gt_box['y2']/fy\n        \n        # obtain box_matched - all false at beginning\n        gt_seen = gt_box['bbox_matched']\n        \n        # ture class != predicted class\n        if gt_class != pred_class:\n            continue\n        # already matched\n        if gt_seen:\n            continue\n        # calculate iou of predicted bbox and real bbox \n        iou = rpn_calculation.iou((pred_x1, pred_y1, pred_x2, pred_y2), (gt_x1, gt_y1, gt_x2, gt_y2))\n        # if iou > 0.5, we will set this prediction correct\n        if iou >= 0.5:\n            found_match = True\n            gt_box['bbox_matched'] = True\n            break\n        else:\n            continue\n    # 1 means this position's bbox correct match with orignal image\n    T[pred_class].append(int(found_match))\n```\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/014_4.JPG)\n\ndiffculttrue10\n```python\n# adding missing object compared to orignal image\nfor gt_box in gt:\n    if not gt_box['bbox_matched'] and not gt_box['difficult']:\n        if gt_box['class'] not in P:\n            P[gt_box['class']] = []\n            T[gt_box['class']] = []\n\n        # T = 1 means there are object, P = 0 means we did't detected that\n        T[gt_box['class']].append(1)\n        P[gt_box['class']].append(0)\n```\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/014_5.JPG)\n\naverage_precision_scoresklearnapmap\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/014_6.JPG)\n\n[jupyter notebook](https://github.com/Trouble404/Object-Detection-System-Based-on-CNN-and-Capsule-Network/blob/master/Faster-RCNN/map.ipynb)\n\n### 06~10/08/2018\n#### adjust\nap\n[jupyter notebook](https://github.com/Trouble404/Object-Detection-System-Based-on-CNN-and-Capsule-Network/blob/master/Faster-RCNN/map_all.ipynb)\n\n##### Project1 all: 9 models:\nALL_2 NO THRESHOLD OTHER WITH THRESHOLD MOST 0.8\n\n| Classes | ALL_1 | ALL_2 | ALL_3 | ALL_4 | ALL_5 | ALL_6 | ALL_7 | ALL_8 | ALL_9 |\n| :---: | :----: | :---: | :----: | :----: | :----: | :----: | :----: | :----: | :----: |\n| motorbike | 0.2305 | 0.2412 | 0.2132 | 0.2220 | 0.2889 | 0.2528 | 0.2204 | 0.2644 | 0.2336 |\n| person | 0.6489 | 0.6735 | 0.7107 | 0.6652 | 0.7120 | 0.7041 | 0.7238 | 0.7003 | 0.7201 |\n| car | 0.1697 | 0.1563 | 0.2032 | 0.2105 | 0.2308 | 0.2221 | 0.2436 | 0.2053 | 0.2080 |\n| aeroplane | 0.7968 | 0.7062 | 0.7941 | 0.6412 | 0.7871 | 0.7331 | 0.7648 | 0.7659 | 0.6902 |\n| bottle | 0.2213 | 0.2428 | 0.2261 | 0.1943 | 0.2570 | 0.2437 | 0.2899 | 0.1442 | 0.2265 |\n| sheep | 0.6162 | 0.5702 | 0.6876 | 0.6295 | 0.6364 | 0.5710 | 0.6536 | 0.6349 | 0.6455 |\n| tvmonitor | 0.1582 | 0.1601 | 0.2231 | 0.1748 | 0.1551 | 0.1603 | 0.1317 | 0.1584 | 0.1678 |\n| boat | 0.3842 | 0.2621 | 0.2261 | 0.1943 | 0.3499 | 0.2437 | 0.2057 | 0.2748 | 0.3509 |\n| chair | 0.2811 | 0.0563| 0.0891 | 0.0621 | 0.1353 | 0.0865 | 0.0907 | 0.0854 | 0.1282 |\n| bicycle | 0.1464 | 0.1224 | 0.1346 | 0.1781 | 0.1406 | 0.1448 | 0.1810 | 0.1071 | 0.1673 |\n| cat | 0.8901 | 0.8565 | 0.9103 | 0.8417 | 0.8289 | 0.8274 | 0.7572 | 0.9143 | 0.8118 |\n| pottedplant | 0.2075 | 0.0926 | 0.1790 | 0.0532 | 0.1517 | 0.1150 | 0.1080 | 0.1022 | 0.0939 |\n| horse | 0.1185 | 0.0588 | 0.0726 | 0.0489 | 0.0696 | 0.0695 | 0.0637 | 0.0651 | 0.0640 |\n| sofa | 0.2797 | 0.2309 | 0.2852 | 0.2966 | 0.3855 | 0.4817 | 0.3659 | 0.3132 | 0.3090 |\n| dog | 0.5359 | 0.5077 | 0.5578 | 0.4413 | 0.4832 | 0.5793 | 0.5687 | 0.4910 | 0.4598 |\n| cow | 0.7582 | 0.6229 | 0.7295 | 0.5420 | 0.5379 | 0.5312 | 0.5147 | 0.5706 | 0.6503 |\n| diningtable | 0.3979 | 0.2734 | 0.3739 | 0.2963 | 0.4715 | 0.4987 | 0.3895 | 0.4983 | 0.4666 |\n| bus | 0.6203 | 0.5572 | 0.6468 | 0.6032 | 0.6320 | 0.6096 | 0.7169 | 0.5938 | 0.5485 |\n| bird | 0.6164 | 0.6662 | 0.5692 | 0.5751 | 0.5407 | 0.4125 | 0.4925 | 0.4347 | 0.5208 |\n| train | 0.8655 | 0.6916 | 0.7141 | 0.7166 | 0.7643 | 0.8107 | 0.7100 | 0.7194 | 0.6263 |\n| **mAP** | **0.4472** | **0.3874** | **0.4341** | **0.3859** | **0.4279** | **0.4141** | **0.4096** | **0.4022** | **0.4045** |\n\n---\n\n##### Project1 epoch_lenght=1000, epoch:1041 : 7 models:\nALL WITH THRESHOLD MOST 0.51\n\n| Classes | ALL_1 | ALL_2 | ALL_3 | ALL_4 | ALL_5 | ALL_6 | ALL_7 |\n| :---: | :----: | :---: | :----: | :----: | :----: | :----: | :----: |\n| motorbike | 0.2433 | 0.2128 | 0.2232 | 0.2262 | 0.2286 | 0.2393 | 0.2279 |\n| person | 0.6560 | 0.6537 | 0.6742 | 0.6952 | 0.6852 | 0.6719 | 0.6636 |\n| car | 0.1562 | 0.1905 | 0.1479 | 0.2024 | 0.2010 | 0.1379 | 0.1583 | \n| aeroplane | 0.7359 | 0.6837 | 0.6729 | 0.6687 | 0.6957 | 0.7339 | 0.6391 |\n| bottle | 0.1913 | 0.1937 | 0.2635 | 0.1843 | 0.2570 | 0.1632 | 0.1863 | \n| sheep | 0.5429 | 0.5579 | 0.6219 | 0.5355 | 0.5881 | 0.5441 | 0.5824 |\n| tvmonitor | 0.1295 | 0.1601 | 0.1368 | 0.1407 | 0.1147 | 0.1349 | 0.1154 | \n| boat | 0.1913 | 0.2880 | 0.2635 | 0.3433 |0.3335 | 0.3422 | 0.3069 | \n| chair | 0.0587 | 0.0657| 0.0342 | 0.0680 | 0.0695 | 0.0752 | 0.0760 |\n| bicycle | 0.1013 | 0.1485 | 0.1225 | 0.1871 | 0.1685 | 0.1037 | 0.1490 | \n| cat | 0.8737 | 0.8557 | 0.8007 | 0.7982 | 0.8045 | 0.8067 | 0.7732 |\n| pottedplant | 0.0694 | 0.1059 | 0.0748 | 0.0878 | 0.0893 | 0.0690 | 0.0865 |\n| horse | 0.0556 | 0.0561 | 0.0581 | 0.0770 | 0.0575 | 0.0539 | 0.0522 |\n| sofa | 0.2177 | 0.2917 | 0.1699 | 0.1940 | 0.3177 | 0.1863 | 0.1857 |\n| dog | 0.6269 | 0.4989 | 0.5015 | 0.5333 | 0.4914 | 0.5572 | 0.4747 |\n| cow | 0.5216 | 0.6229 | 0.5283 | 0.6426 | 0.4358 | 0.4227 | 0.4589 | \n| diningtable | 0.3076 | 0.3889 | 0.3283 | 0.2404 | 0.4219 | 0.4153 | 0.2627 |\n| bus | 0.5865 | 0.5222 | 0.6312 | 0.5853 | 0.5042 | 0.4882 | 0.5576 |\n| bird | 0.5339 | 0.5039 | 0.5150 | 0.5152 | 0.5838 | 0.3890 | 0.4680 |\n| train | 0.4994 | 0.6541 | 0.6702 | 0.6920 | 0.5959 | 0.5893 | 0.6861 |\n| **mAP** | **0.3699** | **0.3765** | **0.3748** | **0.3814** | **0.3786** | **0.3562** | **0.3555** |\n\n\nclassap\n\n**VOC2007**\nVOC2007OpenCVBUG\n\nVOC2012VOC2007Irius\n\n20AP10002Wclass_balance\n\nclass balance\n\nimagenet\n\n\n\n### 13/08/2018\n#### soft-NMS\nBSMBDBMNtaverage precision, AP\n\nM\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/018_1.JPG)\nNMSSoft-NMSNMS**MM**PASCAL VOC  MS-COCOSoft-NMSSoft-NMS\n\n\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/018_2.JPG)\n\n\nNMS\n$$ s_{i}=\\left\\{\n\\begin{aligned}\ns_{i}, \\ \\ \\ \\ iou(M,b_{i}) <  N_{t} \\\\\n0, \\ \\ \\ \\ iou(M,b_{i}) \\geq  N_{t} \n\\end{aligned}\n\\right.\n$$\n\nSOFT NMS\n$$ s_{i}=\\left\\{\n\\begin{aligned}\ns_{i}, \\ \\ \\ \\ iou(M,b_{i}) <  N_{t} \\\\\n1-iou(M,b_{i}), \\ \\ \\ \\ iou(M,b_{i}) \\geq  N_{t} \n\\end{aligned}\n\\right.\n$$\n\nMNtMM\n\nNtsoft-NMS\n\nGaussian penalty:\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/018_3.JPG)\n\n\n```python\n\"\"\" NMS , delete overlapping box\n\n@param boxes: (n,4) box and coresspoding cordinates\n@param probs: (n,) box adn coresspding possibility\n@param overlap_thresh: treshold of delet box overlapping\n@param max_boxes: maximum keeping number of boxes\n@param method: 1 for linear soft NMS, 2 for gaussian soft NMS\n@param sigma: parameter of gaussian soft NMS\nprob_thresh: threshold of probs after soft NMS\n\n\n@return: boxes: boxes cordinates(x1,y1,x2,y2)\n@return: probs: coresspoding possibility\n\"\"\"\ndef soft_nms(boxes, probs, overlap_thresh=0.9, max_boxes=300, method = 1, sigma=0.5, prob_thresh=0.49):\n    # number of input boxes\n    N = boxes.shape[0]\n    # if the bounding boxes integers, convert them to floats --\n    # this is important since we'll be doing a bunch of divisions\n    if boxes.dtype.kind == \"i\":\n        boxes = boxes.astype(\"float\")\n\n    # iterate all boxes\n    for i in range(N):\n        \n        # obtain current boxes' cordinates and probs\n        maxscore = probs[i]\n        maxpos = i\n\n        tx1 = boxes[i,0]\n        ty1 = boxes[i,1]\n        tx2 = boxes[i,2]\n        ty2 = boxes[i,3]\n        ts = probs[i]\n\n        pos = i + 1\n        # get max box\n        while pos < N:\n            if maxscore < probs[pos]:\n                maxscore = probs[pos]\n                maxpos = pos\n            pos = pos + 1\n\n        # add max box as a detection \n        boxes[i,0] = boxes[maxpos,0]\n        boxes[i,1] = boxes[maxpos,1]\n        boxes[i,2] = boxes[maxpos,2]\n        boxes[i,3] = boxes[maxpos,3]\n        probs[i] = probs[maxpos]\n\n        # swap ith box with position of max box\n        boxes[maxpos,0] = tx1\n        boxes[maxpos,1] = ty1\n        boxes[maxpos,2] = tx2\n        boxes[maxpos,3] = ty2\n        probs[maxpos] = ts\n\n        # cordinates of max box\n        tx1 = boxes[i,0]\n        ty1 = boxes[i,1]\n        tx2 = boxes[i,2]\n        ty2 = boxes[i,3]\n        ts = probs[i]\n\n        pos = i + 1\n        # NMS iterations, note that N changes if detection boxes fall below threshold\n        while pos < N:\n            x1 = boxes[pos, 0]\n            y1 = boxes[pos, 1]\n            x2 = boxes[pos, 2]\n            y2 = boxes[pos, 3]\n            s = probs[pos]\n            \n            # calculate the areas, +1 for robatness\n            area = (x2 - x1 + 1) * (y2 - y1 + 1)\n            iw = (min(tx2, x2) - max(tx1, x1) + 1)\n            # # confirm left top cordinates less than top right\n            if iw > 0:\n                ih = (min(ty2, y2) - max(ty1, y1) + 1)\n                # confirm left top cordinates less than top right\n                if ih > 0:\n                    # find the union\n                    ua = float((tx2 - tx1 + 1) * (ty2 - ty1 + 1) + area - iw * ih)\n                    #iou between max box and detection box\n                    ov = iw * ih / ua\n\n                    if method == 1: # linear\n                        if ov > overlap_thresh: \n                            weight = 1 - ov\n                        else:\n                            weight = 1\n                    elif method == 2: # gaussian\n                        weight = np.exp(-(ov * ov)/sigma)\n                    else: # original NMS\n                        if ov > overlap_thresh: \n                            weight = 0\n                        else:\n                            weight = 1\n\n                    # obtain adjusted probs\n                    probs[pos] = weight*probs[pos]\n\n   \n                    # if box score falls below threshold, discard the box by swapping with last box\n                    # update N\n                    if probs[pos] < prob_thresh:\n                        boxes[pos,0] = boxes[N-1, 0]\n                        boxes[pos,1] = boxes[N-1, 1]\n                        boxes[pos,2] = boxes[N-1, 2]\n                        boxes[pos,3] = boxes[N-1, 3]\n                        probs[pos] = probs[N-1]\n                        N = N - 1\n                        pos = pos - 1\n\n            pos = pos + 1\n    # keep is the idx of current keeping objects, front ith objectes\n    keep = [i for i in range(N)]\n    return boxes[keep], probs[keep]\n```\n\n\n### 14/08/2018\n#### OVERLAPPING OBJECT DETECTION\n\n\n### 15/08/2018\n\n\n### 16/08/2018\n\n\n### 17/08/2018\n\n\n## September\n\n\n\n","slug":"LogBook","published":1,"updated":"2020-04-28T12:38:55.395Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck9jwdnmb001ejlrcuz1ek2ga","content":"<h2 id=\"Gantt-chart\"><a href=\"#Gantt-chart\" class=\"headerlink\" title=\"Gantt chart\"></a>Gantt chart</h2><p><img src=\"https://raw.githubusercontent.com/Trouble404/Object-Detection-System-Based-on-CNN-and-Capsule-Network/master/Project%20Plan/gantt%20chart%20of%20project.PNG\" alt=\"image\"></p>\n<a id=\"more\"></a>\n<hr>\n<h2 id=\"Check-list\"><a href=\"#Check-list\" class=\"headerlink\" title=\"Check list\"></a>Check list</h2><ul>\n<li style=\"list-style: none\"><input type=\"checkbox\" checked> <strong>1) preparation</strong></li>\n<li style=\"list-style: none\"><input type=\"checkbox\" checked> <strong><em>1.1) Familiarization with develop tools</em></strong></li>\n<li style=\"list-style: none\"><input type=\"checkbox\" checked> 1.1.1) Keras</li>\n<li style=\"list-style: none\"><input type=\"checkbox\" checked> 1.1.2) Pythrch</li>\n<li style=\"list-style: none\"><input type=\"checkbox\" checked> <strong><em>1.2) Presentation</em></strong></li>\n<li style=\"list-style: none\"><input type=\"checkbox\" checked> 1.2.1) Poster conference</li>\n<li style=\"list-style: none\"><input type=\"checkbox\" checked> <strong>2) Create image database</strong></li>\n<li style=\"list-style: none\"><input type=\"checkbox\" checked> 2.1) Confirmation of detected objects</li>\n<li style=\"list-style: none\"><input type=\"checkbox\" checked> 2.2) Collect and generate the dataset</li>\n<li style=\"list-style: none\"><input type=\"checkbox\" checked> <strong>3) Familiarization with CNN based object detection methods</strong></li>\n<li style=\"list-style: none\"><input type=\"checkbox\" checked> 3.1) R-CNN</li>\n<li style=\"list-style: none\"><input type=\"checkbox\" checked> 3.2) SPP-net</li>\n<li style=\"list-style: none\"><input type=\"checkbox\" checked> 3.3) Fast R-CNN</li>\n<li style=\"list-style: none\"><input type=\"checkbox\" checked> 3.4) Faster R-CNN</li>\n<li style=\"list-style: none\"><input type=\"checkbox\" checked> <strong>4) Implement object detection system based on one chosen CNN method</strong></li>\n<li style=\"list-style: none\"><input type=\"checkbox\" checked> 4.1) Pre-processing of images</li>\n<li style=\"list-style: none\"><input type=\"checkbox\" checked> 4.2) Extracting features</li>\n<li style=\"list-style: none\"><input type=\"checkbox\" checked> 4.3) Mode architecture</li>\n<li style=\"list-style: none\"><input type=\"checkbox\" checked> 4.4) Train model and optimization</li>\n<li style=\"list-style: none\"><input type=\"checkbox\" checked> 4.5) Models ensemble</li>\n<li style=\"list-style: none\"><input type=\"checkbox\" checked> <strong>5) Analysis work</strong></li>\n<li style=\"list-style: none\"><input type=\"checkbox\" checked> 5.1) Evaluation of detection result.</li>\n<li style=\"list-style: none\"><input type=\"checkbox\" checked> <strong>6) Paperwork and bench inspection</strong></li>\n<li style=\"list-style: none\"><input type=\"checkbox\" checked> 6.1) Logbook</li>\n<li style=\"list-style: none\"><input type=\"checkbox\" checked> 6.2) Write the thesis</li>\n<li style=\"list-style: none\"><input type=\"checkbox\" checked> 6.3) Project video</li>\n<li style=\"list-style: none\"><input type=\"checkbox\" checked> 6.4) Speech and ppt of bench inspection</li>\n<li style=\"list-style: none\"><input type=\"checkbox\" checked> <strong>7) Documents</strong></li>\n<li style=\"list-style: none\"><input type=\"checkbox\" checked> 7.1) Project Brief</li>\n</ul>\n<hr>\n<h2 id=\"May\"><a href=\"#May\" class=\"headerlink\" title=\"May\"></a>May</h2><h3 id=\"28-05-2018\"><a href=\"#28-05-2018\" class=\"headerlink\" title=\"28/05/2018\"></a>28/05/2018</h3><p>Keras is a high-level neural networks API, written in Python and capable of running on top of <a href=\"https://github.com/tensorflow/tensorflow\" target=\"_blank\" rel=\"noopener\">TensorFlow</a>, CNTK, or Theano.</p>\n<ul>\n<li><p><strong><a href=\"https://keras.io/\" target=\"_blank\" rel=\"noopener\">Keras document</a></strong></p>\n</li>\n<li><p><strong><a href=\"https://keras-cn.readthedocs.io/en/latest/#keraspython\" target=\"_blank\" rel=\"noopener\">Keras </a></strong></p>\n</li>\n</ul>\n<hr>\n<h4 id=\"Installation\"><a href=\"#Installation\" class=\"headerlink\" title=\"Installation\"></a>Installation</h4><ul>\n<li><p><strong>TensorFlow</strong><br><a href=\"https://www.visualstudio.com/zh-hans/vs/older-downloads/\" target=\"_blank\" rel=\"noopener\">Microsoft Visual Studio 2015</a><br><a href=\"http://docs.nvidia.com/cuda/cuda-installation-guide-microsoft-windows/\" target=\"_blank\" rel=\"noopener\">CUDA 9.0</a><br><a href=\"https://developer.nvidia.com/cudnn\" target=\"_blank\" rel=\"noopener\">cuDNN7</a><br><a href=\"https://www.anaconda.com/download/\" target=\"_blank\" rel=\"noopener\">Anaconda</a></p>\n<ul>\n<li>Step 1: Install VS2015</li>\n<li>Step 2: Install CUDA 9.0 </li>\n<li>Step 3: Install cuDNN7 cudnnbinPATH</li>\n<li>Step 4: Install Anaconda PATH,  <strong>Python 3.5</strong></li>\n<li>Step 5: Anaconda ,jupyterbook<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">conda create  --name tensorflow python=3.5</span><br><span class=\"line\">activate tensorflow</span><br><span class=\"line\">conda install nb_conda</span><br></pre></td></tr></table></figure></li>\n<li>Step 6: Install GPU version TensorFlow.<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">pip install -i https://pypi.tuna.tsinghua.edu.cn/simple --ignore-installed --upgrade tensorflow-gpu </span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n</li>\n<li><p><strong>Keras</strong></p>\n<ul>\n<li>Step 1:   Keras GPU <figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">activate tensorflow</span><br><span class=\"line\">pip install keras -U --pre</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n</li>\n</ul>\n<h4 id=\"Keras\"><a href=\"#Keras\" class=\"headerlink\" title=\"Keras**\"></a>Keras**</h4><ul>\n<li><strong><a href=\"https://github.com/Trouble404/NBA-with-Machine-Learning\" target=\"_blank\" rel=\"noopener\">NBA with Machine Learning</a></strong></li>\n<li><strong><a href=\"https://github.com/Trouble404/kaggle-Job-Salary-Prediction\" target=\"_blank\" rel=\"noopener\">Kaggle- Job salary prediction</a></strong></li>\n</ul>\n<h4 id=\"TensorFlow-CPU-\"><a href=\"#TensorFlow-CPU-\" class=\"headerlink\" title=\"TensorFlow CPU \"></a>TensorFlow CPU </h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> tensorflow <span class=\"keyword\">as</span> tf  </span><br><span class=\"line\"><span class=\"keyword\">import</span> os</span><br><span class=\"line\"><span class=\"keyword\">import</span> keras.backend.tensorflow_backend <span class=\"keyword\">as</span> KTF  </span><br><span class=\"line\"></span><br><span class=\"line\">os.environ[<span class=\"string\">\"CUDA_VISIBLE_DEVICES\"</span>] = <span class=\"string\">\"0\"</span>  <span class=\"comment\">#GPU</span></span><br><span class=\"line\">config = tf.ConfigProto()</span><br><span class=\"line\">config.gpu_options.per_process_gpu_memory_fraction = <span class=\"number\">0.4</span> <span class=\"comment\">#GPUGPU</span></span><br><span class=\"line\">sess = tf.Session(config=config)</span><br><span class=\"line\">KTF.set_session(sess)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">with</span> tf.device(<span class=\"string\">'/cpu:0'</span>):</span><br></pre></td></tr></table></figure>\n<p>GPUCPU</p>\n<h4 id=\"Jupyter-Notebook-\"><a href=\"#Jupyter-Notebook-\" class=\"headerlink\" title=\"Jupyter Notebook \"></a>Jupyter Notebook </h4><p> <br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">jupyter notebook --generate-config</span><br></pre></td></tr></table></figure></p>\n<p><img src=\"https://raw.githubusercontent.com/Trouble404/Object-Detection-System-Based-on-CNN-and-Capsule-Network/master/Learned%20skills/jupyter%20_setting.PNG\" alt=\"image\"></p>\n<h2 id=\"June\"><a href=\"#June\" class=\"headerlink\" title=\"June\"></a>June</h2><h3 id=\"01-06-2018\"><a href=\"#01-06-2018\" class=\"headerlink\" title=\"01/06/2018\"></a>01/06/2018</h3><p><strong><a href=\"https://pytorch.org/about/\" target=\"_blank\" rel=\"noopener\">PyTorch</a></strong> is a python package that provides two high-level features:</p>\n<ul>\n<li>Tensor computation (like numpy) with strong GPU acceleration</li>\n<li>Deep Neural Networks built on a tape-based autodiff system</li>\n</ul>\n<table>\n<thead>\n<tr>\n<th style=\"text-align:left\">Package</th>\n<th style=\"text-align:left\">Description</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:left\">torch</td>\n<td style=\"text-align:left\">a Tensor library like NumPy, with strong GPU support</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">torch.autograd</td>\n<td style=\"text-align:left\">a tape based automatic differentiation library that supports all differentiable Tensor operations in torch</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">torch.nn</td>\n<td style=\"text-align:left\">a neural networks library deeply integrated with autograd designed for maximum flexibility</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">torch.optim</td>\n<td style=\"text-align:left\">an optimization package to be used with torch.nn with standard optimization methods such as SGD, RMSProp, LBFGS, Adam etc.</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">torch.multiprocessing</td>\n<td style=\"text-align:left\">python multiprocessing, but with magical memory sharing of torch Tensors across processes. Useful for data loading and hogwild training.</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">torch.utils</td>\n<td style=\"text-align:left\">DataLoader, Trainer and other utility functions for convenience</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">torch.legacy(.nn/.optim)</td>\n<td style=\"text-align:left\">legacy code that has been ported over from torch for backward compatibility reasons</td>\n</tr>\n</tbody>\n</table>\n<hr>\n<h4 id=\"Installation-1\"><a href=\"#Installation-1\" class=\"headerlink\" title=\"Installation\"></a>Installation</h4><ul>\n<li>Step 1: Anaconda ,jupyterbook<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">conda create  --name pytorch python=3.5</span><br><span class=\"line\">activate pytorch</span><br><span class=\"line\">conda install nb_conda</span><br></pre></td></tr></table></figure></li>\n<li>Step 2: Install GPU version PyTorch.<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">conda install pytorch cuda90 -c pytorch </span><br><span class=\"line\">pip install torchvision</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<h4 id=\"Understanding-of-PyTorch\"><a href=\"#Understanding-of-PyTorch\" class=\"headerlink\" title=\"Understanding of PyTorch\"></a>Understanding of PyTorch</h4><ul>\n<li><p><strong>Tensors</strong><br>Tensorsnumpyndarrays, TensorGPU</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> __future__ <span class=\"keyword\">import</span> print_function</span><br><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\">x = torch.Tensor(<span class=\"number\">5</span>, <span class=\"number\">3</span>)  <span class=\"comment\"># 5*3</span></span><br><span class=\"line\">x = torch.rand(<span class=\"number\">5</span>, <span class=\"number\">3</span>)  <span class=\"comment\"># </span></span><br><span class=\"line\">x <span class=\"comment\"># notebookxx</span></span><br><span class=\"line\">x.size()</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#<span class=\"doctag\">NOTE:</span> torch.Size tuple, *</span></span><br><span class=\"line\">y = torch.rand(<span class=\"number\">5</span>, <span class=\"number\">3</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\">x + y <span class=\"comment\"># </span></span><br><span class=\"line\">torch.add(x, y) <span class=\"comment\"># </span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># tensor</span></span><br><span class=\"line\">result = torch.Tensor(<span class=\"number\">5</span>, <span class=\"number\">3</span>) <span class=\"comment\"># </span></span><br><span class=\"line\">torch.add(x, y, out=result) <span class=\"comment\"># </span></span><br><span class=\"line\">y.add_(x) <span class=\"comment\"># yx</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># tensor'_'</span></span><br><span class=\"line\"><span class=\"comment\"># x.copy_(y), x.t_(), x</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#python</span></span><br><span class=\"line\">x[:,<span class=\"number\">1</span>] <span class=\"comment\">#x</span></span><br></pre></td></tr></table></figure>\n</li>\n<li><p><strong>Numpy</strong><br>TorchTensornumpyarrayTorchTensornumpyarray</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># tensornumpy</span></span><br><span class=\"line\">a = torch.ones(<span class=\"number\">5</span>)</span><br><span class=\"line\">b = a.numpy()</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># numpy,tensor</span></span><br><span class=\"line\">a.add_(<span class=\"number\">1</span>)</span><br><span class=\"line\">print(a)</span><br><span class=\"line\">print(b)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># numpyArraytorchTensor</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\">a = np.ones(<span class=\"number\">5</span>)</span><br><span class=\"line\">b = torch.from_numpy(a)</span><br><span class=\"line\">np.add(a, <span class=\"number\">1</span>, out=a)</span><br><span class=\"line\">print(a)</span><br><span class=\"line\">print(b)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># CharTensortensorCPUGPU</span></span><br><span class=\"line\"><span class=\"comment\"># CUDATensorGPU</span></span><br><span class=\"line\"><span class=\"comment\"># CUDAGPU</span></span><br><span class=\"line\"><span class=\"keyword\">if</span> torch.cuda.is_available():</span><br><span class=\"line\">    x = x.cuda()</span><br><span class=\"line\">    y = y.cuda()</span><br></pre></td></tr></table></figure>\n</li>\n<li><p><strong>PyTorchCIFAR10</strong><br><strong><a href=\"https://github.com/Trouble404/Object-Detection-System-Based-on-CNN-and-Capsule-Network/blob/master/Learned%20skills/pytorch.ipynb\" target=\"_blank\" rel=\"noopener\">code</a></strong></p>\n</li>\n<li><p><strong>MMdnn</strong><br>MMdnn is a set of tools to help users inter-operate among different deep learning frameworks. E.g. model conversion and visualization. Convert models between Caffe, Keras, MXNet, Tensorflow, CNTK, PyTorch Onnx and CoreML.</p>\n<p><img src=\"https://raw.githubusercontent.com/Microsoft/MMdnn/master/docs/supported.jpg\" alt=\"iamge\"></p>\n<p>MMdnn</p>\n<ul>\n<li>DNN</li>\n<li></li>\n<li>DNN</li>\n<li></li>\n</ul>\n<p><strong><a href=\"https://github.com/Microsoft/MMdnn\" target=\"_blank\" rel=\"noopener\">Github</a></strong></p>\n</li>\n</ul>\n<h3 id=\"04-06-2018\"><a href=\"#04-06-2018\" class=\"headerlink\" title=\"04/06/2018\"></a>04/06/2018</h3><h4 id=\"Dataset\"><a href=\"#Dataset\" class=\"headerlink\" title=\"Dataset:\"></a><strong>Dataset:</strong></h4><p> <strong><a href=\"http://host.robots.ox.ac.uk/pascal/VOC/voc2012/index.html\" target=\"_blank\" rel=\"noopener\">VOC 2012 Dataset</a></strong></p>\n<h4 id=\"Introduce\"><a href=\"#Introduce\" class=\"headerlink\" title=\"Introduce:\"></a><strong>Introduce:</strong></h4><p> <strong>Visual Object Classes Challenge 2012 (VOC2012)</strong><br><a href=\"http://host.robots.ox.ac.uk/pascal/VOC/\" target=\"_blank\" rel=\"noopener\">PASCAL</a>s full name is Pattern Analysis, Statistical Modelling and Computational Learning.<br>VOCs full name is <strong>Visual OBject Classes</strong>.<br>The first competition was held in 2005 and terminated in 2012. I will use the last updated dataset which is <a href=\"http://host.robots.ox.ac.uk/pascal/VOC/voc2012/index.html\" target=\"_blank\" rel=\"noopener\">VOC2012</a> dataset.</p>\n<p>The main aim of this competition is object detection, there are 20 classes objects in the dataset:</p>\n<ul>\n<li>person</li>\n<li>bird, cat, cow, dog, horse, sheep</li>\n<li>aeroplane, bicycle, boat, bus, car, motorbike, train</li>\n<li>bottle, chair, dining table, potted plant, sofa, tv/monitor</li>\n</ul>\n<h4 id=\"Detection-Task\"><a href=\"#Detection-Task\" class=\"headerlink\" title=\"Detection Task\"></a><strong>Detection Task</strong></h4><p>Referenced:<br><strong>The PASCAL Visual Object Classes Challenge 2012 (VOC2012) Development Kit</strong><br><strong>Mark Everingham - John Winn</strong><br><a href=\"http://host.robots.ox.ac.uk/pascal/VOC/voc2012/htmldoc/index.html\" target=\"_blank\" rel=\"noopener\">http://host.robots.ox.ac.uk/pascal/VOC/voc2012/htmldoc/index.html</a></p>\n<p><strong>Task:</strong><br>For each of the twenty classes predict the bounding boxes of each object of that class in a test image (if any). Each bounding box should be output with an associated real-valued confidence of the detection so that a precision/recall curve can be drawn. Participants may choose to tackle all, or any subset of object classes, for example cars only or motorbikes and cars.</p>\n<p><strong>Competitions</strong>:<br>Two competitions are defined according to the choice of training data:</p>\n<ul>\n<li>taken from the $VOC_{trainval}$ data provided.</li>\n<li>from any source excluding the $VOC_{test}$ data provided.</li>\n</ul>\n<p><strong>Submission of Results</strong>:<br>A separate text file of results should be generated for each competition and each class e.g. `car. Each line should be a detection output by the detector in the following format:<br>    <figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;image identifier&gt; &lt;confidence&gt; &lt;left&gt; &lt;top&gt; &lt;right&gt; &lt;bottom&gt;</span><br></pre></td></tr></table></figure></p>\n<p>where (left,top)-(right,bottom) defines the bounding box of the detected object. The top-left pixel in the image has coordinates $(1,1)$. Greater confidence values signify greater confidence that the detection is correct. An example file excerpt is shown below. Note that for the image 2009_000032, multiple objects are detected:<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">comp3_det_test_car.txt:</span><br><span class=\"line\">    ...</span><br><span class=\"line\">    2009_000026 0.949297 172.000000 233.000000 191.000000 248.000000</span><br><span class=\"line\">    2009_000032 0.013737 1.000000 147.000000 114.000000 242.000000</span><br><span class=\"line\">    2009_000032 0.013737 1.000000 134.000000 94.000000 168.000000</span><br><span class=\"line\">    2009_000035 0.063948 455.000000 229.000000 491.000000 243.000000</span><br><span class=\"line\">    ...</span><br></pre></td></tr></table></figure></p>\n<p><strong>Evaluation</strong>:<br>The detection task will be judged by the precision/recall curve. The principal quantitative measure used will be the average precision (AP). Detections are considered true or false positives based on the area of overlap with ground truth bounding boxes. To be considered a correct detection, the area of overlap $a_o$ between the predicted bounding box $B_p$ and ground truth bounding box $B_{gt}$ must exceed $50\\%$ by the formula: </p>\n<center> $a_o = \\frac{area(B_p \\cap B_{gt})}{area(B_p \\cup B_{gt})}$ </center>\n\n<h4 id=\"XML\"><a href=\"#XML\" class=\"headerlink\" title=\"XML\"></a><strong>XML</strong></h4><p> xmlgemfield8xml8xmlxml<br> <figure class=\"highlight html\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"> &lt;?xml version=\"1.0\" encoding=\"utf-8\"?&gt;</span><br><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">annotation</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">folder</span>&gt;</span>VOC2007<span class=\"tag\">&lt;/<span class=\"name\">folder</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">filename</span>&gt;</span>test100.mp4_3380.jpeg<span class=\"tag\">&lt;/<span class=\"name\">filename</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">size</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">width</span>&gt;</span>1280<span class=\"tag\">&lt;/<span class=\"name\">width</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">height</span>&gt;</span>720<span class=\"tag\">&lt;/<span class=\"name\">height</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">depth</span>&gt;</span>3<span class=\"tag\">&lt;/<span class=\"name\">depth</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;/<span class=\"name\">size</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">object</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>gemfield<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">bndbox</span>&gt;</span></span><br><span class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">xmin</span>&gt;</span>549<span class=\"tag\">&lt;/<span class=\"name\">xmin</span>&gt;</span></span><br><span class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">xmax</span>&gt;</span>715<span class=\"tag\">&lt;/<span class=\"name\">xmax</span>&gt;</span></span><br><span class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">ymin</span>&gt;</span>257<span class=\"tag\">&lt;/<span class=\"name\">ymin</span>&gt;</span></span><br><span class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">ymax</span>&gt;</span>289<span class=\"tag\">&lt;/<span class=\"name\">ymax</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;/<span class=\"name\">bndbox</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">truncated</span>&gt;</span>0<span class=\"tag\">&lt;/<span class=\"name\">truncated</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">difficult</span>&gt;</span>0<span class=\"tag\">&lt;/<span class=\"name\">difficult</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;/<span class=\"name\">object</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">object</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>civilnet<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">bndbox</span>&gt;</span></span><br><span class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">xmin</span>&gt;</span>842<span class=\"tag\">&lt;/<span class=\"name\">xmin</span>&gt;</span></span><br><span class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">xmax</span>&gt;</span>1009<span class=\"tag\">&lt;/<span class=\"name\">xmax</span>&gt;</span></span><br><span class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">ymin</span>&gt;</span>138<span class=\"tag\">&lt;/<span class=\"name\">ymin</span>&gt;</span></span><br><span class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">ymax</span>&gt;</span>171<span class=\"tag\">&lt;/<span class=\"name\">ymax</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;/<span class=\"name\">bndbox</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">truncated</span>&gt;</span>0<span class=\"tag\">&lt;/<span class=\"name\">truncated</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">difficult</span>&gt;</span>0<span class=\"tag\">&lt;/<span class=\"name\">difficult</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;/<span class=\"name\">object</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">segmented</span>&gt;</span>0<span class=\"tag\">&lt;/<span class=\"name\">segmented</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">annotation</span>&gt;</span></span><br></pre></td></tr></table></figure></p>\n<p>2objectgemfieldcivilnet</p>\n<p>xml</p>\n<ul>\n<li>bndbox</li>\n<li>truncated</li>\n<li>occluded</li>\n<li>difficultdifficult</li>\n</ul>\n<p><strong>objectname SSDpy-faster-rcnn</strong></p>\n<h3 id=\"07-06-2018\"><a href=\"#07-06-2018\" class=\"headerlink\" title=\"07/06/2018\"></a>07/06/2018</h3><h4 id=\"Poster-conference\"><a href=\"#Poster-conference\" class=\"headerlink\" title=\"Poster conference\"></a><strong>Poster conference</strong></h4><p><img src=\"https://raw.githubusercontent.com/Trouble404/Object-Detection-System-Based-on-CNN-and-Capsule-Network/master/Poster/poster.png\" alt=\"iamge\"></p>\n<p>5 People in one group to present their object.<br>I present this object to my supervisor in this conference.</p>\n<h3 id=\"11-06-2018\"><a href=\"#11-06-2018\" class=\"headerlink\" title=\"11/06/2018\"></a>11/06/2018</h3><h4 id=\"R-CNN\"><a href=\"#R-CNN\" class=\"headerlink\" title=\"R-CNN\"></a><strong>R-CNN</strong></h4><p>Paper: <a href=\"https://arxiv.org/abs/1311.2524\" target=\"_blank\" rel=\"noopener\">Rich feature hierarchies for accurate object detection and semantic segmentation</a></p>\n<p><strong></strong></p>\n<ul>\n<li> (Selective Search)(CNN)</li>\n<li>     ImageNet ILSVC 20121000 PASCAL VOC 2007   20 CNN   </li>\n</ul>\n<p><strong></strong></p>\n<ol>\n<li> 1K~2K Selective Search </li>\n<li>  CNN </li>\n<li> SVM </li>\n<li> <center><img src=\"https://raw.githubusercontent.com/Trouble404/Object-Detection-System-Based-on-CNN-and-Capsule-Network/master/Research%20Review/LaTex/1.PNG\" alt=\"image\"></center>\n\n</li>\n</ol>\n<p><strong><a href=\"https://www.koen.me/research/pub/uijlings-ijcv2013-draft.pdf\" target=\"_blank\" rel=\"noopener\">Selective Search</a></strong></p>\n<ol>\n<li> (1k~2k )</li>\n<li></li>\n<li><br> <ul>\n<li></li>\n<li></li>\n<li>  a-b-c-d-e-f-g-hab-cd-ef-gh -&gt; abcd-efgh -&gt; abcdefgh ab-c-d-e-f-g-h -&gt;abcd-e-f-g-h -&gt;abcdef-gh -&gt; abcdefgh</li>\n<li>BBOX </li>\n</ul>\n</li>\n</ol>\n<h3 id=\"12-06-2018\"><a href=\"#12-06-2018\" class=\"headerlink\" title=\"12/06/2018\"></a>12/06/2018</h3><h4 id=\"SPP-CNN\"><a href=\"#SPP-CNN\" class=\"headerlink\" title=\"SPP-CNN\"></a><strong>SPP-CNN</strong></h4><p>Paper: <a href=\"https://arxiv.org/abs/1406.4729\" target=\"_blank\" rel=\"noopener\">Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition</a></p>\n<p><strong></strong></p>\n<p>RCNNCNNRCNN224*224CNN</p>\n<ul>\n<li>region proposalSelective Search2Kregion proposal2KCNN</li>\n<li>region proposal</li>\n</ul>\n<p><strong></strong></p>\n<ol>\n<li>selective searchregion proposal</li>\n<li><br>$s \\in S = {480,576,688,864,1200}$<br>epochmodelmodel1*12*23*36*650bins</li>\n<li>region proposal224*224 </li>\n<li>SVMBoundingBox.<center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/SPP-NET.jpg\" alt=\"image\"></center>\n\n\n</li>\n</ol>\n<h3 id=\"13-06-2018\"><a href=\"#13-06-2018\" class=\"headerlink\" title=\"13/06/2018\"></a>13/06/2018</h3><h4 id=\"FAST-R-CNN\"><a href=\"#FAST-R-CNN\" class=\"headerlink\" title=\"FAST R-CNN\"></a><strong>FAST R-CNN</strong></h4><p>Paper: <a href=\"https://arxiv.org/abs/1504.08083\" target=\"_blank\" rel=\"noopener\">Fast R-CNN</a></p>\n<p><strong></strong></p>\n<ul>\n<li>RCNN</li>\n<li> </li>\n<li>: RCNN</li>\n</ul>\n<p><strong></strong></p>\n<ol>\n<li>convconv</li>\n<li>object proposals region of interestRoI</li>\n<li>fc<br> KbackgroundsoftmaxRoI<br>bbox regressionK4Kbounding-boxtrained end-to-end with a multi-task loss<center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/fast-rcnn.png\" alt=\"image\"></center>\n\n</li>\n</ol>\n<h3 id=\"14-18-06-2018\"><a href=\"#14-18-06-2018\" class=\"headerlink\" title=\"14~18/06/2018\"></a>14~18/06/2018</h3><h4 id=\"FASTER-R-CNN\"><a href=\"#FASTER-R-CNN\" class=\"headerlink\" title=\"FASTER R-CNN\"></a><strong>FASTER R-CNN</strong></h4><p>I want to use <strong>Faster R-cnn</strong> as the first method to implement object detection system.</p>\n<p>Paper: <a href=\"https://arxiv.org/abs/1506.01497\" target=\"_blank\" rel=\"noopener\">Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks</a></p>\n<p>Faster RCNN(feature extraction)proposalbounding box regression(rect refine)classification</p>\n<center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster-rcnn.jpg\" alt=\"image\"></center>\n\n<h4 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h4><ol>\n<li>Conv layersCNNFaster R-CNNconv+relu+poolingimagefeature mapsfeature mapsRPN</li>\n<li>Region Proposal NetworksRPNregion proposalssoftmaxanchorsforegroundbackgroundbounding box regressionanchorsproposals</li>\n<li>Roi Poolingfeature mapsproposalsproposal feature maps</li>\n<li>Classificationproposal feature mapsproposalbounding box regression</li>\n</ol>\n<h4 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h4><p><strong>[1. Conv layers]</strong><br>   <center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_1.jpg\" alt=\"image\"></center><br>   Conv layersconvpoolingrelupythonVGG16faster_rcnn_test.pt,    Conv layers13conv13relu4poolingConv          layers</p>\n<ul>\n<li>conv $kernel_size=3$  $pad=1$  $stride=1$ <br></li>\n<li><p>pooling $kernel_size=2$  $pad=0$  $stride=2$</p>\n<p>Faster RCNN Conv layers $pad=1$ 0                $(M+2)\\times (N+2)$ 3x3 $M\\times N$ Conv layersconv    <br><center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_2.jpg\" alt=\"image\"></center><br>Conv layerspooling $kernel_size=2$  $stride=2$ pooling $M\\times N$  $(M/2) \\times(N/2)$ Conv layersconvrelupooling1/2<br> $M\\times N$ Conv layers $(M/16)\\times (N/16)$ Conv layersfeatuure map</p>\n</li>\n</ul>\n<p><strong>[2. Region Proposal Networks(RPN)]</strong><br>   OpenCV adaboost+R-CNNSS(Selective      Search)Faster RCNNSSRPNFaster R-CNN    <br>   <center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_3.jpg\" alt=\"image\"></center><br>   RPNRPN2softmaxanchorsforeground              backgroundforegroundanchorsbounding box regression          proposalProposalforeground anchorsbounding box regressionproposals    proposalsProposal Layer</p>\n<p>   <strong>2.1 </strong></p>\n<ul>\n<li>+</li>\n<li><p>+</p>\n<center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_4.jpg\" alt=\"image\"></center><br>3233     <br> $1\\times1$      <br><br><strong>2.2 Anchors</strong><br>RPNanchorsanchorsrpn/generate_anchors.pydemo    generate_anchors.py<br>[[ -84.  -40.   99.   55.]<br>[-176.  -88.  191.  103.]<br>[-360. -184.  375.  199.]<br>[ -56.  -56.   71.   71.]<br>[-120. -120.  135.  135.]<br>[-248. -248.  263.  263.]<br>[ -36.  -80.   51.   95.]<br>[ -80. -168.   95.  183.]<br>[-168. -344.  183.  359.]]<br><br>4 $(x1,y1,x2,y2)$ 93 $width:height = [1:1, 1:2, 2:1]$ anchors<br><center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_5.jpg\" alt=\"image\"></center><br>anchors sizepython demoreshape $800\\times600$anchorsanchors 1:2  $352\\times704$  2:1  $736\\times384$ cover $800\\times600$ <br>9anchorsFaster RCNNConv layersfeature maps9anchors2bounding box regression<br><center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_6.jpg\" alt=\"image\"></center>\n\n<p></p>\n</li>\n</ul>\n<ul>\n<li>ZF modelConv Layersconv5num_output=256256feature map256-dimensions</li>\n<li>conv5rpn_conv/3x3num_output=2563x3256-d47</li>\n<li>conv5 feature mapkanchork=9anhcorforegroundbackground256d featurecls=2k scoresanchor[x, y, w, h]4reg=4k coordinates</li>\n<li><p>anchorsanchors128postive anchors+128negative anchorsanchors5.1</p>\n<p> <strong>2.3 softmaxforegroundbackground</strong><br> MxNFaster RCNNRPN(M/16)x(N/16) W=M/16  H=N/16 reshapesoftmax1x1<br> <center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_7.jpg\" alt=\"image\"></center><br> 1x1caffe prototxt<br> <center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_19.PNG\" alt=\"image\"></center><br>num_output=18 $W\\times H \\times 18$ feature maps9anchorsanchorsforegroundbackground $W\\times H\\times (9\\cdot2)$ softmaxforeground anchorsboxforeground anchors<br>RPNanchorssoftmaxforeground anchors</p>\n<p> <strong>2.4 bounding box regression</strong><br>Ground Truth(GT)foreground anchorsforeground anchorsGT<br><center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_8.jpg\" alt=\"image\"></center><br> (x, y, w, h) AForeground AnchorsGGTanchor AGG</p>\n</li>\n<li>$anchor A=(A_{x}, A_{y}, A_{w}, A_{h})$  $GT=[G_{x}, G_{y}, G_{w}, G_{h}]$</li>\n<li><p>F$F(A_{x}, A_{y}, A_{w}, A_{h})=(G_{x}^{}, G_{y}^{}, G_{w}^{}, G_{h}^{})$ $(G_{x}^{}, G_{y}^{}, G_{w}^{}, G_{h}^{}) \\approx (G_{x}, G_{y}, G_{w}, G_{h})$<br><center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_9.jpg\" alt=\"image\"></center><br>F10anchor AG :</p>\n</li>\n<li><p></p>\n<center><br>$G^{}<em>{x} = A</em>{w} \\cdot d_{x}(A) + A_{x} $<br>$G^{}<em>{y} = A</em>{y} \\cdot d_{y}(A) + A_{y} $<br></center></li>\n<li><center><br>$G^{}<em>{w} = A</em>{w} \\cdot exp(d_{w}(A)) $<br>$G^{}<em>{h} = A</em>{h} \\cdot exp(d_{h}(A)) $<br></center>\n\n</li>\n</ul>\n<p>4 $d_{x}(A),d_{y}(A),d_{w}(A),d_{h}(A)$ anchor AGT anchors AGT</p>\n<p> $d_{x}(A),d_{y}(A),d_{w}(A),d_{h}(A)$ X, W, Y$Y=WX$Xcnn feature mapAGT$(t_{x}, t_{y}, t_{w}, t_{h})$$d_{x}(A),d_{y}(A),d_{w}(A),d_{h}(A)$</p>\n<center><br>$d_{<em>}(A) = w^{T}_{</em>} \\cdot \\phi(A)$<br></center>\n\n<p>(A)anchorfeature mapwd(A)* xywh$(t_{x}, t_{y}, t_{w}, t_{h})$</p>\n<center><br>$Loss = \\sum^{N}<em>{i}(t^{i}</em>{<em>} - \\hat{w}^{T}_{</em>} \\cdot \\phi(A^{i}))^{2}$<br></center><br><br><center><br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_20.jpg\" alt=\"image\"><br></center><br>GT<br>Faster RCNNforeground anchorground truth $(t_x, t_y)$  $(t_w, t_h)$ <br><center><br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_21.jpg\" alt=\"image\"><br></center><br>bouding box regressioncnn feature AnchorGT $(t_x, t_y, t_w, t_h)$<br>bouding box regressionAnchor $(t_x, t_y, t_w, t_h)$Anchor<br><br>   <strong>2.5 proposalsbounding box regression</strong><br>bounding box regressionRPN<br><center><br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_10.jpg\" alt=\"image\"><br></center><br> $num_output=36$  $W\\times H\\times 36$ caffe blob [1, 36, H, W] feature maps9anchorsanchors4$d_{x}(A),d_{y}(A),d_{w}(A),d_{h}(A)$<br><br>   <strong>2.6 Proposal Layer</strong><br>Proposal Layer $d_{x}(A),d_{y}(A),d_{w}(A),d_{h}(A)$ foreground anchorsproposalRoI Pooling Layer<br>im_infoPxQFaster RCNNreshape $M\\times N$ im_info=[M, N, scale_factor]Conv Layers4pooling $W\\times H=(M/16)\\times(N/16)$ feature_stride=16anchor<br><center><br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_11.jpg\" alt=\"image\"><br></center>\n\n<p>Proposal Layer forwardcaffe layer</p>\n<ol>\n<li>anchors$[d_{x}(A),d_{y}(A),d_{w}(A),d_{h}(A)]$anchorsbbox regressionanchors</li>\n<li>foreground softmax scoresanchorspre_nms_topN(e.g. 6000)anchorsforeground anchors</li>\n<li>foreground anchorsroi poolingproposal</li>\n<li>width&lt;threshold or height&lt;thresholdforeground anchors</li>\n<li>nonmaximum suppression</li>\n<li>nmsforeground softmax scoresfg anchorspost_nms_topN(e.g. 300)proposal</li>\n</ol>\n<p> proposal=[x1, y1, x2, y2] anchorsproposal $M\\times N$ ~   </p>\n<p><strong>RPN</strong><br><strong>anchors -&gt; softmaxfg anchors -&gt; bbox regfg anchors -&gt; Proposal Layerproposals</strong></p>\n<h3 id=\"19-06-2018\"><a href=\"#19-06-2018\" class=\"headerlink\" title=\"19/06/2018\"></a>19/06/2018</h3><h4 id=\"-XML-\"><a href=\"#-XML-\" class=\"headerlink\" title=\" XML \"></a> XML </h4><p> xml.etree.ElementTree XML list<br></p>\n<ul>\n<li>XML</li>\n<li></li>\n<li>XMLPYTHON<br><center><br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_23.PNG\" alt=\"image\"><br></center><br>Github  jupyter notebook <a href=\"https://github.com/Trouble404/Object-Detection-System-Based-on-CNN-and-Capsule-Network/blob/master/Faster-RCNN/parser_voc2012_xml_and_plotting.ipynb\" target=\"_blank\" rel=\"noopener\"></a> </li>\n</ul>\n<p> trainval.txt <br></p>\n<p>  17125 <br> 11540 </p>\n<p>20</p>\n<center><br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_24.PNG\" alt=\"image\"><br></center> \n\n\n<h3 id=\"20-06-2018\"><a href=\"#20-06-2018\" class=\"headerlink\" title=\"20/06/2018\"></a>20/06/2018</h3><h4 id=\"BBOXES\"><a href=\"#BBOXES\" class=\"headerlink\" title=\"BBOXES\"></a>BBOXES</h4><p> cv2 <br>  <figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">pip install opencv-python</span><br></pre></td></tr></table></figure><br> OpenCV-python  BGR  RGB</p>\n<p>VOC201220 BBOXES</p>\n<table>\n<thead>\n<tr>\n<th style=\"text-align:left\">class</th>\n<th style=\"text-align:left\">class_mapping</th>\n<th style=\"text-align:left\">BGR of bbox</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:left\">Person</td>\n<td style=\"text-align:left\">0</td>\n<td style=\"text-align:left\">(0, 0, 255)</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">Aeroplane</td>\n<td style=\"text-align:left\">1</td>\n<td style=\"text-align:left\">(0, 0, 255)</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">Tvmonitor</td>\n<td style=\"text-align:left\">2</td>\n<td style=\"text-align:left\">(0, 128, 0)</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">Train</td>\n<td style=\"text-align:left\">3</td>\n<td style=\"text-align:left\">(128, 128, 128)</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">Boat</td>\n<td style=\"text-align:left\">4</td>\n<td style=\"text-align:left\">(0, 165, 255)</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">Dog</td>\n<td style=\"text-align:left\">5</td>\n<td style=\"text-align:left\">(0, 255, 255)</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">Chair</td>\n<td style=\"text-align:left\">6</td>\n<td style=\"text-align:left\">(80, 127, 255)</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">Bird</td>\n<td style=\"text-align:left\">7</td>\n<td style=\"text-align:left\">(208, 224, 64)</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">Bicycle</td>\n<td style=\"text-align:left\">8</td>\n<td style=\"text-align:left\">(235, 206, 135)</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">Bottle</td>\n<td style=\"text-align:left\">9</td>\n<td style=\"text-align:left\">(128, 0, 0)</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">Sheep</td>\n<td style=\"text-align:left\">10</td>\n<td style=\"text-align:left\">(140, 180, 210)</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">Diningtable</td>\n<td style=\"text-align:left\">11</td>\n<td style=\"text-align:left\">(0, 255, 0)</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">Horse</td>\n<td style=\"text-align:left\">12</td>\n<td style=\"text-align:left\">(133, 21, 199)</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">Motorbike</td>\n<td style=\"text-align:left\">13</td>\n<td style=\"text-align:left\">(47, 107, 85)</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">Sofa</td>\n<td style=\"text-align:left\">14</td>\n<td style=\"text-align:left\">(19, 69, 139)</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">Cow</td>\n<td style=\"text-align:left\">15</td>\n<td style=\"text-align:left\">(222, 196, 176)</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">Car</td>\n<td style=\"text-align:left\">16</td>\n<td style=\"text-align:left\">(0, 0, 0)</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">Cat</td>\n<td style=\"text-align:left\">17</td>\n<td style=\"text-align:left\">(225, 105, 65)</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">Bus</td>\n<td style=\"text-align:left\">18</td>\n<td style=\"text-align:left\">(255, 255, 255)</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">Pottedplant</td>\n<td style=\"text-align:left\">19</td>\n<td style=\"text-align:left\">(205, 250, 255)</td>\n</tr>\n</tbody>\n</table>\n<p>show_image_with_bboxBBOXESXMLlist:</p>\n<center><br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_22.PNG\" alt=\"image\"><br></center><br>Github  jupyter notebook <a href=\"https://github.com/Trouble404/Object-Detection-System-Based-on-CNN-and-Capsule-Network/blob/master/Faster-RCNN/parser_voc2012_xml_and_plotting.ipynb\" target=\"_blank\" rel=\"noopener\"></a><br><br>EXAMPLE:<br><center><br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/img_with_bboex_1.PNG\" alt=\"image\"><br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/img_with_bboex_2.PNG\" alt=\"image\"><br></center>  \n\n<h3 id=\"21-06-2018\"><a href=\"#21-06-2018\" class=\"headerlink\" title=\"21/06/2018\"></a>21/06/2018</h3><h4 id=\"config-setting\"><a href=\"#config-setting\" class=\"headerlink\" title=\"config setting\"></a>config setting</h4><p>set config class:<br>                 for image enhancement:</p>\n<center><br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/21_1.PNG\" alt=\"image\"><br></center>  \n\n<h4 id=\"image-enhancement\"><a href=\"#image-enhancement\" class=\"headerlink\" title=\"image enhancement\"></a>image enhancement</h4><center><br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/21_2.PNG\" alt=\"image\"><br></center><br>According to the config of three peremeters, users could augment image with 3 different ways or using them all.<br>For horizontal and vertical flips, 1/3 probability to triggle<br>With 0,90,180,270 rotation,<br>This function could increase the number of datasets.<br><br>image flips and rotation are realized by opencv and replace of height and width<br>New cordinates of bboxes are calculated acccording to different change of image<br><br>detailed in Github, jupyter notebook: <a href=\"https://github.com/Trouble404/Object-Detection-System-Based-on-CNN-and-Capsule-Network/blob/master/Faster-RCNN/config_set_and_image_enhance.ipynb\" target=\"_blank\" rel=\"noopener\">address</a><br><br>Orignal image:<br><center><br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/21_3.PNG\" alt=\"image\"><br></center><br>horizontal flip:<br><center><br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/21_4.PNG\" alt=\"image\"><br></center><br>Vertical filp:<br><center><br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/21_5.PNG\" alt=\"image\"><br></center><br>Random rotation:<br><center><br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/21_6.PNG\" alt=\"image\"><br></center><br>Horizontal and then vertical flips:<br><center><br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/21_7.PNG\" alt=\"image\"><br></center>  \n\n<h3 id=\"22-06-2018\"><a href=\"#22-06-2018\" class=\"headerlink\" title=\"22/06/2018\"></a>22/06/2018</h3><h4 id=\"Image-rezise\"><a href=\"#Image-rezise\" class=\"headerlink\" title=\"Image rezise\"></a>Image rezise</h4><p>This function is to rezise input image to a uniform size with same shortest side</p>\n<center><br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/22_2.PNG\" alt=\"image\"><br></center> \n\n<p>According to set the value of shortest side, convergent-divergent or augmented another side proportion</p>\n<p>Test:<br>Left image is resized image, in this case, the orignal image amplified.</p>\n<center><br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/22_1.png\" alt=\"image\"><br></center> \n\n<h4 id=\"Class-Balance\"><a href=\"#Class-Balance\" class=\"headerlink\" title=\"Class Balance\"></a>Class Balance</h4><p>When training the model, if we sent image with no repeating classes, it may help to improve the performance of model. Therefore, this function is to make sure no repeating classes in two closed input image.</p>\n<center><br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/22_3.PNG\" alt=\"image\"><br></center> \n\n<p>Test:</p>\n<p><center><br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/22_4.PNG\" alt=\"image\"><br></center><br>Random output 4 iamge with is function, it could find no repeating classes in two closed image. However, it may reduce the number of trainning image because skip some images.</p>\n<h3 id=\"25-26-06-2018\"><a href=\"#25-26-06-2018\" class=\"headerlink\" title=\"25~26/06/2018\"></a>25~26/06/2018</h3><h4 id=\"Region-Proposal-Networks-RPN\"><a href=\"#Region-Proposal-Networks-RPN\" class=\"headerlink\" title=\"Region Proposal Networks(RPN)\"></a>Region Proposal Networks(RPN)</h4><p><center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_3.jpg\" alt=\"image\"></center><br>RPN2softmaxanchorsforegroundbackgroundforegroundanchorsbounding box regressionproposalProposalforeground anchorsbounding box regressionproposalsproposalsProposal Layer</p>\n<h4 id=\"Anchors\"><a href=\"#Anchors\" class=\"headerlink\" title=\"Anchors\"></a>Anchors</h4><p></p>\n<p><center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_5.jpg\" alt=\"image\"></center><br>4 (x1,y1,x2,y2) 93 width:height = [1:1, 1:2, 2:1]</p>\n<p><center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_6.jpg\" alt=\"image\"></center><br>Conv layersfeature maps9anchors2bounding box regression.</p>\n<h4 id=\"Code\"><a href=\"#Code\" class=\"headerlink\" title=\"Code\"></a>Code</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"string\">\"\"\" intersection of two bboxes</span></span><br><span class=\"line\"><span class=\"string\">@param ai: left top x,y and right bottom x,y coordinates of bbox 1</span></span><br><span class=\"line\"><span class=\"string\">@param bi: left top x,y and right bottom x,y coordinates of bbox 2</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">@return: area_union: whether contain target classes</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">intersection</span><span class=\"params\">(ai, bi)</span>:</span></span><br></pre></td></tr></table></figure>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"string\">\"\"\" union of two bboxes</span></span><br><span class=\"line\"><span class=\"string\">@param au: left top x,y and right bottom x,y coordinates of bbox 1</span></span><br><span class=\"line\"><span class=\"string\">@param bu: left top x,y and right bottom x,y coordinates of bbox 2</span></span><br><span class=\"line\"><span class=\"string\">@param area_intersection: intersection area</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">@return: area_union: whether contain target classes</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">union</span><span class=\"params\">(au, bu, area_intersection)</span>:</span></span><br></pre></td></tr></table></figure>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"string\">\"\"\" calculate ratio of intersection and union</span></span><br><span class=\"line\"><span class=\"string\">@param a: left top x,y and right bottom x,y coordinates of bbox 1</span></span><br><span class=\"line\"><span class=\"string\">@param b: left top x,y and right bottom x,y coordinates of bbox 2</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">@return: ratio of intersection and union of two bboxes</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">iou</span><span class=\"params\">(a, b)</span>:</span></span><br></pre></td></tr></table></figure>\n<p><strong>IOU is used to bounding box regression</strong></p>\n<hr>\n<p><strong> rpn calculation</strong></p>\n<ol>\n<li>Traversal all pre-anchors to calculate IOU with GT bboxes</li>\n<li>Set number and proprty of pre-anchors</li>\n<li>return specity number of result(Anchors)</li>\n</ol>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"string\">\"\"\" </span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">@param C: configuration</span></span><br><span class=\"line\"><span class=\"string\">@param img_data: parsered xml information</span></span><br><span class=\"line\"><span class=\"string\">@param width: orignal width of image</span></span><br><span class=\"line\"><span class=\"string\">@param hegiht: orignal height of image</span></span><br><span class=\"line\"><span class=\"string\">@param resized_width: resized width of image after image processing</span></span><br><span class=\"line\"><span class=\"string\">@param resized_heighth: resized height of image after image processing</span></span><br><span class=\"line\"><span class=\"string\">@param img_length_calc_function: Keras's image_dim_ordering function</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">@return: np.copy(y_rpn_cls): whether contain target classes</span></span><br><span class=\"line\"><span class=\"string\">@return: np.copy(y_rpn_regr): corrspoding return of gradient</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">\"\"\"</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">calc_rpn</span><span class=\"params\">(C, img_data, width, height, resized_width, resized_height, img_length_calc_function)</span>:</span></span><br></pre></td></tr></table></figure>\n<p>num_regions256 </p>\n<p><br>Initialise paramters: see <a href=\"https://github.com/Trouble404/Object-Detection-System-Based-on-CNN-and-Capsule-Network/blob/master/Faster-RCNN/rpn_calculation.ipynb\" target=\"_blank\" rel=\"noopener\">jupyter notebook</a></p>\n<p>Calculate the size of map feature:<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">(output_width, output_height) = img_length_calc_function(resized_width, resized_height)</span><br></pre></td></tr></table></figure></p>\n<p><br><br>Get the GT box coordinates, and resize to account for image resizing<br>after rezised functon, the coordinates of bboxes need to re-calculation:<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">for</span> bbox_num, bbox <span class=\"keyword\">in</span> enumerate(img_data[<span class=\"string\">'bboxes'</span>]):</span><br><span class=\"line\">\tgta[bbox_num, <span class=\"number\">0</span>] = bbox[<span class=\"string\">'x1'</span>] * (resized_width / float(width))</span><br><span class=\"line\">\tgta[bbox_num, <span class=\"number\">1</span>] = bbox[<span class=\"string\">'x2'</span>] * (resized_width / float(width))</span><br><span class=\"line\">\tgta[bbox_num, <span class=\"number\">2</span>] = bbox[<span class=\"string\">'y1'</span>] * (resized_height / float(height))</span><br><span class=\"line\">\tgta[bbox_num, <span class=\"number\">3</span>] = bbox[<span class=\"string\">'y2'</span>] * (resized_height / float(height))</span><br></pre></td></tr></table></figure></p>\n<p>gtax1,x2,y1,y2x1,y1,x2,y2<br><br><br>Traverse all possible group of sizes<br>anchor box scales [128, 256, 512]<br>anchor box ratios [1:1,1:2,2:1]<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">for</span> anchor_size_idx <span class=\"keyword\">in</span> range(len(anchor_sizes)):</span><br><span class=\"line\">\t<span class=\"keyword\">for</span> anchor_ratio_idx <span class=\"keyword\">in</span> range(len(anchor_ratios)):</span><br><span class=\"line\">\t\tanchor_x = anchor_sizes[anchor_size_idx] * anchor_ratios[anchor_ratio_idx][<span class=\"number\">0</span>]</span><br><span class=\"line\">\t\tanchor_y = anchor_sizes[anchor_size_idx] * anchor_ratios[anchor_ratio_idx][<span class=\"number\">1</span>]</span><br></pre></td></tr></table></figure></p>\n<p>Traver one bbox group, all pre boxes generated by anchors</p>\n<p>output_widthoutput_heightwidth and height of map feature<br>downscalemapping ration, defualt 16<br>if to delete box out of iamge</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">for</span> ix <span class=\"keyword\">in</span> range(output_width):</span><br><span class=\"line\">\tx1_anc = downscale * (ix + <span class=\"number\">0.5</span>) - anchor_x / <span class=\"number\">2</span></span><br><span class=\"line\">\tx2_anc = downscale * (ix + <span class=\"number\">0.5</span>) + anchor_x / <span class=\"number\">2</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"keyword\">if</span> x1_anc &lt; <span class=\"number\">0</span> <span class=\"keyword\">or</span> x2_anc &gt; resized_width:</span><br><span class=\"line\">\t\t<span class=\"keyword\">continue</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"keyword\">for</span> jy <span class=\"keyword\">in</span> range(output_height):</span><br><span class=\"line\">\t\ty1_anc = downscale * (jy + <span class=\"number\">0.5</span>) - anchor_y / <span class=\"number\">2</span></span><br><span class=\"line\">\t\ty2_anc = downscale * (jy + <span class=\"number\">0.5</span>) + anchor_y / <span class=\"number\">2</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">if</span> y1_anc &lt; <span class=\"number\">0</span> <span class=\"keyword\">or</span> y2_anc &gt; resized_height:</span><br><span class=\"line\">\t\t\t<span class=\"keyword\">continue</span></span><br></pre></td></tr></table></figure>\n<p><br></p>\n<p></p>\n<p>bboxes bboxIOU<br>curr_ioubbox</p>\n<p>tx<br>ty:tx<br>tw:bbox<br>th:</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">if</span> curr_iou &gt; best_iou_for_bbox[bbox_num] <span class=\"keyword\">or</span> curr_iou &gt; C.rpn_max_overlap:</span><br><span class=\"line\">\tcx = (gta[bbox_num, <span class=\"number\">0</span>] + gta[bbox_num, <span class=\"number\">1</span>]) / <span class=\"number\">2.0</span></span><br><span class=\"line\">\tcy = (gta[bbox_num, <span class=\"number\">2</span>] + gta[bbox_num, <span class=\"number\">3</span>]) / <span class=\"number\">2.0</span></span><br><span class=\"line\">\tcxa = (x1_anc + x2_anc) / <span class=\"number\">2.0</span></span><br><span class=\"line\">\tcya = (y1_anc + y2_anc) / <span class=\"number\">2.0</span></span><br><span class=\"line\"></span><br><span class=\"line\">\ttx = (cx - cxa) / (x2_anc - x1_anc)</span><br><span class=\"line\">\tty = (cy - cya) / (y2_anc - y1_anc)</span><br><span class=\"line\">\ttw = np.log((gta[bbox_num, <span class=\"number\">1</span>] - gta[bbox_num, <span class=\"number\">0</span>]) / (x2_anc - x1_anc))</span><br><span class=\"line\">\tth = np.log((gta[bbox_num, <span class=\"number\">3</span>] - gta[bbox_num, <span class=\"number\">2</span>])) / (y2_anc - y1_anc)</span><br></pre></td></tr></table></figure>\n<p>Faster RCNNforeground anchorground truth $(t_x, t_y)$ </p>\n<p><center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/25_2.jpg\" alt=\"image\"></center><br>bouding box regressioncnn feature AnchorGT $(t_x, t_y, t_w, t_h)$ <br>bouding box regressionAnchor $(t_x, t_y, t_w, t_h)$Anchor</p>\n<p><br><br></p>\n<p>bbox<br>pos<br>best_iou_for_loc<br>num_anchors_for_bbox[bbox_num]bboxpos<br>negneutral<br>negbbox</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">if</span> img_data[<span class=\"string\">'bboxes'</span>][bbox_num][<span class=\"string\">'class'</span>] != <span class=\"string\">'bg'</span>:</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"comment\"># all GT boxes should be mapped to an anchor box, so we keep track of which anchor box was best</span></span><br><span class=\"line\">\t<span class=\"keyword\">if</span> curr_iou &gt; best_iou_for_bbox[bbox_num]:</span><br><span class=\"line\">\t\tbest_anchor_for_bbox[bbox_num] = [jy, ix, anchor_ratio_idx, anchor_size_idx]</span><br><span class=\"line\">\t\tbest_iou_for_bbox[bbox_num] = curr_iou</span><br><span class=\"line\">\t\tbest_x_for_bbox[bbox_num, :] = [x1_anc, x2_anc, y1_anc, y2_anc]</span><br><span class=\"line\">\t\tbest_dx_for_bbox[bbox_num, :] = [tx, ty, tw, th]</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"keyword\">if</span> curr_iou &gt; C.rpn_max_overlap:</span><br><span class=\"line\">\t\tbbox_type = <span class=\"string\">'pos'</span></span><br><span class=\"line\">\t\tnum_anchors_for_bbox[bbox_num] += <span class=\"number\">1</span></span><br><span class=\"line\">\t\t<span class=\"keyword\">if</span> curr_iou &gt; best_iou_for_loc:</span><br><span class=\"line\">\t\t\tbest_iou_for_loc = curr_iou</span><br><span class=\"line\">\t\t\tbest_regr = (tx, ty, tw, th)</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"keyword\">if</span> C.rpn_min_overlap &lt; curr_iou &lt; C.rpn_max_overlap:</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">if</span> bbox_type != <span class=\"string\">'pos'</span>:</span><br><span class=\"line\">\t\t\tbbox_type = <span class=\"string\">'neutral'</span></span><br></pre></td></tr></table></figure>\n<p><br><br>bbox</p>\n<p>y_is_box_validnertual<br>y_rpn_overlap<br>y_rpn_regr:<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">if</span> bbox_type == <span class=\"string\">'neg'</span>:</span><br><span class=\"line\">    y_is_box_valid[jy, ix, anchor_ratio_idx + n_anchratios * anchor_size_idx] = <span class=\"number\">1</span></span><br><span class=\"line\">    y_rpn_overlap[jy, ix, anchor_ratio_idx + n_anchratios * anchor_size_idx] = <span class=\"number\">0</span></span><br><span class=\"line\"><span class=\"keyword\">elif</span> bbox_type == <span class=\"string\">'neutral'</span>:</span><br><span class=\"line\">    y_is_box_valid[jy, ix, anchor_ratio_idx + n_anchratios * anchor_size_idx] = <span class=\"number\">0</span></span><br><span class=\"line\">    y_rpn_overlap[jy, ix, anchor_ratio_idx + n_anchratios * anchor_size_idx] = <span class=\"number\">0</span></span><br><span class=\"line\"><span class=\"keyword\">else</span>:</span><br><span class=\"line\">    y_is_box_valid[jy, ix, anchor_ratio_idx + n_anchratios * anchor_size_idx] = <span class=\"number\">1</span></span><br><span class=\"line\">    y_rpn_overlap[jy, ix, anchor_ratio_idx + n_anchratios * anchor_size_idx] = <span class=\"number\">1</span></span><br><span class=\"line\">    start = <span class=\"number\">4</span> * (anchor_ratio_idx + n_anchratios * anchor_size_idx)</span><br><span class=\"line\">    y_rpn_regr[jy, ix, start:start+<span class=\"number\">4</span>] = best_regr</span><br></pre></td></tr></table></figure></p>\n<p><br><br>bboxposanchorpos<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">for</span> idx <span class=\"keyword\">in</span> range(num_anchors_for_bbox.shape[<span class=\"number\">0</span>]):</span><br><span class=\"line\">\t<span class=\"keyword\">if</span> num_anchors_for_bbox[idx] == <span class=\"number\">0</span>:</span><br><span class=\"line\">\t\t<span class=\"comment\"># no box with an IOU greater than zero ...</span></span><br><span class=\"line\">\t\t<span class=\"keyword\">if</span> best_anchor_for_bbox[idx, <span class=\"number\">0</span>] == <span class=\"number\">-1</span>:</span><br><span class=\"line\">\t\t\t<span class=\"keyword\">continue</span></span><br><span class=\"line\">\t\ty_is_box_valid[best_anchor_for_bbox[idx,<span class=\"number\">0</span>], best_anchor_for_bbox[idx,<span class=\"number\">1</span>], best_anchor_for_bbox[idx,<span class=\"number\">2</span>] + n_anchratios *best_anchor_for_bbox[idx,<span class=\"number\">3</span>]] = <span class=\"number\">1</span></span><br><span class=\"line\">\t\ty_rpn_overlap[best_anchor_for_bbox[idx,<span class=\"number\">0</span>], best_anchor_for_bbox[idx,<span class=\"number\">1</span>], best_anchor_for_bbox[idx,<span class=\"number\">2</span>] + n_anchratios *best_anchor_for_bbox[idx,<span class=\"number\">3</span>]] = <span class=\"number\">1</span></span><br><span class=\"line\">\t\tstart = <span class=\"number\">4</span> * (best_anchor_for_bbox[idx,<span class=\"number\">2</span>] + n_anchratios * best_anchor_for_bbox[idx,<span class=\"number\">3</span>])</span><br><span class=\"line\">\t\ty_rpn_regr[best_anchor_for_bbox[idx,<span class=\"number\">0</span>], best_anchor_for_bbox[idx,<span class=\"number\">1</span>], start:start+<span class=\"number\">4</span>] = best_dx_for_bbox[idx, :]</span><br></pre></td></tr></table></figure></p>\n<p><br><br>, Tensorflow batch size,  <br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">y_rpn_overlap = np.transpose(y_rpn_overlap, (<span class=\"number\">2</span>, <span class=\"number\">0</span>, <span class=\"number\">1</span>))</span><br><span class=\"line\">y_rpn_overlap = np.expand_dims(y_rpn_overlap, axis=<span class=\"number\">0</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">y_is_box_valid = np.transpose(y_is_box_valid, (<span class=\"number\">2</span>, <span class=\"number\">0</span>, <span class=\"number\">1</span>))</span><br><span class=\"line\">y_is_box_valid = np.expand_dims(y_is_box_valid, axis=<span class=\"number\">0</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">y_rpn_regr = np.transpose(y_rpn_regr, (<span class=\"number\">2</span>, <span class=\"number\">0</span>, <span class=\"number\">1</span>))</span><br><span class=\"line\">y_rpn_regr = np.expand_dims(y_rpn_regr, axis=<span class=\"number\">0</span>)</span><br></pre></td></tr></table></figure></p>\n<p><br><br>num_regions<br>posnum_regions / 2pos<br>posnegnum_regionsneg<br> 256128positive,128negative  <br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">pos_locs = np.where(np(y_rpn_overlap[<span class=\"number\">0</span>, :, :, :] =.logical_and= <span class=\"number\">1</span>, y_is_box_valid[<span class=\"number\">0</span>, :, :, :] == <span class=\"number\">1</span>))</span><br><span class=\"line\">neg_locs = np.where(np.logical_and(y_rpn_overlap[<span class=\"number\">0</span>, :, :, :] == <span class=\"number\">0</span>, y_is_box_valid[<span class=\"number\">0</span>, :, :, :] == <span class=\"number\">1</span>))</span><br><span class=\"line\">num_regions = <span class=\"number\">256</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> len(pos_locs[<span class=\"number\">0</span>]) &gt; num_regions / <span class=\"number\">2</span>:</span><br><span class=\"line\">\tval_locs = random.sample(range(len(pos_locs[<span class=\"number\">0</span>])), len(pos_locs[<span class=\"number\">0</span>]) - num_regions / <span class=\"number\">2</span>)</span><br><span class=\"line\">\ty_is_box_valid[<span class=\"number\">0</span>, pos_locs[<span class=\"number\">0</span>][val_locs], pos_locs[<span class=\"number\">1</span>][val_locs], pos_locs[<span class=\"number\">2</span>][val_locs]] = <span class=\"number\">0</span></span><br><span class=\"line\">\tnum_pos = num_regions / <span class=\"number\">2</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> len(neg_locs[<span class=\"number\">0</span>]) + num_pos &gt; num_regions:</span><br><span class=\"line\">\tval_locs = random.sample(range(len(neg_locs[<span class=\"number\">0</span>])), len(neg_locs[<span class=\"number\">0</span>]) - num_pos)</span><br><span class=\"line\">\t y_is_box_valid[<span class=\"number\">0</span>, neg_locs[<span class=\"number\">0</span>][val_locs], neg_locs[<span class=\"number\">1</span>][val_locs], neg_locs[<span class=\"number\">2</span>][val_locs]] = <span class=\"number\">0</span></span><br></pre></td></tr></table></figure></p>\n<p><br></p>\n<h3 id=\"27-06-2018\"><a href=\"#27-06-2018\" class=\"headerlink\" title=\"27/06/2018\"></a>27/06/2018</h3><h4 id=\"project-brief\"><a href=\"#project-brief\" class=\"headerlink\" title=\"project brief\"></a>project brief</h4><p>Re organization of Project plan</p>\n<h4 id=\"Anchors-Iterative\"><a href=\"#Anchors-Iterative\" class=\"headerlink\" title=\"Anchors Iterative\"></a>Anchors Iterative</h4><p>Integration of privous work:<br>In each anchor: config file -&gt; rpn_stride = 16 means generate one anchor in 16 pixels<br><a href=\"https://github.com/Trouble404/Object-Detection-System-Based-on-CNN-and-Capsule-Network/blob/master/Faster-RCNN/anchor_rpn.ipynb\" target=\"_blank\" rel=\"noopener\">Jupyter Notebook address</a></p>\n<p><br>Function description<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">@param all_img_data: Parsered xml file  </span></span><br><span class=\"line\"><span class=\"string\">@param class_count: Counting of the number of all classes objects</span></span><br><span class=\"line\"><span class=\"string\">@param C: Configuration class</span></span><br><span class=\"line\"><span class=\"string\">@param img_length_calc_function: resnet's get_img_output_length() function</span></span><br><span class=\"line\"><span class=\"string\">@param backend: Tensorflow in this project</span></span><br><span class=\"line\"><span class=\"string\">#param mode: train or val</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">yield np.copy(x_img), [np.copy(y_rpn_cls), np.copy(y_rpn_regr)], img_data_aug</span></span><br><span class=\"line\"><span class=\"string\">@return: np.copy(x_img): image's matrix data</span></span><br><span class=\"line\"><span class=\"string\">@return: [np.copy(y_rpn_cls), np.copy(y_rpn_regr)]: calculated rpn class and radient</span></span><br><span class=\"line\"><span class=\"string\">@return: img_data_aug: correspoding parsed xml information</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">\"\"\"</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">get_anchor_gt</span><span class=\"params\">(all_img_data, class_count, C, img_length_calc_function, backend, mode=<span class=\"string\">'train'</span>)</span>:</span></span><br></pre></td></tr></table></figure></p>\n<p><br><br><strong>Traverse all input image based on input xml information</strong></p>\n<ul>\n<li>Apply class balance function: <figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">C.balanced_classes = <span class=\"keyword\">True</span></span><br><span class=\"line\">sample_selector = image_processing.SampleSelector(class_count)</span><br><span class=\"line\"><span class=\"keyword\">if</span> C.balanced_classes <span class=\"keyword\">and</span> sample_selector.skip_sample_for_balanced_class(img_data):</span><br><span class=\"line\">    <span class=\"keyword\">continue</span></span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<p><br></p>\n<ul>\n<li>Apply image enhance<br>if input mode is train, apply image enhance to obtain augmented image xml and matrix, if mode is val, obtain image orignal xml and matrix<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">if</span> mode == <span class=\"string\">'train'</span>:</span><br><span class=\"line\">    img_data_aug, x_img = image_enhance.augment(img_data, C, augment=<span class=\"keyword\">True</span>)</span><br><span class=\"line\"><span class=\"keyword\">else</span>:</span><br><span class=\"line\">    img_data_aug, x_img = image_enhance.augment(img_data, C, augment=<span class=\"keyword\">False</span>)</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<p>verifacation width and hegiht in xml and matrix<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">(width, height) = (img_data_aug[<span class=\"string\">'width'</span>], img_data_aug[<span class=\"string\">'height'</span>])</span><br><span class=\"line\">(rows, cols, _) = x_img.shape</span><br><span class=\"line\"><span class=\"keyword\">assert</span> cols == width</span><br><span class=\"line\"><span class=\"keyword\">assert</span> rows == height</span><br></pre></td></tr></table></figure></p>\n<p><br></p>\n<ul>\n<li>Apply rezise function<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">(resized_width, resized_height) = image_processing.get_new_img_size(width, height, C.im_size)</span><br><span class=\"line\">x_img = cv2.resize(x_img, (resized_width, resized_height), interpolation=cv2.INTER_CUBIC)</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<p><br></p>\n<ul>\n<li>Apply rpn calculation<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">y_rpn_cls, y_rpn_regr = rpn_calculation.calc_rpn(C, img_data_aug, width, height, resized_width, resized_height, img_length_calc_function)</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<p><br></p>\n<ul>\n<li><p>Zero-center by mean pixel, and preprocess image format<br>BGR -&gt; RGB because when apply resnet, it need RGB but in cv2, it use BGR</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">x_img = x_img[:,:, (<span class=\"number\">2</span>, <span class=\"number\">1</span>, <span class=\"number\">0</span>)]</span><br></pre></td></tr></table></figure>\n<p> For using pre-trainning model, needs to mins mean channel in each dim</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">x_img = x_img.astype(np.float32)</span><br><span class=\"line\">x_img[:, :, <span class=\"number\">0</span>] -= C.img_channel_mean[<span class=\"number\">0</span>]</span><br><span class=\"line\">x_img[:, :, <span class=\"number\">1</span>] -= C.img_channel_mean[<span class=\"number\">1</span>]</span><br><span class=\"line\">x_img[:, :, <span class=\"number\">2</span>] -= C.img_channel_mean[<span class=\"number\">2</span>]</span><br><span class=\"line\">x_img /= C.img_scaling_factor <span class=\"comment\"># default to 1,so no change here</span></span><br></pre></td></tr></table></figure>\n<p> expand for batch size</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">x_img = np.expand_dims(x_img, axis=<span class=\"number\">0</span>)</span><br></pre></td></tr></table></figure>\n<p>for using pre-trainning model, need to sclaling the std to match pre trained model</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">y_rpn_regr[:, y_rpn_regr.shape[<span class=\"number\">1</span>]//<span class=\"number\">2</span>:, :, :] *= C.std_scaling <span class=\"comment\"># scaling is 4 here</span></span><br></pre></td></tr></table></figure>\n<p>in tensorflow, sort as batch size, width, height, deep</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">if</span> backend == <span class=\"string\">'tf'</span>:</span><br><span class=\"line\">    x_img = np.transpose(x_img, (<span class=\"number\">0</span>, <span class=\"number\">3</span>, <span class=\"number\">2</span>, <span class=\"number\">1</span>))</span><br><span class=\"line\">    y_rpn_cls = np.transpose(y_rpn_cls, (<span class=\"number\">0</span>, <span class=\"number\">3</span>, <span class=\"number\">2</span>, <span class=\"number\">1</span>))</span><br><span class=\"line\">\ty_rpn_regr = np.transpose(y_rpn_regr, (<span class=\"number\">0</span>, <span class=\"number\">3</span>, <span class=\"number\">2</span>, <span class=\"number\">1</span>))</span><br></pre></td></tr></table></figure>\n<p>generator to iteror, using next() to loop</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">yield</span> np.copy(x_img), [np.copy(y_rpn_cls), np.copy(y_rpn_regr)], img_data_aug</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<p><br><br><br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">data_gen_train = get_anchor_gt(train_imgs, classes_count, C, nn.get_img_output_length, K.image_dim_ordering(), mode=<span class=\"string\">'train'</span>)</span><br><span class=\"line\">data_gen_val = get_anchor_gt(val_imgs, classes_count, C, nn.get_img_output_length,K.image_dim_ordering(), mode=<span class=\"string\">'val'</span>)</span><br></pre></td></tr></table></figure></p>\n<p>Test:<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">img,rpn,img_aug = next(data_gen_train)</span><br></pre></td></tr></table></figure></p>\n<p><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/26_1.PNG\" alt=\"image\"></p>\n<h3 id=\"28-06-2018\"><a href=\"#28-06-2018\" class=\"headerlink\" title=\"28/06/2018\"></a>28/06/2018</h3><h4 id=\"Resnet50-structure\"><a href=\"#Resnet50-structure\" class=\"headerlink\" title=\"Resnet50 structure\"></a>Resnet50 structure</h4><p>: <a href=\"https://arxiv.org/abs/1512.03385\" target=\"_blank\" rel=\"noopener\">https://arxiv.org/abs/1512.03385</a></p>\n<p><br><strong>Is learning better networks as easy as stacking more layers?</strong></p>\n<p><br><strong>vanishing/exploding gradients</strong>//normalized initialization and intermediate normalization layers<br><strong>degradation</strong>accuracyfigure156-layererror20-layererror</p>\n<p>He kaiMinglayersdegradationidentity mapping </p>\n<p><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/01_2.jpg\" alt=\"image\"></p>\n<p>plain layer<br><br>1) Our extremely deep residual nets are easy to optimize, but the counterpart plain nets (that simply stack layers) exhibit higher training error when the depth increases;<br>2) Our deep residual nets can easily enjoy accuracy gains from greatly increased depth, producing results substantially better than previous networks.</p>\n<h3 id=\"29-06-2018\"><a href=\"#29-06-2018\" class=\"headerlink\" title=\"29/06/2018\"></a>29/06/2018</h3><h4 id=\"Resnet50-image-structure\"><a href=\"#Resnet50-image-structure\" class=\"headerlink\" title=\"Resnet50 image structure\"></a>Resnet50 image structure</h4><p><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/1_01.png\" alt=\"iamge\"><br>ResNet2blockIdentity BlockdimensionblockConv Blockdimensionfeature vectordimension</p>\n<p>CNNimageconvertdepthfeature mapkernelVGG3x3outputchannelIdentity BlockConv BlockIdentity Block.</p>\n<p>Conv Block:<br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/01_3.png\" alt=\"image\"><br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/01_4.png\" alt=\"image\"><br>shortcut pathconv2D layer1x1 filter sizemain pathdimensionshortcut path.</p>\n<h2 id=\"July\"><a href=\"#July\" class=\"headerlink\" title=\"July\"></a>July</h2><h3 id=\"02-07-2018\"><a href=\"#02-07-2018\" class=\"headerlink\" title=\"02/07/2018\"></a>02/07/2018</h3><h4 id=\"Construct-resnet-by-keras\"><a href=\"#Construct-resnet-by-keras\" class=\"headerlink\" title=\"Construct resnet by keras\"></a>Construct resnet by keras</h4><p>xF(x)shape</p>\n<p>kerasidentity_blockconv_block</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> keras.models <span class=\"keyword\">import</span> Model</span><br><span class=\"line\"><span class=\"keyword\">from</span> keras.layers <span class=\"keyword\">import</span> Input,Dense,BatchNormalization,Conv2D,MaxPooling2D,AveragePooling2D,ZeroPadding2D</span><br><span class=\"line\"><span class=\"keyword\">from</span> keras.layers <span class=\"keyword\">import</span> add,Flatten</span><br><span class=\"line\"><span class=\"keyword\">from</span> keras.optimizers <span class=\"keyword\">import</span> SGD</span><br></pre></td></tr></table></figure>\n<p>identity_block, convshortcut<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">Conv2d_BN</span><span class=\"params\">(x, nb_filter,kernel_size, strides=<span class=\"params\">(<span class=\"number\">1</span>,<span class=\"number\">1</span>)</span>, padding=<span class=\"string\">'same'</span>,name=None)</span>:</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> name <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"keyword\">None</span>:</span><br><span class=\"line\">        bn_name = name + <span class=\"string\">'_bn'</span></span><br><span class=\"line\">        conv_name = name + <span class=\"string\">'_conv'</span></span><br><span class=\"line\">    <span class=\"keyword\">else</span>:</span><br><span class=\"line\">        bn_name = <span class=\"keyword\">None</span></span><br><span class=\"line\">        conv_name = <span class=\"keyword\">None</span></span><br><span class=\"line\"> </span><br><span class=\"line\">    x = Conv2D(nb_filter,kernel_size,padding=padding,strides=strides,activation=<span class=\"string\">'relu'</span>,name=conv_name)(x)</span><br><span class=\"line\">    x = BatchNormalization(axis=<span class=\"number\">3</span>,name=bn_name)(x)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> x</span><br><span class=\"line\">    </span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">Conv_Block</span><span class=\"params\">(inpt,nb_filter,kernel_size,strides=<span class=\"params\">(<span class=\"number\">1</span>,<span class=\"number\">1</span>)</span>, with_conv_shortcut=False)</span>:</span></span><br><span class=\"line\">    x = Conv2d_BN(inpt,nb_filter=nb_filter[<span class=\"number\">0</span>],kernel_size=(<span class=\"number\">1</span>,<span class=\"number\">1</span>),strides=strides,padding=<span class=\"string\">'same'</span>)</span><br><span class=\"line\">    x = Conv2d_BN(x, nb_filter=nb_filter[<span class=\"number\">1</span>], kernel_size=(<span class=\"number\">3</span>,<span class=\"number\">3</span>), padding=<span class=\"string\">'same'</span>)</span><br><span class=\"line\">    x = Conv2d_BN(x, nb_filter=nb_filter[<span class=\"number\">2</span>], kernel_size=(<span class=\"number\">1</span>,<span class=\"number\">1</span>), padding=<span class=\"string\">'same'</span>)</span><br><span class=\"line\">    <span class=\"keyword\">if</span> with_conv_shortcut:</span><br><span class=\"line\">        shortcut = Conv2d_BN(inpt,nb_filter=nb_filter[<span class=\"number\">2</span>],strides=strides,kernel_size=kernel_size)</span><br><span class=\"line\">        x = add([x,shortcut])</span><br><span class=\"line\">        <span class=\"keyword\">return</span> x</span><br><span class=\"line\">    <span class=\"keyword\">else</span>:</span><br><span class=\"line\">        x = add([x,inpt])</span><br><span class=\"line\">        <span class=\"keyword\">return</span> x</span><br></pre></td></tr></table></figure></p>\n<p>identity_blockconv_block<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">inpt = Input(shape=(<span class=\"number\">224</span>,<span class=\"number\">224</span>,<span class=\"number\">3</span>))</span><br><span class=\"line\">x = ZeroPadding2D((<span class=\"number\">3</span>,<span class=\"number\">3</span>))(inpt)</span><br><span class=\"line\">x = Conv2d_BN(x,nb_filter=<span class=\"number\">64</span>,kernel_size=(<span class=\"number\">7</span>,<span class=\"number\">7</span>),strides=(<span class=\"number\">2</span>,<span class=\"number\">2</span>),padding=<span class=\"string\">'valid'</span>)</span><br><span class=\"line\">x = MaxPooling2D(pool_size=(<span class=\"number\">3</span>,<span class=\"number\">3</span>),strides=(<span class=\"number\">2</span>,<span class=\"number\">2</span>),padding=<span class=\"string\">'same'</span>)(x)</span><br><span class=\"line\"> </span><br><span class=\"line\">x = Conv_Block(x,nb_filter=[<span class=\"number\">64</span>,<span class=\"number\">64</span>,<span class=\"number\">256</span>],kernel_size=(<span class=\"number\">3</span>,<span class=\"number\">3</span>),strides=(<span class=\"number\">1</span>,<span class=\"number\">1</span>),with_conv_shortcut=<span class=\"keyword\">True</span>)</span><br><span class=\"line\">x = Conv_Block(x,nb_filter=[<span class=\"number\">64</span>,<span class=\"number\">64</span>,<span class=\"number\">256</span>],kernel_size=(<span class=\"number\">3</span>,<span class=\"number\">3</span>))</span><br><span class=\"line\">x = Conv_Block(x,nb_filter=[<span class=\"number\">64</span>,<span class=\"number\">64</span>,<span class=\"number\">256</span>],kernel_size=(<span class=\"number\">3</span>,<span class=\"number\">3</span>))</span><br><span class=\"line\"> </span><br><span class=\"line\">x = Conv_Block(x,nb_filter=[<span class=\"number\">128</span>,<span class=\"number\">128</span>,<span class=\"number\">512</span>],kernel_size=(<span class=\"number\">3</span>,<span class=\"number\">3</span>),strides=(<span class=\"number\">2</span>,<span class=\"number\">2</span>),with_conv_shortcut=<span class=\"keyword\">True</span>)</span><br><span class=\"line\">x = Conv_Block(x,nb_filter=[<span class=\"number\">128</span>,<span class=\"number\">128</span>,<span class=\"number\">512</span>],kernel_size=(<span class=\"number\">3</span>,<span class=\"number\">3</span>))</span><br><span class=\"line\">x = Conv_Block(x,nb_filter=[<span class=\"number\">128</span>,<span class=\"number\">128</span>,<span class=\"number\">512</span>],kernel_size=(<span class=\"number\">3</span>,<span class=\"number\">3</span>))</span><br><span class=\"line\">x = Conv_Block(x,nb_filter=[<span class=\"number\">128</span>,<span class=\"number\">128</span>,<span class=\"number\">512</span>],kernel_size=(<span class=\"number\">3</span>,<span class=\"number\">3</span>))</span><br><span class=\"line\"> </span><br><span class=\"line\">x = Conv_Block(x,nb_filter=[<span class=\"number\">256</span>,<span class=\"number\">256</span>,<span class=\"number\">1024</span>],kernel_size=(<span class=\"number\">3</span>,<span class=\"number\">3</span>),strides=(<span class=\"number\">2</span>,<span class=\"number\">2</span>),with_conv_shortcut=<span class=\"keyword\">True</span>)</span><br><span class=\"line\">x = Conv_Block(x,nb_filter=[<span class=\"number\">256</span>,<span class=\"number\">256</span>,<span class=\"number\">1024</span>],kernel_size=(<span class=\"number\">3</span>,<span class=\"number\">3</span>))</span><br><span class=\"line\">x = Conv_Block(x,nb_filter=[<span class=\"number\">256</span>,<span class=\"number\">256</span>,<span class=\"number\">1024</span>],kernel_size=(<span class=\"number\">3</span>,<span class=\"number\">3</span>))</span><br><span class=\"line\">x = Conv_Block(x,nb_filter=[<span class=\"number\">256</span>,<span class=\"number\">256</span>,<span class=\"number\">1024</span>],kernel_size=(<span class=\"number\">3</span>,<span class=\"number\">3</span>))</span><br><span class=\"line\">x = Conv_Block(x,nb_filter=[<span class=\"number\">256</span>,<span class=\"number\">256</span>,<span class=\"number\">1024</span>],kernel_size=(<span class=\"number\">3</span>,<span class=\"number\">3</span>))</span><br><span class=\"line\">x = Conv_Block(x,nb_filter=[<span class=\"number\">256</span>,<span class=\"number\">256</span>,<span class=\"number\">1024</span>],kernel_size=(<span class=\"number\">3</span>,<span class=\"number\">3</span>))</span><br><span class=\"line\"> </span><br><span class=\"line\">x = Conv_Block(x,nb_filter=[<span class=\"number\">512</span>,<span class=\"number\">512</span>,<span class=\"number\">2048</span>],kernel_size=(<span class=\"number\">3</span>,<span class=\"number\">3</span>),strides=(<span class=\"number\">2</span>,<span class=\"number\">2</span>),with_conv_shortcut=<span class=\"keyword\">True</span>)</span><br><span class=\"line\">x = Conv_Block(x,nb_filter=[<span class=\"number\">512</span>,<span class=\"number\">512</span>,<span class=\"number\">2048</span>],kernel_size=(<span class=\"number\">3</span>,<span class=\"number\">3</span>))</span><br><span class=\"line\">x = Conv_Block(x,nb_filter=[<span class=\"number\">512</span>,<span class=\"number\">512</span>,<span class=\"number\">2048</span>],kernel_size=(<span class=\"number\">3</span>,<span class=\"number\">3</span>))</span><br><span class=\"line\">x = AveragePooling2D(pool_size=(<span class=\"number\">7</span>,<span class=\"number\">7</span>))(x)</span><br><span class=\"line\">x = Flatten()(x)</span><br><span class=\"line\">x = Dense(<span class=\"number\">1000</span>,activation=<span class=\"string\">'softmax'</span>)(x)</span><br><span class=\"line\"></span><br><span class=\"line\">model = Model(inputs=inpt,outputs=x)</span><br><span class=\"line\">sgd = SGD(decay=<span class=\"number\">0.0001</span>,momentum=<span class=\"number\">0.9</span>)</span><br><span class=\"line\">model.compile(loss=<span class=\"string\">'categorical_crossentropy'</span>,optimizer=sgd,metrics=[<span class=\"string\">'accuracy'</span>])</span><br><span class=\"line\">model.summary()</span><br></pre></td></tr></table></figure></p>\n<p><a href=\"https://github.com/Trouble404/Object-Detection-System-Based-on-CNN-and-Capsule-Network/blob/master/Faster-RCNN/Resnet50/resnet50_keras.ipynb\" target=\"_blank\" rel=\"noopener\">jupyter notebook</a></p>\n<h3 id=\"03-07-2018\"><a href=\"#03-07-2018\" class=\"headerlink\" title=\"03/07/2018\"></a>03/07/2018</h3><h4 id=\"load-pre-trained-model-of-resnet50\"><a href=\"#load-pre-trained-model-of-resnet50\" class=\"headerlink\" title=\"load pre-trained model of resnet50\"></a>load pre-trained model of resnet50</h4><p></p>\n<ul>\n<li>ResNet50resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5</li>\n<li>ResNet50</li>\n<li></li>\n<li></li>\n<li>blockblockResNet50 fine-tune</li>\n</ul>\n<p><a href=\"https://github.com/fchollet/deep-learning-models/releases\" target=\"_blank\" rel=\"noopener\"></a></p>\n<p><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/01_7.JPG\" alt=\"image\"></p>\n<p><br>identity<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">identity_block</span><span class=\"params\">(X, f, filters, stage, block)</span>:</span></span><br><span class=\"line\">    <span class=\"comment\"># defining name basis</span></span><br><span class=\"line\">    conv_name_base = <span class=\"string\">'res'</span> + str(stage) + block + <span class=\"string\">'_branch'</span></span><br><span class=\"line\">    bn_name_base = <span class=\"string\">'bn'</span> + str(stage) + block + <span class=\"string\">'_branch'</span></span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\"># Retrieve Filters</span></span><br><span class=\"line\">    F1, F2, F3 = filters</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\"># Save the input value. You'll need this later to add back to the main path. </span></span><br><span class=\"line\">    X_shortcut = X</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\"># First component of main path</span></span><br><span class=\"line\">    X = Conv2D(filters = F1, kernel_size = (<span class=\"number\">1</span>, <span class=\"number\">1</span>), strides = (<span class=\"number\">1</span>,<span class=\"number\">1</span>), padding = <span class=\"string\">'valid'</span>, name = conv_name_base + <span class=\"string\">'2a'</span>)(X)</span><br><span class=\"line\">    X = BatchNormalization(axis = <span class=\"number\">3</span>, name = bn_name_base + <span class=\"string\">'2a'</span>)(X)</span><br><span class=\"line\">    X = Activation(<span class=\"string\">'relu'</span>)(X)</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\"># Second component of main path (3 lines)</span></span><br><span class=\"line\">    X = Conv2D(filters= F2, kernel_size=(f,f),strides=(<span class=\"number\">1</span>,<span class=\"number\">1</span>),padding=<span class=\"string\">'same'</span>,name=conv_name_base + <span class=\"string\">'2b'</span>)(X)</span><br><span class=\"line\">    X = BatchNormalization(axis=<span class=\"number\">3</span>,name=bn_name_base+<span class=\"string\">'2b'</span>)(X)</span><br><span class=\"line\">    X = Activation(<span class=\"string\">'relu'</span>)(X)</span><br><span class=\"line\"> </span><br><span class=\"line\">    <span class=\"comment\"># Third component of main path (2 lines)</span></span><br><span class=\"line\">    X = Conv2D(filters=F3,kernel_size=(<span class=\"number\">1</span>,<span class=\"number\">1</span>),strides=(<span class=\"number\">1</span>,<span class=\"number\">1</span>),padding=<span class=\"string\">'valid'</span>,name=conv_name_base+<span class=\"string\">'2c'</span>)(X)</span><br><span class=\"line\">    X = BatchNormalization(axis=<span class=\"number\">3</span>,name=bn_name_base+<span class=\"string\">'2c'</span>)(X)</span><br><span class=\"line\"> </span><br><span class=\"line\">    <span class=\"comment\"># Final step: Add shortcut value to main path, and pass it through a RELU activation (2 lines)</span></span><br><span class=\"line\">    X = Add()([X, X_shortcut])</span><br><span class=\"line\">    X = Activation(<span class=\"string\">'relu'</span>)(X)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> X</span><br></pre></td></tr></table></figure></p>\n<p>conv<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">convolutional_block</span><span class=\"params\">(X, f, filters, stage, block, s = <span class=\"number\">2</span>)</span>:</span></span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\"># defining name basis</span></span><br><span class=\"line\">    conv_name_base = <span class=\"string\">'res'</span> + str(stage) + block + <span class=\"string\">'_branch'</span></span><br><span class=\"line\">    bn_name_base = <span class=\"string\">'bn'</span> + str(stage) + block + <span class=\"string\">'_branch'</span></span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\"># Retrieve Filters</span></span><br><span class=\"line\">    F1, F2, F3 = filters</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\"># Save the input value</span></span><br><span class=\"line\">    X_shortcut = X</span><br><span class=\"line\"> </span><br><span class=\"line\">    <span class=\"comment\">##### MAIN PATH #####</span></span><br><span class=\"line\">    <span class=\"comment\"># First component of main path </span></span><br><span class=\"line\">    X = Conv2D(F1, (<span class=\"number\">1</span>, <span class=\"number\">1</span>), strides = (s,s),padding=<span class=\"string\">'valid'</span>,name = conv_name_base + <span class=\"string\">'2a'</span>)(X)</span><br><span class=\"line\">    X = BatchNormalization(axis = <span class=\"number\">3</span>, name = bn_name_base + <span class=\"string\">'2a'</span>)(X)</span><br><span class=\"line\">    X = Activation(<span class=\"string\">'relu'</span>)(X)</span><br><span class=\"line\"> </span><br><span class=\"line\">    <span class=\"comment\"># Second component of main path (3 lines)</span></span><br><span class=\"line\">    X = Conv2D(F2,(f,f),strides=(<span class=\"number\">1</span>,<span class=\"number\">1</span>),padding=<span class=\"string\">'same'</span>,name=conv_name_base+<span class=\"string\">'2b'</span>)(X)</span><br><span class=\"line\">    X = BatchNormalization(axis=<span class=\"number\">3</span>,name=bn_name_base+<span class=\"string\">'2b'</span>)(X)</span><br><span class=\"line\">    X = Activation(<span class=\"string\">'relu'</span>)(X)</span><br><span class=\"line\"> </span><br><span class=\"line\">    <span class=\"comment\"># Third component of main path (2 lines)</span></span><br><span class=\"line\">    X = Conv2D(F3,(<span class=\"number\">1</span>,<span class=\"number\">1</span>),strides=(<span class=\"number\">1</span>,<span class=\"number\">1</span>),padding=<span class=\"string\">'valid'</span>,name=conv_name_base+<span class=\"string\">'2c'</span>)(X)</span><br><span class=\"line\">    X = BatchNormalization(axis=<span class=\"number\">3</span>,name=bn_name_base+<span class=\"string\">'2c'</span>)(X)</span><br><span class=\"line\"> </span><br><span class=\"line\">    <span class=\"comment\">##### SHORTCUT PATH #### (2 lines)</span></span><br><span class=\"line\">    X_shortcut = Conv2D(F3,(<span class=\"number\">1</span>,<span class=\"number\">1</span>),strides=(s,s),padding=<span class=\"string\">'valid'</span>,name=conv_name_base+<span class=\"string\">'1'</span>)(X_shortcut)</span><br><span class=\"line\">    X_shortcut = BatchNormalization(axis=<span class=\"number\">3</span>,name =bn_name_base+<span class=\"string\">'1'</span>)(X_shortcut)</span><br><span class=\"line\"> </span><br><span class=\"line\">    <span class=\"comment\"># Final step: Add shortcut value to main path, and pass it through a RELU activation (2 lines)</span></span><br><span class=\"line\">    X = Add()([X,X_shortcut])</span><br><span class=\"line\">    X = Activation(<span class=\"string\">'relu'</span>)(X)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> X</span><br></pre></td></tr></table></figure></p>\n<p>resnet50<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">ResNet50</span><span class=\"params\">(input_shape = <span class=\"params\">(<span class=\"number\">64</span>, <span class=\"number\">64</span>, <span class=\"number\">3</span>)</span>, classes = <span class=\"number\">30</span>)</span>:</span></span><br><span class=\"line\">    <span class=\"comment\"># Define the input as a tensor with shape input_shape</span></span><br><span class=\"line\">    X_input = Input(input_shape)</span><br><span class=\"line\"> </span><br><span class=\"line\">    <span class=\"comment\"># Zero-Padding</span></span><br><span class=\"line\">    X = ZeroPadding2D((<span class=\"number\">3</span>, <span class=\"number\">3</span>))(X_input)</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\"># Stage 1</span></span><br><span class=\"line\">    X = Conv2D(<span class=\"number\">64</span>, (<span class=\"number\">7</span>, <span class=\"number\">7</span>), strides = (<span class=\"number\">2</span>, <span class=\"number\">2</span>), name = <span class=\"string\">'conv1'</span>)(X)</span><br><span class=\"line\">    X = BatchNormalization(axis = <span class=\"number\">3</span>, name = <span class=\"string\">'bn_conv1'</span>)(X)</span><br><span class=\"line\">    X = Activation(<span class=\"string\">'relu'</span>)(X)</span><br><span class=\"line\">    X = MaxPooling2D((<span class=\"number\">3</span>, <span class=\"number\">3</span>), strides=(<span class=\"number\">2</span>, <span class=\"number\">2</span>))(X)</span><br><span class=\"line\"> </span><br><span class=\"line\">    <span class=\"comment\"># Stage 2</span></span><br><span class=\"line\">    X = convolutional_block(X, f = <span class=\"number\">3</span>, filters = [<span class=\"number\">64</span>, <span class=\"number\">64</span>, <span class=\"number\">256</span>], stage = <span class=\"number\">2</span>, block=<span class=\"string\">'a'</span>, s = <span class=\"number\">1</span>)</span><br><span class=\"line\">    X = identity_block(X, <span class=\"number\">3</span>, [<span class=\"number\">64</span>, <span class=\"number\">64</span>, <span class=\"number\">256</span>], stage=<span class=\"number\">2</span>, block=<span class=\"string\">'b'</span>)</span><br><span class=\"line\">    X = identity_block(X, <span class=\"number\">3</span>, [<span class=\"number\">64</span>, <span class=\"number\">64</span>, <span class=\"number\">256</span>], stage=<span class=\"number\">2</span>, block=<span class=\"string\">'c'</span>)</span><br><span class=\"line\"> </span><br><span class=\"line\">    <span class=\"comment\">### START CODE HERE ###</span></span><br><span class=\"line\"> </span><br><span class=\"line\">    <span class=\"comment\"># Stage 3 (4 lines)</span></span><br><span class=\"line\">    X = convolutional_block(X, f = <span class=\"number\">3</span>,filters= [<span class=\"number\">128</span>,<span class=\"number\">128</span>,<span class=\"number\">512</span>],stage=<span class=\"number\">3</span>,block=<span class=\"string\">'a'</span>,s=<span class=\"number\">2</span>)</span><br><span class=\"line\">    X = identity_block(X,<span class=\"number\">3</span>,[<span class=\"number\">128</span>,<span class=\"number\">128</span>,<span class=\"number\">512</span>],stage=<span class=\"number\">3</span>,block=<span class=\"string\">'b'</span>)</span><br><span class=\"line\">    X = identity_block(X,<span class=\"number\">3</span>,[<span class=\"number\">128</span>,<span class=\"number\">128</span>,<span class=\"number\">512</span>],stage=<span class=\"number\">3</span>,block=<span class=\"string\">'c'</span>)</span><br><span class=\"line\">    X = identity_block(X,<span class=\"number\">3</span>,[<span class=\"number\">128</span>,<span class=\"number\">128</span>,<span class=\"number\">512</span>],stage=<span class=\"number\">3</span>,block=<span class=\"string\">'d'</span>)</span><br><span class=\"line\"> </span><br><span class=\"line\">    <span class=\"comment\"># Stage 4 (6 lines)</span></span><br><span class=\"line\">    X = convolutional_block(X,f=<span class=\"number\">3</span>,filters=[<span class=\"number\">256</span>,<span class=\"number\">256</span>,<span class=\"number\">1024</span>],stage=<span class=\"number\">4</span>,block=<span class=\"string\">'a'</span>,s=<span class=\"number\">2</span>)</span><br><span class=\"line\">    X = identity_block(X,<span class=\"number\">3</span>,[<span class=\"number\">256</span>,<span class=\"number\">256</span>,<span class=\"number\">1024</span>],stage=<span class=\"number\">4</span>,block=<span class=\"string\">'b'</span>)</span><br><span class=\"line\">    X = identity_block(X,<span class=\"number\">3</span>,[<span class=\"number\">256</span>,<span class=\"number\">256</span>,<span class=\"number\">1024</span>],stage=<span class=\"number\">4</span>,block=<span class=\"string\">'c'</span>)</span><br><span class=\"line\">    X = identity_block(X,<span class=\"number\">3</span>,[<span class=\"number\">256</span>,<span class=\"number\">256</span>,<span class=\"number\">1024</span>],stage=<span class=\"number\">4</span>,block=<span class=\"string\">'d'</span>)</span><br><span class=\"line\">    X = identity_block(X,<span class=\"number\">3</span>,[<span class=\"number\">256</span>,<span class=\"number\">256</span>,<span class=\"number\">1024</span>],stage=<span class=\"number\">4</span>,block=<span class=\"string\">'e'</span>)</span><br><span class=\"line\">    X = identity_block(X,<span class=\"number\">3</span>,[<span class=\"number\">256</span>,<span class=\"number\">256</span>,<span class=\"number\">1024</span>],stage=<span class=\"number\">4</span>,block=<span class=\"string\">'f'</span>)</span><br><span class=\"line\"> </span><br><span class=\"line\">    <span class=\"comment\"># Stage 5 (3 lines)</span></span><br><span class=\"line\">    X = convolutional_block(X, f = <span class=\"number\">3</span>,filters= [<span class=\"number\">512</span>,<span class=\"number\">512</span>,<span class=\"number\">2048</span>],stage=<span class=\"number\">5</span>,block=<span class=\"string\">'a'</span>,s=<span class=\"number\">2</span>)</span><br><span class=\"line\">    X = identity_block(X,<span class=\"number\">3</span>,[<span class=\"number\">512</span>,<span class=\"number\">512</span>,<span class=\"number\">2048</span>],stage=<span class=\"number\">5</span>,block=<span class=\"string\">'b'</span>)</span><br><span class=\"line\">    X = identity_block(X,<span class=\"number\">3</span>,[<span class=\"number\">512</span>,<span class=\"number\">512</span>,<span class=\"number\">2048</span>],stage=<span class=\"number\">5</span>,block=<span class=\"string\">'c'</span>)</span><br><span class=\"line\"> </span><br><span class=\"line\">    <span class=\"comment\"># AVGPOOL (1 line). Use \"X = AveragePooling2D(...)(X)\"</span></span><br><span class=\"line\">    X = AveragePooling2D((<span class=\"number\">2</span>,<span class=\"number\">2</span>),strides=(<span class=\"number\">2</span>,<span class=\"number\">2</span>))(X)</span><br><span class=\"line\"> </span><br><span class=\"line\">    <span class=\"comment\"># output layer</span></span><br><span class=\"line\">    X = Flatten()(X)</span><br><span class=\"line\">    model = Model(inputs = X_input, outputs = X, name=<span class=\"string\">'ResNet50'</span>)</span><br><span class=\"line\"> </span><br><span class=\"line\">    <span class=\"keyword\">return</span> model</span><br></pre></td></tr></table></figure></p>\n<p><br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">base_model = ResNet50(input_shape=(<span class=\"number\">224</span>,<span class=\"number\">224</span>,<span class=\"number\">3</span>),classes=<span class=\"number\">30</span>) </span><br><span class=\"line\">base_model.load_weights(<span class=\"string\">'resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5'</span>)</span><br></pre></td></tr></table></figure></p>\n<p><br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/01_8.JPG\" alt=\"image\"></p>\n<h3 id=\"04-07-2018\"><a href=\"#04-07-2018\" class=\"headerlink\" title=\"04/07/2018\"></a>04/07/2018</h3><h4 id=\"Loading-pre-trained-model\"><a href=\"#Loading-pre-trained-model\" class=\"headerlink\" title=\"Loading pre-trained model\"></a>Loading pre-trained model</h4><p>kerasmodel.load_weightsmodel.load_weights(my_model_weights.h5, by_name=True) x= Dense(100, weights=oldModel.layers[1].get_weights())(x)</p>\n<p><br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">try</span>:</span><br><span class=\"line\">    base_model.load_weights(<span class=\"string\">'resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5'</span>,by_name=<span class=\"keyword\">True</span>)</span><br><span class=\"line\">    print(<span class=\"string\">\"load successful\"</span>)</span><br><span class=\"line\"><span class=\"keyword\">except</span>:</span><br><span class=\"line\">    print(<span class=\"string\">\"load failed\"</span>)</span><br></pre></td></tr></table></figure></p>\n<p><a href=\"https://github.com/Trouble404/Object-Detection-System-Based-on-CNN-and-Capsule-Network/blob/master/Faster-RCNN/Resnet50/resnet50_pre_load.ipynb\" target=\"_blank\" rel=\"noopener\">jupyter notebook</a></p>\n<h3 id=\"05-06-07-2018\"><a href=\"#05-06-07-2018\" class=\"headerlink\" title=\"05~06/07/2018\"></a>05~06/07/2018</h3><h4 id=\"construct-faster-rcnn-net\"><a href=\"#construct-faster-rcnn-net\" class=\"headerlink\" title=\"construct faster rcnn net\"></a>construct faster rcnn net</h4><p><strong>RoiPoolingConv</strong><br><br>ROI<br>ROIRegion of Interest<br>1Fast RCNN RoISelective Search<br>2Faster RCNNRPNRoIs<br>kerasLayer<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">RoiPoolingConv</span><span class=\"params\">(Layer)</span>:</span></span><br></pre></td></tr></table></figure></p>\n<p><a href=\"http://keras-cn.readthedocs.io/en/latest/layers/writting_layer/\" target=\"_blank\" rel=\"noopener\"></a></p>\n<p><br>**<a href=\"https://www.cnblogs.com/xuyuanyuan123/p/6674645.html\" target=\"_blank\" rel=\"noopener\">kwargs</a><br><a href=\"https://link.zhihu.com/?target=http%3A//python.jobbole.com/86787/\" target=\"_blank\" rel=\"noopener\">super</a>:<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"string\">'''ROI pooling layer for 2D inputs.</span></span><br><span class=\"line\"><span class=\"string\">    # Arguments</span></span><br><span class=\"line\"><span class=\"string\">        pool_size: int</span></span><br><span class=\"line\"><span class=\"string\">            Size of pooling region to use. pool_size = 7 will result in a 7x7 region.</span></span><br><span class=\"line\"><span class=\"string\">        num_rois: number of regions of interest to be used</span></span><br><span class=\"line\"><span class=\"string\">    '''</span></span><br><span class=\"line\"><span class=\"comment\">#  </span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self, pool_size, num_rois, **kwargs)</span>:</span></span><br><span class=\"line\"></span><br><span class=\"line\">        self.dim_ordering = K.image_dim_ordering()</span><br><span class=\"line\">        <span class=\"comment\"># print error when kernel not tensorflow or thoean</span></span><br><span class=\"line\">        <span class=\"keyword\">assert</span> self.dim_ordering <span class=\"keyword\">in</span> &#123;<span class=\"string\">'tf'</span>&#125;, <span class=\"string\">'dim_ordering must be in tf'</span></span><br><span class=\"line\"></span><br><span class=\"line\">        self.pool_size = pool_size</span><br><span class=\"line\">        self.num_rois = num_rois</span><br><span class=\"line\"></span><br><span class=\"line\">        super(RoiPoolingConv, self).__init__(**kwargs)</span><br></pre></td></tr></table></figure></p>\n<p>:<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">build</span><span class=\"params\">(self, input_shape)</span>:</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> self.dim_ordering == <span class=\"string\">'tf'</span>:</span><br><span class=\"line\">            self.nb_channels = input_shape[<span class=\"number\">0</span>][<span class=\"number\">3</span>]</span><br></pre></td></tr></table></figure></p>\n<p><br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">compute_output_shape</span><span class=\"params\">(self, input_shape)</span>:</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> self.dim_ordering == <span class=\"string\">'tf'</span>:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"keyword\">None</span>, self.num_rois, self.pool_size, self.pool_size, self.nb_channels</span><br></pre></td></tr></table></figure></p>\n<p>,, output:<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">call</span><span class=\"params\">(self, x, mask=None)</span>:</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">assert</span>(len(x) == <span class=\"number\">2</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">        img = x[<span class=\"number\">0</span>]</span><br><span class=\"line\">        rois = x[<span class=\"number\">1</span>]</span><br><span class=\"line\"></span><br><span class=\"line\">        input_shape = K.shape(img)</span><br><span class=\"line\"></span><br><span class=\"line\">        outputs = []</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">for</span> roi_idx <span class=\"keyword\">in</span> range(self.num_rois):</span><br><span class=\"line\"></span><br><span class=\"line\">            x = rois[<span class=\"number\">0</span>, roi_idx, <span class=\"number\">0</span>]</span><br><span class=\"line\">            y = rois[<span class=\"number\">0</span>, roi_idx, <span class=\"number\">1</span>]</span><br><span class=\"line\">            w = rois[<span class=\"number\">0</span>, roi_idx, <span class=\"number\">2</span>]</span><br><span class=\"line\">            h = rois[<span class=\"number\">0</span>, roi_idx, <span class=\"number\">3</span>]</span><br><span class=\"line\">            </span><br><span class=\"line\">            row_length = w / float(self.pool_size)</span><br><span class=\"line\">            col_length = h / float(self.pool_size)</span><br><span class=\"line\"></span><br><span class=\"line\">            num_pool_regions = self.pool_size</span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"keyword\">if</span> self.dim_ordering == <span class=\"string\">'tf'</span>:</span><br><span class=\"line\">                x = K.cast(x, <span class=\"string\">'int32'</span>)</span><br><span class=\"line\">                y = K.cast(y, <span class=\"string\">'int32'</span>)</span><br><span class=\"line\">                w = K.cast(w, <span class=\"string\">'int32'</span>)</span><br><span class=\"line\">                h = K.cast(h, <span class=\"string\">'int32'</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">                <span class=\"comment\"># resize porposal of feature map</span></span><br><span class=\"line\">                rs = tf.image.resize_images(img[:, y:y+h, x:x+w, :], (self.pool_size, self.pool_size))</span><br><span class=\"line\">                outputs.append(rs)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># outputsshape:(?, 7, 7, 512)</span></span><br><span class=\"line\">        final_output = K.concatenate(outputs, axis=<span class=\"number\">0</span>)</span><br><span class=\"line\">        final_output = K.reshape(final_output, (<span class=\"number\">1</span>, self.num_rois, self.pool_size, self.pool_size, self.nb_channels))</span><br><span class=\"line\">        <span class=\"comment\"># shape:(1, 32, 7, 7, 512)</span></span><br><span class=\"line\">        final_output = K.permute_dimensions(final_output, (<span class=\"number\">0</span>, <span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">4</span>))</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">return</span> final_output</span><br></pre></td></tr></table></figure></p>\n<p>batchvectorbatchRoIvectorchannel * w * hRoI Poolingboxw * h.</p>\n<p><strong>TimeDistributed </strong><br>FastRcnnROIpoolingRoiKerasTimeDistributed</p>\n<p>RelutensorTimeDistributedconv2DTimeDistributedROIROI</p>\n<p>Faster RCNNbboxnum_roisnum_roisTimeDistributedwrap</p>\n<p>conv  identity<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">conv_block_td</span><span class=\"params\">(input_tensor, kernel_size, filters, stage, block, input_shape, strides=<span class=\"params\">(<span class=\"number\">2</span>, <span class=\"number\">2</span>)</span>, trainable=True)</span>:</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># conv block time distributed</span></span><br><span class=\"line\"></span><br><span class=\"line\">    nb_filter1, nb_filter2, nb_filter3 = filters</span><br><span class=\"line\"></span><br><span class=\"line\">    bn_axis = <span class=\"number\">3</span></span><br><span class=\"line\"></span><br><span class=\"line\">    conv_name_base = <span class=\"string\">'res'</span> + str(stage) + block + <span class=\"string\">'_branch'</span></span><br><span class=\"line\">    bn_name_base = <span class=\"string\">'bn'</span> + str(stage) + block + <span class=\"string\">'_branch'</span></span><br><span class=\"line\"></span><br><span class=\"line\">    x = TimeDistributed(Convolution2D(nb_filter1, (<span class=\"number\">1</span>, <span class=\"number\">1</span>), strides=strides, trainable=trainable, kernel_initializer=<span class=\"string\">'normal'</span>), input_shape=input_shape, name=conv_name_base + <span class=\"string\">'2a'</span>)(input_tensor)</span><br><span class=\"line\">    x = TimeDistributed(BatchNormalization(axis=bn_axis), name=bn_name_base + <span class=\"string\">'2a'</span>)(x)</span><br><span class=\"line\">    x = Activation(<span class=\"string\">'relu'</span>)(x)</span><br><span class=\"line\"></span><br><span class=\"line\">    x = TimeDistributed(Convolution2D(nb_filter2, (kernel_size, kernel_size), padding=<span class=\"string\">'same'</span>, trainable=trainable, kernel_initializer=<span class=\"string\">'normal'</span>), name=conv_name_base + <span class=\"string\">'2b'</span>)(x)</span><br><span class=\"line\">    x = TimeDistributed(BatchNormalization(axis=bn_axis), name=bn_name_base + <span class=\"string\">'2b'</span>)(x)</span><br><span class=\"line\">    x = Activation(<span class=\"string\">'relu'</span>)(x)</span><br><span class=\"line\"></span><br><span class=\"line\">    x = TimeDistributed(Convolution2D(nb_filter3, (<span class=\"number\">1</span>, <span class=\"number\">1</span>), kernel_initializer=<span class=\"string\">'normal'</span>), name=conv_name_base + <span class=\"string\">'2c'</span>, trainable=trainable)(x)</span><br><span class=\"line\">    x = TimeDistributed(BatchNormalization(axis=bn_axis), name=bn_name_base + <span class=\"string\">'2c'</span>)(x)</span><br><span class=\"line\"></span><br><span class=\"line\">    shortcut = TimeDistributed(Convolution2D(nb_filter3, (<span class=\"number\">1</span>, <span class=\"number\">1</span>), strides=strides, trainable=trainable, kernel_initializer=<span class=\"string\">'normal'</span>), name=conv_name_base + <span class=\"string\">'1'</span>)(input_tensor)</span><br><span class=\"line\">    shortcut = TimeDistributed(BatchNormalization(axis=bn_axis), name=bn_name_base + <span class=\"string\">'1'</span>)(shortcut)</span><br><span class=\"line\"></span><br><span class=\"line\">    x = Add()([x, shortcut])</span><br><span class=\"line\">    x = Activation(<span class=\"string\">'relu'</span>)(x)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> x</span><br></pre></td></tr></table></figure></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">identity_block_td</span><span class=\"params\">(input_tensor, kernel_size, filters, stage, block, trainable=True)</span>:</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># identity block time distributed</span></span><br><span class=\"line\"></span><br><span class=\"line\">    nb_filter1, nb_filter2, nb_filter3 = filters</span><br><span class=\"line\"></span><br><span class=\"line\">    bn_axis = <span class=\"number\">3</span></span><br><span class=\"line\"></span><br><span class=\"line\">    conv_name_base = <span class=\"string\">'res'</span> + str(stage) + block + <span class=\"string\">'_branch'</span></span><br><span class=\"line\">    bn_name_base = <span class=\"string\">'bn'</span> + str(stage) + block + <span class=\"string\">'_branch'</span></span><br><span class=\"line\"></span><br><span class=\"line\">    x = TimeDistributed(Convolution2D(nb_filter1, (<span class=\"number\">1</span>, <span class=\"number\">1</span>), trainable=trainable, kernel_initializer=<span class=\"string\">'normal'</span>), name=conv_name_base + <span class=\"string\">'2a'</span>)(input_tensor)</span><br><span class=\"line\">    x = TimeDistributed(BatchNormalization(axis=bn_axis), name=bn_name_base + <span class=\"string\">'2a'</span>)(x)</span><br><span class=\"line\">    x = Activation(<span class=\"string\">'relu'</span>)(x)</span><br><span class=\"line\"></span><br><span class=\"line\">    x = TimeDistributed(Convolution2D(nb_filter2, (kernel_size, kernel_size), trainable=trainable, kernel_initializer=<span class=\"string\">'normal'</span>,padding=<span class=\"string\">'same'</span>), name=conv_name_base + <span class=\"string\">'2b'</span>)(x)</span><br><span class=\"line\">    x = TimeDistributed(BatchNormalization(axis=bn_axis), name=bn_name_base + <span class=\"string\">'2b'</span>)(x)</span><br><span class=\"line\">    x = Activation(<span class=\"string\">'relu'</span>)(x)</span><br><span class=\"line\"></span><br><span class=\"line\">    x = TimeDistributed(Convolution2D(nb_filter3, (<span class=\"number\">1</span>, <span class=\"number\">1</span>), trainable=trainable, kernel_initializer=<span class=\"string\">'normal'</span>), name=conv_name_base + <span class=\"string\">'2c'</span>)(x)</span><br><span class=\"line\">    x = TimeDistributed(BatchNormalization(axis=bn_axis), name=bn_name_base + <span class=\"string\">'2c'</span>)(x)</span><br><span class=\"line\"></span><br><span class=\"line\">    x = Add()([x, input_tensor])</span><br><span class=\"line\">    x = Activation(<span class=\"string\">'relu'</span>)(x)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">return</span> x</span><br></pre></td></tr></table></figure>\n<p>2DTimeDistributedDense</p>\n<p><strong>resnet50stage</strong><br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">classifier_layers</span><span class=\"params\">(x, input_shape, trainable=False)</span>:</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># Stage 5</span></span><br><span class=\"line\">    x = conv_block_td(x, <span class=\"number\">3</span>, [<span class=\"number\">512</span>, <span class=\"number\">512</span>, <span class=\"number\">2048</span>], stage=<span class=\"number\">5</span>, block=<span class=\"string\">'a'</span>, input_shape=input_shape, strides=(<span class=\"number\">2</span>, <span class=\"number\">2</span>), trainable=trainable)</span><br><span class=\"line\">    x = identity_block_td(x, <span class=\"number\">3</span>, [<span class=\"number\">512</span>, <span class=\"number\">512</span>, <span class=\"number\">2048</span>], stage=<span class=\"number\">5</span>, block=<span class=\"string\">'b'</span>, trainable=trainable)</span><br><span class=\"line\">    x = identity_block_td(x, <span class=\"number\">3</span>, [<span class=\"number\">512</span>, <span class=\"number\">512</span>, <span class=\"number\">2048</span>], stage=<span class=\"number\">5</span>, block=<span class=\"string\">'c'</span>, trainable=trainable)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># AVGPOOL</span></span><br><span class=\"line\">    x = TimeDistributed(AveragePooling2D((<span class=\"number\">7</span>, <span class=\"number\">7</span>)), name=<span class=\"string\">'avg_pool'</span>)(x)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">return</span> x</span><br></pre></td></tr></table></figure></p>\n<ul>\n<li>RoiPoolingConvshape(1, 32, 7, 7, 512)batch_size,</li>\n<li>TimeDistributed3D1323232</li>\n<li>out_classshape:(?, 32, 21); out_regrshape:(?, 32, 80)<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">classifier</span><span class=\"params\">(base_layers, input_rois, num_rois, nb_classes = <span class=\"number\">21</span>, trainable=False)</span>:</span></span><br><span class=\"line\"></span><br><span class=\"line\">    pooling_regions = <span class=\"number\">14</span></span><br><span class=\"line\">    input_shape = (num_rois,<span class=\"number\">14</span>,<span class=\"number\">14</span>,<span class=\"number\">1024</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    out_roi_pool = RoiPoolingConv(pooling_regions, num_rois)([base_layers, input_rois])</span><br><span class=\"line\">    out = classifier_layers(out_roi_pool, input_shape=input_shape, trainable=<span class=\"keyword\">True</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    out = TimeDistributed(Flatten())(out)</span><br><span class=\"line\"></span><br><span class=\"line\">    out_class = TimeDistributed(Dense(nb_classes, activation=<span class=\"string\">'softmax'</span>, kernel_initializer=<span class=\"string\">'zero'</span>), name=<span class=\"string\">'dense_class_&#123;&#125;'</span>.format(nb_classes))(out)</span><br><span class=\"line\">    <span class=\"comment\"># note: no regression target for bg class</span></span><br><span class=\"line\">    out_regr = TimeDistributed(Dense(<span class=\"number\">4</span> * (nb_classes<span class=\"number\">-1</span>), activation=<span class=\"string\">'linear'</span>, kernel_initializer=<span class=\"string\">'zero'</span>), name=<span class=\"string\">'dense_regress_&#123;&#125;'</span>.format(nb_classes))(out)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> [out_class, out_regr]</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<p><strong>RPN</strong></p>\n<ul>\n<li>x_class:sigmoidnum_anchors</li>\n<li>x_regr<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">rpn</span><span class=\"params\">(base_layers,num_anchors)</span>:</span></span><br><span class=\"line\"></span><br><span class=\"line\">    x = Convolution2D(<span class=\"number\">512</span>, (<span class=\"number\">3</span>, <span class=\"number\">3</span>), padding=<span class=\"string\">'same'</span>, activation=<span class=\"string\">'relu'</span>, kernel_initializer=<span class=\"string\">'normal'</span>, name=<span class=\"string\">'rpn_conv1'</span>)(base_layers)</span><br><span class=\"line\"></span><br><span class=\"line\">    x_class = Convolution2D(num_anchors, (<span class=\"number\">1</span>, <span class=\"number\">1</span>), activation=<span class=\"string\">'sigmoid'</span>, kernel_initializer=<span class=\"string\">'uniform'</span>, name=<span class=\"string\">'rpn_out_class'</span>)(x)</span><br><span class=\"line\">    x_regr = Convolution2D(num_anchors * <span class=\"number\">4</span>, (<span class=\"number\">1</span>, <span class=\"number\">1</span>), activation=<span class=\"string\">'linear'</span>, kernel_initializer=<span class=\"string\">'zero'</span>, name=<span class=\"string\">'rpn_out_regress'</span>)(x)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">return</span> [x_class, x_regr, base_layers]</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<p><strong>resnet</strong><br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">nn_base</span><span class=\"params\">(input_tensor=None, trainable=False)</span>:</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># Determine proper input shape</span></span><br><span class=\"line\"></span><br><span class=\"line\">    input_shape = (<span class=\"keyword\">None</span>, <span class=\"keyword\">None</span>, <span class=\"number\">3</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">if</span> input_tensor <span class=\"keyword\">is</span> <span class=\"keyword\">None</span>:</span><br><span class=\"line\">        img_input = Input(shape=input_shape)</span><br><span class=\"line\">    <span class=\"keyword\">else</span>:</span><br><span class=\"line\">        <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> K.is_keras_tensor(input_tensor):</span><br><span class=\"line\">            img_input = Input(tensor=input_tensor, shape=input_shape)</span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            img_input = input_tensor</span><br><span class=\"line\"></span><br><span class=\"line\">    bn_axis = <span class=\"number\">3</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># Zero-Padding</span></span><br><span class=\"line\">    x = ZeroPadding2D((<span class=\"number\">3</span>, <span class=\"number\">3</span>))(img_input)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># Stage 1</span></span><br><span class=\"line\">    x = Convolution2D(<span class=\"number\">64</span>, (<span class=\"number\">7</span>, <span class=\"number\">7</span>), strides=(<span class=\"number\">2</span>, <span class=\"number\">2</span>), name=<span class=\"string\">'conv1'</span>, trainable = trainable)(x)</span><br><span class=\"line\">    x = BatchNormalization(axis=bn_axis, name=<span class=\"string\">'bn_conv1'</span>)(x)</span><br><span class=\"line\">    x = Activation(<span class=\"string\">'relu'</span>)(x)</span><br><span class=\"line\">    x = MaxPooling2D((<span class=\"number\">3</span>, <span class=\"number\">3</span>), strides=(<span class=\"number\">2</span>, <span class=\"number\">2</span>))(x)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># Stage 2</span></span><br><span class=\"line\">    x = conv_block(x, <span class=\"number\">3</span>, [<span class=\"number\">64</span>, <span class=\"number\">64</span>, <span class=\"number\">256</span>], stage=<span class=\"number\">2</span>, block=<span class=\"string\">'a'</span>, strides=(<span class=\"number\">1</span>, <span class=\"number\">1</span>), trainable = trainable)</span><br><span class=\"line\">    x = identity_block(x, <span class=\"number\">3</span>, [<span class=\"number\">64</span>, <span class=\"number\">64</span>, <span class=\"number\">256</span>], stage=<span class=\"number\">2</span>, block=<span class=\"string\">'b'</span>, trainable = trainable)</span><br><span class=\"line\">    x = identity_block(x, <span class=\"number\">3</span>, [<span class=\"number\">64</span>, <span class=\"number\">64</span>, <span class=\"number\">256</span>], stage=<span class=\"number\">2</span>, block=<span class=\"string\">'c'</span>, trainable = trainable)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># Stage 3</span></span><br><span class=\"line\">    x = conv_block(x, <span class=\"number\">3</span>, [<span class=\"number\">128</span>, <span class=\"number\">128</span>, <span class=\"number\">512</span>], stage=<span class=\"number\">3</span>, block=<span class=\"string\">'a'</span>, trainable = trainable)</span><br><span class=\"line\">    x = identity_block(x, <span class=\"number\">3</span>, [<span class=\"number\">128</span>, <span class=\"number\">128</span>, <span class=\"number\">512</span>], stage=<span class=\"number\">3</span>, block=<span class=\"string\">'b'</span>, trainable = trainable)</span><br><span class=\"line\">    x = identity_block(x, <span class=\"number\">3</span>, [<span class=\"number\">128</span>, <span class=\"number\">128</span>, <span class=\"number\">512</span>], stage=<span class=\"number\">3</span>, block=<span class=\"string\">'c'</span>, trainable = trainable)</span><br><span class=\"line\">    x = identity_block(x, <span class=\"number\">3</span>, [<span class=\"number\">128</span>, <span class=\"number\">128</span>, <span class=\"number\">512</span>], stage=<span class=\"number\">3</span>, block=<span class=\"string\">'d'</span>, trainable = trainable)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># Stage 4</span></span><br><span class=\"line\">    x = conv_block(x, <span class=\"number\">3</span>, [<span class=\"number\">256</span>, <span class=\"number\">256</span>, <span class=\"number\">1024</span>], stage=<span class=\"number\">4</span>, block=<span class=\"string\">'a'</span>, trainable = trainable)</span><br><span class=\"line\">    x = identity_block(x, <span class=\"number\">3</span>, [<span class=\"number\">256</span>, <span class=\"number\">256</span>, <span class=\"number\">1024</span>], stage=<span class=\"number\">4</span>, block=<span class=\"string\">'b'</span>, trainable = trainable)</span><br><span class=\"line\">    x = identity_block(x, <span class=\"number\">3</span>, [<span class=\"number\">256</span>, <span class=\"number\">256</span>, <span class=\"number\">1024</span>], stage=<span class=\"number\">4</span>, block=<span class=\"string\">'c'</span>, trainable = trainable)</span><br><span class=\"line\">    x = identity_block(x, <span class=\"number\">3</span>, [<span class=\"number\">256</span>, <span class=\"number\">256</span>, <span class=\"number\">1024</span>], stage=<span class=\"number\">4</span>, block=<span class=\"string\">'d'</span>, trainable = trainable)</span><br><span class=\"line\">    x = identity_block(x, <span class=\"number\">3</span>, [<span class=\"number\">256</span>, <span class=\"number\">256</span>, <span class=\"number\">1024</span>], stage=<span class=\"number\">4</span>, block=<span class=\"string\">'e'</span>, trainable = trainable)</span><br><span class=\"line\">    x = identity_block(x, <span class=\"number\">3</span>, [<span class=\"number\">256</span>, <span class=\"number\">256</span>, <span class=\"number\">1024</span>], stage=<span class=\"number\">4</span>, block=<span class=\"string\">'f'</span>, trainable = trainable)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">return</span> x</span><br></pre></td></tr></table></figure></p>\n<p><strong></strong><br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># define the base network (resnet here)</span></span><br><span class=\"line\">shared_layers = nn.nn_base(img_input, trainable=<span class=\"keyword\">True</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># define the RPN, built on the base layers</span></span><br><span class=\"line\"><span class=\"comment\"># 9 types of anchors</span></span><br><span class=\"line\">num_anchors = len(C.anchor_box_scales) * len(C.anchor_box_ratios)</span><br><span class=\"line\">rpn = nn.rpn(shared_layers, num_anchors)</span><br><span class=\"line\"></span><br><span class=\"line\">classifier = nn.classifier(shared_layers, roi_input, C.num_rois, nb_classes=len(classes_count), trainable=<span class=\"keyword\">True</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">model_rpn = Model(img_input, rpn[:<span class=\"number\">2</span>])</span><br><span class=\"line\">model_classifier = Model([img_input, roi_input], classifier)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># this is a model that holds both the RPN and the classifier, used to load/save weights for the models</span></span><br><span class=\"line\">model_all = Model([img_input, roi_input], rpn[:<span class=\"number\">2</span>] + classifier)</span><br></pre></td></tr></table></figure></p>\n<p><a href=\"https://github.com/Trouble404/Object-Detection-System-Based-on-CNN-and-Capsule-Network/blob/master/Faster-RCNN/Resnet50/workflow_model.ipynb\" target=\"_blank\" rel=\"noopener\">jupyter notebook</a></p>\n<h3 id=\"09-07-2018\"><a href=\"#09-07-2018\" class=\"headerlink\" title=\"09/07/2018\"></a>09/07/2018</h3><h4 id=\"Loss-define\"><a href=\"#Loss-define\" class=\"headerlink\" title=\"Loss define\"></a>Loss define</h4><p><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/6_1.JPG\" alt=\"image\"></p>\n<p>(Multi-task Loss Function)Softmax Classification LossBounding Box Regression Loss</p>\n<p>$L({p_{i}},{t_{i}}) = \\frac{1}{N_{cls}}\\sum_{i}L_{cls}(p_{i},p_{i}^{\\ast}) + \\lambda\\frac{1}{N_{reg}}\\sum_{i}L_{reg}(t_{i},t_{i}^{\\ast})$</p>\n<p><strong>Softmax Classification</strong><br>RPN(cls)2k = 18conv5-3WH18conv5-39anchorsanchorscore(fg/bg)anchorSoftmaxreshape<br>$p_{i}$$p_{i}^{\\ast}$(label)anchor$p_{i}^{\\ast}$1$p_{i}^{\\ast}$0$L_{cls}$(log loss)</p>\n<p><strong>Bounding Box Regression</strong><br>RPN4k = 36$[x,y,w,h]$box(predicted box)$[x,y,w,h]$anchor$[x_{a},y_{a},w_{a},h_{a}]$ground truth$[x^{\\ast},y^{\\ast},w^{\\ast},h^{\\ast}]$anchor{$t$}ground truthanchor{$t^{\\ast}$}</p>\n<p>$t_{x}=\\frac{(x-x_{a})}{w_{a}}$ , $t_{y}=\\frac{(y-y_{a})}{h_{a}}$ , $t_{w}=log(\\frac{w}{w_{a}})$ , $t_{h}=log(\\frac{h}{h_{a}})$ (1)<br>$t_{x}^{\\ast}=\\frac{(x^{\\ast}-x_{a})}{w_{a}}$ , $t_{y}^{\\ast}=\\frac{(y^{\\ast}-y_{a})}{h_{a}}$ , $t_{w}^{\\ast}=log(\\frac{w^{\\ast}}{w_{a}})$ , $t_{h}^{\\ast}=log(\\frac{h^{\\ast}}{h_{a}})$ (2)</p>\n<p>{t}${t^{\\ast}}$${t}$${t^{\\ast}}$${t}$(1)</p>\n<p>Smooth L1:</p>\n<p>$$ Smooth_{L1}(x) =\\left{<br>\\begin{aligned}<br>0.5x^{2} \\ \\ |x| \\leqslant 1\\<br>|x| - 0.5 \\ \\ otherwise<br>\\end{aligned}<br>\\right.<br>$$</p>\n<p>$L_{reg} = Smooth_{L1}(t-t^{\\ast})$<br>Smooth L1L2L1<br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/7_1.JPG\" alt=\"iamge\"></p>\n<p>$p_{i}^{\\ast}L_{reg}$anchor$(p_{i}^{\\ast}=1)$anchorbboxground truth(IoU)bboxground truthwork(cls)(reg){p}{t}$N_{cls}$$N_{reg}$</p>\n<h3 id=\"10-07-2018\"><a href=\"#10-07-2018\" class=\"headerlink\" title=\"10/07/2018\"></a>10/07/2018</h3><h4 id=\"loss-code\"><a href=\"#loss-code\" class=\"headerlink\" title=\"loss code\"></a>loss code</h4><p>  generator to iteror, using next() to loop<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">yield</span> np.copy(x_img), [np.copy(y_rpn_cls), np.copy(y_rpn_regr)], img_data_aug</span><br></pre></td></tr></table></figure></p>\n<p>Rpn calculation:<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">img,rpn,img_aug = next(data_gen_train)</span><br></pre></td></tr></table></figure></p>\n<p> <img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/7_2.JPG\" alt=\"image\"></p>\n<p>def <br><br><br></p>\n<p>$L$  cls <br>$L({p_{i}},{t_{i}}) = \\frac{1}{N_{cls}}\\sum_{i}L_{cls}(p_{i},p_{i}^{\\ast})$</p>\n<p>$p_{i}$$p_{i}^{\\ast}$(label)anchor$p_{i}^{\\ast}$1$p_{i}^{\\ast}$0$L_{cls}$(log loss)</p>\n<p>  rpn loss cls:<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">rpn_loss_cls</span><span class=\"params\">(num_anchors)</span>:</span></span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">rpn_loss_cls_fixed_num</span><span class=\"params\">(y_true, y_pred)</span>:</span></span><br><span class=\"line\">            <span class=\"comment\"># binary_crossentropy -&gt; logloss</span></span><br><span class=\"line\">            <span class=\"comment\"># epsilon to increase robustness</span></span><br><span class=\"line\">\t\t<span class=\"keyword\">return</span> lambda_rpn_class * K.sum(y_true[:, :, :, :num_anchors] * K.binary_crossentropy(y_pred[:, :, :, :], y_true[:, :, :, num_anchors:])) / K.sum(epsilon + y_true[:, :, :, :num_anchors])</span><br><span class=\"line\">\t<span class=\"keyword\">return</span> rpn_loss_cls_fixed_num</span><br></pre></td></tr></table></figure></p>\n<p>$L$  reg <br>$L({p_{i}},{t_{i}}) =  \\lambda\\frac{1}{N_{reg}}\\sum_{i}L_{reg}(t_{i},t_{i}^{\\ast})$<br>Smooth L1:</p>\n<p>$$ Smooth_{L1}(x) =\\left{<br>\\begin{aligned}<br>0.5x^{2} \\ \\ |x| \\leqslant 1\\<br>|x| - 0.5 \\ \\ otherwise<br>\\end{aligned}<br>\\right.<br>$$<br>$L_{reg} = Smooth_{L1}(t-t^{\\ast})$</p>\n<p>  rpn loss reg:<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">rpn_loss_regr</span><span class=\"params\">(num_anchors)</span>:</span></span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">rpn_loss_regr_fixed_num</span><span class=\"params\">(y_true, y_pred)</span>:</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"comment\"># difference of ture value and predicted value</span></span><br><span class=\"line\">\t\tx = y_true[:, :, :, <span class=\"number\">4</span> * num_anchors:] - y_pred</span><br><span class=\"line\">\t\t<span class=\"comment\"># absulote value of difference</span></span><br><span class=\"line\">\t\tx_abs = K.abs(x)</span><br><span class=\"line\">\t\t<span class=\"comment\"># if absulote value less than 1, x_bool == 1, else x_bool = 0</span></span><br><span class=\"line\">\t\tx_bool = K.cast(K.less_equal(x_abs, <span class=\"number\">1.0</span>), tf.float32)</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">return</span> lambda_rpn_regr * K.sum(y_true[:, :, :, :<span class=\"number\">4</span> * num_anchors] * (x_bool * (<span class=\"number\">0.5</span> * x * x) + (<span class=\"number\">1</span> - x_bool) * (x_abs - <span class=\"number\">0.5</span>))) / K.sum(epsilon + y_true[:, :, :, :<span class=\"number\">4</span> * num_anchors])</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"keyword\">return</span> rpn_loss_regr_fixed_num</span><br></pre></td></tr></table></figure></p>\n<p>classlossclass_loss_clslossK.meanloss<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">class_loss_regr</span><span class=\"params\">(num_classes)</span>:</span></span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">class_loss_regr_fixed_num</span><span class=\"params\">(y_true, y_pred)</span>:</span></span><br><span class=\"line\">\t\tx = y_true[:, :, <span class=\"number\">4</span>*num_classes:] - y_pred</span><br><span class=\"line\">\t\tx_abs = K.abs(x)</span><br><span class=\"line\">\t\tx_bool = K.cast(K.less_equal(x_abs, <span class=\"number\">1.0</span>), <span class=\"string\">'float32'</span>)</span><br><span class=\"line\">\t\t<span class=\"keyword\">return</span> lambda_cls_regr * K.sum(y_true[:, :, :<span class=\"number\">4</span>*num_classes] * (x_bool * (<span class=\"number\">0.5</span> * x * x) + (<span class=\"number\">1</span> - x_bool) * (x_abs - <span class=\"number\">0.5</span>))) / K.sum(epsilon + y_true[:, :, :<span class=\"number\">4</span>*num_classes])</span><br><span class=\"line\">\t<span class=\"keyword\">return</span> class_loss_regr_fixed_num</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">class_loss_cls</span><span class=\"params\">(y_true, y_pred)</span>:</span></span><br><span class=\"line\">\t<span class=\"keyword\">return</span> lambda_cls_class * K.mean(categorical_crossentropy(y_true[<span class=\"number\">0</span>, :, :], y_pred[<span class=\"number\">0</span>, :, :]))</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"11-07-2018\"><a href=\"#11-07-2018\" class=\"headerlink\" title=\"11/07/2018\"></a>11/07/2018</h3><h4 id=\"Iridis\"><a href=\"#Iridis\" class=\"headerlink\" title=\"Iridis\"></a>Iridis</h4><h4 id=\"High-Performance-Computing-HPC\"><a href=\"#High-Performance-Computing-HPC\" class=\"headerlink\" title=\"High Performance Computing (HPC)\"></a>High Performance Computing (HPC)</h4><p><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/8_1.jpg\" alt=\"image\"><br><a href=\"https://www.southampton.ac.uk/isolutions/staff/high-performance-computing.page\" target=\"_blank\" rel=\"noopener\">Introduction</a></p>\n<p>Iridis 5 specifications</p>\n<ul>\n<li>#251 in hte world(Based on July 2018 TOPP500 list) with $R_{peak}\\sim1305.6\\ TFlops/s$</li>\n<li>464 2.0 GHz nodes with 40 cores per node, 192 GB memeory</li>\n<li>10 nodes with 4xGTX1080TI GPUs, 28 cores(hyper-threaded), 128 GB memeory</li>\n<li>10 nodes with 2xVolta Tesia GPUs, same as thandard compute</li>\n<li>2.2 PB disk with paraller file system (&gt;12GB\\s)</li>\n<li>5M Project delivered by OCF/IBM</li>\n</ul>\n<p><a href=\"https://mobaxterm.mobatek.net/\" target=\"_blank\" rel=\"noopener\">MobaXterm</a></p>\n<h4 id=\"create-my-own-conda-envieroment\"><a href=\"#create-my-own-conda-envieroment\" class=\"headerlink\" title=\"create my own conda envieroment\"></a>create my own conda envieroment</h4><p>Fllowing instroduction before</p>\n<h4 id=\"Slurm-command\"><a href=\"#Slurm-command\" class=\"headerlink\" title=\"Slurm command\"></a>Slurm command</h4><table>\n<thead>\n<tr>\n<th>Command</th>\n<th>Definition</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>sbatch</td>\n<td>Submits job scripts into system for execution (queued)</td>\n</tr>\n<tr>\n<td>scancel</td>\n<td>Cancels a job</td>\n</tr>\n<tr>\n<td>scontrol</td>\n<td>Used to display Slurm state, several options only available to root</td>\n</tr>\n<tr>\n<td>sinfo</td>\n<td>Display state of partitions and nodes</td>\n</tr>\n<tr>\n<td>squeue</td>\n<td>Display state of jobs</td>\n</tr>\n<tr>\n<td>salloc</td>\n<td>Submit a job for execution, or initiate job in real time</td>\n</tr>\n</tbody>\n</table>\n<p><strong> Bash script</strong><br><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#!/bin/bash</span></span><br><span class=\"line\"><span class=\"comment\">#SBATCH -J faster_rcnn </span></span><br><span class=\"line\"><span class=\"comment\">#SBATCH -o train_7.out</span></span><br><span class=\"line\"><span class=\"comment\">#SBATCH --ntasks=28</span></span><br><span class=\"line\"><span class=\"comment\">#SBATCH --nodes=1</span></span><br><span class=\"line\"><span class=\"comment\">#SBATCH --ntasks-per-node=8</span></span><br><span class=\"line\"><span class=\"comment\">#SBATCH --time=00:05:00</span></span><br><span class=\"line\"><span class=\"comment\">#SBATCH --gres=gpu:1</span></span><br><span class=\"line\"><span class=\"comment\">#SBATCH -p lyceum</span></span><br><span class=\"line\"></span><br><span class=\"line\">module load conda</span><br><span class=\"line\">module load cuda</span><br><span class=\"line\"><span class=\"built_in\">source</span> activate project</span><br><span class=\"line\">python test_frcnn.py</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"12-13-07-2018\"><a href=\"#12-13-07-2018\" class=\"headerlink\" title=\"12~13/07/2018\"></a>12~13/07/2018</h3><h4 id=\"change-plan\"><a href=\"#change-plan\" class=\"headerlink\" title=\"change plan\"></a>change plan</h4><p>faster r-cnncapsulefaster-rcnnfine turn</p>\n<p>1<br>ResNetIncRes V2ResNeXt  VGG </p>\n<p>2RPN<br>  RPN  Proposal </p>\n<p>3<br>   </p>\n<hr>\n<p>@1ION<br>Inside outside net: Detecting objects in context with skip pooling and recurrent neural networks<br></p>\n<p>1Inside Net<br> Inside  ROI  Scale  Feature Map<br> Skip-Pooling conv3-4-5-context <br> </p>\n<p>2Outside Net<br> Outside  ROI  Contextual<br> RNN  IRNN<br> </p>\n<hr>\n<p>@2 HyperNet<br>Hypernet: Towards accurate region proposal generation and joint object detection<br> Region Proposal <br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/8_2.JPG\" alt=\"image\"><br>   </p>\n<p>1Hyper Feature Extraction <br> Feature  Max Pooling  Deconv<br> Feature ConCat  LRN ION  L2 Norm</p>\n<p>2Region Proposal Generation<br> ConvNet RPN )<br> ROI Pooling Conv  FC  Position  ROI Pooling  13*13  bin Conv3*3*4 13*13*4  Cube FC  256d <br> Score+ BBox_Reg  Faster  Location OffSet<br> Overlap Greedy NMS  IOU 0.7 Image  1k  Region Top-200  Detetcion<br> Edge Box  Deep Box Deep Proposal </p>\n<p>3Object Detection<br> Fast RCNN<br>a FC  3<em>3</em>63<br>b DropOut  0.5  0.25<br> Proposal NMS  Box</p>\n<hr>\n<p>@3 MSCNN<br>A Unified Multi-scale Deep Convolutional Neural Network for Fast Object Detection<br>aScaleScaleFeature<br>ScaleScaleFeature</p>\n<p>b<br></p>\n<p>cScale-&gt;-&gt;Model<br> a b Trade-Off</p>\n<p>dScale-&gt;-&gt;-&gt;1Model</p>\n<p>eRCNNProposalCNN<br> aPatch</p>\n<p>fRPNCNN<br> b</p>\n<p>g<br> cover</p>\n<hr>\n<p>NMSsoft-nms<br>Repulsion loss overlapping  loss<br>faster rcnn</p>\n<h3 id=\"16-20-07-2018\"><a href=\"#16-20-07-2018\" class=\"headerlink\" title=\"16~20/07/2018\"></a>16~20/07/2018</h3><p></p>\n<h3 id=\"23-07-2018\"><a href=\"#23-07-2018\" class=\"headerlink\" title=\"23/07/2018\"></a>23/07/2018</h3><h4 id=\"fix-boxes-location-by-regrident\"><a href=\"#fix-boxes-location-by-regrident\" class=\"headerlink\" title=\"fix boxes location by regrident\"></a>fix boxes location by regrident</h4><p>regranchor</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"string\">\"\"\" fix boxes with grident</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">@param X: current cordinates of box</span></span><br><span class=\"line\"><span class=\"string\">@param T: coresspoding grident</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">@return: Fixed cordinates of box</span></span><br><span class=\"line\"><span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">apply_regr_np</span><span class=\"params\">(X, T)</span>:</span></span><br><span class=\"line\">\t<span class=\"keyword\">try</span>:</span><br><span class=\"line\">\t\tx = X[<span class=\"number\">0</span>, :, :]</span><br><span class=\"line\">\t\ty = X[<span class=\"number\">1</span>, :, :]</span><br><span class=\"line\">\t\tw = X[<span class=\"number\">2</span>, :, :]</span><br><span class=\"line\">\t\th = X[<span class=\"number\">3</span>, :, :]</span><br><span class=\"line\"></span><br><span class=\"line\">\t\ttx = T[<span class=\"number\">0</span>, :, :]</span><br><span class=\"line\">\t\tty = T[<span class=\"number\">1</span>, :, :]</span><br><span class=\"line\">\t\ttw = T[<span class=\"number\">2</span>, :, :]</span><br><span class=\"line\">\t\tth = T[<span class=\"number\">3</span>, :, :]</span><br></pre></td></tr></table></figure>\n<p>$t_{x}=\\frac{(x-x_{a})}{w_{a}}$ , $t_{y}=\\frac{(y-y_{a})}{h_{a}}$ , $t_{w}=log(\\frac{w}{w_{a}})$ , $t_{h}=log(\\frac{h}{h_{a}})$ (1)<br>$t_{x}^{\\ast}=\\frac{(x^{\\ast}-x_{a})}{w_{a}}$ , $t_{y}^{\\ast}=\\frac{(y^{\\ast}-y_{a})}{h_{a}}$ , $t_{w}^{\\ast}=log(\\frac{w^{\\ast}}{w_{a}})$ , $t_{h}^{\\ast}=log(\\frac{h^{\\ast}}{h_{a}})$ (2)</p>\n<p>{t}${t^{\\ast}}$${t}$${t^{\\ast}}$${t}$(1)</p>\n<p><br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">\t<span class=\"comment\"># centre cordinate</span></span><br><span class=\"line\">\tcx = x + w/<span class=\"number\">2.</span></span><br><span class=\"line\">\tcy = y + h/<span class=\"number\">2.</span></span><br><span class=\"line\">\t<span class=\"comment\"># fixed centre cordinate</span></span><br><span class=\"line\">\tcx1 = tx * w + cx</span><br><span class=\"line\">\tcy1 = ty * h + cy</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"comment\"># fixed wdith and height</span></span><br><span class=\"line\">\tw1 = np.exp(tw.astype(np.float64)) * w</span><br><span class=\"line\">\th1 = np.exp(th.astype(np.float64)) * h</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"comment\"># fixed left top corner's cordinate</span></span><br><span class=\"line\">\tx1 = cx1 - w1/<span class=\"number\">2.</span></span><br><span class=\"line\">\ty1 = cy1 - h1/<span class=\"number\">2.</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"comment\"># apporximate</span></span><br><span class=\"line\">\tx1 = np.round(x1)</span><br><span class=\"line\">\ty1 = np.round(y1)</span><br><span class=\"line\">\tw1 = np.round(w1)</span><br><span class=\"line\">\th1 = np.round(h1)</span><br><span class=\"line\">\t<span class=\"keyword\">return</span> np.stack([x1, y1, w1, h1])</span><br><span class=\"line\"><span class=\"keyword\">except</span> Exception <span class=\"keyword\">as</span> e:</span><br><span class=\"line\">\tprint(e)</span><br><span class=\"line\">\t<span class=\"keyword\">return</span> X</span><br></pre></td></tr></table></figure></p>\n<h4 id=\"NMS-no-max-suppression\"><a href=\"#NMS-no-max-suppression\" class=\"headerlink\" title=\"NMS no max suppression\"></a>NMS no max suppression</h4><p></p>\n<p><br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"string\">\"\"\" NMS , delete overlapping box</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">@param boxes: (n,4) box and coresspoding cordinates</span></span><br><span class=\"line\"><span class=\"string\">@param probs: (n,) box adn coresspding possibility</span></span><br><span class=\"line\"><span class=\"string\">@param overlap_thresh: treshold of delet box overlapping</span></span><br><span class=\"line\"><span class=\"string\">@param max_boxes: maximum keeping number of boxes</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">@return: boxes: boxes cordinates(x1,y1,x2,y2)</span></span><br><span class=\"line\"><span class=\"string\">@return: probs: coresspoding possibility</span></span><br><span class=\"line\"><span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">non_max_suppression_fast</span><span class=\"params\">(boxes, probs, overlap_thresh=<span class=\"number\">0.9</span>, max_boxes=<span class=\"number\">300</span>)</span>:</span></span><br></pre></td></tr></table></figure></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">if</span> len(boxes) == <span class=\"number\">0</span>:</span><br><span class=\"line\">   <span class=\"keyword\">return</span> []</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># grab the coordinates of the bounding boxes</span></span><br><span class=\"line\">x1 = boxes[:, <span class=\"number\">0</span>]</span><br><span class=\"line\">y1 = boxes[:, <span class=\"number\">1</span>]</span><br><span class=\"line\">x2 = boxes[:, <span class=\"number\">2</span>]</span><br><span class=\"line\">y2 = boxes[:, <span class=\"number\">3</span>]</span><br><span class=\"line\"></span><br><span class=\"line\">np.testing.assert_array_less(x1, x2)</span><br><span class=\"line\">np.testing.assert_array_less(y1, y2)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># if the bounding boxes integers, convert them to floats --</span></span><br><span class=\"line\"><span class=\"comment\"># this is important since we'll be doing a bunch of divisions</span></span><br><span class=\"line\"><span class=\"keyword\">if</span> boxes.dtype.kind == <span class=\"string\">\"i\"</span>:</span><br><span class=\"line\">\tboxes = boxes.astype(<span class=\"string\">\"float\"</span>)</span><br></pre></td></tr></table></figure>\n<p><br><br><br><br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># initialize the list of picked indexes\t</span></span><br><span class=\"line\">pick = []</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># calculate the areas</span></span><br><span class=\"line\">area = (x2 - x1) * (y2 - y1)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># sort the bounding boxes </span></span><br><span class=\"line\">idxs = np.argsort(probs)</span><br></pre></td></tr></table></figure></p>\n<p>pick<br><br>probs<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">while</span> len(idxs) &gt; <span class=\"number\">0</span>:</span><br><span class=\"line\"><span class=\"comment\"># grab the last index in the indexes list and add the</span></span><br><span class=\"line\"><span class=\"comment\"># index value to the list of picked indexes</span></span><br><span class=\"line\">last = len(idxs) - <span class=\"number\">1</span></span><br><span class=\"line\">i = idxs[last]</span><br><span class=\"line\">pick.append(i)</span><br></pre></td></tr></table></figure></p>\n<p>overlap_thresh</p>\n<p>idxs<br>overlap_thresh<br>max_boxes<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># find the intersection</span></span><br><span class=\"line\"></span><br><span class=\"line\">xx1_int = np.maximum(x1[i], x1[idxs[:last]])</span><br><span class=\"line\">yy1_int = np.maximum(y1[i], y1[idxs[:last]])</span><br><span class=\"line\">xx2_int = np.minimum(x2[i], x2[idxs[:last]])</span><br><span class=\"line\">yy2_int = np.minimum(y2[i], y2[idxs[:last]])</span><br><span class=\"line\"></span><br><span class=\"line\">ww_int = np.maximum(<span class=\"number\">0</span>, xx2_int - xx1_int)</span><br><span class=\"line\">hh_int = np.maximum(<span class=\"number\">0</span>, yy2_int - yy1_int)</span><br><span class=\"line\"></span><br><span class=\"line\">area_int = ww_int * hh_int</span><br></pre></td></tr></table></figure></p>\n<p>idxspick<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># find the union</span></span><br><span class=\"line\">area_union = area[i] + area[idxs[:last]] - area_int</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># compute the ratio of overlap</span></span><br><span class=\"line\">overlap = area_int/(area_union + <span class=\"number\">1e-6</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># delete all indexes from the index list that have</span></span><br><span class=\"line\">idxs = np.delete(idxs, np.concatenate(([last],np.where(overlap &gt; overlap_thresh)[<span class=\"number\">0</span>])))</span><br></pre></td></tr></table></figure></p>\n<p><br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">if</span> len(pick) &gt;= max_boxes:</span><br><span class=\"line\">   <span class=\"keyword\">break</span></span><br></pre></td></tr></table></figure></p>\n<p>[np.concatest]<br>max_boxes<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">boxes = boxes[pick].astype(<span class=\"string\">\"int\"</span>)</span><br><span class=\"line\">probs = probs[pick]</span><br><span class=\"line\"><span class=\"keyword\">return</span> boxes, probs</span><br></pre></td></tr></table></figure></p>\n<p>pick</p>\n<h3 id=\"24-07-2018\"><a href=\"#24-07-2018\" class=\"headerlink\" title=\"24/07/2018\"></a>24/07/2018</h3><h4 id=\"rpn-to-porposal-fixed\"><a href=\"#rpn-to-porposal-fixed\" class=\"headerlink\" title=\"rpn to porposal fixed\"></a>rpn to porposal fixed</h4><p>rpn<br><br>anchor_sizeanchor_ratio</p>\n<p></p>\n<p><br>1regr_layer[0, :, :, 4 * curr_layer:4 * curr_layer + 4]<br>2curr_layer</p>\n<p>anchorx,y,w,h</p>\n<p>regranchor</p>\n<p><br>0<br>x1,y1,x2,y2</p>\n<p>all_boxesn,4all_probsn,</p>\n<p><br>np.where() <br>np.delete(all_boxes, idxs, 0)</p>\n<p>9<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"string\">\"\"\" rpn to porposal</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">@param rpn_layer: porposal's coresspoding possibiliy, whether item exciseted</span></span><br><span class=\"line\"><span class=\"string\">@param regr_layer: porposal's coresspoding regrident</span></span><br><span class=\"line\"><span class=\"string\">@param C: Configuration</span></span><br><span class=\"line\"><span class=\"string\">@param dim_ordering: Dimensional organization</span></span><br><span class=\"line\"><span class=\"string\">@param use_regr=True: wether use regurident to fix proposal</span></span><br><span class=\"line\"><span class=\"string\">@param max_boxes=300: max boxes after apply this function</span></span><br><span class=\"line\"><span class=\"string\">@param overlap_thresh=0.9: threshold of overlapping</span></span><br><span class=\"line\"><span class=\"string\">@param C: Configuration</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">@return: max_boxes proposal with format (x1,y1,x2,y2)</span></span><br><span class=\"line\"><span class=\"string\">\"\"\"</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">rpn_to_roi</span><span class=\"params\">(rpn_layer, regr_layer, C, dim_ordering, use_regr=True, max_boxes=<span class=\"number\">300</span>,overlap_thresh=<span class=\"number\">0.9</span>)</span>:</span></span><br><span class=\"line\">\t<span class=\"comment\"># std_scaling default 4</span></span><br><span class=\"line\">\tregr_layer = regr_layer / C.std_scaling</span><br><span class=\"line\"></span><br><span class=\"line\">\tanchor_sizes = C.anchor_box_scales</span><br><span class=\"line\">\tanchor_ratios = C.anchor_box_ratios</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"keyword\">assert</span> rpn_layer.shape[<span class=\"number\">0</span>] == <span class=\"number\">1</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"comment\"># obtain img's width and height's matrix</span></span><br><span class=\"line\">\t(rows, cols) = rpn_layer.shape[<span class=\"number\">1</span>:<span class=\"number\">3</span>]</span><br><span class=\"line\"></span><br><span class=\"line\">\tcurr_layer = <span class=\"number\">0</span></span><br><span class=\"line\"></span><br><span class=\"line\">\tA = np.zeros((<span class=\"number\">4</span>, rpn_layer.shape[<span class=\"number\">1</span>], rpn_layer.shape[<span class=\"number\">2</span>], rpn_layer.shape[<span class=\"number\">3</span>]))</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"comment\"># anchor size is [128, 256, 512]</span></span><br><span class=\"line\">\t<span class=\"keyword\">for</span> anchor_size <span class=\"keyword\">in</span> anchor_sizes:</span><br><span class=\"line\">\t\t<span class=\"comment\"># anchor ratio is [1,2,1]</span></span><br><span class=\"line\">\t\t<span class=\"keyword\">for</span> anchor_ratio <span class=\"keyword\">in</span> anchor_ratios:</span><br><span class=\"line\">\t\t\t<span class=\"comment\"># rpn_stride = 16</span></span><br><span class=\"line\">\t\t\t<span class=\"comment\"># obatin anchor's weidth and height on feature map</span></span><br><span class=\"line\">\t\t\tanchor_x = (anchor_size * anchor_ratio[<span class=\"number\">0</span>])/C.rpn_stride</span><br><span class=\"line\">\t\t\tanchor_y = (anchor_size * anchor_ratio[<span class=\"number\">1</span>])/C.rpn_stride</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t<span class=\"comment\"># obtain current regrident</span></span><br><span class=\"line\">\t\t\t<span class=\"comment\"># when one dimentional obtain a value, the new varirant will decrease one dimenttion</span></span><br><span class=\"line\">\t\t\tregr = regr_layer[<span class=\"number\">0</span>, :, :, <span class=\"number\">4</span> * curr_layer:<span class=\"number\">4</span> * curr_layer + <span class=\"number\">4</span>]</span><br><span class=\"line\">\t\t\t<span class=\"comment\"># put depth to first bacause tensorflow as backend</span></span><br><span class=\"line\">\t\t\tregr = np.transpose(regr, (<span class=\"number\">2</span>, <span class=\"number\">0</span>, <span class=\"number\">1</span>))</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t<span class=\"comment\"># The rows of the output array X are copies of the vector x; columns of the output array Y are copies of the vector y</span></span><br><span class=\"line\">\t\t\t<span class=\"comment\"># each cordinartes of matrix cls and rows</span></span><br><span class=\"line\">\t\t\tX, Y = np.meshgrid(np.arange(cols),np. arange(rows))</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t<span class=\"comment\"># obtain anchors's (x,y,w,h)</span></span><br><span class=\"line\">\t\t\tA[<span class=\"number\">0</span>, :, :, curr_layer] = X - anchor_x/<span class=\"number\">2</span></span><br><span class=\"line\">\t\t\tA[<span class=\"number\">1</span>, :, :, curr_layer] = Y - anchor_y/<span class=\"number\">2</span></span><br><span class=\"line\">\t\t\tA[<span class=\"number\">2</span>, :, :, curr_layer] = anchor_x</span><br><span class=\"line\">\t\t\tA[<span class=\"number\">3</span>, :, :, curr_layer] = anchor_y</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t<span class=\"comment\"># fix boxes with grident</span></span><br><span class=\"line\">\t\t\t<span class=\"keyword\">if</span> use_regr:</span><br><span class=\"line\">\t\t\t\t<span class=\"comment\"># fixed corinates of box</span></span><br><span class=\"line\">\t\t\t\tA[:, :, :, curr_layer] = apply_regr_np(A[:, :, :, curr_layer], regr)</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t<span class=\"comment\"># fix unreasonable cordinates</span></span><br><span class=\"line\">\t\t\t<span class=\"comment\"># np.maximum(1,[]) will set the value less than 1 in [] to 1</span></span><br><span class=\"line\">\t\t\t<span class=\"comment\"># box's width and height can't less than 0</span></span><br><span class=\"line\">\t\t\tA[<span class=\"number\">2</span>, :, :, curr_layer] = np.maximum(<span class=\"number\">1</span>, A[<span class=\"number\">2</span>, :, :, curr_layer])</span><br><span class=\"line\">\t\t\tA[<span class=\"number\">3</span>, :, :, curr_layer] = np.maximum(<span class=\"number\">1</span>, A[<span class=\"number\">3</span>, :, :, curr_layer])</span><br><span class=\"line\">\t\t\t<span class=\"comment\"># fixed right bottom cordinates</span></span><br><span class=\"line\">\t\t\tA[<span class=\"number\">2</span>, :, :, curr_layer] += A[<span class=\"number\">0</span>, :, :, curr_layer]</span><br><span class=\"line\">\t\t\tA[<span class=\"number\">3</span>, :, :, curr_layer] += A[<span class=\"number\">1</span>, :, :, curr_layer]</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t<span class=\"comment\"># left top corner cordinates can't out image</span></span><br><span class=\"line\">\t\t\tA[<span class=\"number\">0</span>, :, :, curr_layer] = np.maximum(<span class=\"number\">0</span>, A[<span class=\"number\">0</span>, :, :, curr_layer])</span><br><span class=\"line\">\t\t\tA[<span class=\"number\">1</span>, :, :, curr_layer] = np.maximum(<span class=\"number\">0</span>, A[<span class=\"number\">1</span>, :, :, curr_layer])</span><br><span class=\"line\">\t\t\t<span class=\"comment\"># right bottom corner cordinates can't out img</span></span><br><span class=\"line\">\t\t\tA[<span class=\"number\">2</span>, :, :, curr_layer] = np.minimum(cols<span class=\"number\">-1</span>, A[<span class=\"number\">2</span>, :, :, curr_layer])</span><br><span class=\"line\">\t\t\tA[<span class=\"number\">3</span>, :, :, curr_layer] = np.minimum(rows<span class=\"number\">-1</span>, A[<span class=\"number\">3</span>, :, :, curr_layer])</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t<span class=\"comment\"># next layer</span></span><br><span class=\"line\">\t\t\tcurr_layer += <span class=\"number\">1</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"comment\"># obtain (n,4) object and coresspoding cordinate</span></span><br><span class=\"line\">\tall_boxes = np.reshape(A.transpose((<span class=\"number\">0</span>, <span class=\"number\">3</span>, <span class=\"number\">1</span>,<span class=\"number\">2</span>)), (<span class=\"number\">4</span>, <span class=\"number\">-1</span>)).transpose((<span class=\"number\">1</span>, <span class=\"number\">0</span>))</span><br><span class=\"line\">\t<span class=\"comment\"># obtain(n,) object and creoespdoing possibility</span></span><br><span class=\"line\">\tall_probs = rpn_layer.transpose((<span class=\"number\">0</span>, <span class=\"number\">3</span>, <span class=\"number\">1</span>, <span class=\"number\">2</span>)).reshape((<span class=\"number\">-1</span>))</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"comment\"># cordinates of left top and right bottom of box</span></span><br><span class=\"line\">\tx1 = all_boxes[:, <span class=\"number\">0</span>]</span><br><span class=\"line\">\ty1 = all_boxes[:, <span class=\"number\">1</span>]</span><br><span class=\"line\">\tx2 = all_boxes[:, <span class=\"number\">2</span>]</span><br><span class=\"line\">\ty2 = all_boxes[:, <span class=\"number\">3</span>]</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"comment\"># find where right cordinate bigger than left cordinate</span></span><br><span class=\"line\">\tidxs = np.where((x1 - x2 &gt;= <span class=\"number\">0</span>) | (y1 - y2 &gt;= <span class=\"number\">0</span>))</span><br><span class=\"line\">\t<span class=\"comment\"># delete thoese point at 0 dimentional -&gt; all boxes</span></span><br><span class=\"line\">\tall_boxes = np.delete(all_boxes, idxs, <span class=\"number\">0</span>)</span><br><span class=\"line\">\tall_probs = np.delete(all_probs, idxs, <span class=\"number\">0</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"comment\"># apply NMS to reduce overlapping boxes</span></span><br><span class=\"line\">\tresult = non_max_suppression_fast(all_boxes, all_probs, overlap_thresh=overlap_thresh, max_boxes=max_boxes)[<span class=\"number\">0</span>]</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"keyword\">return</span> result</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"25-07-2018\"><a href=\"#25-07-2018\" class=\"headerlink\" title=\"25/07/2018\"></a>25/07/2018</h3><h4 id=\"generate-classifiers-trainning-data\"><a href=\"#generate-classifiers-trainning-data\" class=\"headerlink\" title=\"generate classifiers trainning data\"></a>generate classifiers trainning data</h4><p>classifier,</p>\n<p><br>bboxes</p>\n<p>R, bboxes</p>\n<p>:<br><br><br></p>\n<p><br>1one-hot<br>y_class_num<br>coordslabelsloss<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">class_num = <span class=\"number\">2</span></span><br><span class=\"line\">class_label = <span class=\"number\">10</span> * [<span class=\"number\">0</span>]</span><br><span class=\"line\">print(class_label)</span><br><span class=\"line\">class_label[class_num] = <span class=\"number\">1</span></span><br><span class=\"line\">print(class_label)</span><br><span class=\"line\"></span><br><span class=\"line\">[<span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>]</span><br><span class=\"line\">[<span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">1</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>]</span><br></pre></td></tr></table></figure></p>\n<p></p>\n<p></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br><span class=\"line\">107</span><br><span class=\"line\">108</span><br><span class=\"line\">109</span><br><span class=\"line\">110</span><br><span class=\"line\">111</span><br><span class=\"line\">112</span><br><span class=\"line\">113</span><br><span class=\"line\">114</span><br><span class=\"line\">115</span><br><span class=\"line\">116</span><br><span class=\"line\">117</span><br><span class=\"line\">118</span><br><span class=\"line\">119</span><br><span class=\"line\">120</span><br><span class=\"line\">121</span><br><span class=\"line\">122</span><br><span class=\"line\">123</span><br><span class=\"line\">124</span><br><span class=\"line\">125</span><br><span class=\"line\">126</span><br><span class=\"line\">127</span><br><span class=\"line\">128</span><br><span class=\"line\">129</span><br><span class=\"line\">130</span><br><span class=\"line\">131</span><br><span class=\"line\">132</span><br><span class=\"line\">133</span><br><span class=\"line\">134</span><br><span class=\"line\">135</span><br><span class=\"line\">136</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"string\">\"\"\" generate classifier training data</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">@param R: porposal -&gt; boxes</span></span><br><span class=\"line\"><span class=\"string\">@param img_data: image data</span></span><br><span class=\"line\"><span class=\"string\">@param C: configuration</span></span><br><span class=\"line\"><span class=\"string\">@param class_mapping: classes and coresspoding numbers</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">@return: np.expand_dims(X, axis=0): boxes after filter</span></span><br><span class=\"line\"><span class=\"string\">@return: np.expand_dims(Y1, axis=0): boxes coresspoding class</span></span><br><span class=\"line\"><span class=\"string\">@return: np.expand_dims(Y2, axis=0): boxes coresspoding regurident</span></span><br><span class=\"line\"><span class=\"string\">@return IoUs: IOU</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">calc_iou</span><span class=\"params\">(R, img_data, C, class_mapping)</span>:</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"comment\"># obtain boxxes information from img data</span></span><br><span class=\"line\">\tbboxes = img_data[<span class=\"string\">'bboxes'</span>]</span><br><span class=\"line\">\t<span class=\"comment\"># obtain width and height of img</span></span><br><span class=\"line\">\t(width, height) = (img_data[<span class=\"string\">'width'</span>], img_data[<span class=\"string\">'height'</span>])</span><br><span class=\"line\">\t<span class=\"comment\"># get image dimensions for resizing</span></span><br><span class=\"line\">\t<span class=\"comment\"># Fix image's shortest edge to config setting: eg: 600</span></span><br><span class=\"line\">\t(resized_width, resized_height) = get_new_img_size(width, height, C.im_size)</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"comment\"># record parameters, bboxes cordinates on feature map</span></span><br><span class=\"line\">\tgta = np.zeros((len(bboxes), <span class=\"number\">4</span>))</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"comment\"># change bboxes's width and height because the img was rezised</span></span><br><span class=\"line\">\t<span class=\"keyword\">for</span> bbox_num, bbox <span class=\"keyword\">in</span> enumerate(bboxes):</span><br><span class=\"line\">\t\t<span class=\"comment\"># get the GT box coordinates, and resize to account for image resizing</span></span><br><span class=\"line\">\t\t<span class=\"comment\"># /C.rpn_stride mapping to feature map</span></span><br><span class=\"line\">\t\tgta[bbox_num, <span class=\"number\">0</span>] = int(round(bbox[<span class=\"string\">'x1'</span>] * (resized_width / float(width))/C.rpn_stride))</span><br><span class=\"line\">\t\tgta[bbox_num, <span class=\"number\">1</span>] = int(round(bbox[<span class=\"string\">'x2'</span>] * (resized_width / float(width))/C.rpn_stride))</span><br><span class=\"line\">\t\tgta[bbox_num, <span class=\"number\">2</span>] = int(round(bbox[<span class=\"string\">'y1'</span>] * (resized_height / float(height))/C.rpn_stride))</span><br><span class=\"line\">\t\tgta[bbox_num, <span class=\"number\">3</span>] = int(round(bbox[<span class=\"string\">'y2'</span>] * (resized_height / float(height))/C.rpn_stride))</span><br><span class=\"line\"></span><br><span class=\"line\">\tx_roi = []</span><br><span class=\"line\">\ty_class_num = []</span><br><span class=\"line\">\ty_class_regr_coords = []</span><br><span class=\"line\">\ty_class_regr_label = []</span><br><span class=\"line\">\tIoUs = [] <span class=\"comment\"># for debugging only</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"comment\"># for all given proposals -&gt; boxes</span></span><br><span class=\"line\">\t<span class=\"keyword\">for</span> ix <span class=\"keyword\">in</span> range(R.shape[<span class=\"number\">0</span>]):</span><br><span class=\"line\">\t\t<span class=\"comment\"># current boxes's cordinates</span></span><br><span class=\"line\">\t\t(x1, y1, x2, y2) = R[ix, :]</span><br><span class=\"line\">\t\tx1 = int(round(x1))</span><br><span class=\"line\">\t\ty1 = int(round(y1))</span><br><span class=\"line\">\t\tx2 = int(round(x2))</span><br><span class=\"line\">\t\ty2 = int(round(y2))</span><br><span class=\"line\"></span><br><span class=\"line\">\t\tbest_iou = <span class=\"number\">0.0</span></span><br><span class=\"line\">\t\tbest_bbox = <span class=\"number\">-1</span></span><br><span class=\"line\">\t\t<span class=\"comment\"># using current proposal to compare with given xml's boxes</span></span><br><span class=\"line\">\t\t<span class=\"keyword\">for</span> bbox_num <span class=\"keyword\">in</span> range(len(bboxes)):</span><br><span class=\"line\">\t\t\t<span class=\"comment\"># calculate current iou</span></span><br><span class=\"line\">\t\t\tcurr_iou = iou([gta[bbox_num, <span class=\"number\">0</span>], gta[bbox_num, <span class=\"number\">2</span>], gta[bbox_num, <span class=\"number\">1</span>], gta[bbox_num, <span class=\"number\">3</span>]], [x1, y1, x2, y2])</span><br><span class=\"line\">\t\t\t<span class=\"comment\"># update parameters</span></span><br><span class=\"line\">\t\t\t<span class=\"keyword\">if</span> curr_iou &gt; best_iou:</span><br><span class=\"line\">\t\t\t\tbest_iou = curr_iou</span><br><span class=\"line\">\t\t\t\tbest_bbox = bbox_num</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"comment\"># if iou to small, we don't put it in trainning because it should be backgroud</span></span><br><span class=\"line\">\t\t<span class=\"keyword\">if</span> best_iou &lt; C.classifier_min_overlap:</span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">continue</span></span><br><span class=\"line\">\t\t<span class=\"keyword\">else</span>:</span><br><span class=\"line\">\t\t\t<span class=\"comment\"># saveing left top cordinates, width and height</span></span><br><span class=\"line\">\t\t\tw = x2 - x1</span><br><span class=\"line\">\t\t\th = y2 - y1</span><br><span class=\"line\">\t\t\tx_roi.append([x1, y1, w, h])</span><br><span class=\"line\">\t\t\t<span class=\"comment\"># saving this bbox's iou</span></span><br><span class=\"line\">\t\t\tIoUs.append(best_iou)</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t<span class=\"comment\"># hard to classfier -&gt; set it to backgroud</span></span><br><span class=\"line\">\t\t\t<span class=\"keyword\">if</span> C.classifier_min_overlap &lt;= best_iou &lt; C.classifier_max_overlap:</span><br><span class=\"line\">\t\t\t\t<span class=\"comment\"># hard negative example</span></span><br><span class=\"line\">\t\t\t\tcls_name = <span class=\"string\">'bg'</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t<span class=\"comment\"># valid proposal</span></span><br><span class=\"line\">\t\t\t<span class=\"keyword\">elif</span> C.classifier_max_overlap &lt;= best_iou:</span><br><span class=\"line\">\t\t\t\t<span class=\"comment\"># coresspoding class name</span></span><br><span class=\"line\">\t\t\t\tcls_name = bboxes[best_bbox][<span class=\"string\">'class'</span>]</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t<span class=\"comment\"># calculate rpn graident with true cordinates given by xml file</span></span><br><span class=\"line\">\t\t\t\tcxg = (gta[best_bbox, <span class=\"number\">0</span>] + gta[best_bbox, <span class=\"number\">1</span>]) / <span class=\"number\">2.0</span></span><br><span class=\"line\">\t\t\t\tcyg = (gta[best_bbox, <span class=\"number\">2</span>] + gta[best_bbox, <span class=\"number\">3</span>]) / <span class=\"number\">2.0</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\tcx = x1 + w / <span class=\"number\">2.0</span></span><br><span class=\"line\">\t\t\t\tcy = y1 + h / <span class=\"number\">2.0</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\ttx = (cxg - cx) / float(w)</span><br><span class=\"line\">\t\t\t\tty = (cyg - cy) / float(h)</span><br><span class=\"line\">\t\t\t\ttw = np.log((gta[best_bbox, <span class=\"number\">1</span>] - gta[best_bbox, <span class=\"number\">0</span>]) / float(w))</span><br><span class=\"line\">\t\t\t\tth = np.log((gta[best_bbox, <span class=\"number\">3</span>] - gta[best_bbox, <span class=\"number\">2</span>]) / float(h))</span><br><span class=\"line\">\t\t\t<span class=\"keyword\">else</span>:</span><br><span class=\"line\">\t\t\t\tprint(<span class=\"string\">'roi = &#123;&#125;'</span>.format(best_iou))</span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">raise</span> RuntimeError</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"comment\"># class name's mapping number</span></span><br><span class=\"line\">\t\tclass_num = class_mapping[cls_name]</span><br><span class=\"line\">\t\t<span class=\"comment\"># list of calss label</span></span><br><span class=\"line\">\t\tclass_label = len(class_mapping) * [<span class=\"number\">0</span>]</span><br><span class=\"line\">\t\t<span class=\"comment\"># set class_num's coresspoding location to 1</span></span><br><span class=\"line\">\t\tclass_label[class_num] = <span class=\"number\">1</span></span><br><span class=\"line\">\t\t<span class=\"comment\"># privous is one-hot vector</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"comment\"># saving the one-hot vector</span></span><br><span class=\"line\">\t\ty_class_num.append(copy.deepcopy(class_label))</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"comment\"># coords used to saving calculated graident</span></span><br><span class=\"line\">\t\tcoords = [<span class=\"number\">0</span>] * <span class=\"number\">4</span> * (len(class_mapping) - <span class=\"number\">1</span>)</span><br><span class=\"line\">\t\t<span class=\"comment\"># labels used to decide whether adding to loss calculation</span></span><br><span class=\"line\">\t\tlabels = [<span class=\"number\">0</span>] * <span class=\"number\">4</span> * (len(class_mapping) - <span class=\"number\">1</span>)</span><br><span class=\"line\">\t\t<span class=\"keyword\">if</span> cls_name != <span class=\"string\">'bg'</span>:</span><br><span class=\"line\">\t\t\tlabel_pos = <span class=\"number\">4</span> * class_num</span><br><span class=\"line\">\t\t\tsx, sy, sw, sh = C.classifier_regr_std</span><br><span class=\"line\">\t\t\tcoords[label_pos:<span class=\"number\">4</span>+label_pos] = [sx*tx, sy*ty, sw*tw, sh*th]</span><br><span class=\"line\">\t\t\tlabels[label_pos:<span class=\"number\">4</span>+label_pos] = [<span class=\"number\">1</span>, <span class=\"number\">1</span>, <span class=\"number\">1</span>, <span class=\"number\">1</span>]</span><br><span class=\"line\">\t\t\ty_class_regr_coords.append(copy.deepcopy(coords))</span><br><span class=\"line\">\t\t\ty_class_regr_label.append(copy.deepcopy(labels))</span><br><span class=\"line\">\t\t<span class=\"keyword\">else</span>:</span><br><span class=\"line\">\t\t\ty_class_regr_coords.append(copy.deepcopy(coords))</span><br><span class=\"line\">\t\t\ty_class_regr_label.append(copy.deepcopy(labels))</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"comment\"># no bboxes</span></span><br><span class=\"line\">\t<span class=\"keyword\">if</span> len(x_roi) == <span class=\"number\">0</span>:</span><br><span class=\"line\">\t\t<span class=\"keyword\">return</span> <span class=\"keyword\">None</span>, <span class=\"keyword\">None</span>, <span class=\"keyword\">None</span>, <span class=\"keyword\">None</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"comment\"># matrix with [x1, y1, w, h]</span></span><br><span class=\"line\">\tX = np.array(x_roi)</span><br><span class=\"line\">\t<span class=\"comment\"># boxxes coresspoding class number</span></span><br><span class=\"line\">\tY1 = np.array(y_class_num)</span><br><span class=\"line\">\t<span class=\"comment\"># matrix of whether adding to calculation and coresspoding regrident</span></span><br><span class=\"line\">\tY2 = np.concatenate([np.array(y_class_regr_label),np.array(y_class_regr_coords)],axis=<span class=\"number\">1</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"comment\"># adding batch size dimention</span></span><br><span class=\"line\">\t<span class=\"keyword\">return</span> np.expand_dims(X, axis=<span class=\"number\">0</span>), np.expand_dims(Y1, axis=<span class=\"number\">0</span>), np.expand_dims(Y2, axis=<span class=\"number\">0</span>), IoUs</span><br></pre></td></tr></table></figure>\n<h3 id=\"26-27-07-2018\"><a href=\"#26-27-07-2018\" class=\"headerlink\" title=\"26~27/07/2018\"></a>26~27/07/2018</h3><h4 id=\"model-parameters\"><a href=\"#model-parameters\" class=\"headerlink\" title=\"model parameters\"></a>model parameters</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># rpn optimizer</span></span><br><span class=\"line\">optimizer = Adam(lr=<span class=\"number\">1e-5</span>)</span><br><span class=\"line\"><span class=\"comment\"># classifier optimizer</span></span><br><span class=\"line\">optimizer_classifier = Adam(lr=<span class=\"number\">1e-5</span>)</span><br><span class=\"line\"><span class=\"comment\"># defined loss apply, metrics used to print accury</span></span><br><span class=\"line\">model_rpn.compile(optimizer=optimizer, loss=[losses.rpn_loss_cls(num_anchors), losses.rpn_loss_regr(num_anchors)])</span><br><span class=\"line\">model_classifier.compile(optimizer=optimizer_classifier, loss=[losses.class_loss_cls, losses.class_loss_regr(len(classes_count)<span class=\"number\">-1</span>)], metrics=&#123;<span class=\"string\">'dense_class_&#123;&#125;'</span>.format(len(classes_count)): <span class=\"string\">'accuracy'</span>&#125;)</span><br><span class=\"line\"><span class=\"comment\"># for saving weight</span></span><br><span class=\"line\">model_all.compile(optimizer=<span class=\"string\">'sgd'</span>, loss=<span class=\"string\">'mae'</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># traing time of each epochs</span></span><br><span class=\"line\">epoch_length = <span class=\"number\">1000</span></span><br><span class=\"line\"><span class=\"comment\"># totoal epochs</span></span><br><span class=\"line\">num_epochs = <span class=\"number\">2000</span></span><br><span class=\"line\"><span class=\"comment\">#</span></span><br><span class=\"line\">iter_num = <span class=\"number\">0</span></span><br><span class=\"line\"><span class=\"comment\"># losses saving matrix</span></span><br><span class=\"line\">losses = np.zeros((epoch_length, <span class=\"number\">5</span>))</span><br><span class=\"line\">rpn_accuracy_rpn_monitor = []</span><br><span class=\"line\">rpn_accuracy_for_epoch = []</span><br><span class=\"line\">start_time = time.time()</span><br><span class=\"line\"><span class=\"comment\"># current total loss</span></span><br><span class=\"line\">best_loss = np.Inf</span><br><span class=\"line\"><span class=\"comment\"># sorted classing mapping</span></span><br><span class=\"line\">class_mapping_inv = &#123;v: k <span class=\"keyword\">for</span> k, v <span class=\"keyword\">in</span> class_mapping.items()&#125;</span><br></pre></td></tr></table></figure>\n<h4 id=\"Training-process\"><a href=\"#Training-process\" class=\"headerlink\" title=\"Training process\"></a>Training process</h4><p><br><strong>rpn</strong><br>RPN,XY</p>\n<p><strong>rpnclassifier:</strong><br><br><br><br>Y1[0, :, -1]0batch-1<br>neg_samples = neg_samples[0]<br>C.num_roisclassifierC.num_rois11</p>\n<p><strong>classifier:</strong><br>Lossaccury<br>loss_class[3]<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">classiferloss</span><br><span class=\"line\">[<span class=\"number\">1.4640709</span>, <span class=\"number\">1.0986123</span>, <span class=\"number\">0.36545864</span>, <span class=\"number\">0.15625</span>]</span><br></pre></td></tr></table></figure></p>\n<p>losslistnumpy<br>epochepochlossepochepoch.</p>\n<hr>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br><span class=\"line\">107</span><br><span class=\"line\">108</span><br><span class=\"line\">109</span><br><span class=\"line\">110</span><br><span class=\"line\">111</span><br><span class=\"line\">112</span><br><span class=\"line\">113</span><br><span class=\"line\">114</span><br><span class=\"line\">115</span><br><span class=\"line\">116</span><br><span class=\"line\">117</span><br><span class=\"line\">118</span><br><span class=\"line\">119</span><br><span class=\"line\">120</span><br><span class=\"line\">121</span><br><span class=\"line\">122</span><br><span class=\"line\">123</span><br><span class=\"line\">124</span><br><span class=\"line\">125</span><br><span class=\"line\">126</span><br><span class=\"line\">127</span><br><span class=\"line\">128</span><br><span class=\"line\">129</span><br><span class=\"line\">130</span><br><span class=\"line\">131</span><br><span class=\"line\">132</span><br><span class=\"line\">133</span><br><span class=\"line\">134</span><br><span class=\"line\">135</span><br><span class=\"line\">136</span><br><span class=\"line\">137</span><br><span class=\"line\">138</span><br><span class=\"line\">139</span><br><span class=\"line\">140</span><br><span class=\"line\">141</span><br><span class=\"line\">142</span><br><span class=\"line\">143</span><br><span class=\"line\">144</span><br><span class=\"line\">145</span><br><span class=\"line\">146</span><br><span class=\"line\">147</span><br><span class=\"line\">148</span><br><span class=\"line\">149</span><br><span class=\"line\">150</span><br><span class=\"line\">151</span><br><span class=\"line\">152</span><br><span class=\"line\">153</span><br><span class=\"line\">154</span><br><span class=\"line\">155</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># Training Process</span></span><br><span class=\"line\">print(<span class=\"string\">'Starting training'</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">for</span> epoch_num <span class=\"keyword\">in</span> range(num_epochs):</span><br><span class=\"line\">    <span class=\"comment\">#progbar is used to print % of processing</span></span><br><span class=\"line\">\tprogbar = generic_utils.Progbar(epoch_length)</span><br><span class=\"line\">    <span class=\"comment\"># print current process</span></span><br><span class=\"line\">\tprint(<span class=\"string\">'Epoch &#123;&#125;/&#123;&#125;'</span>.format(epoch_num + <span class=\"number\">1</span>, num_epochs))</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"keyword\">while</span> <span class=\"keyword\">True</span>:</span><br><span class=\"line\">\t\t<span class=\"keyword\">try</span>:</span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"comment\"># verbose True to print RPN situation, if can't generate boxes on positivate object, it will print error</span></span><br><span class=\"line\">\t\t\t<span class=\"keyword\">if</span> len(rpn_accuracy_rpn_monitor) == epoch_length <span class=\"keyword\">and</span> C.verbose:</span><br><span class=\"line\">                <span class=\"comment\"># postivate boxes / all boxes</span></span><br><span class=\"line\">\t\t\t\tmean_overlapping_bboxes = float(sum(rpn_accuracy_rpn_monitor))/len(rpn_accuracy_rpn_monitor)</span><br><span class=\"line\">\t\t\t\trpn_accuracy_rpn_monitor = []</span><br><span class=\"line\">\t\t\t\tprint(<span class=\"string\">'Average number of overlapping bounding boxes from RPN = &#123;&#125; for &#123;&#125; previous iterations'</span>.format(mean_overlapping_bboxes, epoch_length))</span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">if</span> mean_overlapping_bboxes == <span class=\"number\">0</span>:</span><br><span class=\"line\">\t\t\t\t\tprint(<span class=\"string\">'RPN is not producing bounding boxes that overlap the ground truth boxes. Check RPN settings or keep training.'</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"comment\"># obtain img, rpn information and img xml format</span></span><br><span class=\"line\">\t\t\tX, Y, img_data = next(data_gen_train)</span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"comment\"># train RPN net, X is img, Y is correspoding class type and graident</span></span><br><span class=\"line\">\t\t\tloss_rpn = model_rpn.train_on_batch(X, Y)</span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"comment\"># predict new Y from privious rpn model</span></span><br><span class=\"line\">\t\t\tP_rpn = model_rpn.predict_on_batch(X)</span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"comment\"># transform predicted rpn to cordinates of boxes</span></span><br><span class=\"line\">\t\t\tR = rpn_to_boxes.rpn_to_roi(P_rpn[<span class=\"number\">0</span>], P_rpn[<span class=\"number\">1</span>], C, K.image_dim_ordering(), use_regr=<span class=\"keyword\">True</span>, overlap_thresh=<span class=\"number\">0.7</span>, max_boxes=<span class=\"number\">300</span>)</span><br><span class=\"line\">\t\t\t<span class=\"comment\"># note: calc_iou converts from (x1,y1,x2,y2) to (x,y,w,h) format</span></span><br><span class=\"line\">            <span class=\"comment\"># X2: [x,y,w,h]</span></span><br><span class=\"line\">            <span class=\"comment\"># Y1: coresspoding class number -&gt; one hot vector</span></span><br><span class=\"line\">            <span class=\"comment\"># Y2: boxes coresspoding regrident</span></span><br><span class=\"line\">\t\t\tX2, Y1, Y2, IouS = rpn_to_classifier.calc_iou(R, img_data, C, class_mapping)</span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"comment\"># no box, stop this epoch</span></span><br><span class=\"line\">\t\t\t<span class=\"keyword\">if</span> X2 <span class=\"keyword\">is</span> <span class=\"keyword\">None</span>:</span><br><span class=\"line\">\t\t\t\trpn_accuracy_rpn_monitor.append(<span class=\"number\">0</span>)</span><br><span class=\"line\">\t\t\t\trpn_accuracy_for_epoch.append(<span class=\"number\">0</span>)</span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">continue</span></span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"comment\"># if last position of one-hot is 1 -&gt; is background</span></span><br><span class=\"line\">\t\t\tneg_samples = np.where(Y1[<span class=\"number\">0</span>, :, <span class=\"number\">-1</span>] == <span class=\"number\">1</span>)</span><br><span class=\"line\">            <span class=\"comment\"># else is postivate sample</span></span><br><span class=\"line\">\t\t\tpos_samples = np.where(Y1[<span class=\"number\">0</span>, :, <span class=\"number\">-1</span>] == <span class=\"number\">0</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"comment\"># obtain backgourd samples's coresspoding rows</span></span><br><span class=\"line\">\t\t\t<span class=\"keyword\">if</span> len(neg_samples) &gt; <span class=\"number\">0</span>:</span><br><span class=\"line\">\t\t\t\tneg_samples = neg_samples[<span class=\"number\">0</span>]</span><br><span class=\"line\">\t\t\t<span class=\"keyword\">else</span>:</span><br><span class=\"line\">\t\t\t\tneg_samples = []</span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"comment\"># obtain posivate samples's coresspoding rows</span></span><br><span class=\"line\">\t\t\t<span class=\"keyword\">if</span> len(pos_samples) &gt; <span class=\"number\">0</span>:</span><br><span class=\"line\">\t\t\t\tpos_samples = pos_samples[<span class=\"number\">0</span>]</span><br><span class=\"line\">\t\t\t<span class=\"keyword\">else</span>:</span><br><span class=\"line\">\t\t\t\tpos_samples = []</span><br><span class=\"line\">\t\t\t<span class=\"comment\"># saving posivate samples's number</span></span><br><span class=\"line\">\t\t\trpn_accuracy_rpn_monitor.append(len(pos_samples))</span><br><span class=\"line\">\t\t\trpn_accuracy_for_epoch.append((len(pos_samples)))</span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"comment\"># default 4 here</span></span><br><span class=\"line\">\t\t\t<span class=\"keyword\">if</span> C.num_rois &gt; <span class=\"number\">1</span>:</span><br><span class=\"line\">                <span class=\"comment\"># wehn postivate samples less than 2</span></span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">if</span> len(pos_samples) &lt; C.num_rois//<span class=\"number\">2</span>:</span><br><span class=\"line\">                    <span class=\"comment\"># chosse all samples</span></span><br><span class=\"line\">\t\t\t\t\tselected_pos_samples = pos_samples.tolist()</span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">else</span>:</span><br><span class=\"line\">                    <span class=\"comment\"># random choose 2 samples</span></span><br><span class=\"line\">\t\t\t\t\tselected_pos_samples = np.random.choice(pos_samples, C.num_rois//<span class=\"number\">2</span>, replace=<span class=\"keyword\">False</span>).tolist()</span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">try</span>:</span><br><span class=\"line\">                    <span class=\"comment\"># random choose num_rois - positave samples naegivate samples</span></span><br><span class=\"line\">\t\t\t\t\tselected_neg_samples = np.random.choice(neg_samples, C.num_rois - len(selected_pos_samples), replace=<span class=\"keyword\">False</span>).tolist()</span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">except</span>:</span><br><span class=\"line\">                    <span class=\"comment\"># if no enought neg samples, copy priouvs neg sample</span></span><br><span class=\"line\">\t\t\t\t\tselected_neg_samples = np.random.choice(neg_samples, C.num_rois - len(selected_pos_samples), replace=<span class=\"keyword\">True</span>).tolist()</span><br><span class=\"line\"></span><br><span class=\"line\">                <span class=\"comment\"># samples picked to classifier network</span></span><br><span class=\"line\">\t\t\t\tsel_samples = selected_pos_samples + selected_neg_samples</span><br><span class=\"line\">\t\t\t<span class=\"keyword\">else</span>:</span><br><span class=\"line\">\t\t\t\t<span class=\"comment\"># in the extreme case where num_rois = 1, we pick a random pos or neg sample</span></span><br><span class=\"line\">\t\t\t\tselected_pos_samples = pos_samples.tolist()</span><br><span class=\"line\">\t\t\t\tselected_neg_samples = neg_samples.tolist()</span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">if</span> np.random.randint(<span class=\"number\">0</span>, <span class=\"number\">2</span>):</span><br><span class=\"line\">\t\t\t\t\tsel_samples = random.choice(neg_samples)</span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">else</span>:</span><br><span class=\"line\">\t\t\t\t\tsel_samples = random.choice(pos_samples)</span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"comment\"># train classifier, img data, selceted samples' cordinates, mapping number of selected samples, coresspoding regreident</span></span><br><span class=\"line\">\t\t\tloss_class = model_classifier.train_on_batch([X, X2[:, sel_samples, :]], [Y1[:, sel_samples, :], Y2[:, sel_samples, :]])</span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"comment\"># in Keras, if loss part bigger than 1, it will return sum of part losses, each loss and accury</span></span><br><span class=\"line\">            <span class=\"comment\"># put each losses and accury into losses</span></span><br><span class=\"line\">\t\t\tlosses[iter_num, <span class=\"number\">0</span>] = loss_rpn[<span class=\"number\">1</span>]</span><br><span class=\"line\">\t\t\tlosses[iter_num, <span class=\"number\">1</span>] = loss_rpn[<span class=\"number\">2</span>]</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\tlosses[iter_num, <span class=\"number\">2</span>] = loss_class[<span class=\"number\">1</span>]</span><br><span class=\"line\">\t\t\tlosses[iter_num, <span class=\"number\">3</span>] = loss_class[<span class=\"number\">2</span>]</span><br><span class=\"line\">\t\t\tlosses[iter_num, <span class=\"number\">4</span>] = loss_class[<span class=\"number\">3</span>]</span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"comment\"># next iter</span></span><br><span class=\"line\">\t\t\titer_num += <span class=\"number\">1</span></span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"comment\"># display and update current mean value of losses</span></span><br><span class=\"line\">\t\t\tprogbar.update(iter_num, [(<span class=\"string\">'rpn_cls'</span>, np.mean(losses[:iter_num, <span class=\"number\">0</span>])), (<span class=\"string\">'rpn_regr'</span>, np.mean(losses[:iter_num, <span class=\"number\">1</span>])),</span><br><span class=\"line\">\t\t\t\t\t\t\t\t\t  (<span class=\"string\">'detector_cls'</span>, np.mean(losses[:iter_num, <span class=\"number\">2</span>])), (<span class=\"string\">'detector_regr'</span>, np.mean(losses[:iter_num, <span class=\"number\">3</span>]))])</span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"comment\"># reach epoch_length</span></span><br><span class=\"line\">\t\t\t<span class=\"keyword\">if</span> iter_num == epoch_length:</span><br><span class=\"line\">\t\t\t\tloss_rpn_cls = np.mean(losses[:, <span class=\"number\">0</span>])</span><br><span class=\"line\">\t\t\t\tloss_rpn_regr = np.mean(losses[:, <span class=\"number\">1</span>])</span><br><span class=\"line\">\t\t\t\tloss_class_cls = np.mean(losses[:, <span class=\"number\">2</span>])</span><br><span class=\"line\">\t\t\t\tloss_class_regr = np.mean(losses[:, <span class=\"number\">3</span>])</span><br><span class=\"line\">\t\t\t\tclass_acc = np.mean(losses[:, <span class=\"number\">4</span>])</span><br><span class=\"line\"></span><br><span class=\"line\">                <span class=\"comment\"># negativate samples / all samples</span></span><br><span class=\"line\">\t\t\t\tmean_overlapping_bboxes = float(sum(rpn_accuracy_for_epoch)) / len(rpn_accuracy_for_epoch)</span><br><span class=\"line\">                <span class=\"comment\"># reset</span></span><br><span class=\"line\">\t\t\t\trpn_accuracy_for_epoch = []</span><br><span class=\"line\"></span><br><span class=\"line\">                <span class=\"comment\"># print trainning loss and accrury</span></span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">if</span> C.verbose:</span><br><span class=\"line\">\t\t\t\t\tprint(<span class=\"string\">'Mean number of bounding boxes from RPN overlapping ground truth boxes: &#123;&#125;'</span>.format(mean_overlapping_bboxes))</span><br><span class=\"line\">\t\t\t\t\tprint(<span class=\"string\">'Classifier accuracy for bounding boxes from RPN: &#123;&#125;'</span>.format(class_acc))</span><br><span class=\"line\">\t\t\t\t\tprint(<span class=\"string\">'Loss RPN classifier: &#123;&#125;'</span>.format(loss_rpn_cls))</span><br><span class=\"line\">\t\t\t\t\tprint(<span class=\"string\">'Loss RPN regression: &#123;&#125;'</span>.format(loss_rpn_regr))</span><br><span class=\"line\">\t\t\t\t\tprint(<span class=\"string\">'Loss Detector classifier: &#123;&#125;'</span>.format(loss_class_cls))</span><br><span class=\"line\">\t\t\t\t\tprint(<span class=\"string\">'Loss Detector regression: &#123;&#125;'</span>.format(loss_class_regr))</span><br><span class=\"line\">                    <span class=\"comment\"># trainng time of one epoch</span></span><br><span class=\"line\">\t\t\t\t\tprint(<span class=\"string\">'Elapsed time: &#123;&#125;'</span>.format(time.time() - start_time))</span><br><span class=\"line\">                    </span><br><span class=\"line\">                <span class=\"comment\"># total loss</span></span><br><span class=\"line\">\t\t\t\tcurr_loss = loss_rpn_cls + loss_rpn_regr + loss_class_cls + loss_class_regr</span><br><span class=\"line\">                <span class=\"comment\"># reset</span></span><br><span class=\"line\">\t\t\t\titer_num = <span class=\"number\">0</span></span><br><span class=\"line\">                <span class=\"comment\"># reset time</span></span><br><span class=\"line\">\t\t\t\tstart_time = time.time()</span><br><span class=\"line\"></span><br><span class=\"line\">                <span class=\"comment\"># if obtain smaller total loss, save weight of current model</span></span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">if</span> curr_loss &lt; best_loss:</span><br><span class=\"line\">\t\t\t\t\t<span class=\"keyword\">if</span> C.verbose:</span><br><span class=\"line\">\t\t\t\t\t\tprint(<span class=\"string\">'Total loss decreased from &#123;&#125; to &#123;&#125;, saving weights'</span>.format(best_loss,curr_loss))</span><br><span class=\"line\">\t\t\t\t\tbest_loss = curr_loss</span><br><span class=\"line\">\t\t\t\t\tmodel_all.save_weights(C.model_path)</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">break</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">except</span> Exception <span class=\"keyword\">as</span> e:</span><br><span class=\"line\">\t\t\tprint(<span class=\"string\">'Exception: &#123;&#125;'</span>.format(e))</span><br><span class=\"line\">\t\t\t<span class=\"keyword\">continue</span></span><br><span class=\"line\"></span><br><span class=\"line\">print(<span class=\"string\">'Training complete, exiting.'</span>)</span><br></pre></td></tr></table></figure>\n<p><a href=\"https://github.com/Trouble404/Object-Detection-System-Based-on-CNN-and-Capsule-Network/blob/master/Faster-RCNN/workflow_train.ipynb\" target=\"_blank\" rel=\"noopener\">jupyter notebook</a></p>\n<h3 id=\"30-07-2018\"><a href=\"#30-07-2018\" class=\"headerlink\" title=\"30/07/2018\"></a>30/07/2018</h3><h4 id=\"Running-at-GPU-enviorment\"><a href=\"#Running-at-GPU-enviorment\" class=\"headerlink\" title=\"Running at GPU enviorment\"></a>Running at GPU enviorment</h4><p><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/9_1.JPG\" alt=\"image\"><br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/9_2.JPG\" alt=\"image\"><br>Meet error in GPU version tensorflow<br>No enough memory.</p>\n<p>Try to Running at Irius:</p>\n<p>Setting 3 differnet configration:<br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/9_3.JPG\" alt=\"image\"><br>at Prjoect1 file:<br>set epoch_length to number of training img<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">epoch_length = <span class=\"number\">11540</span></span><br><span class=\"line\">num_epochs = <span class=\"number\">100</span></span><br></pre></td></tr></table></figure></p>\n<p>Apply img enhance function<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">C.use_horizontal_flips = <span class=\"keyword\">True</span></span><br><span class=\"line\">C.use_vertical_flips = <span class=\"keyword\">True</span></span><br><span class=\"line\">C.rot_90 = <span class=\"keyword\">True</span></span><br></pre></td></tr></table></figure></p>\n<hr>\n<p>at Prjoect file:<br>set epoch_length to 1000, increase epoch<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">epoch_length = <span class=\"number\">1000</span></span><br><span class=\"line\">num_epochs = <span class=\"number\">2000</span></span><br></pre></td></tr></table></figure></p>\n<p>Apply img enhance and class balance function<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">C.use_horizontal_flips = <span class=\"keyword\">True</span></span><br><span class=\"line\">C.use_vertical_flips = <span class=\"keyword\">True</span></span><br><span class=\"line\">C.rot_90 = <span class=\"keyword\">True</span></span><br><span class=\"line\">C.balanced_classes = <span class=\"keyword\">True</span></span><br></pre></td></tr></table></figure></p>\n<hr>\n<p>at Prjoect3 file:<br>set epoch_length to 1000, increase epoch<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">epoch_length = <span class=\"number\">1000</span></span><br><span class=\"line\">num_epochs = <span class=\"number\">2000</span></span><br></pre></td></tr></table></figure></p>\n<p>Apply img enhance<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">C.use_horizontal_flips = <span class=\"keyword\">True</span></span><br><span class=\"line\">C.use_vertical_flips = <span class=\"keyword\">True</span></span><br><span class=\"line\">C.rot_90 = <span class=\"keyword\">True</span></span><br></pre></td></tr></table></figure></p>\n<hr>\n<h4 id=\"check-irdius-work\"><a href=\"#check-irdius-work\" class=\"headerlink\" title=\"check irdius work\"></a>check irdius work</h4><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">myqueue</span><br></pre></td></tr></table></figure>\n<p><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/9_4.JPG\" alt=\"image\"></p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ssh pink59</span><br><span class=\"line\">nvidia-smi</span><br></pre></td></tr></table></figure>\n<p><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/9_5.JPG\" alt=\"image\"></p>\n<h3 id=\"31-07-2018\"><a href=\"#31-07-2018\" class=\"headerlink\" title=\"31/07/2018\"></a>31/07/2018</h3><h4 id=\"obtain-trained-model-and-log-file\"><a href=\"#obtain-trained-model-and-log-file\" class=\"headerlink\" title=\"obtain trained model and log file\"></a>obtain trained model and log file</h4><p> Iriuds GPU24<br>LogBook.</p>\n<h4 id=\"plot-rpn-and-classfier-loss\"><a href=\"#plot-rpn-and-classfier-loss\" class=\"headerlink\" title=\"plot rpn and classfier loss\"></a>plot rpn and classfier loss</h4><p>epochrpn_cls, rpn_regr, detc_cls, detc_regr<br>List<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">obtain_each_batch</span><span class=\"params\">(filename)</span>:</span></span><br><span class=\"line\">    n = <span class=\"number\">0</span></span><br><span class=\"line\">    rpn_cls = []</span><br><span class=\"line\">    rpn_regr = []</span><br><span class=\"line\">    detector_cls = []</span><br><span class=\"line\">    detector_regr = []</span><br><span class=\"line\">    f = open(filename,<span class=\"string\">'r'</span>,buffering=<span class=\"number\">-1</span>)</span><br><span class=\"line\">    lines = f.readlines()</span><br><span class=\"line\">    <span class=\"keyword\">for</span> line <span class=\"keyword\">in</span> lines:</span><br><span class=\"line\">        n = n + <span class=\"number\">1</span></span><br><span class=\"line\">        match = re.match(<span class=\"string\">r'.* - rpn_cls: (.*) - rpn_regr: .*'</span>, line, re.M|re.I)</span><br><span class=\"line\">        <span class=\"keyword\">if</span> match <span class=\"keyword\">is</span> <span class=\"keyword\">None</span>:</span><br><span class=\"line\">            <span class=\"keyword\">continue</span></span><br><span class=\"line\">        <span class=\"keyword\">else</span>: </span><br><span class=\"line\">            rpn_cls.append(float(match.group(<span class=\"number\">1</span>)))</span><br><span class=\"line\"></span><br><span class=\"line\">        match = re.match(<span class=\"string\">r'.* - rpn_regr: (.*) - detector_cls: .*'</span>, line, re.M|re.I)</span><br><span class=\"line\">        <span class=\"keyword\">if</span> match <span class=\"keyword\">is</span> <span class=\"keyword\">None</span>:</span><br><span class=\"line\">            <span class=\"keyword\">continue</span></span><br><span class=\"line\">        <span class=\"keyword\">else</span>: </span><br><span class=\"line\">            rpn_regr.append(float(match.group(<span class=\"number\">1</span>)))            </span><br><span class=\"line\">            </span><br><span class=\"line\">        match = re.match(<span class=\"string\">r'.* - detector_cls: (.*) - detector_regr: .*'</span>, line, re.M|re.I)</span><br><span class=\"line\">        <span class=\"keyword\">if</span> match <span class=\"keyword\">is</span> <span class=\"keyword\">None</span>:</span><br><span class=\"line\">            <span class=\"keyword\">continue</span></span><br><span class=\"line\">        <span class=\"keyword\">else</span>: </span><br><span class=\"line\">            detector_cls.append(float(match.group(<span class=\"number\">1</span>))) </span><br><span class=\"line\">            </span><br><span class=\"line\">        match = re.match(<span class=\"string\">r'.* - detector_regr: (.*)\\n'</span>, line, re.M|re.I)</span><br><span class=\"line\">        <span class=\"keyword\">if</span> match <span class=\"keyword\">is</span> <span class=\"keyword\">None</span>:</span><br><span class=\"line\">            <span class=\"keyword\">continue</span></span><br><span class=\"line\">        <span class=\"keyword\">else</span>: </span><br><span class=\"line\">            det_regr = match.group(<span class=\"number\">1</span>)[<span class=\"number\">0</span>:<span class=\"number\">6</span>]</span><br><span class=\"line\">            detector_regr.append(float(det_regr))</span><br><span class=\"line\"></span><br><span class=\"line\">    f.close()</span><br><span class=\"line\">    print(n)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> rpn_cls, rpn_regr, detector_cls, detector_regr</span><br></pre></td></tr></table></figure></p>\n<p>epochaccury, loss of rpn cls, loss of rpn regr, loss of detc cls, loss of detc regr<br>list<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">obtain_batch</span><span class=\"params\">(filename)</span>:</span></span><br><span class=\"line\">    n = <span class=\"number\">0</span></span><br><span class=\"line\">    accuracy = []</span><br><span class=\"line\">    loss_rpn_cls = []</span><br><span class=\"line\">    loss_rpn_regr = []</span><br><span class=\"line\">    loss_detc_cls = []</span><br><span class=\"line\">    loss_detc_regr = []</span><br><span class=\"line\">    f = open(filename,<span class=\"string\">'r'</span>,buffering=<span class=\"number\">-1</span>)</span><br><span class=\"line\">    lines = f.readlines()</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"keyword\">for</span> line <span class=\"keyword\">in</span> lines:</span><br><span class=\"line\">        n = n + <span class=\"number\">1</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> <span class=\"string\">'Classifier accuracy for bounding boxes from RPN'</span> <span class=\"keyword\">in</span> line:</span><br><span class=\"line\">            result = re.findall(<span class=\"string\">r\"\\d+\\.?\\d*\"</span>,line)</span><br><span class=\"line\">            accuracy.append(float(result[<span class=\"number\">0</span>]))</span><br><span class=\"line\">            </span><br><span class=\"line\">        <span class=\"keyword\">if</span> <span class=\"string\">'Loss RPN classifier'</span> <span class=\"keyword\">in</span> line:</span><br><span class=\"line\">            result = re.findall(<span class=\"string\">r\"\\d+\\.?\\d*\"</span>,line)</span><br><span class=\"line\">            loss_rpn_cls.append(float(result[<span class=\"number\">0</span>]))       </span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">if</span> <span class=\"string\">'Loss RPN regression'</span> <span class=\"keyword\">in</span> line:</span><br><span class=\"line\">            result = re.findall(<span class=\"string\">r\"\\d+\\.?\\d*\"</span>,line)</span><br><span class=\"line\">            loss_rpn_regr.append(float(result[<span class=\"number\">0</span>]))</span><br><span class=\"line\">            </span><br><span class=\"line\">        <span class=\"keyword\">if</span> <span class=\"string\">'Loss Detector classifier'</span> <span class=\"keyword\">in</span> line:</span><br><span class=\"line\">            result = re.findall(<span class=\"string\">r\"\\d+\\.?\\d*\"</span>,line)</span><br><span class=\"line\">            loss_detc_cls.append(float(result[<span class=\"number\">0</span>]))</span><br><span class=\"line\">            </span><br><span class=\"line\">        <span class=\"keyword\">if</span> <span class=\"string\">'Loss Detector regression'</span> <span class=\"keyword\">in</span> line:</span><br><span class=\"line\">            result = re.findall(<span class=\"string\">r\"\\d+\\.?\\d*\"</span>,line)</span><br><span class=\"line\">            loss_detc_regr.append(float(result[<span class=\"number\">0</span>])) </span><br><span class=\"line\">            </span><br><span class=\"line\">    f.close()</span><br><span class=\"line\">    print(n)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> accuracy, loss_rpn_cls, loss_rpn_regr, loss_detc_cls, loss_detc_regr</span><br></pre></td></tr></table></figure></p>\n<h4 id=\"plot-epoch-loss-and-accury\"><a href=\"#plot-epoch-loss-and-accury\" class=\"headerlink\" title=\"plot epoch loss and accury\"></a>plot epoch loss and accury</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">filename = <span class=\"string\">r'F:\\desktop\\\\1000-no_balance\\train1.out'</span></span><br><span class=\"line\">aa,bb,cc,dd,ee = obtain_batch(filename)</span><br><span class=\"line\">x_cor = np.arange(<span class=\"number\">0</span>,len(aa),<span class=\"number\">1</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">plt.plot(x_cor,aa, c=<span class=\"string\">'b'</span>, label = <span class=\"string\">\"Accuracy\"</span>)</span><br><span class=\"line\">plt.plot(x_cor,bb, c=<span class=\"string\">'c'</span>, label = <span class=\"string\">\"Loss RPN classifier\"</span>)</span><br><span class=\"line\">plt.plot(x_cor,cc, c=<span class=\"string\">'g'</span>, label = <span class=\"string\">\"Loss RPN regression\"</span>)</span><br><span class=\"line\">plt.plot(x_cor,dd, c=<span class=\"string\">'k'</span>, label = <span class=\"string\">\"Loss Detector classifier\"</span>)</span><br><span class=\"line\">plt.plot(x_cor,ee, c=<span class=\"string\">'m'</span>, label = <span class=\"string\">\"Loss Detector regression\"</span>)</span><br><span class=\"line\">plt.ylabel(<span class=\"string\">\"Value of Accuracy and Loss\"</span>) </span><br><span class=\"line\">plt.xlabel(<span class=\"string\">\"Number of Epoch\"</span>)</span><br><span class=\"line\">plt.title(<span class=\"string\">'Loss and Accuracy for Totoal Epochs'</span>)  </span><br><span class=\"line\">plt.legend()</span><br><span class=\"line\">plt.ylim(<span class=\"number\">0</span>,<span class=\"number\">2</span>)</span><br><span class=\"line\"><span class=\"comment\">#plt.xlim(0,11540)</span></span><br><span class=\"line\">plt.savefig(<span class=\"string\">\"pic1.PNG\"</span>, dpi = <span class=\"number\">600</span>)</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n<p><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/pic1.PNG\" alt=\"image\"></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">filename = <span class=\"string\">r'F:\\desktop\\\\1000-no_balance\\train1.out'</span></span><br><span class=\"line\">a,b,c,d = obtain_each_batch(filename)</span><br><span class=\"line\">x_cor = np.arange(<span class=\"number\">0</span>,len(a),<span class=\"number\">1</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">plt.plot(x_cor,a, c=<span class=\"string\">'b'</span>, label = <span class=\"string\">\"rpn_cls\"</span>)</span><br><span class=\"line\">plt.plot(x_cor,b, c=<span class=\"string\">'c'</span>, label = <span class=\"string\">\"rpn_regr\"</span>)</span><br><span class=\"line\">plt.plot(x_cor,c, c=<span class=\"string\">'g'</span>, label = <span class=\"string\">\"detector_cls\"</span>)</span><br><span class=\"line\">plt.plot(x_cor,d, c=<span class=\"string\">'k'</span>, label = <span class=\"string\">\"detector_regr\"</span>)</span><br><span class=\"line\">plt.ylabel(<span class=\"string\">\"Value of Loss\"</span>) </span><br><span class=\"line\">plt.xlabel(<span class=\"string\">\"Epoch Length\"</span>)</span><br><span class=\"line\">plt.title(<span class=\"string\">'Loss for Lenght of Epoch'</span>)  </span><br><span class=\"line\">plt.legend()</span><br><span class=\"line\"><span class=\"comment\">#plt.ylim(0,2)</span></span><br><span class=\"line\">plt.xlim(<span class=\"number\">80787</span>,<span class=\"number\">92327</span>)</span><br><span class=\"line\">plt.savefig(<span class=\"string\">\"pic2.PNG\"</span>, dpi = <span class=\"number\">600</span>)</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n<p><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/pic2.PNG\" alt=\"image\"></p>\n<p><a href=\"https://github.com/Trouble404/Object-Detection-System-Based-on-CNN-and-Capsule-Network/blob/master/Plot/logbook_plot.ipynb\" target=\"_blank\" rel=\"noopener\">Jupyter notebook</a></p>\n<h2 id=\"August\"><a href=\"#August\" class=\"headerlink\" title=\"August\"></a>August</h2><h3 id=\"01-02-08-2018\"><a href=\"#01-02-08-2018\" class=\"headerlink\" title=\"01~02/08/2018\"></a>01~02/08/2018</h3><h4 id=\"test-network\"><a href=\"#test-network\" class=\"headerlink\" title=\"test network\"></a>test network</h4><p>train<br></p>\n<p><strong>rpn</strong><br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">shared_layers = nn.nn_base(img_input, trainable=<span class=\"keyword\">True</span>)</span><br><span class=\"line\">num_anchors = len(C.anchor_box_scales)*len(C.anchor_box_ratios)</span><br><span class=\"line\">rpn_layers = nn.rpn(shared_layers,num_anchors)</span><br></pre></td></tr></table></figure></p>\n<p><strong>classifier</strong><br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">classifier = nn.classifier(feature_map_input, roi_input, C.num_rois,nb_classes=len(class_mapping), trainable=<span class=\"keyword\">True</span>)</span><br></pre></td></tr></table></figure></p>\n<p><strong></strong><br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">C.model_path = <span class=\"string\">'gpu_resnet50_weights.h5'</span></span><br><span class=\"line\"><span class=\"keyword\">try</span>:</span><br><span class=\"line\">    print(<span class=\"string\">'Loading weights from &#123;&#125;'</span>.format(C.model_path))</span><br><span class=\"line\">    model_rpn.load_weights(C.model_path, by_name=<span class=\"keyword\">True</span>)</span><br><span class=\"line\">    model_classifier.load_weights(C.model_path, by_name=<span class=\"keyword\">True</span>)</span><br><span class=\"line\"><span class=\"keyword\">except</span>:</span><br><span class=\"line\">    print(<span class=\"string\">'can not load'</span>)</span><br></pre></td></tr></table></figure></p>\n<p><strong></strong><br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/test/test4.jpg\" alt=\"image\"><br></p>\n<ol>\n<li><p><br> <br> <br> <br> img</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">format_img_size</span><span class=\"params\">(img, C)</span>:</span></span><br><span class=\"line\">    (height,width,_) = img.shape</span><br><span class=\"line\">    <span class=\"keyword\">if</span> width &lt;= height:</span><br><span class=\"line\">        ratio = C.im_size/width</span><br><span class=\"line\">    <span class=\"keyword\">else</span>:</span><br><span class=\"line\">        ratio = C.im_size/height</span><br><span class=\"line\">    new_width, new_height = image_processing.get_new_img_size(width,height, C.im_size)</span><br><span class=\"line\">    img = cv2.resize(img, (new_width, new_height), interpolation=cv2.INTER_CUBIC)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> img, ratio</span><br></pre></td></tr></table></figure>\n</li>\n<li><p><br>  BGRRGBRESNET<br> np.float32<br> 1<br> <br> </p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">format_img_channels</span><span class=\"params\">(img, C)</span>:</span></span><br><span class=\"line\">\t<span class=\"string\">\"\"\" formats the image channels based on config \"\"\"</span></span><br><span class=\"line\">\timg = img[:, :, (<span class=\"number\">2</span>, <span class=\"number\">1</span>, <span class=\"number\">0</span>)]</span><br><span class=\"line\">\timg = img.astype(np.float32)</span><br><span class=\"line\">\timg[:, :, <span class=\"number\">0</span>] -= C.img_channel_mean[<span class=\"number\">0</span>]</span><br><span class=\"line\">\timg[:, :, <span class=\"number\">1</span>] -= C.img_channel_mean[<span class=\"number\">1</span>]</span><br><span class=\"line\">\timg[:, :, <span class=\"number\">2</span>] -= C.img_channel_mean[<span class=\"number\">2</span>]</span><br><span class=\"line\">\timg /= C.img_scaling_factor</span><br><span class=\"line\">\timg = np.transpose(img, (<span class=\"number\">2</span>, <span class=\"number\">0</span>, <span class=\"number\">1</span>))</span><br><span class=\"line\">\timg = np.expand_dims(img, axis=<span class=\"number\">0</span>)</span><br><span class=\"line\">\t<span class=\"keyword\">return</span> img</span><br></pre></td></tr></table></figure>\n</li>\n</ol>\n<p>tensorflow</p>\n<p><strong></strong><br>Y1:anchor<br>Y2:anchor<br>F:</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[Y1, Y2, F] = model_rpn.predict(X)</span><br></pre></td></tr></table></figure>\n<p>rpn16anchorrpn<br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/pic_anchors.png\" alt=\"image\"><br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/25_1.jpg\" alt=\"image\"></p>\n<p><strong>rpn:</strong><br>300(x1,y1,x2,y2)<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># transform predicted rpn to cordinates of boxes</span></span><br><span class=\"line\">R = rpn_to_boxes.rpn_to_roi(Y1, Y2, C, K.image_dim_ordering(), use_regr=<span class=\"keyword\">True</span>, overlap_thresh=<span class=\"number\">0.7</span>)</span><br></pre></td></tr></table></figure></p>\n<p><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/pic_rois.png\" alt=\"image\"></p>\n<p>(x1,y1,x2,y2)  (x,y,w,h)<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">R[:, <span class=\"number\">2</span>] -= R[:, <span class=\"number\">0</span>]</span><br><span class=\"line\">R[:, <span class=\"number\">3</span>] -= R[:, <span class=\"number\">1</span>]</span><br></pre></td></tr></table></figure></p>\n<p><strong></strong><br>C.num_rois<br>32300/32, 10<br>3232<br><br>3232<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># divided 32 bboxes as one group</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> jk <span class=\"keyword\">in</span> range(R.shape[<span class=\"number\">0</span>]//C.num_rois + <span class=\"number\">1</span>):</span><br><span class=\"line\">    <span class=\"comment\"># pick num_rios(32) bboxes one time, only pick to last bboxes in last group</span></span><br><span class=\"line\">    ROIs = np.expand_dims(R[C.num_rois*jk:C.num_rois*(jk+<span class=\"number\">1</span>), :], axis=<span class=\"number\">0</span>)</span><br><span class=\"line\">    <span class=\"comment\">#print(ROIs.shape)</span></span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\"># no proposals, out iter</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> ROIs.shape[<span class=\"number\">1</span>] == <span class=\"number\">0</span>:</span><br><span class=\"line\">        <span class=\"keyword\">break</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># when last time can't obtain num_rios(32) bboxes, adding bboxes with 0 to fill to 32 bboxes</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> jk == R.shape[<span class=\"number\">0</span>]//C.num_rois:</span><br><span class=\"line\">        <span class=\"comment\">#pad R</span></span><br><span class=\"line\">        curr_shape = ROIs.shape</span><br><span class=\"line\">        target_shape = (curr_shape[<span class=\"number\">0</span>],C.num_rois,curr_shape[<span class=\"number\">2</span>])</span><br><span class=\"line\">        ROIs_padded = np.zeros(target_shape).astype(ROIs.dtype)</span><br><span class=\"line\">        ROIs_padded[:, :curr_shape[<span class=\"number\">1</span>], :] = ROIs</span><br><span class=\"line\">        ROIs_padded[<span class=\"number\">0</span>, curr_shape[<span class=\"number\">1</span>]:, :] = ROIs[<span class=\"number\">0</span>, <span class=\"number\">0</span>, :]</span><br><span class=\"line\">        <span class=\"comment\"># 10 group with 320 bboxes</span></span><br><span class=\"line\">        ROIs = ROIs_padded</span><br></pre></td></tr></table></figure></p>\n<p></p>\n<p><strong></strong></p>\n<p><br>P_cls<br>P_regr<br>F:rpn<br>ROIS:<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[P_cls, P_regr] = model_classifier_only.predict([F, ROIs])</span><br></pre></td></tr></table></figure></p>\n<p><br><br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">for</span> ii <span class=\"keyword\">in</span> range(P_cls.shape[<span class=\"number\">1</span>]):</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># if smaller than setting threshold, we think this bbox invalid</span></span><br><span class=\"line\">    <span class=\"comment\"># and if this bbox's class is background, we don't need to care about it</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> np.max(P_cls[<span class=\"number\">0</span>, ii, :]) &lt; bbox_threshold <span class=\"keyword\">or</span> np.argmax(P_cls[<span class=\"number\">0</span>, ii, :]) == (P_cls.shape[<span class=\"number\">2</span>] - <span class=\"number\">1</span>):</span><br><span class=\"line\">        <span class=\"keyword\">continue</span></span><br></pre></td></tr></table></figure></p>\n<p><br>list<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># obatain max possibility's class name by class mapping</span></span><br><span class=\"line\">cls_name = class_mapping[np.argmax(P_cls[<span class=\"number\">0</span>, ii, :])]</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># saving bboxes and probs</span></span><br><span class=\"line\"><span class=\"keyword\">if</span> cls_name <span class=\"keyword\">not</span> <span class=\"keyword\">in</span> bboxes:</span><br><span class=\"line\">    bboxes[cls_name] = []</span><br><span class=\"line\">    probs[cls_name] = []</span><br></pre></td></tr></table></figure></p>\n<p><br><br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># obtain current cordinates of proposal</span></span><br><span class=\"line\">(x, y, w, h) = ROIs[<span class=\"number\">0</span>, ii, :]</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># obtain the position with max possibility</span></span><br><span class=\"line\">cls_num = np.argmax(P_cls[<span class=\"number\">0</span>, ii, :])</span><br></pre></td></tr></table></figure></p>\n<p><br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/pic_cls1.png\" alt=\"iamge\"></p>\n<p><br><br><br> C.rpn_stride<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">try</span>:</span><br><span class=\"line\">    <span class=\"comment\"># obtain privous position's bbox's regrient</span></span><br><span class=\"line\">    (tx, ty, tw, th) = P_regr[<span class=\"number\">0</span>, ii, <span class=\"number\">4</span>*cls_num:<span class=\"number\">4</span>*(cls_num+<span class=\"number\">1</span>)]</span><br><span class=\"line\">    <span class=\"comment\"># waiting test</span></span><br><span class=\"line\">    tx /= C.classifier_regr_std[<span class=\"number\">0</span>]</span><br><span class=\"line\">    ty /= C.classifier_regr_std[<span class=\"number\">1</span>]</span><br><span class=\"line\">    tw /= C.classifier_regr_std[<span class=\"number\">2</span>]</span><br><span class=\"line\">    th /= C.classifier_regr_std[<span class=\"number\">3</span>]</span><br><span class=\"line\">    <span class=\"comment\"># fix box with regreient</span></span><br><span class=\"line\">    x, y, w, h = rpn_to_boxes.apply_regr(x, y, w, h, tx, ty, tw, th)</span><br><span class=\"line\"><span class=\"keyword\">except</span>:</span><br><span class=\"line\">    <span class=\"keyword\">pass</span></span><br><span class=\"line\"><span class=\"comment\"># cordinates of current's box on real img</span></span><br><span class=\"line\">bboxes[cls_name].append([C.rpn_stride*x, C.rpn_stride*y, C.rpn_stride*(x+w), C.rpn_stride*(y+h)])</span><br><span class=\"line\"><span class=\"comment\"># coresspoding posbility</span></span><br><span class=\"line\">probs[cls_name].append(np.max(P_cls[<span class=\"number\">0</span>, ii, :]))</span><br></pre></td></tr></table></figure></p>\n<p><br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/pic_cls2.png\" alt=\"image\"></p>\n<p>bboxesbbox<br>No Max Supression<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># for all classes in current boxes</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> key <span class=\"keyword\">in</span> bboxes:</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># bboxes's cordinates</span></span><br><span class=\"line\">    bbox = np.array(bboxes[key])</span><br><span class=\"line\">    <span class=\"comment\"># apply NMX to merge some  overlapping boxes</span></span><br><span class=\"line\">    new_boxes, new_probs = rpn_to_boxes.non_max_suppression_fast(bbox, np.array(probs[key]), overlap_thresh=<span class=\"number\">0.5</span>)</span><br></pre></td></tr></table></figure></p>\n<p><br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/pic_cls3.png\" alt=\"image\"></p>\n<p><a href=\"https://github.com/Trouble404/Object-Detection-System-Based-on-CNN-and-Capsule-Network/blob/master/Faster-RCNN/test.ipynb\" target=\"_blank\" rel=\"noopener\">Jupyter notebook</a></p>\n<h4 id=\"result\"><a href=\"#result\" class=\"headerlink\" title=\"result\"></a>result</h4><p>Small img, only 8k</p>\n<div align=\"center\"><br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/test/test1.jpg\" height=\"200px\"><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/test/test1.png\" height=\"200px\"><br></div>\n\n<hr>\n<p>Overlapping img</p>\n<div align=\"center\"><br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/test/test2.jpg\" height=\"200px\"><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/test/test2.png\" height=\"200px\"><br></div>\n\n<hr>\n<p>Crowed People</p>\n<div align=\"center\"><br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/test/test3.jpg\" height=\"260px\"><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/test/test3.png\" height=\"260px\"><br></div>\n\n<hr>\n<p>cow and people</p>\n<div align=\"center\"><br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/test/test4.jpg\" height=\"260px\"><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/test/test4.png\" height=\"260px\"><br></div>\n\n<hr>\n<p>car and plane</p>\n<div align=\"center\"><br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/test/test5.jpg\" height=\"220px\"><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/test/test5.png\" height=\"220px\"><br></div>\n\n<hr>\n<p>Street img</p>\n<div align=\"center\"><br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/test/test6.jpg\" height=\"270px\"><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/test/test6.png\" height=\"270px\"><br></div>\n\n<hr>\n<p>Lots Dogs</p>\n<div align=\"center\"><br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/test/test7.jpg\" height=\"250px\"><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/test/test7.png\" height=\"250px\"><br></div>\n\n<hr>\n<p>Overlapping car and people</p>\n<div align=\"center\"><br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/test/test8.jpg\" height=\"290px\"><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/test/test8.png\" height=\"290px\"><br></div>\n\n<p></p>\n<h3 id=\"03-08-2018\"><a href=\"#03-08-2018\" class=\"headerlink\" title=\"03/08/2018\"></a>03/08/2018</h3><h4 id=\"evaluation\"><a href=\"#evaluation\" class=\"headerlink\" title=\"evaluation\"></a>evaluation</h4><p><strong>mAP</strong><br>mAPPrecisionRecallobject detectionobjectPrecisionRecall/ P-RAPmeanAPmAPmAP[0,1] </p>\n<p><strong>AP</strong>:PrecisionRecallsklearn.metrics.average_precision_score </p>\n<p>resizesklearn.metrics.average_precision_score</p>\n<hr>\n<p>rpntrain<br>feature map<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">num_features = <span class=\"number\">1024</span></span><br><span class=\"line\"></span><br><span class=\"line\">input_shape_img = (<span class=\"keyword\">None</span>, <span class=\"keyword\">None</span>, <span class=\"number\">3</span>)</span><br><span class=\"line\">input_shape_features = (<span class=\"keyword\">None</span>, <span class=\"keyword\">None</span>, num_features)</span><br><span class=\"line\"></span><br><span class=\"line\">img_input = Input(shape=input_shape_img)</span><br><span class=\"line\">roi_input = Input(shape=(C.num_rois, <span class=\"number\">4</span>))</span><br><span class=\"line\">feature_map_input = Input(shape=input_shape_features)</span><br><span class=\"line\"></span><br><span class=\"line\">classifier = nn.classifier(feature_map_input, roi_input, C.num_rois, nb_classes=len(class_mapping), trainable=<span class=\"keyword\">True</span>)</span><br></pre></td></tr></table></figure></p>\n<p></p>\n<p>VOC<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">train_imgs = []</span><br><span class=\"line\">test_imgs = []</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">for</span> each <span class=\"keyword\">in</span> all_imgs:</span><br><span class=\"line\">\t<span class=\"keyword\">if</span> each[<span class=\"string\">'imageset'</span>] == <span class=\"string\">'trainval'</span>:</span><br><span class=\"line\">\t\ttrain_imgs.append(each)</span><br><span class=\"line\">\t<span class=\"keyword\">if</span> each[<span class=\"string\">'imageset'</span>] == <span class=\"string\">'test'</span>:</span><br><span class=\"line\">\t\ttest_imgs.append(each)</span><br></pre></td></tr></table></figure></p>\n<p>xml<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">for</span> jk <span class=\"keyword\">in</span> range(new_boxes.shape[<span class=\"number\">0</span>]):</span><br><span class=\"line\">    (x1, y1, x2, y2) = new_boxes[jk, :]</span><br><span class=\"line\">    det = &#123;<span class=\"string\">'x1'</span>: x1, <span class=\"string\">'x2'</span>: x2, <span class=\"string\">'y1'</span>: y1, <span class=\"string\">'y2'</span>: y2, <span class=\"string\">'class'</span>: key, <span class=\"string\">'prob'</span>: new_probs[jk]&#125;</span><br><span class=\"line\">    all_dets.append(det)</span><br></pre></td></tr></table></figure></p>\n<p><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/014_1.JPG\" alt=\"image\"></p>\n<p><br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/014_2.JPG\" alt=\"image\"></p>\n<p>bbox_matchedFALSETrue<br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/14_3.JPG\" alt=\"image\"></p>\n<p>idx<br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/014_3.JPG\" alt=\"image\"></p>\n<p>iou0.5</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># process each bbox with hightest prob</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> box_idx <span class=\"keyword\">in</span> box_idx_sorted_by_prob:</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\"># obtain current box's cordinates, class and prob</span></span><br><span class=\"line\">    pred_box = pred[box_idx]</span><br><span class=\"line\">    pred_class = pred_box[<span class=\"string\">'class'</span>]</span><br><span class=\"line\">    pred_x1 = pred_box[<span class=\"string\">'x1'</span>]</span><br><span class=\"line\">    pred_x2 = pred_box[<span class=\"string\">'x2'</span>]</span><br><span class=\"line\">    pred_y1 = pred_box[<span class=\"string\">'y1'</span>]</span><br><span class=\"line\">    pred_y2 = pred_box[<span class=\"string\">'y2'</span>]</span><br><span class=\"line\">    pred_prob = pred_box[<span class=\"string\">'prob'</span>]</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\"># if not in P list, save current class infomration to it</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> pred_class <span class=\"keyword\">not</span> <span class=\"keyword\">in</span> P:</span><br><span class=\"line\">        P[pred_class] = []</span><br><span class=\"line\">        T[pred_class] = []</span><br><span class=\"line\">        <span class=\"comment\"># put porb to P</span></span><br><span class=\"line\">    P[pred_class].append(pred_prob)</span><br><span class=\"line\">    <span class=\"comment\"># used to check whether find current object</span></span><br><span class=\"line\">    found_match = <span class=\"keyword\">False</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># compare each real bbox</span></span><br><span class=\"line\">    <span class=\"comment\"># obtain real box's cordinates, class and prob</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> gt_box <span class=\"keyword\">in</span> gt:</span><br><span class=\"line\">        gt_class = gt_box[<span class=\"string\">'class'</span>]</span><br><span class=\"line\">        <span class=\"comment\"># bacause the image is rezied, so calculate the real cordinates</span></span><br><span class=\"line\">        gt_x1 = gt_box[<span class=\"string\">'x1'</span>]/fx</span><br><span class=\"line\">        gt_x2 = gt_box[<span class=\"string\">'x2'</span>]/fx</span><br><span class=\"line\">        gt_y1 = gt_box[<span class=\"string\">'y1'</span>]/fy</span><br><span class=\"line\">        gt_y2 = gt_box[<span class=\"string\">'y2'</span>]/fy</span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"comment\"># obtain box_matched - all false at beginning</span></span><br><span class=\"line\">        gt_seen = gt_box[<span class=\"string\">'bbox_matched'</span>]</span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"comment\"># ture class != predicted class</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> gt_class != pred_class:</span><br><span class=\"line\">            <span class=\"keyword\">continue</span></span><br><span class=\"line\">        <span class=\"comment\"># already matched</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> gt_seen:</span><br><span class=\"line\">            <span class=\"keyword\">continue</span></span><br><span class=\"line\">        <span class=\"comment\"># calculate iou of predicted bbox and real bbox </span></span><br><span class=\"line\">        iou = rpn_calculation.iou((pred_x1, pred_y1, pred_x2, pred_y2), (gt_x1, gt_y1, gt_x2, gt_y2))</span><br><span class=\"line\">        <span class=\"comment\"># if iou &gt; 0.5, we will set this prediction correct</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> iou &gt;= <span class=\"number\">0.5</span>:</span><br><span class=\"line\">            found_match = <span class=\"keyword\">True</span></span><br><span class=\"line\">            gt_box[<span class=\"string\">'bbox_matched'</span>] = <span class=\"keyword\">True</span></span><br><span class=\"line\">            <span class=\"keyword\">break</span></span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            <span class=\"keyword\">continue</span></span><br><span class=\"line\">    <span class=\"comment\"># 1 means this position's bbox correct match with orignal image</span></span><br><span class=\"line\">    T[pred_class].append(int(found_match))</span><br></pre></td></tr></table></figure>\n<p><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/014_4.JPG\" alt=\"image\"></p>\n<p>diffculttrue10<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># adding missing object compared to orignal image</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> gt_box <span class=\"keyword\">in</span> gt:</span><br><span class=\"line\">    <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> gt_box[<span class=\"string\">'bbox_matched'</span>] <span class=\"keyword\">and</span> <span class=\"keyword\">not</span> gt_box[<span class=\"string\">'difficult'</span>]:</span><br><span class=\"line\">        <span class=\"keyword\">if</span> gt_box[<span class=\"string\">'class'</span>] <span class=\"keyword\">not</span> <span class=\"keyword\">in</span> P:</span><br><span class=\"line\">            P[gt_box[<span class=\"string\">'class'</span>]] = []</span><br><span class=\"line\">            T[gt_box[<span class=\"string\">'class'</span>]] = []</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># T = 1 means there are object, P = 0 means we did't detected that</span></span><br><span class=\"line\">        T[gt_box[<span class=\"string\">'class'</span>]].append(<span class=\"number\">1</span>)</span><br><span class=\"line\">        P[gt_box[<span class=\"string\">'class'</span>]].append(<span class=\"number\">0</span>)</span><br></pre></td></tr></table></figure></p>\n<p><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/014_5.JPG\" alt=\"image\"></p>\n<p>average_precision_scoresklearnapmap<br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/014_6.JPG\" alt=\"image\"></p>\n<p><a href=\"https://github.com/Trouble404/Object-Detection-System-Based-on-CNN-and-Capsule-Network/blob/master/Faster-RCNN/map.ipynb\" target=\"_blank\" rel=\"noopener\">jupyter notebook</a></p>\n<h3 id=\"06-10-08-2018\"><a href=\"#06-10-08-2018\" class=\"headerlink\" title=\"06~10/08/2018\"></a>06~10/08/2018</h3><h4 id=\"adjust\"><a href=\"#adjust\" class=\"headerlink\" title=\"adjust\"></a>adjust</h4><p>ap<br><a href=\"https://github.com/Trouble404/Object-Detection-System-Based-on-CNN-and-Capsule-Network/blob/master/Faster-RCNN/map_all.ipynb\" target=\"_blank\" rel=\"noopener\">jupyter notebook</a></p>\n<h5 id=\"Project1-all-9-models\"><a href=\"#Project1-all-9-models\" class=\"headerlink\" title=\"Project1 all: 9 models:\"></a>Project1 all: 9 models:</h5><p>ALL_2 NO THRESHOLD OTHER WITH THRESHOLD MOST 0.8</p>\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">Classes</th>\n<th style=\"text-align:center\">ALL_1</th>\n<th style=\"text-align:center\">ALL_2</th>\n<th style=\"text-align:center\">ALL_3</th>\n<th style=\"text-align:center\">ALL_4</th>\n<th style=\"text-align:center\">ALL_5</th>\n<th style=\"text-align:center\">ALL_6</th>\n<th style=\"text-align:center\">ALL_7</th>\n<th style=\"text-align:center\">ALL_8</th>\n<th style=\"text-align:center\">ALL_9</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">motorbike</td>\n<td style=\"text-align:center\">0.2305</td>\n<td style=\"text-align:center\">0.2412</td>\n<td style=\"text-align:center\">0.2132</td>\n<td style=\"text-align:center\">0.2220</td>\n<td style=\"text-align:center\">0.2889</td>\n<td style=\"text-align:center\">0.2528</td>\n<td style=\"text-align:center\">0.2204</td>\n<td style=\"text-align:center\">0.2644</td>\n<td style=\"text-align:center\">0.2336</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">person</td>\n<td style=\"text-align:center\">0.6489</td>\n<td style=\"text-align:center\">0.6735</td>\n<td style=\"text-align:center\">0.7107</td>\n<td style=\"text-align:center\">0.6652</td>\n<td style=\"text-align:center\">0.7120</td>\n<td style=\"text-align:center\">0.7041</td>\n<td style=\"text-align:center\">0.7238</td>\n<td style=\"text-align:center\">0.7003</td>\n<td style=\"text-align:center\">0.7201</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">car</td>\n<td style=\"text-align:center\">0.1697</td>\n<td style=\"text-align:center\">0.1563</td>\n<td style=\"text-align:center\">0.2032</td>\n<td style=\"text-align:center\">0.2105</td>\n<td style=\"text-align:center\">0.2308</td>\n<td style=\"text-align:center\">0.2221</td>\n<td style=\"text-align:center\">0.2436</td>\n<td style=\"text-align:center\">0.2053</td>\n<td style=\"text-align:center\">0.2080</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">aeroplane</td>\n<td style=\"text-align:center\">0.7968</td>\n<td style=\"text-align:center\">0.7062</td>\n<td style=\"text-align:center\">0.7941</td>\n<td style=\"text-align:center\">0.6412</td>\n<td style=\"text-align:center\">0.7871</td>\n<td style=\"text-align:center\">0.7331</td>\n<td style=\"text-align:center\">0.7648</td>\n<td style=\"text-align:center\">0.7659</td>\n<td style=\"text-align:center\">0.6902</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">bottle</td>\n<td style=\"text-align:center\">0.2213</td>\n<td style=\"text-align:center\">0.2428</td>\n<td style=\"text-align:center\">0.2261</td>\n<td style=\"text-align:center\">0.1943</td>\n<td style=\"text-align:center\">0.2570</td>\n<td style=\"text-align:center\">0.2437</td>\n<td style=\"text-align:center\">0.2899</td>\n<td style=\"text-align:center\">0.1442</td>\n<td style=\"text-align:center\">0.2265</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">sheep</td>\n<td style=\"text-align:center\">0.6162</td>\n<td style=\"text-align:center\">0.5702</td>\n<td style=\"text-align:center\">0.6876</td>\n<td style=\"text-align:center\">0.6295</td>\n<td style=\"text-align:center\">0.6364</td>\n<td style=\"text-align:center\">0.5710</td>\n<td style=\"text-align:center\">0.6536</td>\n<td style=\"text-align:center\">0.6349</td>\n<td style=\"text-align:center\">0.6455</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">tvmonitor</td>\n<td style=\"text-align:center\">0.1582</td>\n<td style=\"text-align:center\">0.1601</td>\n<td style=\"text-align:center\">0.2231</td>\n<td style=\"text-align:center\">0.1748</td>\n<td style=\"text-align:center\">0.1551</td>\n<td style=\"text-align:center\">0.1603</td>\n<td style=\"text-align:center\">0.1317</td>\n<td style=\"text-align:center\">0.1584</td>\n<td style=\"text-align:center\">0.1678</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">boat</td>\n<td style=\"text-align:center\">0.3842</td>\n<td style=\"text-align:center\">0.2621</td>\n<td style=\"text-align:center\">0.2261</td>\n<td style=\"text-align:center\">0.1943</td>\n<td style=\"text-align:center\">0.3499</td>\n<td style=\"text-align:center\">0.2437</td>\n<td style=\"text-align:center\">0.2057</td>\n<td style=\"text-align:center\">0.2748</td>\n<td style=\"text-align:center\">0.3509</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">chair</td>\n<td style=\"text-align:center\">0.2811</td>\n<td style=\"text-align:center\">0.0563</td>\n<td style=\"text-align:center\">0.0891</td>\n<td style=\"text-align:center\">0.0621</td>\n<td style=\"text-align:center\">0.1353</td>\n<td style=\"text-align:center\">0.0865</td>\n<td style=\"text-align:center\">0.0907</td>\n<td style=\"text-align:center\">0.0854</td>\n<td style=\"text-align:center\">0.1282</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">bicycle</td>\n<td style=\"text-align:center\">0.1464</td>\n<td style=\"text-align:center\">0.1224</td>\n<td style=\"text-align:center\">0.1346</td>\n<td style=\"text-align:center\">0.1781</td>\n<td style=\"text-align:center\">0.1406</td>\n<td style=\"text-align:center\">0.1448</td>\n<td style=\"text-align:center\">0.1810</td>\n<td style=\"text-align:center\">0.1071</td>\n<td style=\"text-align:center\">0.1673</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">cat</td>\n<td style=\"text-align:center\">0.8901</td>\n<td style=\"text-align:center\">0.8565</td>\n<td style=\"text-align:center\">0.9103</td>\n<td style=\"text-align:center\">0.8417</td>\n<td style=\"text-align:center\">0.8289</td>\n<td style=\"text-align:center\">0.8274</td>\n<td style=\"text-align:center\">0.7572</td>\n<td style=\"text-align:center\">0.9143</td>\n<td style=\"text-align:center\">0.8118</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">pottedplant</td>\n<td style=\"text-align:center\">0.2075</td>\n<td style=\"text-align:center\">0.0926</td>\n<td style=\"text-align:center\">0.1790</td>\n<td style=\"text-align:center\">0.0532</td>\n<td style=\"text-align:center\">0.1517</td>\n<td style=\"text-align:center\">0.1150</td>\n<td style=\"text-align:center\">0.1080</td>\n<td style=\"text-align:center\">0.1022</td>\n<td style=\"text-align:center\">0.0939</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">horse</td>\n<td style=\"text-align:center\">0.1185</td>\n<td style=\"text-align:center\">0.0588</td>\n<td style=\"text-align:center\">0.0726</td>\n<td style=\"text-align:center\">0.0489</td>\n<td style=\"text-align:center\">0.0696</td>\n<td style=\"text-align:center\">0.0695</td>\n<td style=\"text-align:center\">0.0637</td>\n<td style=\"text-align:center\">0.0651</td>\n<td style=\"text-align:center\">0.0640</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">sofa</td>\n<td style=\"text-align:center\">0.2797</td>\n<td style=\"text-align:center\">0.2309</td>\n<td style=\"text-align:center\">0.2852</td>\n<td style=\"text-align:center\">0.2966</td>\n<td style=\"text-align:center\">0.3855</td>\n<td style=\"text-align:center\">0.4817</td>\n<td style=\"text-align:center\">0.3659</td>\n<td style=\"text-align:center\">0.3132</td>\n<td style=\"text-align:center\">0.3090</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">dog</td>\n<td style=\"text-align:center\">0.5359</td>\n<td style=\"text-align:center\">0.5077</td>\n<td style=\"text-align:center\">0.5578</td>\n<td style=\"text-align:center\">0.4413</td>\n<td style=\"text-align:center\">0.4832</td>\n<td style=\"text-align:center\">0.5793</td>\n<td style=\"text-align:center\">0.5687</td>\n<td style=\"text-align:center\">0.4910</td>\n<td style=\"text-align:center\">0.4598</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">cow</td>\n<td style=\"text-align:center\">0.7582</td>\n<td style=\"text-align:center\">0.6229</td>\n<td style=\"text-align:center\">0.7295</td>\n<td style=\"text-align:center\">0.5420</td>\n<td style=\"text-align:center\">0.5379</td>\n<td style=\"text-align:center\">0.5312</td>\n<td style=\"text-align:center\">0.5147</td>\n<td style=\"text-align:center\">0.5706</td>\n<td style=\"text-align:center\">0.6503</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">diningtable</td>\n<td style=\"text-align:center\">0.3979</td>\n<td style=\"text-align:center\">0.2734</td>\n<td style=\"text-align:center\">0.3739</td>\n<td style=\"text-align:center\">0.2963</td>\n<td style=\"text-align:center\">0.4715</td>\n<td style=\"text-align:center\">0.4987</td>\n<td style=\"text-align:center\">0.3895</td>\n<td style=\"text-align:center\">0.4983</td>\n<td style=\"text-align:center\">0.4666</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">bus</td>\n<td style=\"text-align:center\">0.6203</td>\n<td style=\"text-align:center\">0.5572</td>\n<td style=\"text-align:center\">0.6468</td>\n<td style=\"text-align:center\">0.6032</td>\n<td style=\"text-align:center\">0.6320</td>\n<td style=\"text-align:center\">0.6096</td>\n<td style=\"text-align:center\">0.7169</td>\n<td style=\"text-align:center\">0.5938</td>\n<td style=\"text-align:center\">0.5485</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">bird</td>\n<td style=\"text-align:center\">0.6164</td>\n<td style=\"text-align:center\">0.6662</td>\n<td style=\"text-align:center\">0.5692</td>\n<td style=\"text-align:center\">0.5751</td>\n<td style=\"text-align:center\">0.5407</td>\n<td style=\"text-align:center\">0.4125</td>\n<td style=\"text-align:center\">0.4925</td>\n<td style=\"text-align:center\">0.4347</td>\n<td style=\"text-align:center\">0.5208</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">train</td>\n<td style=\"text-align:center\">0.8655</td>\n<td style=\"text-align:center\">0.6916</td>\n<td style=\"text-align:center\">0.7141</td>\n<td style=\"text-align:center\">0.7166</td>\n<td style=\"text-align:center\">0.7643</td>\n<td style=\"text-align:center\">0.8107</td>\n<td style=\"text-align:center\">0.7100</td>\n<td style=\"text-align:center\">0.7194</td>\n<td style=\"text-align:center\">0.6263</td>\n</tr>\n<tr>\n<td style=\"text-align:center\"><strong>mAP</strong></td>\n<td style=\"text-align:center\"><strong>0.4472</strong></td>\n<td style=\"text-align:center\"><strong>0.3874</strong></td>\n<td style=\"text-align:center\"><strong>0.4341</strong></td>\n<td style=\"text-align:center\"><strong>0.3859</strong></td>\n<td style=\"text-align:center\"><strong>0.4279</strong></td>\n<td style=\"text-align:center\"><strong>0.4141</strong></td>\n<td style=\"text-align:center\"><strong>0.4096</strong></td>\n<td style=\"text-align:center\"><strong>0.4022</strong></td>\n<td style=\"text-align:center\"><strong>0.4045</strong></td>\n</tr>\n</tbody>\n</table>\n<hr>\n<h5 id=\"Project1-epoch-lenght-1000-epoch-1041-7-models\"><a href=\"#Project1-epoch-lenght-1000-epoch-1041-7-models\" class=\"headerlink\" title=\"Project1 epoch_lenght=1000, epoch:1041 : 7 models:\"></a>Project1 epoch_lenght=1000, epoch:1041 : 7 models:</h5><p>ALL WITH THRESHOLD MOST 0.51</p>\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">Classes</th>\n<th style=\"text-align:center\">ALL_1</th>\n<th style=\"text-align:center\">ALL_2</th>\n<th style=\"text-align:center\">ALL_3</th>\n<th style=\"text-align:center\">ALL_4</th>\n<th style=\"text-align:center\">ALL_5</th>\n<th style=\"text-align:center\">ALL_6</th>\n<th style=\"text-align:center\">ALL_7</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">motorbike</td>\n<td style=\"text-align:center\">0.2433</td>\n<td style=\"text-align:center\">0.2128</td>\n<td style=\"text-align:center\">0.2232</td>\n<td style=\"text-align:center\">0.2262</td>\n<td style=\"text-align:center\">0.2286</td>\n<td style=\"text-align:center\">0.2393</td>\n<td style=\"text-align:center\">0.2279</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">person</td>\n<td style=\"text-align:center\">0.6560</td>\n<td style=\"text-align:center\">0.6537</td>\n<td style=\"text-align:center\">0.6742</td>\n<td style=\"text-align:center\">0.6952</td>\n<td style=\"text-align:center\">0.6852</td>\n<td style=\"text-align:center\">0.6719</td>\n<td style=\"text-align:center\">0.6636</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">car</td>\n<td style=\"text-align:center\">0.1562</td>\n<td style=\"text-align:center\">0.1905</td>\n<td style=\"text-align:center\">0.1479</td>\n<td style=\"text-align:center\">0.2024</td>\n<td style=\"text-align:center\">0.2010</td>\n<td style=\"text-align:center\">0.1379</td>\n<td style=\"text-align:center\">0.1583</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">aeroplane</td>\n<td style=\"text-align:center\">0.7359</td>\n<td style=\"text-align:center\">0.6837</td>\n<td style=\"text-align:center\">0.6729</td>\n<td style=\"text-align:center\">0.6687</td>\n<td style=\"text-align:center\">0.6957</td>\n<td style=\"text-align:center\">0.7339</td>\n<td style=\"text-align:center\">0.6391</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">bottle</td>\n<td style=\"text-align:center\">0.1913</td>\n<td style=\"text-align:center\">0.1937</td>\n<td style=\"text-align:center\">0.2635</td>\n<td style=\"text-align:center\">0.1843</td>\n<td style=\"text-align:center\">0.2570</td>\n<td style=\"text-align:center\">0.1632</td>\n<td style=\"text-align:center\">0.1863</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">sheep</td>\n<td style=\"text-align:center\">0.5429</td>\n<td style=\"text-align:center\">0.5579</td>\n<td style=\"text-align:center\">0.6219</td>\n<td style=\"text-align:center\">0.5355</td>\n<td style=\"text-align:center\">0.5881</td>\n<td style=\"text-align:center\">0.5441</td>\n<td style=\"text-align:center\">0.5824</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">tvmonitor</td>\n<td style=\"text-align:center\">0.1295</td>\n<td style=\"text-align:center\">0.1601</td>\n<td style=\"text-align:center\">0.1368</td>\n<td style=\"text-align:center\">0.1407</td>\n<td style=\"text-align:center\">0.1147</td>\n<td style=\"text-align:center\">0.1349</td>\n<td style=\"text-align:center\">0.1154</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">boat</td>\n<td style=\"text-align:center\">0.1913</td>\n<td style=\"text-align:center\">0.2880</td>\n<td style=\"text-align:center\">0.2635</td>\n<td style=\"text-align:center\">0.3433</td>\n<td style=\"text-align:center\">0.3335</td>\n<td style=\"text-align:center\">0.3422</td>\n<td style=\"text-align:center\">0.3069</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">chair</td>\n<td style=\"text-align:center\">0.0587</td>\n<td style=\"text-align:center\">0.0657</td>\n<td style=\"text-align:center\">0.0342</td>\n<td style=\"text-align:center\">0.0680</td>\n<td style=\"text-align:center\">0.0695</td>\n<td style=\"text-align:center\">0.0752</td>\n<td style=\"text-align:center\">0.0760</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">bicycle</td>\n<td style=\"text-align:center\">0.1013</td>\n<td style=\"text-align:center\">0.1485</td>\n<td style=\"text-align:center\">0.1225</td>\n<td style=\"text-align:center\">0.1871</td>\n<td style=\"text-align:center\">0.1685</td>\n<td style=\"text-align:center\">0.1037</td>\n<td style=\"text-align:center\">0.1490</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">cat</td>\n<td style=\"text-align:center\">0.8737</td>\n<td style=\"text-align:center\">0.8557</td>\n<td style=\"text-align:center\">0.8007</td>\n<td style=\"text-align:center\">0.7982</td>\n<td style=\"text-align:center\">0.8045</td>\n<td style=\"text-align:center\">0.8067</td>\n<td style=\"text-align:center\">0.7732</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">pottedplant</td>\n<td style=\"text-align:center\">0.0694</td>\n<td style=\"text-align:center\">0.1059</td>\n<td style=\"text-align:center\">0.0748</td>\n<td style=\"text-align:center\">0.0878</td>\n<td style=\"text-align:center\">0.0893</td>\n<td style=\"text-align:center\">0.0690</td>\n<td style=\"text-align:center\">0.0865</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">horse</td>\n<td style=\"text-align:center\">0.0556</td>\n<td style=\"text-align:center\">0.0561</td>\n<td style=\"text-align:center\">0.0581</td>\n<td style=\"text-align:center\">0.0770</td>\n<td style=\"text-align:center\">0.0575</td>\n<td style=\"text-align:center\">0.0539</td>\n<td style=\"text-align:center\">0.0522</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">sofa</td>\n<td style=\"text-align:center\">0.2177</td>\n<td style=\"text-align:center\">0.2917</td>\n<td style=\"text-align:center\">0.1699</td>\n<td style=\"text-align:center\">0.1940</td>\n<td style=\"text-align:center\">0.3177</td>\n<td style=\"text-align:center\">0.1863</td>\n<td style=\"text-align:center\">0.1857</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">dog</td>\n<td style=\"text-align:center\">0.6269</td>\n<td style=\"text-align:center\">0.4989</td>\n<td style=\"text-align:center\">0.5015</td>\n<td style=\"text-align:center\">0.5333</td>\n<td style=\"text-align:center\">0.4914</td>\n<td style=\"text-align:center\">0.5572</td>\n<td style=\"text-align:center\">0.4747</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">cow</td>\n<td style=\"text-align:center\">0.5216</td>\n<td style=\"text-align:center\">0.6229</td>\n<td style=\"text-align:center\">0.5283</td>\n<td style=\"text-align:center\">0.6426</td>\n<td style=\"text-align:center\">0.4358</td>\n<td style=\"text-align:center\">0.4227</td>\n<td style=\"text-align:center\">0.4589</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">diningtable</td>\n<td style=\"text-align:center\">0.3076</td>\n<td style=\"text-align:center\">0.3889</td>\n<td style=\"text-align:center\">0.3283</td>\n<td style=\"text-align:center\">0.2404</td>\n<td style=\"text-align:center\">0.4219</td>\n<td style=\"text-align:center\">0.4153</td>\n<td style=\"text-align:center\">0.2627</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">bus</td>\n<td style=\"text-align:center\">0.5865</td>\n<td style=\"text-align:center\">0.5222</td>\n<td style=\"text-align:center\">0.6312</td>\n<td style=\"text-align:center\">0.5853</td>\n<td style=\"text-align:center\">0.5042</td>\n<td style=\"text-align:center\">0.4882</td>\n<td style=\"text-align:center\">0.5576</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">bird</td>\n<td style=\"text-align:center\">0.5339</td>\n<td style=\"text-align:center\">0.5039</td>\n<td style=\"text-align:center\">0.5150</td>\n<td style=\"text-align:center\">0.5152</td>\n<td style=\"text-align:center\">0.5838</td>\n<td style=\"text-align:center\">0.3890</td>\n<td style=\"text-align:center\">0.4680</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">train</td>\n<td style=\"text-align:center\">0.4994</td>\n<td style=\"text-align:center\">0.6541</td>\n<td style=\"text-align:center\">0.6702</td>\n<td style=\"text-align:center\">0.6920</td>\n<td style=\"text-align:center\">0.5959</td>\n<td style=\"text-align:center\">0.5893</td>\n<td style=\"text-align:center\">0.6861</td>\n</tr>\n<tr>\n<td style=\"text-align:center\"><strong>mAP</strong></td>\n<td style=\"text-align:center\"><strong>0.3699</strong></td>\n<td style=\"text-align:center\"><strong>0.3765</strong></td>\n<td style=\"text-align:center\"><strong>0.3748</strong></td>\n<td style=\"text-align:center\"><strong>0.3814</strong></td>\n<td style=\"text-align:center\"><strong>0.3786</strong></td>\n<td style=\"text-align:center\"><strong>0.3562</strong></td>\n<td style=\"text-align:center\"><strong>0.3555</strong></td>\n</tr>\n</tbody>\n</table>\n<p>classap</p>\n<p><strong>VOC2007</strong><br>VOC2007OpenCVBUG<br><br>VOC2012VOC2007Irius</p>\n<p>20AP10002Wclass_balance</p>\n<p>class balance</p>\n<p>imagenet</p>\n<p></p>\n<h3 id=\"13-08-2018\"><a href=\"#13-08-2018\" class=\"headerlink\" title=\"13/08/2018\"></a>13/08/2018</h3><h4 id=\"soft-NMS\"><a href=\"#soft-NMS\" class=\"headerlink\" title=\"soft-NMS\"></a>soft-NMS</h4><p>BSMBDBMNtaverage precision, AP</p>\n<p>M<br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/018_1.JPG\" alt=\"image\"><br>NMSSoft-NMSNMS<strong>MM</strong>PASCAL VOC  MS-COCOSoft-NMSSoft-NMS</p>\n<p><br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/018_2.JPG\" alt=\"image\"></p>\n<p><br>NMS<br>$$ s_{i}=\\left{<br>\\begin{aligned}<br>s_{i}, \\ \\ \\ \\ iou(M,b_{i}) &lt;  N_{t} \\<br>0, \\ \\ \\ \\ iou(M,b_{i}) \\geq  N_{t}<br>\\end{aligned}<br>\\right.<br>$$</p>\n<p>SOFT NMS<br>$$ s_{i}=\\left{<br>\\begin{aligned}<br>s_{i}, \\ \\ \\ \\ iou(M,b_{i}) &lt;  N_{t} \\<br>1-iou(M,b_{i}), \\ \\ \\ \\ iou(M,b_{i}) \\geq  N_{t}<br>\\end{aligned}<br>\\right.<br>$$</p>\n<p>MNtMM</p>\n<p>Ntsoft-NMS</p>\n<p>Gaussian penalty:<br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/018_3.JPG\" alt=\"image\"></p>\n<p><br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br><span class=\"line\">107</span><br><span class=\"line\">108</span><br><span class=\"line\">109</span><br><span class=\"line\">110</span><br><span class=\"line\">111</span><br><span class=\"line\">112</span><br><span class=\"line\">113</span><br><span class=\"line\">114</span><br><span class=\"line\">115</span><br><span class=\"line\">116</span><br><span class=\"line\">117</span><br><span class=\"line\">118</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"string\">\"\"\" NMS , delete overlapping box</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">@param boxes: (n,4) box and coresspoding cordinates</span></span><br><span class=\"line\"><span class=\"string\">@param probs: (n,) box adn coresspding possibility</span></span><br><span class=\"line\"><span class=\"string\">@param overlap_thresh: treshold of delet box overlapping</span></span><br><span class=\"line\"><span class=\"string\">@param max_boxes: maximum keeping number of boxes</span></span><br><span class=\"line\"><span class=\"string\">@param method: 1 for linear soft NMS, 2 for gaussian soft NMS</span></span><br><span class=\"line\"><span class=\"string\">@param sigma: parameter of gaussian soft NMS</span></span><br><span class=\"line\"><span class=\"string\">prob_thresh: threshold of probs after soft NMS</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">@return: boxes: boxes cordinates(x1,y1,x2,y2)</span></span><br><span class=\"line\"><span class=\"string\">@return: probs: coresspoding possibility</span></span><br><span class=\"line\"><span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">soft_nms</span><span class=\"params\">(boxes, probs, overlap_thresh=<span class=\"number\">0.9</span>, max_boxes=<span class=\"number\">300</span>, method = <span class=\"number\">1</span>, sigma=<span class=\"number\">0.5</span>, prob_thresh=<span class=\"number\">0.49</span>)</span>:</span></span><br><span class=\"line\">    <span class=\"comment\"># number of input boxes</span></span><br><span class=\"line\">    N = boxes.shape[<span class=\"number\">0</span>]</span><br><span class=\"line\">    <span class=\"comment\"># if the bounding boxes integers, convert them to floats --</span></span><br><span class=\"line\">    <span class=\"comment\"># this is important since we'll be doing a bunch of divisions</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> boxes.dtype.kind == <span class=\"string\">\"i\"</span>:</span><br><span class=\"line\">        boxes = boxes.astype(<span class=\"string\">\"float\"</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># iterate all boxes</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(N):</span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"comment\"># obtain current boxes' cordinates and probs</span></span><br><span class=\"line\">        maxscore = probs[i]</span><br><span class=\"line\">        maxpos = i</span><br><span class=\"line\"></span><br><span class=\"line\">        tx1 = boxes[i,<span class=\"number\">0</span>]</span><br><span class=\"line\">        ty1 = boxes[i,<span class=\"number\">1</span>]</span><br><span class=\"line\">        tx2 = boxes[i,<span class=\"number\">2</span>]</span><br><span class=\"line\">        ty2 = boxes[i,<span class=\"number\">3</span>]</span><br><span class=\"line\">        ts = probs[i]</span><br><span class=\"line\"></span><br><span class=\"line\">        pos = i + <span class=\"number\">1</span></span><br><span class=\"line\">        <span class=\"comment\"># get max box</span></span><br><span class=\"line\">        <span class=\"keyword\">while</span> pos &lt; N:</span><br><span class=\"line\">            <span class=\"keyword\">if</span> maxscore &lt; probs[pos]:</span><br><span class=\"line\">                maxscore = probs[pos]</span><br><span class=\"line\">                maxpos = pos</span><br><span class=\"line\">            pos = pos + <span class=\"number\">1</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># add max box as a detection </span></span><br><span class=\"line\">        boxes[i,<span class=\"number\">0</span>] = boxes[maxpos,<span class=\"number\">0</span>]</span><br><span class=\"line\">        boxes[i,<span class=\"number\">1</span>] = boxes[maxpos,<span class=\"number\">1</span>]</span><br><span class=\"line\">        boxes[i,<span class=\"number\">2</span>] = boxes[maxpos,<span class=\"number\">2</span>]</span><br><span class=\"line\">        boxes[i,<span class=\"number\">3</span>] = boxes[maxpos,<span class=\"number\">3</span>]</span><br><span class=\"line\">        probs[i] = probs[maxpos]</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># swap ith box with position of max box</span></span><br><span class=\"line\">        boxes[maxpos,<span class=\"number\">0</span>] = tx1</span><br><span class=\"line\">        boxes[maxpos,<span class=\"number\">1</span>] = ty1</span><br><span class=\"line\">        boxes[maxpos,<span class=\"number\">2</span>] = tx2</span><br><span class=\"line\">        boxes[maxpos,<span class=\"number\">3</span>] = ty2</span><br><span class=\"line\">        probs[maxpos] = ts</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># cordinates of max box</span></span><br><span class=\"line\">        tx1 = boxes[i,<span class=\"number\">0</span>]</span><br><span class=\"line\">        ty1 = boxes[i,<span class=\"number\">1</span>]</span><br><span class=\"line\">        tx2 = boxes[i,<span class=\"number\">2</span>]</span><br><span class=\"line\">        ty2 = boxes[i,<span class=\"number\">3</span>]</span><br><span class=\"line\">        ts = probs[i]</span><br><span class=\"line\"></span><br><span class=\"line\">        pos = i + <span class=\"number\">1</span></span><br><span class=\"line\">        <span class=\"comment\"># NMS iterations, note that N changes if detection boxes fall below threshold</span></span><br><span class=\"line\">        <span class=\"keyword\">while</span> pos &lt; N:</span><br><span class=\"line\">            x1 = boxes[pos, <span class=\"number\">0</span>]</span><br><span class=\"line\">            y1 = boxes[pos, <span class=\"number\">1</span>]</span><br><span class=\"line\">            x2 = boxes[pos, <span class=\"number\">2</span>]</span><br><span class=\"line\">            y2 = boxes[pos, <span class=\"number\">3</span>]</span><br><span class=\"line\">            s = probs[pos]</span><br><span class=\"line\">            </span><br><span class=\"line\">            <span class=\"comment\"># calculate the areas, +1 for robatness</span></span><br><span class=\"line\">            area = (x2 - x1 + <span class=\"number\">1</span>) * (y2 - y1 + <span class=\"number\">1</span>)</span><br><span class=\"line\">            iw = (min(tx2, x2) - max(tx1, x1) + <span class=\"number\">1</span>)</span><br><span class=\"line\">            <span class=\"comment\"># # confirm left top cordinates less than top right</span></span><br><span class=\"line\">            <span class=\"keyword\">if</span> iw &gt; <span class=\"number\">0</span>:</span><br><span class=\"line\">                ih = (min(ty2, y2) - max(ty1, y1) + <span class=\"number\">1</span>)</span><br><span class=\"line\">                <span class=\"comment\"># confirm left top cordinates less than top right</span></span><br><span class=\"line\">                <span class=\"keyword\">if</span> ih &gt; <span class=\"number\">0</span>:</span><br><span class=\"line\">                    <span class=\"comment\"># find the union</span></span><br><span class=\"line\">                    ua = float((tx2 - tx1 + <span class=\"number\">1</span>) * (ty2 - ty1 + <span class=\"number\">1</span>) + area - iw * ih)</span><br><span class=\"line\">                    <span class=\"comment\">#iou between max box and detection box</span></span><br><span class=\"line\">                    ov = iw * ih / ua</span><br><span class=\"line\"></span><br><span class=\"line\">                    <span class=\"keyword\">if</span> method == <span class=\"number\">1</span>: <span class=\"comment\"># linear</span></span><br><span class=\"line\">                        <span class=\"keyword\">if</span> ov &gt; overlap_thresh: </span><br><span class=\"line\">                            weight = <span class=\"number\">1</span> - ov</span><br><span class=\"line\">                        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">                            weight = <span class=\"number\">1</span></span><br><span class=\"line\">                    <span class=\"keyword\">elif</span> method == <span class=\"number\">2</span>: <span class=\"comment\"># gaussian</span></span><br><span class=\"line\">                        weight = np.exp(-(ov * ov)/sigma)</span><br><span class=\"line\">                    <span class=\"keyword\">else</span>: <span class=\"comment\"># original NMS</span></span><br><span class=\"line\">                        <span class=\"keyword\">if</span> ov &gt; overlap_thresh: </span><br><span class=\"line\">                            weight = <span class=\"number\">0</span></span><br><span class=\"line\">                        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">                            weight = <span class=\"number\">1</span></span><br><span class=\"line\"></span><br><span class=\"line\">                    <span class=\"comment\"># obtain adjusted probs</span></span><br><span class=\"line\">                    probs[pos] = weight*probs[pos]</span><br><span class=\"line\"></span><br><span class=\"line\">   </span><br><span class=\"line\">                    <span class=\"comment\"># if box score falls below threshold, discard the box by swapping with last box</span></span><br><span class=\"line\">                    <span class=\"comment\"># update N</span></span><br><span class=\"line\">                    <span class=\"keyword\">if</span> probs[pos] &lt; prob_thresh:</span><br><span class=\"line\">                        boxes[pos,<span class=\"number\">0</span>] = boxes[N<span class=\"number\">-1</span>, <span class=\"number\">0</span>]</span><br><span class=\"line\">                        boxes[pos,<span class=\"number\">1</span>] = boxes[N<span class=\"number\">-1</span>, <span class=\"number\">1</span>]</span><br><span class=\"line\">                        boxes[pos,<span class=\"number\">2</span>] = boxes[N<span class=\"number\">-1</span>, <span class=\"number\">2</span>]</span><br><span class=\"line\">                        boxes[pos,<span class=\"number\">3</span>] = boxes[N<span class=\"number\">-1</span>, <span class=\"number\">3</span>]</span><br><span class=\"line\">                        probs[pos] = probs[N<span class=\"number\">-1</span>]</span><br><span class=\"line\">                        N = N - <span class=\"number\">1</span></span><br><span class=\"line\">                        pos = pos - <span class=\"number\">1</span></span><br><span class=\"line\"></span><br><span class=\"line\">            pos = pos + <span class=\"number\">1</span></span><br><span class=\"line\">    <span class=\"comment\"># keep is the idx of current keeping objects, front ith objectes</span></span><br><span class=\"line\">    keep = [i <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(N)]</span><br><span class=\"line\">    <span class=\"keyword\">return</span> boxes[keep], probs[keep]</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"14-08-2018\"><a href=\"#14-08-2018\" class=\"headerlink\" title=\"14/08/2018\"></a>14/08/2018</h3><h4 id=\"OVERLAPPING-OBJECT-DETECTION\"><a href=\"#OVERLAPPING-OBJECT-DETECTION\" class=\"headerlink\" title=\"OVERLAPPING OBJECT DETECTION\"></a>OVERLAPPING OBJECT DETECTION</h4><h3 id=\"15-08-2018\"><a href=\"#15-08-2018\" class=\"headerlink\" title=\"15/08/2018\"></a>15/08/2018</h3><h3 id=\"16-08-2018\"><a href=\"#16-08-2018\" class=\"headerlink\" title=\"16/08/2018\"></a>16/08/2018</h3><h3 id=\"17-08-2018\"><a href=\"#17-08-2018\" class=\"headerlink\" title=\"17/08/2018\"></a>17/08/2018</h3><h2 id=\"September\"><a href=\"#September\" class=\"headerlink\" title=\"September\"></a>September</h2>","site":{"data":{}},"excerpt":"<h2 id=\"Gantt-chart\"><a href=\"#Gantt-chart\" class=\"headerlink\" title=\"Gantt chart\"></a>Gantt chart</h2><p><img src=\"https://raw.githubusercontent.com/Trouble404/Object-Detection-System-Based-on-CNN-and-Capsule-Network/master/Project%20Plan/gantt%20chart%20of%20project.PNG\" alt=\"image\"></p>","more":"<hr>\n<h2 id=\"Check-list\"><a href=\"#Check-list\" class=\"headerlink\" title=\"Check list\"></a>Check list</h2><ul>\n<li style=\"list-style: none\"><input type=\"checkbox\" checked> <strong>1) preparation</strong></li>\n<li style=\"list-style: none\"><input type=\"checkbox\" checked> <strong><em>1.1) Familiarization with develop tools</em></strong></li>\n<li style=\"list-style: none\"><input type=\"checkbox\" checked> 1.1.1) Keras</li>\n<li style=\"list-style: none\"><input type=\"checkbox\" checked> 1.1.2) Pythrch</li>\n<li style=\"list-style: none\"><input type=\"checkbox\" checked> <strong><em>1.2) Presentation</em></strong></li>\n<li style=\"list-style: none\"><input type=\"checkbox\" checked> 1.2.1) Poster conference</li>\n<li style=\"list-style: none\"><input type=\"checkbox\" checked> <strong>2) Create image database</strong></li>\n<li style=\"list-style: none\"><input type=\"checkbox\" checked> 2.1) Confirmation of detected objects</li>\n<li style=\"list-style: none\"><input type=\"checkbox\" checked> 2.2) Collect and generate the dataset</li>\n<li style=\"list-style: none\"><input type=\"checkbox\" checked> <strong>3) Familiarization with CNN based object detection methods</strong></li>\n<li style=\"list-style: none\"><input type=\"checkbox\" checked> 3.1) R-CNN</li>\n<li style=\"list-style: none\"><input type=\"checkbox\" checked> 3.2) SPP-net</li>\n<li style=\"list-style: none\"><input type=\"checkbox\" checked> 3.3) Fast R-CNN</li>\n<li style=\"list-style: none\"><input type=\"checkbox\" checked> 3.4) Faster R-CNN</li>\n<li style=\"list-style: none\"><input type=\"checkbox\" checked> <strong>4) Implement object detection system based on one chosen CNN method</strong></li>\n<li style=\"list-style: none\"><input type=\"checkbox\" checked> 4.1) Pre-processing of images</li>\n<li style=\"list-style: none\"><input type=\"checkbox\" checked> 4.2) Extracting features</li>\n<li style=\"list-style: none\"><input type=\"checkbox\" checked> 4.3) Mode architecture</li>\n<li style=\"list-style: none\"><input type=\"checkbox\" checked> 4.4) Train model and optimization</li>\n<li style=\"list-style: none\"><input type=\"checkbox\" checked> 4.5) Models ensemble</li>\n<li style=\"list-style: none\"><input type=\"checkbox\" checked> <strong>5) Analysis work</strong></li>\n<li style=\"list-style: none\"><input type=\"checkbox\" checked> 5.1) Evaluation of detection result.</li>\n<li style=\"list-style: none\"><input type=\"checkbox\" checked> <strong>6) Paperwork and bench inspection</strong></li>\n<li style=\"list-style: none\"><input type=\"checkbox\" checked> 6.1) Logbook</li>\n<li style=\"list-style: none\"><input type=\"checkbox\" checked> 6.2) Write the thesis</li>\n<li style=\"list-style: none\"><input type=\"checkbox\" checked> 6.3) Project video</li>\n<li style=\"list-style: none\"><input type=\"checkbox\" checked> 6.4) Speech and ppt of bench inspection</li>\n<li style=\"list-style: none\"><input type=\"checkbox\" checked> <strong>7) Documents</strong></li>\n<li style=\"list-style: none\"><input type=\"checkbox\" checked> 7.1) Project Brief</li>\n</ul>\n<hr>\n<h2 id=\"May\"><a href=\"#May\" class=\"headerlink\" title=\"May\"></a>May</h2><h3 id=\"28-05-2018\"><a href=\"#28-05-2018\" class=\"headerlink\" title=\"28/05/2018\"></a>28/05/2018</h3><p>Keras is a high-level neural networks API, written in Python and capable of running on top of <a href=\"https://github.com/tensorflow/tensorflow\" target=\"_blank\" rel=\"noopener\">TensorFlow</a>, CNTK, or Theano.</p>\n<ul>\n<li><p><strong><a href=\"https://keras.io/\" target=\"_blank\" rel=\"noopener\">Keras document</a></strong></p>\n</li>\n<li><p><strong><a href=\"https://keras-cn.readthedocs.io/en/latest/#keraspython\" target=\"_blank\" rel=\"noopener\">Keras </a></strong></p>\n</li>\n</ul>\n<hr>\n<h4 id=\"Installation\"><a href=\"#Installation\" class=\"headerlink\" title=\"Installation\"></a>Installation</h4><ul>\n<li><p><strong>TensorFlow</strong><br><a href=\"https://www.visualstudio.com/zh-hans/vs/older-downloads/\" target=\"_blank\" rel=\"noopener\">Microsoft Visual Studio 2015</a><br><a href=\"http://docs.nvidia.com/cuda/cuda-installation-guide-microsoft-windows/\" target=\"_blank\" rel=\"noopener\">CUDA 9.0</a><br><a href=\"https://developer.nvidia.com/cudnn\" target=\"_blank\" rel=\"noopener\">cuDNN7</a><br><a href=\"https://www.anaconda.com/download/\" target=\"_blank\" rel=\"noopener\">Anaconda</a></p>\n<ul>\n<li>Step 1: Install VS2015</li>\n<li>Step 2: Install CUDA 9.0 </li>\n<li>Step 3: Install cuDNN7 cudnnbinPATH</li>\n<li>Step 4: Install Anaconda PATH,  <strong>Python 3.5</strong></li>\n<li>Step 5: Anaconda ,jupyterbook<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">conda create  --name tensorflow python=3.5</span><br><span class=\"line\">activate tensorflow</span><br><span class=\"line\">conda install nb_conda</span><br></pre></td></tr></table></figure></li>\n<li>Step 6: Install GPU version TensorFlow.<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">pip install -i https://pypi.tuna.tsinghua.edu.cn/simple --ignore-installed --upgrade tensorflow-gpu </span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n</li>\n<li><p><strong>Keras</strong></p>\n<ul>\n<li>Step 1:   Keras GPU <figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">activate tensorflow</span><br><span class=\"line\">pip install keras -U --pre</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n</li>\n</ul>\n<h4 id=\"Keras\"><a href=\"#Keras\" class=\"headerlink\" title=\"Keras**\"></a>Keras**</h4><ul>\n<li><strong><a href=\"https://github.com/Trouble404/NBA-with-Machine-Learning\" target=\"_blank\" rel=\"noopener\">NBA with Machine Learning</a></strong></li>\n<li><strong><a href=\"https://github.com/Trouble404/kaggle-Job-Salary-Prediction\" target=\"_blank\" rel=\"noopener\">Kaggle- Job salary prediction</a></strong></li>\n</ul>\n<h4 id=\"TensorFlow-CPU-\"><a href=\"#TensorFlow-CPU-\" class=\"headerlink\" title=\"TensorFlow CPU \"></a>TensorFlow CPU </h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> tensorflow <span class=\"keyword\">as</span> tf  </span><br><span class=\"line\"><span class=\"keyword\">import</span> os</span><br><span class=\"line\"><span class=\"keyword\">import</span> keras.backend.tensorflow_backend <span class=\"keyword\">as</span> KTF  </span><br><span class=\"line\"></span><br><span class=\"line\">os.environ[<span class=\"string\">\"CUDA_VISIBLE_DEVICES\"</span>] = <span class=\"string\">\"0\"</span>  <span class=\"comment\">#GPU</span></span><br><span class=\"line\">config = tf.ConfigProto()</span><br><span class=\"line\">config.gpu_options.per_process_gpu_memory_fraction = <span class=\"number\">0.4</span> <span class=\"comment\">#GPUGPU</span></span><br><span class=\"line\">sess = tf.Session(config=config)</span><br><span class=\"line\">KTF.set_session(sess)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">with</span> tf.device(<span class=\"string\">'/cpu:0'</span>):</span><br></pre></td></tr></table></figure>\n<p>GPUCPU</p>\n<h4 id=\"Jupyter-Notebook-\"><a href=\"#Jupyter-Notebook-\" class=\"headerlink\" title=\"Jupyter Notebook \"></a>Jupyter Notebook </h4><p> <br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">jupyter notebook --generate-config</span><br></pre></td></tr></table></figure></p>\n<p><img src=\"https://raw.githubusercontent.com/Trouble404/Object-Detection-System-Based-on-CNN-and-Capsule-Network/master/Learned%20skills/jupyter%20_setting.PNG\" alt=\"image\"></p>\n<h2 id=\"June\"><a href=\"#June\" class=\"headerlink\" title=\"June\"></a>June</h2><h3 id=\"01-06-2018\"><a href=\"#01-06-2018\" class=\"headerlink\" title=\"01/06/2018\"></a>01/06/2018</h3><p><strong><a href=\"https://pytorch.org/about/\" target=\"_blank\" rel=\"noopener\">PyTorch</a></strong> is a python package that provides two high-level features:</p>\n<ul>\n<li>Tensor computation (like numpy) with strong GPU acceleration</li>\n<li>Deep Neural Networks built on a tape-based autodiff system</li>\n</ul>\n<table>\n<thead>\n<tr>\n<th style=\"text-align:left\">Package</th>\n<th style=\"text-align:left\">Description</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:left\">torch</td>\n<td style=\"text-align:left\">a Tensor library like NumPy, with strong GPU support</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">torch.autograd</td>\n<td style=\"text-align:left\">a tape based automatic differentiation library that supports all differentiable Tensor operations in torch</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">torch.nn</td>\n<td style=\"text-align:left\">a neural networks library deeply integrated with autograd designed for maximum flexibility</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">torch.optim</td>\n<td style=\"text-align:left\">an optimization package to be used with torch.nn with standard optimization methods such as SGD, RMSProp, LBFGS, Adam etc.</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">torch.multiprocessing</td>\n<td style=\"text-align:left\">python multiprocessing, but with magical memory sharing of torch Tensors across processes. Useful for data loading and hogwild training.</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">torch.utils</td>\n<td style=\"text-align:left\">DataLoader, Trainer and other utility functions for convenience</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">torch.legacy(.nn/.optim)</td>\n<td style=\"text-align:left\">legacy code that has been ported over from torch for backward compatibility reasons</td>\n</tr>\n</tbody>\n</table>\n<hr>\n<h4 id=\"Installation-1\"><a href=\"#Installation-1\" class=\"headerlink\" title=\"Installation\"></a>Installation</h4><ul>\n<li>Step 1: Anaconda ,jupyterbook<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">conda create  --name pytorch python=3.5</span><br><span class=\"line\">activate pytorch</span><br><span class=\"line\">conda install nb_conda</span><br></pre></td></tr></table></figure></li>\n<li>Step 2: Install GPU version PyTorch.<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">conda install pytorch cuda90 -c pytorch </span><br><span class=\"line\">pip install torchvision</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<h4 id=\"Understanding-of-PyTorch\"><a href=\"#Understanding-of-PyTorch\" class=\"headerlink\" title=\"Understanding of PyTorch\"></a>Understanding of PyTorch</h4><ul>\n<li><p><strong>Tensors</strong><br>Tensorsnumpyndarrays, TensorGPU</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> __future__ <span class=\"keyword\">import</span> print_function</span><br><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\">x = torch.Tensor(<span class=\"number\">5</span>, <span class=\"number\">3</span>)  <span class=\"comment\"># 5*3</span></span><br><span class=\"line\">x = torch.rand(<span class=\"number\">5</span>, <span class=\"number\">3</span>)  <span class=\"comment\"># </span></span><br><span class=\"line\">x <span class=\"comment\"># notebookxx</span></span><br><span class=\"line\">x.size()</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#<span class=\"doctag\">NOTE:</span> torch.Size tuple, *</span></span><br><span class=\"line\">y = torch.rand(<span class=\"number\">5</span>, <span class=\"number\">3</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\">x + y <span class=\"comment\"># </span></span><br><span class=\"line\">torch.add(x, y) <span class=\"comment\"># </span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># tensor</span></span><br><span class=\"line\">result = torch.Tensor(<span class=\"number\">5</span>, <span class=\"number\">3</span>) <span class=\"comment\"># </span></span><br><span class=\"line\">torch.add(x, y, out=result) <span class=\"comment\"># </span></span><br><span class=\"line\">y.add_(x) <span class=\"comment\"># yx</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># tensor'_'</span></span><br><span class=\"line\"><span class=\"comment\"># x.copy_(y), x.t_(), x</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#python</span></span><br><span class=\"line\">x[:,<span class=\"number\">1</span>] <span class=\"comment\">#x</span></span><br></pre></td></tr></table></figure>\n</li>\n<li><p><strong>Numpy</strong><br>TorchTensornumpyarrayTorchTensornumpyarray</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># tensornumpy</span></span><br><span class=\"line\">a = torch.ones(<span class=\"number\">5</span>)</span><br><span class=\"line\">b = a.numpy()</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># numpy,tensor</span></span><br><span class=\"line\">a.add_(<span class=\"number\">1</span>)</span><br><span class=\"line\">print(a)</span><br><span class=\"line\">print(b)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># numpyArraytorchTensor</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\">a = np.ones(<span class=\"number\">5</span>)</span><br><span class=\"line\">b = torch.from_numpy(a)</span><br><span class=\"line\">np.add(a, <span class=\"number\">1</span>, out=a)</span><br><span class=\"line\">print(a)</span><br><span class=\"line\">print(b)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># CharTensortensorCPUGPU</span></span><br><span class=\"line\"><span class=\"comment\"># CUDATensorGPU</span></span><br><span class=\"line\"><span class=\"comment\"># CUDAGPU</span></span><br><span class=\"line\"><span class=\"keyword\">if</span> torch.cuda.is_available():</span><br><span class=\"line\">    x = x.cuda()</span><br><span class=\"line\">    y = y.cuda()</span><br></pre></td></tr></table></figure>\n</li>\n<li><p><strong>PyTorchCIFAR10</strong><br><strong><a href=\"https://github.com/Trouble404/Object-Detection-System-Based-on-CNN-and-Capsule-Network/blob/master/Learned%20skills/pytorch.ipynb\" target=\"_blank\" rel=\"noopener\">code</a></strong></p>\n</li>\n<li><p><strong>MMdnn</strong><br>MMdnn is a set of tools to help users inter-operate among different deep learning frameworks. E.g. model conversion and visualization. Convert models between Caffe, Keras, MXNet, Tensorflow, CNTK, PyTorch Onnx and CoreML.</p>\n<p><img src=\"https://raw.githubusercontent.com/Microsoft/MMdnn/master/docs/supported.jpg\" alt=\"iamge\"></p>\n<p>MMdnn</p>\n<ul>\n<li>DNN</li>\n<li></li>\n<li>DNN</li>\n<li></li>\n</ul>\n<p><strong><a href=\"https://github.com/Microsoft/MMdnn\" target=\"_blank\" rel=\"noopener\">Github</a></strong></p>\n</li>\n</ul>\n<h3 id=\"04-06-2018\"><a href=\"#04-06-2018\" class=\"headerlink\" title=\"04/06/2018\"></a>04/06/2018</h3><h4 id=\"Dataset\"><a href=\"#Dataset\" class=\"headerlink\" title=\"Dataset:\"></a><strong>Dataset:</strong></h4><p> <strong><a href=\"http://host.robots.ox.ac.uk/pascal/VOC/voc2012/index.html\" target=\"_blank\" rel=\"noopener\">VOC 2012 Dataset</a></strong></p>\n<h4 id=\"Introduce\"><a href=\"#Introduce\" class=\"headerlink\" title=\"Introduce:\"></a><strong>Introduce:</strong></h4><p> <strong>Visual Object Classes Challenge 2012 (VOC2012)</strong><br><a href=\"http://host.robots.ox.ac.uk/pascal/VOC/\" target=\"_blank\" rel=\"noopener\">PASCAL</a>s full name is Pattern Analysis, Statistical Modelling and Computational Learning.<br>VOCs full name is <strong>Visual OBject Classes</strong>.<br>The first competition was held in 2005 and terminated in 2012. I will use the last updated dataset which is <a href=\"http://host.robots.ox.ac.uk/pascal/VOC/voc2012/index.html\" target=\"_blank\" rel=\"noopener\">VOC2012</a> dataset.</p>\n<p>The main aim of this competition is object detection, there are 20 classes objects in the dataset:</p>\n<ul>\n<li>person</li>\n<li>bird, cat, cow, dog, horse, sheep</li>\n<li>aeroplane, bicycle, boat, bus, car, motorbike, train</li>\n<li>bottle, chair, dining table, potted plant, sofa, tv/monitor</li>\n</ul>\n<h4 id=\"Detection-Task\"><a href=\"#Detection-Task\" class=\"headerlink\" title=\"Detection Task\"></a><strong>Detection Task</strong></h4><p>Referenced:<br><strong>The PASCAL Visual Object Classes Challenge 2012 (VOC2012) Development Kit</strong><br><strong>Mark Everingham - John Winn</strong><br><a href=\"http://host.robots.ox.ac.uk/pascal/VOC/voc2012/htmldoc/index.html\" target=\"_blank\" rel=\"noopener\">http://host.robots.ox.ac.uk/pascal/VOC/voc2012/htmldoc/index.html</a></p>\n<p><strong>Task:</strong><br>For each of the twenty classes predict the bounding boxes of each object of that class in a test image (if any). Each bounding box should be output with an associated real-valued confidence of the detection so that a precision/recall curve can be drawn. Participants may choose to tackle all, or any subset of object classes, for example cars only or motorbikes and cars.</p>\n<p><strong>Competitions</strong>:<br>Two competitions are defined according to the choice of training data:</p>\n<ul>\n<li>taken from the $VOC_{trainval}$ data provided.</li>\n<li>from any source excluding the $VOC_{test}$ data provided.</li>\n</ul>\n<p><strong>Submission of Results</strong>:<br>A separate text file of results should be generated for each competition and each class e.g. `car. Each line should be a detection output by the detector in the following format:<br>    <figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;image identifier&gt; &lt;confidence&gt; &lt;left&gt; &lt;top&gt; &lt;right&gt; &lt;bottom&gt;</span><br></pre></td></tr></table></figure></p>\n<p>where (left,top)-(right,bottom) defines the bounding box of the detected object. The top-left pixel in the image has coordinates $(1,1)$. Greater confidence values signify greater confidence that the detection is correct. An example file excerpt is shown below. Note that for the image 2009_000032, multiple objects are detected:<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">comp3_det_test_car.txt:</span><br><span class=\"line\">    ...</span><br><span class=\"line\">    2009_000026 0.949297 172.000000 233.000000 191.000000 248.000000</span><br><span class=\"line\">    2009_000032 0.013737 1.000000 147.000000 114.000000 242.000000</span><br><span class=\"line\">    2009_000032 0.013737 1.000000 134.000000 94.000000 168.000000</span><br><span class=\"line\">    2009_000035 0.063948 455.000000 229.000000 491.000000 243.000000</span><br><span class=\"line\">    ...</span><br></pre></td></tr></table></figure></p>\n<p><strong>Evaluation</strong>:<br>The detection task will be judged by the precision/recall curve. The principal quantitative measure used will be the average precision (AP). Detections are considered true or false positives based on the area of overlap with ground truth bounding boxes. To be considered a correct detection, the area of overlap $a_o$ between the predicted bounding box $B_p$ and ground truth bounding box $B_{gt}$ must exceed $50\\%$ by the formula: </p>\n<center> $a_o = \\frac{area(B_p \\cap B_{gt})}{area(B_p \\cup B_{gt})}$ </center>\n\n<h4 id=\"XML\"><a href=\"#XML\" class=\"headerlink\" title=\"XML\"></a><strong>XML</strong></h4><p> xmlgemfield8xml8xmlxml<br> <figure class=\"highlight html\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"> &lt;?xml version=\"1.0\" encoding=\"utf-8\"?&gt;</span><br><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">annotation</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">folder</span>&gt;</span>VOC2007<span class=\"tag\">&lt;/<span class=\"name\">folder</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">filename</span>&gt;</span>test100.mp4_3380.jpeg<span class=\"tag\">&lt;/<span class=\"name\">filename</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">size</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">width</span>&gt;</span>1280<span class=\"tag\">&lt;/<span class=\"name\">width</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">height</span>&gt;</span>720<span class=\"tag\">&lt;/<span class=\"name\">height</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">depth</span>&gt;</span>3<span class=\"tag\">&lt;/<span class=\"name\">depth</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;/<span class=\"name\">size</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">object</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>gemfield<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">bndbox</span>&gt;</span></span><br><span class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">xmin</span>&gt;</span>549<span class=\"tag\">&lt;/<span class=\"name\">xmin</span>&gt;</span></span><br><span class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">xmax</span>&gt;</span>715<span class=\"tag\">&lt;/<span class=\"name\">xmax</span>&gt;</span></span><br><span class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">ymin</span>&gt;</span>257<span class=\"tag\">&lt;/<span class=\"name\">ymin</span>&gt;</span></span><br><span class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">ymax</span>&gt;</span>289<span class=\"tag\">&lt;/<span class=\"name\">ymax</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;/<span class=\"name\">bndbox</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">truncated</span>&gt;</span>0<span class=\"tag\">&lt;/<span class=\"name\">truncated</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">difficult</span>&gt;</span>0<span class=\"tag\">&lt;/<span class=\"name\">difficult</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;/<span class=\"name\">object</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">object</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>civilnet<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">bndbox</span>&gt;</span></span><br><span class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">xmin</span>&gt;</span>842<span class=\"tag\">&lt;/<span class=\"name\">xmin</span>&gt;</span></span><br><span class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">xmax</span>&gt;</span>1009<span class=\"tag\">&lt;/<span class=\"name\">xmax</span>&gt;</span></span><br><span class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">ymin</span>&gt;</span>138<span class=\"tag\">&lt;/<span class=\"name\">ymin</span>&gt;</span></span><br><span class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">ymax</span>&gt;</span>171<span class=\"tag\">&lt;/<span class=\"name\">ymax</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;/<span class=\"name\">bndbox</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">truncated</span>&gt;</span>0<span class=\"tag\">&lt;/<span class=\"name\">truncated</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">difficult</span>&gt;</span>0<span class=\"tag\">&lt;/<span class=\"name\">difficult</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;/<span class=\"name\">object</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">segmented</span>&gt;</span>0<span class=\"tag\">&lt;/<span class=\"name\">segmented</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">annotation</span>&gt;</span></span><br></pre></td></tr></table></figure></p>\n<p>2objectgemfieldcivilnet</p>\n<p>xml</p>\n<ul>\n<li>bndbox</li>\n<li>truncated</li>\n<li>occluded</li>\n<li>difficultdifficult</li>\n</ul>\n<p><strong>objectname SSDpy-faster-rcnn</strong></p>\n<h3 id=\"07-06-2018\"><a href=\"#07-06-2018\" class=\"headerlink\" title=\"07/06/2018\"></a>07/06/2018</h3><h4 id=\"Poster-conference\"><a href=\"#Poster-conference\" class=\"headerlink\" title=\"Poster conference\"></a><strong>Poster conference</strong></h4><p><img src=\"https://raw.githubusercontent.com/Trouble404/Object-Detection-System-Based-on-CNN-and-Capsule-Network/master/Poster/poster.png\" alt=\"iamge\"></p>\n<p>5 People in one group to present their object.<br>I present this object to my supervisor in this conference.</p>\n<h3 id=\"11-06-2018\"><a href=\"#11-06-2018\" class=\"headerlink\" title=\"11/06/2018\"></a>11/06/2018</h3><h4 id=\"R-CNN\"><a href=\"#R-CNN\" class=\"headerlink\" title=\"R-CNN\"></a><strong>R-CNN</strong></h4><p>Paper: <a href=\"https://arxiv.org/abs/1311.2524\" target=\"_blank\" rel=\"noopener\">Rich feature hierarchies for accurate object detection and semantic segmentation</a></p>\n<p><strong></strong></p>\n<ul>\n<li> (Selective Search)(CNN)</li>\n<li>     ImageNet ILSVC 20121000 PASCAL VOC 2007   20 CNN   </li>\n</ul>\n<p><strong></strong></p>\n<ol>\n<li> 1K~2K Selective Search </li>\n<li>  CNN </li>\n<li> SVM </li>\n<li> <center><img src=\"https://raw.githubusercontent.com/Trouble404/Object-Detection-System-Based-on-CNN-and-Capsule-Network/master/Research%20Review/LaTex/1.PNG\" alt=\"image\"></center>\n\n</li>\n</ol>\n<p><strong><a href=\"https://www.koen.me/research/pub/uijlings-ijcv2013-draft.pdf\" target=\"_blank\" rel=\"noopener\">Selective Search</a></strong></p>\n<ol>\n<li> (1k~2k )</li>\n<li></li>\n<li><br> <ul>\n<li></li>\n<li></li>\n<li>  a-b-c-d-e-f-g-hab-cd-ef-gh -&gt; abcd-efgh -&gt; abcdefgh ab-c-d-e-f-g-h -&gt;abcd-e-f-g-h -&gt;abcdef-gh -&gt; abcdefgh</li>\n<li>BBOX </li>\n</ul>\n</li>\n</ol>\n<h3 id=\"12-06-2018\"><a href=\"#12-06-2018\" class=\"headerlink\" title=\"12/06/2018\"></a>12/06/2018</h3><h4 id=\"SPP-CNN\"><a href=\"#SPP-CNN\" class=\"headerlink\" title=\"SPP-CNN\"></a><strong>SPP-CNN</strong></h4><p>Paper: <a href=\"https://arxiv.org/abs/1406.4729\" target=\"_blank\" rel=\"noopener\">Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition</a></p>\n<p><strong></strong></p>\n<p>RCNNCNNRCNN224*224CNN</p>\n<ul>\n<li>region proposalSelective Search2Kregion proposal2KCNN</li>\n<li>region proposal</li>\n</ul>\n<p><strong></strong></p>\n<ol>\n<li>selective searchregion proposal</li>\n<li><br>$s \\in S = {480,576,688,864,1200}$<br>epochmodelmodel1*12*23*36*650bins</li>\n<li>region proposal224*224 </li>\n<li>SVMBoundingBox.<center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/SPP-NET.jpg\" alt=\"image\"></center>\n\n\n</li>\n</ol>\n<h3 id=\"13-06-2018\"><a href=\"#13-06-2018\" class=\"headerlink\" title=\"13/06/2018\"></a>13/06/2018</h3><h4 id=\"FAST-R-CNN\"><a href=\"#FAST-R-CNN\" class=\"headerlink\" title=\"FAST R-CNN\"></a><strong>FAST R-CNN</strong></h4><p>Paper: <a href=\"https://arxiv.org/abs/1504.08083\" target=\"_blank\" rel=\"noopener\">Fast R-CNN</a></p>\n<p><strong></strong></p>\n<ul>\n<li>RCNN</li>\n<li> </li>\n<li>: RCNN</li>\n</ul>\n<p><strong></strong></p>\n<ol>\n<li>convconv</li>\n<li>object proposals region of interestRoI</li>\n<li>fc<br> KbackgroundsoftmaxRoI<br>bbox regressionK4Kbounding-boxtrained end-to-end with a multi-task loss<center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/fast-rcnn.png\" alt=\"image\"></center>\n\n</li>\n</ol>\n<h3 id=\"14-18-06-2018\"><a href=\"#14-18-06-2018\" class=\"headerlink\" title=\"14~18/06/2018\"></a>14~18/06/2018</h3><h4 id=\"FASTER-R-CNN\"><a href=\"#FASTER-R-CNN\" class=\"headerlink\" title=\"FASTER R-CNN\"></a><strong>FASTER R-CNN</strong></h4><p>I want to use <strong>Faster R-cnn</strong> as the first method to implement object detection system.</p>\n<p>Paper: <a href=\"https://arxiv.org/abs/1506.01497\" target=\"_blank\" rel=\"noopener\">Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks</a></p>\n<p>Faster RCNN(feature extraction)proposalbounding box regression(rect refine)classification</p>\n<center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster-rcnn.jpg\" alt=\"image\"></center>\n\n<h4 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h4><ol>\n<li>Conv layersCNNFaster R-CNNconv+relu+poolingimagefeature mapsfeature mapsRPN</li>\n<li>Region Proposal NetworksRPNregion proposalssoftmaxanchorsforegroundbackgroundbounding box regressionanchorsproposals</li>\n<li>Roi Poolingfeature mapsproposalsproposal feature maps</li>\n<li>Classificationproposal feature mapsproposalbounding box regression</li>\n</ol>\n<h4 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h4><p><strong>[1. Conv layers]</strong><br>   <center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_1.jpg\" alt=\"image\"></center><br>   Conv layersconvpoolingrelupythonVGG16faster_rcnn_test.pt,    Conv layers13conv13relu4poolingConv          layers</p>\n<ul>\n<li>conv $kernel_size=3$  $pad=1$  $stride=1$ <br></li>\n<li><p>pooling $kernel_size=2$  $pad=0$  $stride=2$</p>\n<p>Faster RCNN Conv layers $pad=1$ 0                $(M+2)\\times (N+2)$ 3x3 $M\\times N$ Conv layersconv    <br><center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_2.jpg\" alt=\"image\"></center><br>Conv layerspooling $kernel_size=2$  $stride=2$ pooling $M\\times N$  $(M/2) \\times(N/2)$ Conv layersconvrelupooling1/2<br> $M\\times N$ Conv layers $(M/16)\\times (N/16)$ Conv layersfeatuure map</p>\n</li>\n</ul>\n<p><strong>[2. Region Proposal Networks(RPN)]</strong><br>   OpenCV adaboost+R-CNNSS(Selective      Search)Faster RCNNSSRPNFaster R-CNN    <br>   <center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_3.jpg\" alt=\"image\"></center><br>   RPNRPN2softmaxanchorsforeground              backgroundforegroundanchorsbounding box regression          proposalProposalforeground anchorsbounding box regressionproposals    proposalsProposal Layer</p>\n<p>   <strong>2.1 </strong></p>\n<ul>\n<li>+</li>\n<li><p>+</p>\n<center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_4.jpg\" alt=\"image\"></center><br>3233     <br> $1\\times1$      <br><br><strong>2.2 Anchors</strong><br>RPNanchorsanchorsrpn/generate_anchors.pydemo    generate_anchors.py<br>[[ -84.  -40.   99.   55.]<br>[-176.  -88.  191.  103.]<br>[-360. -184.  375.  199.]<br>[ -56.  -56.   71.   71.]<br>[-120. -120.  135.  135.]<br>[-248. -248.  263.  263.]<br>[ -36.  -80.   51.   95.]<br>[ -80. -168.   95.  183.]<br>[-168. -344.  183.  359.]]<br><br>4 $(x1,y1,x2,y2)$ 93 $width:height = [1:1, 1:2, 2:1]$ anchors<br><center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_5.jpg\" alt=\"image\"></center><br>anchors sizepython demoreshape $800\\times600$anchorsanchors 1:2  $352\\times704$  2:1  $736\\times384$ cover $800\\times600$ <br>9anchorsFaster RCNNConv layersfeature maps9anchors2bounding box regression<br><center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_6.jpg\" alt=\"image\"></center>\n\n<p></p>\n</li>\n</ul>\n<ul>\n<li>ZF modelConv Layersconv5num_output=256256feature map256-dimensions</li>\n<li>conv5rpn_conv/3x3num_output=2563x3256-d47</li>\n<li>conv5 feature mapkanchork=9anhcorforegroundbackground256d featurecls=2k scoresanchor[x, y, w, h]4reg=4k coordinates</li>\n<li><p>anchorsanchors128postive anchors+128negative anchorsanchors5.1</p>\n<p> <strong>2.3 softmaxforegroundbackground</strong><br> MxNFaster RCNNRPN(M/16)x(N/16) W=M/16  H=N/16 reshapesoftmax1x1<br> <center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_7.jpg\" alt=\"image\"></center><br> 1x1caffe prototxt<br> <center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_19.PNG\" alt=\"image\"></center><br>num_output=18 $W\\times H \\times 18$ feature maps9anchorsanchorsforegroundbackground $W\\times H\\times (9\\cdot2)$ softmaxforeground anchorsboxforeground anchors<br>RPNanchorssoftmaxforeground anchors</p>\n<p> <strong>2.4 bounding box regression</strong><br>Ground Truth(GT)foreground anchorsforeground anchorsGT<br><center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_8.jpg\" alt=\"image\"></center><br> (x, y, w, h) AForeground AnchorsGGTanchor AGG</p>\n</li>\n<li>$anchor A=(A_{x}, A_{y}, A_{w}, A_{h})$  $GT=[G_{x}, G_{y}, G_{w}, G_{h}]$</li>\n<li><p>F$F(A_{x}, A_{y}, A_{w}, A_{h})=(G_{x}^{}, G_{y}^{}, G_{w}^{}, G_{h}^{})$ $(G_{x}^{}, G_{y}^{}, G_{w}^{}, G_{h}^{}) \\approx (G_{x}, G_{y}, G_{w}, G_{h})$<br><center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_9.jpg\" alt=\"image\"></center><br>F10anchor AG :</p>\n</li>\n<li><p></p>\n<center><br>$G^{}<em>{x} = A</em>{w} \\cdot d_{x}(A) + A_{x} $<br>$G^{}<em>{y} = A</em>{y} \\cdot d_{y}(A) + A_{y} $<br></center></li>\n<li><center><br>$G^{}<em>{w} = A</em>{w} \\cdot exp(d_{w}(A)) $<br>$G^{}<em>{h} = A</em>{h} \\cdot exp(d_{h}(A)) $<br></center>\n\n</li>\n</ul>\n<p>4 $d_{x}(A),d_{y}(A),d_{w}(A),d_{h}(A)$ anchor AGT anchors AGT</p>\n<p> $d_{x}(A),d_{y}(A),d_{w}(A),d_{h}(A)$ X, W, Y$Y=WX$Xcnn feature mapAGT$(t_{x}, t_{y}, t_{w}, t_{h})$$d_{x}(A),d_{y}(A),d_{w}(A),d_{h}(A)$</p>\n<center><br>$d_{<em>}(A) = w^{T}_{</em>} \\cdot \\phi(A)$<br></center>\n\n<p>(A)anchorfeature mapwd(A)* xywh$(t_{x}, t_{y}, t_{w}, t_{h})$</p>\n<center><br>$Loss = \\sum^{N}<em>{i}(t^{i}</em>{<em>} - \\hat{w}^{T}_{</em>} \\cdot \\phi(A^{i}))^{2}$<br></center><br><br><center><br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_20.jpg\" alt=\"image\"><br></center><br>GT<br>Faster RCNNforeground anchorground truth $(t_x, t_y)$  $(t_w, t_h)$ <br><center><br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_21.jpg\" alt=\"image\"><br></center><br>bouding box regressioncnn feature AnchorGT $(t_x, t_y, t_w, t_h)$<br>bouding box regressionAnchor $(t_x, t_y, t_w, t_h)$Anchor<br><br>   <strong>2.5 proposalsbounding box regression</strong><br>bounding box regressionRPN<br><center><br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_10.jpg\" alt=\"image\"><br></center><br> $num_output=36$  $W\\times H\\times 36$ caffe blob [1, 36, H, W] feature maps9anchorsanchors4$d_{x}(A),d_{y}(A),d_{w}(A),d_{h}(A)$<br><br>   <strong>2.6 Proposal Layer</strong><br>Proposal Layer $d_{x}(A),d_{y}(A),d_{w}(A),d_{h}(A)$ foreground anchorsproposalRoI Pooling Layer<br>im_infoPxQFaster RCNNreshape $M\\times N$ im_info=[M, N, scale_factor]Conv Layers4pooling $W\\times H=(M/16)\\times(N/16)$ feature_stride=16anchor<br><center><br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_11.jpg\" alt=\"image\"><br></center>\n\n<p>Proposal Layer forwardcaffe layer</p>\n<ol>\n<li>anchors$[d_{x}(A),d_{y}(A),d_{w}(A),d_{h}(A)]$anchorsbbox regressionanchors</li>\n<li>foreground softmax scoresanchorspre_nms_topN(e.g. 6000)anchorsforeground anchors</li>\n<li>foreground anchorsroi poolingproposal</li>\n<li>width&lt;threshold or height&lt;thresholdforeground anchors</li>\n<li>nonmaximum suppression</li>\n<li>nmsforeground softmax scoresfg anchorspost_nms_topN(e.g. 300)proposal</li>\n</ol>\n<p> proposal=[x1, y1, x2, y2] anchorsproposal $M\\times N$ ~   </p>\n<p><strong>RPN</strong><br><strong>anchors -&gt; softmaxfg anchors -&gt; bbox regfg anchors -&gt; Proposal Layerproposals</strong></p>\n<h3 id=\"19-06-2018\"><a href=\"#19-06-2018\" class=\"headerlink\" title=\"19/06/2018\"></a>19/06/2018</h3><h4 id=\"-XML-\"><a href=\"#-XML-\" class=\"headerlink\" title=\" XML \"></a> XML </h4><p> xml.etree.ElementTree XML list<br></p>\n<ul>\n<li>XML</li>\n<li></li>\n<li>XMLPYTHON<br><center><br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_23.PNG\" alt=\"image\"><br></center><br>Github  jupyter notebook <a href=\"https://github.com/Trouble404/Object-Detection-System-Based-on-CNN-and-Capsule-Network/blob/master/Faster-RCNN/parser_voc2012_xml_and_plotting.ipynb\" target=\"_blank\" rel=\"noopener\"></a> </li>\n</ul>\n<p> trainval.txt <br></p>\n<p>  17125 <br> 11540 </p>\n<p>20</p>\n<center><br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_24.PNG\" alt=\"image\"><br></center> \n\n\n<h3 id=\"20-06-2018\"><a href=\"#20-06-2018\" class=\"headerlink\" title=\"20/06/2018\"></a>20/06/2018</h3><h4 id=\"BBOXES\"><a href=\"#BBOXES\" class=\"headerlink\" title=\"BBOXES\"></a>BBOXES</h4><p> cv2 <br>  <figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">pip install opencv-python</span><br></pre></td></tr></table></figure><br> OpenCV-python  BGR  RGB</p>\n<p>VOC201220 BBOXES</p>\n<table>\n<thead>\n<tr>\n<th style=\"text-align:left\">class</th>\n<th style=\"text-align:left\">class_mapping</th>\n<th style=\"text-align:left\">BGR of bbox</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:left\">Person</td>\n<td style=\"text-align:left\">0</td>\n<td style=\"text-align:left\">(0, 0, 255)</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">Aeroplane</td>\n<td style=\"text-align:left\">1</td>\n<td style=\"text-align:left\">(0, 0, 255)</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">Tvmonitor</td>\n<td style=\"text-align:left\">2</td>\n<td style=\"text-align:left\">(0, 128, 0)</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">Train</td>\n<td style=\"text-align:left\">3</td>\n<td style=\"text-align:left\">(128, 128, 128)</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">Boat</td>\n<td style=\"text-align:left\">4</td>\n<td style=\"text-align:left\">(0, 165, 255)</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">Dog</td>\n<td style=\"text-align:left\">5</td>\n<td style=\"text-align:left\">(0, 255, 255)</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">Chair</td>\n<td style=\"text-align:left\">6</td>\n<td style=\"text-align:left\">(80, 127, 255)</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">Bird</td>\n<td style=\"text-align:left\">7</td>\n<td style=\"text-align:left\">(208, 224, 64)</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">Bicycle</td>\n<td style=\"text-align:left\">8</td>\n<td style=\"text-align:left\">(235, 206, 135)</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">Bottle</td>\n<td style=\"text-align:left\">9</td>\n<td style=\"text-align:left\">(128, 0, 0)</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">Sheep</td>\n<td style=\"text-align:left\">10</td>\n<td style=\"text-align:left\">(140, 180, 210)</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">Diningtable</td>\n<td style=\"text-align:left\">11</td>\n<td style=\"text-align:left\">(0, 255, 0)</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">Horse</td>\n<td style=\"text-align:left\">12</td>\n<td style=\"text-align:left\">(133, 21, 199)</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">Motorbike</td>\n<td style=\"text-align:left\">13</td>\n<td style=\"text-align:left\">(47, 107, 85)</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">Sofa</td>\n<td style=\"text-align:left\">14</td>\n<td style=\"text-align:left\">(19, 69, 139)</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">Cow</td>\n<td style=\"text-align:left\">15</td>\n<td style=\"text-align:left\">(222, 196, 176)</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">Car</td>\n<td style=\"text-align:left\">16</td>\n<td style=\"text-align:left\">(0, 0, 0)</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">Cat</td>\n<td style=\"text-align:left\">17</td>\n<td style=\"text-align:left\">(225, 105, 65)</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">Bus</td>\n<td style=\"text-align:left\">18</td>\n<td style=\"text-align:left\">(255, 255, 255)</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">Pottedplant</td>\n<td style=\"text-align:left\">19</td>\n<td style=\"text-align:left\">(205, 250, 255)</td>\n</tr>\n</tbody>\n</table>\n<p>show_image_with_bboxBBOXESXMLlist:</p>\n<center><br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_22.PNG\" alt=\"image\"><br></center><br>Github  jupyter notebook <a href=\"https://github.com/Trouble404/Object-Detection-System-Based-on-CNN-and-Capsule-Network/blob/master/Faster-RCNN/parser_voc2012_xml_and_plotting.ipynb\" target=\"_blank\" rel=\"noopener\"></a><br><br>EXAMPLE:<br><center><br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/img_with_bboex_1.PNG\" alt=\"image\"><br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/img_with_bboex_2.PNG\" alt=\"image\"><br></center>  \n\n<h3 id=\"21-06-2018\"><a href=\"#21-06-2018\" class=\"headerlink\" title=\"21/06/2018\"></a>21/06/2018</h3><h4 id=\"config-setting\"><a href=\"#config-setting\" class=\"headerlink\" title=\"config setting\"></a>config setting</h4><p>set config class:<br>                 for image enhancement:</p>\n<center><br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/21_1.PNG\" alt=\"image\"><br></center>  \n\n<h4 id=\"image-enhancement\"><a href=\"#image-enhancement\" class=\"headerlink\" title=\"image enhancement\"></a>image enhancement</h4><center><br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/21_2.PNG\" alt=\"image\"><br></center><br>According to the config of three peremeters, users could augment image with 3 different ways or using them all.<br>For horizontal and vertical flips, 1/3 probability to triggle<br>With 0,90,180,270 rotation,<br>This function could increase the number of datasets.<br><br>image flips and rotation are realized by opencv and replace of height and width<br>New cordinates of bboxes are calculated acccording to different change of image<br><br>detailed in Github, jupyter notebook: <a href=\"https://github.com/Trouble404/Object-Detection-System-Based-on-CNN-and-Capsule-Network/blob/master/Faster-RCNN/config_set_and_image_enhance.ipynb\" target=\"_blank\" rel=\"noopener\">address</a><br><br>Orignal image:<br><center><br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/21_3.PNG\" alt=\"image\"><br></center><br>horizontal flip:<br><center><br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/21_4.PNG\" alt=\"image\"><br></center><br>Vertical filp:<br><center><br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/21_5.PNG\" alt=\"image\"><br></center><br>Random rotation:<br><center><br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/21_6.PNG\" alt=\"image\"><br></center><br>Horizontal and then vertical flips:<br><center><br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/21_7.PNG\" alt=\"image\"><br></center>  \n\n<h3 id=\"22-06-2018\"><a href=\"#22-06-2018\" class=\"headerlink\" title=\"22/06/2018\"></a>22/06/2018</h3><h4 id=\"Image-rezise\"><a href=\"#Image-rezise\" class=\"headerlink\" title=\"Image rezise\"></a>Image rezise</h4><p>This function is to rezise input image to a uniform size with same shortest side</p>\n<center><br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/22_2.PNG\" alt=\"image\"><br></center> \n\n<p>According to set the value of shortest side, convergent-divergent or augmented another side proportion</p>\n<p>Test:<br>Left image is resized image, in this case, the orignal image amplified.</p>\n<center><br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/22_1.png\" alt=\"image\"><br></center> \n\n<h4 id=\"Class-Balance\"><a href=\"#Class-Balance\" class=\"headerlink\" title=\"Class Balance\"></a>Class Balance</h4><p>When training the model, if we sent image with no repeating classes, it may help to improve the performance of model. Therefore, this function is to make sure no repeating classes in two closed input image.</p>\n<center><br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/22_3.PNG\" alt=\"image\"><br></center> \n\n<p>Test:</p>\n<p><center><br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/22_4.PNG\" alt=\"image\"><br></center><br>Random output 4 iamge with is function, it could find no repeating classes in two closed image. However, it may reduce the number of trainning image because skip some images.</p>\n<h3 id=\"25-26-06-2018\"><a href=\"#25-26-06-2018\" class=\"headerlink\" title=\"25~26/06/2018\"></a>25~26/06/2018</h3><h4 id=\"Region-Proposal-Networks-RPN\"><a href=\"#Region-Proposal-Networks-RPN\" class=\"headerlink\" title=\"Region Proposal Networks(RPN)\"></a>Region Proposal Networks(RPN)</h4><p><center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_3.jpg\" alt=\"image\"></center><br>RPN2softmaxanchorsforegroundbackgroundforegroundanchorsbounding box regressionproposalProposalforeground anchorsbounding box regressionproposalsproposalsProposal Layer</p>\n<h4 id=\"Anchors\"><a href=\"#Anchors\" class=\"headerlink\" title=\"Anchors\"></a>Anchors</h4><p></p>\n<p><center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_5.jpg\" alt=\"image\"></center><br>4 (x1,y1,x2,y2) 93 width:height = [1:1, 1:2, 2:1]</p>\n<p><center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_6.jpg\" alt=\"image\"></center><br>Conv layersfeature maps9anchors2bounding box regression.</p>\n<h4 id=\"Code\"><a href=\"#Code\" class=\"headerlink\" title=\"Code\"></a>Code</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"string\">\"\"\" intersection of two bboxes</span></span><br><span class=\"line\"><span class=\"string\">@param ai: left top x,y and right bottom x,y coordinates of bbox 1</span></span><br><span class=\"line\"><span class=\"string\">@param bi: left top x,y and right bottom x,y coordinates of bbox 2</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">@return: area_union: whether contain target classes</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">intersection</span><span class=\"params\">(ai, bi)</span>:</span></span><br></pre></td></tr></table></figure>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"string\">\"\"\" union of two bboxes</span></span><br><span class=\"line\"><span class=\"string\">@param au: left top x,y and right bottom x,y coordinates of bbox 1</span></span><br><span class=\"line\"><span class=\"string\">@param bu: left top x,y and right bottom x,y coordinates of bbox 2</span></span><br><span class=\"line\"><span class=\"string\">@param area_intersection: intersection area</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">@return: area_union: whether contain target classes</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">union</span><span class=\"params\">(au, bu, area_intersection)</span>:</span></span><br></pre></td></tr></table></figure>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"string\">\"\"\" calculate ratio of intersection and union</span></span><br><span class=\"line\"><span class=\"string\">@param a: left top x,y and right bottom x,y coordinates of bbox 1</span></span><br><span class=\"line\"><span class=\"string\">@param b: left top x,y and right bottom x,y coordinates of bbox 2</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">@return: ratio of intersection and union of two bboxes</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">iou</span><span class=\"params\">(a, b)</span>:</span></span><br></pre></td></tr></table></figure>\n<p><strong>IOU is used to bounding box regression</strong></p>\n<hr>\n<p><strong> rpn calculation</strong></p>\n<ol>\n<li>Traversal all pre-anchors to calculate IOU with GT bboxes</li>\n<li>Set number and proprty of pre-anchors</li>\n<li>return specity number of result(Anchors)</li>\n</ol>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"string\">\"\"\" </span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">@param C: configuration</span></span><br><span class=\"line\"><span class=\"string\">@param img_data: parsered xml information</span></span><br><span class=\"line\"><span class=\"string\">@param width: orignal width of image</span></span><br><span class=\"line\"><span class=\"string\">@param hegiht: orignal height of image</span></span><br><span class=\"line\"><span class=\"string\">@param resized_width: resized width of image after image processing</span></span><br><span class=\"line\"><span class=\"string\">@param resized_heighth: resized height of image after image processing</span></span><br><span class=\"line\"><span class=\"string\">@param img_length_calc_function: Keras's image_dim_ordering function</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">@return: np.copy(y_rpn_cls): whether contain target classes</span></span><br><span class=\"line\"><span class=\"string\">@return: np.copy(y_rpn_regr): corrspoding return of gradient</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">\"\"\"</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">calc_rpn</span><span class=\"params\">(C, img_data, width, height, resized_width, resized_height, img_length_calc_function)</span>:</span></span><br></pre></td></tr></table></figure>\n<p>num_regions256 </p>\n<p><br>Initialise paramters: see <a href=\"https://github.com/Trouble404/Object-Detection-System-Based-on-CNN-and-Capsule-Network/blob/master/Faster-RCNN/rpn_calculation.ipynb\" target=\"_blank\" rel=\"noopener\">jupyter notebook</a></p>\n<p>Calculate the size of map feature:<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">(output_width, output_height) = img_length_calc_function(resized_width, resized_height)</span><br></pre></td></tr></table></figure></p>\n<p><br><br>Get the GT box coordinates, and resize to account for image resizing<br>after rezised functon, the coordinates of bboxes need to re-calculation:<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">for</span> bbox_num, bbox <span class=\"keyword\">in</span> enumerate(img_data[<span class=\"string\">'bboxes'</span>]):</span><br><span class=\"line\">\tgta[bbox_num, <span class=\"number\">0</span>] = bbox[<span class=\"string\">'x1'</span>] * (resized_width / float(width))</span><br><span class=\"line\">\tgta[bbox_num, <span class=\"number\">1</span>] = bbox[<span class=\"string\">'x2'</span>] * (resized_width / float(width))</span><br><span class=\"line\">\tgta[bbox_num, <span class=\"number\">2</span>] = bbox[<span class=\"string\">'y1'</span>] * (resized_height / float(height))</span><br><span class=\"line\">\tgta[bbox_num, <span class=\"number\">3</span>] = bbox[<span class=\"string\">'y2'</span>] * (resized_height / float(height))</span><br></pre></td></tr></table></figure></p>\n<p>gtax1,x2,y1,y2x1,y1,x2,y2<br><br><br>Traverse all possible group of sizes<br>anchor box scales [128, 256, 512]<br>anchor box ratios [1:1,1:2,2:1]<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">for</span> anchor_size_idx <span class=\"keyword\">in</span> range(len(anchor_sizes)):</span><br><span class=\"line\">\t<span class=\"keyword\">for</span> anchor_ratio_idx <span class=\"keyword\">in</span> range(len(anchor_ratios)):</span><br><span class=\"line\">\t\tanchor_x = anchor_sizes[anchor_size_idx] * anchor_ratios[anchor_ratio_idx][<span class=\"number\">0</span>]</span><br><span class=\"line\">\t\tanchor_y = anchor_sizes[anchor_size_idx] * anchor_ratios[anchor_ratio_idx][<span class=\"number\">1</span>]</span><br></pre></td></tr></table></figure></p>\n<p>Traver one bbox group, all pre boxes generated by anchors</p>\n<p>output_widthoutput_heightwidth and height of map feature<br>downscalemapping ration, defualt 16<br>if to delete box out of iamge</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">for</span> ix <span class=\"keyword\">in</span> range(output_width):</span><br><span class=\"line\">\tx1_anc = downscale * (ix + <span class=\"number\">0.5</span>) - anchor_x / <span class=\"number\">2</span></span><br><span class=\"line\">\tx2_anc = downscale * (ix + <span class=\"number\">0.5</span>) + anchor_x / <span class=\"number\">2</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"keyword\">if</span> x1_anc &lt; <span class=\"number\">0</span> <span class=\"keyword\">or</span> x2_anc &gt; resized_width:</span><br><span class=\"line\">\t\t<span class=\"keyword\">continue</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"keyword\">for</span> jy <span class=\"keyword\">in</span> range(output_height):</span><br><span class=\"line\">\t\ty1_anc = downscale * (jy + <span class=\"number\">0.5</span>) - anchor_y / <span class=\"number\">2</span></span><br><span class=\"line\">\t\ty2_anc = downscale * (jy + <span class=\"number\">0.5</span>) + anchor_y / <span class=\"number\">2</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">if</span> y1_anc &lt; <span class=\"number\">0</span> <span class=\"keyword\">or</span> y2_anc &gt; resized_height:</span><br><span class=\"line\">\t\t\t<span class=\"keyword\">continue</span></span><br></pre></td></tr></table></figure>\n<p><br></p>\n<p></p>\n<p>bboxes bboxIOU<br>curr_ioubbox</p>\n<p>tx<br>ty:tx<br>tw:bbox<br>th:</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">if</span> curr_iou &gt; best_iou_for_bbox[bbox_num] <span class=\"keyword\">or</span> curr_iou &gt; C.rpn_max_overlap:</span><br><span class=\"line\">\tcx = (gta[bbox_num, <span class=\"number\">0</span>] + gta[bbox_num, <span class=\"number\">1</span>]) / <span class=\"number\">2.0</span></span><br><span class=\"line\">\tcy = (gta[bbox_num, <span class=\"number\">2</span>] + gta[bbox_num, <span class=\"number\">3</span>]) / <span class=\"number\">2.0</span></span><br><span class=\"line\">\tcxa = (x1_anc + x2_anc) / <span class=\"number\">2.0</span></span><br><span class=\"line\">\tcya = (y1_anc + y2_anc) / <span class=\"number\">2.0</span></span><br><span class=\"line\"></span><br><span class=\"line\">\ttx = (cx - cxa) / (x2_anc - x1_anc)</span><br><span class=\"line\">\tty = (cy - cya) / (y2_anc - y1_anc)</span><br><span class=\"line\">\ttw = np.log((gta[bbox_num, <span class=\"number\">1</span>] - gta[bbox_num, <span class=\"number\">0</span>]) / (x2_anc - x1_anc))</span><br><span class=\"line\">\tth = np.log((gta[bbox_num, <span class=\"number\">3</span>] - gta[bbox_num, <span class=\"number\">2</span>])) / (y2_anc - y1_anc)</span><br></pre></td></tr></table></figure>\n<p>Faster RCNNforeground anchorground truth $(t_x, t_y)$ </p>\n<p><center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/25_2.jpg\" alt=\"image\"></center><br>bouding box regressioncnn feature AnchorGT $(t_x, t_y, t_w, t_h)$ <br>bouding box regressionAnchor $(t_x, t_y, t_w, t_h)$Anchor</p>\n<p><br><br></p>\n<p>bbox<br>pos<br>best_iou_for_loc<br>num_anchors_for_bbox[bbox_num]bboxpos<br>negneutral<br>negbbox</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">if</span> img_data[<span class=\"string\">'bboxes'</span>][bbox_num][<span class=\"string\">'class'</span>] != <span class=\"string\">'bg'</span>:</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"comment\"># all GT boxes should be mapped to an anchor box, so we keep track of which anchor box was best</span></span><br><span class=\"line\">\t<span class=\"keyword\">if</span> curr_iou &gt; best_iou_for_bbox[bbox_num]:</span><br><span class=\"line\">\t\tbest_anchor_for_bbox[bbox_num] = [jy, ix, anchor_ratio_idx, anchor_size_idx]</span><br><span class=\"line\">\t\tbest_iou_for_bbox[bbox_num] = curr_iou</span><br><span class=\"line\">\t\tbest_x_for_bbox[bbox_num, :] = [x1_anc, x2_anc, y1_anc, y2_anc]</span><br><span class=\"line\">\t\tbest_dx_for_bbox[bbox_num, :] = [tx, ty, tw, th]</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"keyword\">if</span> curr_iou &gt; C.rpn_max_overlap:</span><br><span class=\"line\">\t\tbbox_type = <span class=\"string\">'pos'</span></span><br><span class=\"line\">\t\tnum_anchors_for_bbox[bbox_num] += <span class=\"number\">1</span></span><br><span class=\"line\">\t\t<span class=\"keyword\">if</span> curr_iou &gt; best_iou_for_loc:</span><br><span class=\"line\">\t\t\tbest_iou_for_loc = curr_iou</span><br><span class=\"line\">\t\t\tbest_regr = (tx, ty, tw, th)</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"keyword\">if</span> C.rpn_min_overlap &lt; curr_iou &lt; C.rpn_max_overlap:</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">if</span> bbox_type != <span class=\"string\">'pos'</span>:</span><br><span class=\"line\">\t\t\tbbox_type = <span class=\"string\">'neutral'</span></span><br></pre></td></tr></table></figure>\n<p><br><br>bbox</p>\n<p>y_is_box_validnertual<br>y_rpn_overlap<br>y_rpn_regr:<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">if</span> bbox_type == <span class=\"string\">'neg'</span>:</span><br><span class=\"line\">    y_is_box_valid[jy, ix, anchor_ratio_idx + n_anchratios * anchor_size_idx] = <span class=\"number\">1</span></span><br><span class=\"line\">    y_rpn_overlap[jy, ix, anchor_ratio_idx + n_anchratios * anchor_size_idx] = <span class=\"number\">0</span></span><br><span class=\"line\"><span class=\"keyword\">elif</span> bbox_type == <span class=\"string\">'neutral'</span>:</span><br><span class=\"line\">    y_is_box_valid[jy, ix, anchor_ratio_idx + n_anchratios * anchor_size_idx] = <span class=\"number\">0</span></span><br><span class=\"line\">    y_rpn_overlap[jy, ix, anchor_ratio_idx + n_anchratios * anchor_size_idx] = <span class=\"number\">0</span></span><br><span class=\"line\"><span class=\"keyword\">else</span>:</span><br><span class=\"line\">    y_is_box_valid[jy, ix, anchor_ratio_idx + n_anchratios * anchor_size_idx] = <span class=\"number\">1</span></span><br><span class=\"line\">    y_rpn_overlap[jy, ix, anchor_ratio_idx + n_anchratios * anchor_size_idx] = <span class=\"number\">1</span></span><br><span class=\"line\">    start = <span class=\"number\">4</span> * (anchor_ratio_idx + n_anchratios * anchor_size_idx)</span><br><span class=\"line\">    y_rpn_regr[jy, ix, start:start+<span class=\"number\">4</span>] = best_regr</span><br></pre></td></tr></table></figure></p>\n<p><br><br>bboxposanchorpos<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">for</span> idx <span class=\"keyword\">in</span> range(num_anchors_for_bbox.shape[<span class=\"number\">0</span>]):</span><br><span class=\"line\">\t<span class=\"keyword\">if</span> num_anchors_for_bbox[idx] == <span class=\"number\">0</span>:</span><br><span class=\"line\">\t\t<span class=\"comment\"># no box with an IOU greater than zero ...</span></span><br><span class=\"line\">\t\t<span class=\"keyword\">if</span> best_anchor_for_bbox[idx, <span class=\"number\">0</span>] == <span class=\"number\">-1</span>:</span><br><span class=\"line\">\t\t\t<span class=\"keyword\">continue</span></span><br><span class=\"line\">\t\ty_is_box_valid[best_anchor_for_bbox[idx,<span class=\"number\">0</span>], best_anchor_for_bbox[idx,<span class=\"number\">1</span>], best_anchor_for_bbox[idx,<span class=\"number\">2</span>] + n_anchratios *best_anchor_for_bbox[idx,<span class=\"number\">3</span>]] = <span class=\"number\">1</span></span><br><span class=\"line\">\t\ty_rpn_overlap[best_anchor_for_bbox[idx,<span class=\"number\">0</span>], best_anchor_for_bbox[idx,<span class=\"number\">1</span>], best_anchor_for_bbox[idx,<span class=\"number\">2</span>] + n_anchratios *best_anchor_for_bbox[idx,<span class=\"number\">3</span>]] = <span class=\"number\">1</span></span><br><span class=\"line\">\t\tstart = <span class=\"number\">4</span> * (best_anchor_for_bbox[idx,<span class=\"number\">2</span>] + n_anchratios * best_anchor_for_bbox[idx,<span class=\"number\">3</span>])</span><br><span class=\"line\">\t\ty_rpn_regr[best_anchor_for_bbox[idx,<span class=\"number\">0</span>], best_anchor_for_bbox[idx,<span class=\"number\">1</span>], start:start+<span class=\"number\">4</span>] = best_dx_for_bbox[idx, :]</span><br></pre></td></tr></table></figure></p>\n<p><br><br>, Tensorflow batch size,  <br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">y_rpn_overlap = np.transpose(y_rpn_overlap, (<span class=\"number\">2</span>, <span class=\"number\">0</span>, <span class=\"number\">1</span>))</span><br><span class=\"line\">y_rpn_overlap = np.expand_dims(y_rpn_overlap, axis=<span class=\"number\">0</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">y_is_box_valid = np.transpose(y_is_box_valid, (<span class=\"number\">2</span>, <span class=\"number\">0</span>, <span class=\"number\">1</span>))</span><br><span class=\"line\">y_is_box_valid = np.expand_dims(y_is_box_valid, axis=<span class=\"number\">0</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">y_rpn_regr = np.transpose(y_rpn_regr, (<span class=\"number\">2</span>, <span class=\"number\">0</span>, <span class=\"number\">1</span>))</span><br><span class=\"line\">y_rpn_regr = np.expand_dims(y_rpn_regr, axis=<span class=\"number\">0</span>)</span><br></pre></td></tr></table></figure></p>\n<p><br><br>num_regions<br>posnum_regions / 2pos<br>posnegnum_regionsneg<br> 256128positive,128negative  <br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">pos_locs = np.where(np(y_rpn_overlap[<span class=\"number\">0</span>, :, :, :] =.logical_and= <span class=\"number\">1</span>, y_is_box_valid[<span class=\"number\">0</span>, :, :, :] == <span class=\"number\">1</span>))</span><br><span class=\"line\">neg_locs = np.where(np.logical_and(y_rpn_overlap[<span class=\"number\">0</span>, :, :, :] == <span class=\"number\">0</span>, y_is_box_valid[<span class=\"number\">0</span>, :, :, :] == <span class=\"number\">1</span>))</span><br><span class=\"line\">num_regions = <span class=\"number\">256</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> len(pos_locs[<span class=\"number\">0</span>]) &gt; num_regions / <span class=\"number\">2</span>:</span><br><span class=\"line\">\tval_locs = random.sample(range(len(pos_locs[<span class=\"number\">0</span>])), len(pos_locs[<span class=\"number\">0</span>]) - num_regions / <span class=\"number\">2</span>)</span><br><span class=\"line\">\ty_is_box_valid[<span class=\"number\">0</span>, pos_locs[<span class=\"number\">0</span>][val_locs], pos_locs[<span class=\"number\">1</span>][val_locs], pos_locs[<span class=\"number\">2</span>][val_locs]] = <span class=\"number\">0</span></span><br><span class=\"line\">\tnum_pos = num_regions / <span class=\"number\">2</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> len(neg_locs[<span class=\"number\">0</span>]) + num_pos &gt; num_regions:</span><br><span class=\"line\">\tval_locs = random.sample(range(len(neg_locs[<span class=\"number\">0</span>])), len(neg_locs[<span class=\"number\">0</span>]) - num_pos)</span><br><span class=\"line\">\t y_is_box_valid[<span class=\"number\">0</span>, neg_locs[<span class=\"number\">0</span>][val_locs], neg_locs[<span class=\"number\">1</span>][val_locs], neg_locs[<span class=\"number\">2</span>][val_locs]] = <span class=\"number\">0</span></span><br></pre></td></tr></table></figure></p>\n<p><br></p>\n<h3 id=\"27-06-2018\"><a href=\"#27-06-2018\" class=\"headerlink\" title=\"27/06/2018\"></a>27/06/2018</h3><h4 id=\"project-brief\"><a href=\"#project-brief\" class=\"headerlink\" title=\"project brief\"></a>project brief</h4><p>Re organization of Project plan</p>\n<h4 id=\"Anchors-Iterative\"><a href=\"#Anchors-Iterative\" class=\"headerlink\" title=\"Anchors Iterative\"></a>Anchors Iterative</h4><p>Integration of privous work:<br>In each anchor: config file -&gt; rpn_stride = 16 means generate one anchor in 16 pixels<br><a href=\"https://github.com/Trouble404/Object-Detection-System-Based-on-CNN-and-Capsule-Network/blob/master/Faster-RCNN/anchor_rpn.ipynb\" target=\"_blank\" rel=\"noopener\">Jupyter Notebook address</a></p>\n<p><br>Function description<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">@param all_img_data: Parsered xml file  </span></span><br><span class=\"line\"><span class=\"string\">@param class_count: Counting of the number of all classes objects</span></span><br><span class=\"line\"><span class=\"string\">@param C: Configuration class</span></span><br><span class=\"line\"><span class=\"string\">@param img_length_calc_function: resnet's get_img_output_length() function</span></span><br><span class=\"line\"><span class=\"string\">@param backend: Tensorflow in this project</span></span><br><span class=\"line\"><span class=\"string\">#param mode: train or val</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">yield np.copy(x_img), [np.copy(y_rpn_cls), np.copy(y_rpn_regr)], img_data_aug</span></span><br><span class=\"line\"><span class=\"string\">@return: np.copy(x_img): image's matrix data</span></span><br><span class=\"line\"><span class=\"string\">@return: [np.copy(y_rpn_cls), np.copy(y_rpn_regr)]: calculated rpn class and radient</span></span><br><span class=\"line\"><span class=\"string\">@return: img_data_aug: correspoding parsed xml information</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">\"\"\"</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">get_anchor_gt</span><span class=\"params\">(all_img_data, class_count, C, img_length_calc_function, backend, mode=<span class=\"string\">'train'</span>)</span>:</span></span><br></pre></td></tr></table></figure></p>\n<p><br><br><strong>Traverse all input image based on input xml information</strong></p>\n<ul>\n<li>Apply class balance function: <figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">C.balanced_classes = <span class=\"keyword\">True</span></span><br><span class=\"line\">sample_selector = image_processing.SampleSelector(class_count)</span><br><span class=\"line\"><span class=\"keyword\">if</span> C.balanced_classes <span class=\"keyword\">and</span> sample_selector.skip_sample_for_balanced_class(img_data):</span><br><span class=\"line\">    <span class=\"keyword\">continue</span></span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<p><br></p>\n<ul>\n<li>Apply image enhance<br>if input mode is train, apply image enhance to obtain augmented image xml and matrix, if mode is val, obtain image orignal xml and matrix<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">if</span> mode == <span class=\"string\">'train'</span>:</span><br><span class=\"line\">    img_data_aug, x_img = image_enhance.augment(img_data, C, augment=<span class=\"keyword\">True</span>)</span><br><span class=\"line\"><span class=\"keyword\">else</span>:</span><br><span class=\"line\">    img_data_aug, x_img = image_enhance.augment(img_data, C, augment=<span class=\"keyword\">False</span>)</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<p>verifacation width and hegiht in xml and matrix<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">(width, height) = (img_data_aug[<span class=\"string\">'width'</span>], img_data_aug[<span class=\"string\">'height'</span>])</span><br><span class=\"line\">(rows, cols, _) = x_img.shape</span><br><span class=\"line\"><span class=\"keyword\">assert</span> cols == width</span><br><span class=\"line\"><span class=\"keyword\">assert</span> rows == height</span><br></pre></td></tr></table></figure></p>\n<p><br></p>\n<ul>\n<li>Apply rezise function<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">(resized_width, resized_height) = image_processing.get_new_img_size(width, height, C.im_size)</span><br><span class=\"line\">x_img = cv2.resize(x_img, (resized_width, resized_height), interpolation=cv2.INTER_CUBIC)</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<p><br></p>\n<ul>\n<li>Apply rpn calculation<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">y_rpn_cls, y_rpn_regr = rpn_calculation.calc_rpn(C, img_data_aug, width, height, resized_width, resized_height, img_length_calc_function)</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<p><br></p>\n<ul>\n<li><p>Zero-center by mean pixel, and preprocess image format<br>BGR -&gt; RGB because when apply resnet, it need RGB but in cv2, it use BGR</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">x_img = x_img[:,:, (<span class=\"number\">2</span>, <span class=\"number\">1</span>, <span class=\"number\">0</span>)]</span><br></pre></td></tr></table></figure>\n<p> For using pre-trainning model, needs to mins mean channel in each dim</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">x_img = x_img.astype(np.float32)</span><br><span class=\"line\">x_img[:, :, <span class=\"number\">0</span>] -= C.img_channel_mean[<span class=\"number\">0</span>]</span><br><span class=\"line\">x_img[:, :, <span class=\"number\">1</span>] -= C.img_channel_mean[<span class=\"number\">1</span>]</span><br><span class=\"line\">x_img[:, :, <span class=\"number\">2</span>] -= C.img_channel_mean[<span class=\"number\">2</span>]</span><br><span class=\"line\">x_img /= C.img_scaling_factor <span class=\"comment\"># default to 1,so no change here</span></span><br></pre></td></tr></table></figure>\n<p> expand for batch size</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">x_img = np.expand_dims(x_img, axis=<span class=\"number\">0</span>)</span><br></pre></td></tr></table></figure>\n<p>for using pre-trainning model, need to sclaling the std to match pre trained model</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">y_rpn_regr[:, y_rpn_regr.shape[<span class=\"number\">1</span>]//<span class=\"number\">2</span>:, :, :] *= C.std_scaling <span class=\"comment\"># scaling is 4 here</span></span><br></pre></td></tr></table></figure>\n<p>in tensorflow, sort as batch size, width, height, deep</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">if</span> backend == <span class=\"string\">'tf'</span>:</span><br><span class=\"line\">    x_img = np.transpose(x_img, (<span class=\"number\">0</span>, <span class=\"number\">3</span>, <span class=\"number\">2</span>, <span class=\"number\">1</span>))</span><br><span class=\"line\">    y_rpn_cls = np.transpose(y_rpn_cls, (<span class=\"number\">0</span>, <span class=\"number\">3</span>, <span class=\"number\">2</span>, <span class=\"number\">1</span>))</span><br><span class=\"line\">\ty_rpn_regr = np.transpose(y_rpn_regr, (<span class=\"number\">0</span>, <span class=\"number\">3</span>, <span class=\"number\">2</span>, <span class=\"number\">1</span>))</span><br></pre></td></tr></table></figure>\n<p>generator to iteror, using next() to loop</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">yield</span> np.copy(x_img), [np.copy(y_rpn_cls), np.copy(y_rpn_regr)], img_data_aug</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<p><br><br><br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">data_gen_train = get_anchor_gt(train_imgs, classes_count, C, nn.get_img_output_length, K.image_dim_ordering(), mode=<span class=\"string\">'train'</span>)</span><br><span class=\"line\">data_gen_val = get_anchor_gt(val_imgs, classes_count, C, nn.get_img_output_length,K.image_dim_ordering(), mode=<span class=\"string\">'val'</span>)</span><br></pre></td></tr></table></figure></p>\n<p>Test:<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">img,rpn,img_aug = next(data_gen_train)</span><br></pre></td></tr></table></figure></p>\n<p><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/26_1.PNG\" alt=\"image\"></p>\n<h3 id=\"28-06-2018\"><a href=\"#28-06-2018\" class=\"headerlink\" title=\"28/06/2018\"></a>28/06/2018</h3><h4 id=\"Resnet50-structure\"><a href=\"#Resnet50-structure\" class=\"headerlink\" title=\"Resnet50 structure\"></a>Resnet50 structure</h4><p>: <a href=\"https://arxiv.org/abs/1512.03385\" target=\"_blank\" rel=\"noopener\">https://arxiv.org/abs/1512.03385</a></p>\n<p><br><strong>Is learning better networks as easy as stacking more layers?</strong></p>\n<p><br><strong>vanishing/exploding gradients</strong>//normalized initialization and intermediate normalization layers<br><strong>degradation</strong>accuracyfigure156-layererror20-layererror</p>\n<p>He kaiMinglayersdegradationidentity mapping </p>\n<p><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/01_2.jpg\" alt=\"image\"></p>\n<p>plain layer<br><br>1) Our extremely deep residual nets are easy to optimize, but the counterpart plain nets (that simply stack layers) exhibit higher training error when the depth increases;<br>2) Our deep residual nets can easily enjoy accuracy gains from greatly increased depth, producing results substantially better than previous networks.</p>\n<h3 id=\"29-06-2018\"><a href=\"#29-06-2018\" class=\"headerlink\" title=\"29/06/2018\"></a>29/06/2018</h3><h4 id=\"Resnet50-image-structure\"><a href=\"#Resnet50-image-structure\" class=\"headerlink\" title=\"Resnet50 image structure\"></a>Resnet50 image structure</h4><p><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/1_01.png\" alt=\"iamge\"><br>ResNet2blockIdentity BlockdimensionblockConv Blockdimensionfeature vectordimension</p>\n<p>CNNimageconvertdepthfeature mapkernelVGG3x3outputchannelIdentity BlockConv BlockIdentity Block.</p>\n<p>Conv Block:<br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/01_3.png\" alt=\"image\"><br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/01_4.png\" alt=\"image\"><br>shortcut pathconv2D layer1x1 filter sizemain pathdimensionshortcut path.</p>\n<h2 id=\"July\"><a href=\"#July\" class=\"headerlink\" title=\"July\"></a>July</h2><h3 id=\"02-07-2018\"><a href=\"#02-07-2018\" class=\"headerlink\" title=\"02/07/2018\"></a>02/07/2018</h3><h4 id=\"Construct-resnet-by-keras\"><a href=\"#Construct-resnet-by-keras\" class=\"headerlink\" title=\"Construct resnet by keras\"></a>Construct resnet by keras</h4><p>xF(x)shape</p>\n<p>kerasidentity_blockconv_block</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> keras.models <span class=\"keyword\">import</span> Model</span><br><span class=\"line\"><span class=\"keyword\">from</span> keras.layers <span class=\"keyword\">import</span> Input,Dense,BatchNormalization,Conv2D,MaxPooling2D,AveragePooling2D,ZeroPadding2D</span><br><span class=\"line\"><span class=\"keyword\">from</span> keras.layers <span class=\"keyword\">import</span> add,Flatten</span><br><span class=\"line\"><span class=\"keyword\">from</span> keras.optimizers <span class=\"keyword\">import</span> SGD</span><br></pre></td></tr></table></figure>\n<p>identity_block, convshortcut<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">Conv2d_BN</span><span class=\"params\">(x, nb_filter,kernel_size, strides=<span class=\"params\">(<span class=\"number\">1</span>,<span class=\"number\">1</span>)</span>, padding=<span class=\"string\">'same'</span>,name=None)</span>:</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> name <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"keyword\">None</span>:</span><br><span class=\"line\">        bn_name = name + <span class=\"string\">'_bn'</span></span><br><span class=\"line\">        conv_name = name + <span class=\"string\">'_conv'</span></span><br><span class=\"line\">    <span class=\"keyword\">else</span>:</span><br><span class=\"line\">        bn_name = <span class=\"keyword\">None</span></span><br><span class=\"line\">        conv_name = <span class=\"keyword\">None</span></span><br><span class=\"line\"> </span><br><span class=\"line\">    x = Conv2D(nb_filter,kernel_size,padding=padding,strides=strides,activation=<span class=\"string\">'relu'</span>,name=conv_name)(x)</span><br><span class=\"line\">    x = BatchNormalization(axis=<span class=\"number\">3</span>,name=bn_name)(x)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> x</span><br><span class=\"line\">    </span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">Conv_Block</span><span class=\"params\">(inpt,nb_filter,kernel_size,strides=<span class=\"params\">(<span class=\"number\">1</span>,<span class=\"number\">1</span>)</span>, with_conv_shortcut=False)</span>:</span></span><br><span class=\"line\">    x = Conv2d_BN(inpt,nb_filter=nb_filter[<span class=\"number\">0</span>],kernel_size=(<span class=\"number\">1</span>,<span class=\"number\">1</span>),strides=strides,padding=<span class=\"string\">'same'</span>)</span><br><span class=\"line\">    x = Conv2d_BN(x, nb_filter=nb_filter[<span class=\"number\">1</span>], kernel_size=(<span class=\"number\">3</span>,<span class=\"number\">3</span>), padding=<span class=\"string\">'same'</span>)</span><br><span class=\"line\">    x = Conv2d_BN(x, nb_filter=nb_filter[<span class=\"number\">2</span>], kernel_size=(<span class=\"number\">1</span>,<span class=\"number\">1</span>), padding=<span class=\"string\">'same'</span>)</span><br><span class=\"line\">    <span class=\"keyword\">if</span> with_conv_shortcut:</span><br><span class=\"line\">        shortcut = Conv2d_BN(inpt,nb_filter=nb_filter[<span class=\"number\">2</span>],strides=strides,kernel_size=kernel_size)</span><br><span class=\"line\">        x = add([x,shortcut])</span><br><span class=\"line\">        <span class=\"keyword\">return</span> x</span><br><span class=\"line\">    <span class=\"keyword\">else</span>:</span><br><span class=\"line\">        x = add([x,inpt])</span><br><span class=\"line\">        <span class=\"keyword\">return</span> x</span><br></pre></td></tr></table></figure></p>\n<p>identity_blockconv_block<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">inpt = Input(shape=(<span class=\"number\">224</span>,<span class=\"number\">224</span>,<span class=\"number\">3</span>))</span><br><span class=\"line\">x = ZeroPadding2D((<span class=\"number\">3</span>,<span class=\"number\">3</span>))(inpt)</span><br><span class=\"line\">x = Conv2d_BN(x,nb_filter=<span class=\"number\">64</span>,kernel_size=(<span class=\"number\">7</span>,<span class=\"number\">7</span>),strides=(<span class=\"number\">2</span>,<span class=\"number\">2</span>),padding=<span class=\"string\">'valid'</span>)</span><br><span class=\"line\">x = MaxPooling2D(pool_size=(<span class=\"number\">3</span>,<span class=\"number\">3</span>),strides=(<span class=\"number\">2</span>,<span class=\"number\">2</span>),padding=<span class=\"string\">'same'</span>)(x)</span><br><span class=\"line\"> </span><br><span class=\"line\">x = Conv_Block(x,nb_filter=[<span class=\"number\">64</span>,<span class=\"number\">64</span>,<span class=\"number\">256</span>],kernel_size=(<span class=\"number\">3</span>,<span class=\"number\">3</span>),strides=(<span class=\"number\">1</span>,<span class=\"number\">1</span>),with_conv_shortcut=<span class=\"keyword\">True</span>)</span><br><span class=\"line\">x = Conv_Block(x,nb_filter=[<span class=\"number\">64</span>,<span class=\"number\">64</span>,<span class=\"number\">256</span>],kernel_size=(<span class=\"number\">3</span>,<span class=\"number\">3</span>))</span><br><span class=\"line\">x = Conv_Block(x,nb_filter=[<span class=\"number\">64</span>,<span class=\"number\">64</span>,<span class=\"number\">256</span>],kernel_size=(<span class=\"number\">3</span>,<span class=\"number\">3</span>))</span><br><span class=\"line\"> </span><br><span class=\"line\">x = Conv_Block(x,nb_filter=[<span class=\"number\">128</span>,<span class=\"number\">128</span>,<span class=\"number\">512</span>],kernel_size=(<span class=\"number\">3</span>,<span class=\"number\">3</span>),strides=(<span class=\"number\">2</span>,<span class=\"number\">2</span>),with_conv_shortcut=<span class=\"keyword\">True</span>)</span><br><span class=\"line\">x = Conv_Block(x,nb_filter=[<span class=\"number\">128</span>,<span class=\"number\">128</span>,<span class=\"number\">512</span>],kernel_size=(<span class=\"number\">3</span>,<span class=\"number\">3</span>))</span><br><span class=\"line\">x = Conv_Block(x,nb_filter=[<span class=\"number\">128</span>,<span class=\"number\">128</span>,<span class=\"number\">512</span>],kernel_size=(<span class=\"number\">3</span>,<span class=\"number\">3</span>))</span><br><span class=\"line\">x = Conv_Block(x,nb_filter=[<span class=\"number\">128</span>,<span class=\"number\">128</span>,<span class=\"number\">512</span>],kernel_size=(<span class=\"number\">3</span>,<span class=\"number\">3</span>))</span><br><span class=\"line\"> </span><br><span class=\"line\">x = Conv_Block(x,nb_filter=[<span class=\"number\">256</span>,<span class=\"number\">256</span>,<span class=\"number\">1024</span>],kernel_size=(<span class=\"number\">3</span>,<span class=\"number\">3</span>),strides=(<span class=\"number\">2</span>,<span class=\"number\">2</span>),with_conv_shortcut=<span class=\"keyword\">True</span>)</span><br><span class=\"line\">x = Conv_Block(x,nb_filter=[<span class=\"number\">256</span>,<span class=\"number\">256</span>,<span class=\"number\">1024</span>],kernel_size=(<span class=\"number\">3</span>,<span class=\"number\">3</span>))</span><br><span class=\"line\">x = Conv_Block(x,nb_filter=[<span class=\"number\">256</span>,<span class=\"number\">256</span>,<span class=\"number\">1024</span>],kernel_size=(<span class=\"number\">3</span>,<span class=\"number\">3</span>))</span><br><span class=\"line\">x = Conv_Block(x,nb_filter=[<span class=\"number\">256</span>,<span class=\"number\">256</span>,<span class=\"number\">1024</span>],kernel_size=(<span class=\"number\">3</span>,<span class=\"number\">3</span>))</span><br><span class=\"line\">x = Conv_Block(x,nb_filter=[<span class=\"number\">256</span>,<span class=\"number\">256</span>,<span class=\"number\">1024</span>],kernel_size=(<span class=\"number\">3</span>,<span class=\"number\">3</span>))</span><br><span class=\"line\">x = Conv_Block(x,nb_filter=[<span class=\"number\">256</span>,<span class=\"number\">256</span>,<span class=\"number\">1024</span>],kernel_size=(<span class=\"number\">3</span>,<span class=\"number\">3</span>))</span><br><span class=\"line\"> </span><br><span class=\"line\">x = Conv_Block(x,nb_filter=[<span class=\"number\">512</span>,<span class=\"number\">512</span>,<span class=\"number\">2048</span>],kernel_size=(<span class=\"number\">3</span>,<span class=\"number\">3</span>),strides=(<span class=\"number\">2</span>,<span class=\"number\">2</span>),with_conv_shortcut=<span class=\"keyword\">True</span>)</span><br><span class=\"line\">x = Conv_Block(x,nb_filter=[<span class=\"number\">512</span>,<span class=\"number\">512</span>,<span class=\"number\">2048</span>],kernel_size=(<span class=\"number\">3</span>,<span class=\"number\">3</span>))</span><br><span class=\"line\">x = Conv_Block(x,nb_filter=[<span class=\"number\">512</span>,<span class=\"number\">512</span>,<span class=\"number\">2048</span>],kernel_size=(<span class=\"number\">3</span>,<span class=\"number\">3</span>))</span><br><span class=\"line\">x = AveragePooling2D(pool_size=(<span class=\"number\">7</span>,<span class=\"number\">7</span>))(x)</span><br><span class=\"line\">x = Flatten()(x)</span><br><span class=\"line\">x = Dense(<span class=\"number\">1000</span>,activation=<span class=\"string\">'softmax'</span>)(x)</span><br><span class=\"line\"></span><br><span class=\"line\">model = Model(inputs=inpt,outputs=x)</span><br><span class=\"line\">sgd = SGD(decay=<span class=\"number\">0.0001</span>,momentum=<span class=\"number\">0.9</span>)</span><br><span class=\"line\">model.compile(loss=<span class=\"string\">'categorical_crossentropy'</span>,optimizer=sgd,metrics=[<span class=\"string\">'accuracy'</span>])</span><br><span class=\"line\">model.summary()</span><br></pre></td></tr></table></figure></p>\n<p><a href=\"https://github.com/Trouble404/Object-Detection-System-Based-on-CNN-and-Capsule-Network/blob/master/Faster-RCNN/Resnet50/resnet50_keras.ipynb\" target=\"_blank\" rel=\"noopener\">jupyter notebook</a></p>\n<h3 id=\"03-07-2018\"><a href=\"#03-07-2018\" class=\"headerlink\" title=\"03/07/2018\"></a>03/07/2018</h3><h4 id=\"load-pre-trained-model-of-resnet50\"><a href=\"#load-pre-trained-model-of-resnet50\" class=\"headerlink\" title=\"load pre-trained model of resnet50\"></a>load pre-trained model of resnet50</h4><p></p>\n<ul>\n<li>ResNet50resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5</li>\n<li>ResNet50</li>\n<li></li>\n<li></li>\n<li>blockblockResNet50 fine-tune</li>\n</ul>\n<p><a href=\"https://github.com/fchollet/deep-learning-models/releases\" target=\"_blank\" rel=\"noopener\"></a></p>\n<p><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/01_7.JPG\" alt=\"image\"></p>\n<p><br>identity<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">identity_block</span><span class=\"params\">(X, f, filters, stage, block)</span>:</span></span><br><span class=\"line\">    <span class=\"comment\"># defining name basis</span></span><br><span class=\"line\">    conv_name_base = <span class=\"string\">'res'</span> + str(stage) + block + <span class=\"string\">'_branch'</span></span><br><span class=\"line\">    bn_name_base = <span class=\"string\">'bn'</span> + str(stage) + block + <span class=\"string\">'_branch'</span></span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\"># Retrieve Filters</span></span><br><span class=\"line\">    F1, F2, F3 = filters</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\"># Save the input value. You'll need this later to add back to the main path. </span></span><br><span class=\"line\">    X_shortcut = X</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\"># First component of main path</span></span><br><span class=\"line\">    X = Conv2D(filters = F1, kernel_size = (<span class=\"number\">1</span>, <span class=\"number\">1</span>), strides = (<span class=\"number\">1</span>,<span class=\"number\">1</span>), padding = <span class=\"string\">'valid'</span>, name = conv_name_base + <span class=\"string\">'2a'</span>)(X)</span><br><span class=\"line\">    X = BatchNormalization(axis = <span class=\"number\">3</span>, name = bn_name_base + <span class=\"string\">'2a'</span>)(X)</span><br><span class=\"line\">    X = Activation(<span class=\"string\">'relu'</span>)(X)</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\"># Second component of main path (3 lines)</span></span><br><span class=\"line\">    X = Conv2D(filters= F2, kernel_size=(f,f),strides=(<span class=\"number\">1</span>,<span class=\"number\">1</span>),padding=<span class=\"string\">'same'</span>,name=conv_name_base + <span class=\"string\">'2b'</span>)(X)</span><br><span class=\"line\">    X = BatchNormalization(axis=<span class=\"number\">3</span>,name=bn_name_base+<span class=\"string\">'2b'</span>)(X)</span><br><span class=\"line\">    X = Activation(<span class=\"string\">'relu'</span>)(X)</span><br><span class=\"line\"> </span><br><span class=\"line\">    <span class=\"comment\"># Third component of main path (2 lines)</span></span><br><span class=\"line\">    X = Conv2D(filters=F3,kernel_size=(<span class=\"number\">1</span>,<span class=\"number\">1</span>),strides=(<span class=\"number\">1</span>,<span class=\"number\">1</span>),padding=<span class=\"string\">'valid'</span>,name=conv_name_base+<span class=\"string\">'2c'</span>)(X)</span><br><span class=\"line\">    X = BatchNormalization(axis=<span class=\"number\">3</span>,name=bn_name_base+<span class=\"string\">'2c'</span>)(X)</span><br><span class=\"line\"> </span><br><span class=\"line\">    <span class=\"comment\"># Final step: Add shortcut value to main path, and pass it through a RELU activation (2 lines)</span></span><br><span class=\"line\">    X = Add()([X, X_shortcut])</span><br><span class=\"line\">    X = Activation(<span class=\"string\">'relu'</span>)(X)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> X</span><br></pre></td></tr></table></figure></p>\n<p>conv<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">convolutional_block</span><span class=\"params\">(X, f, filters, stage, block, s = <span class=\"number\">2</span>)</span>:</span></span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\"># defining name basis</span></span><br><span class=\"line\">    conv_name_base = <span class=\"string\">'res'</span> + str(stage) + block + <span class=\"string\">'_branch'</span></span><br><span class=\"line\">    bn_name_base = <span class=\"string\">'bn'</span> + str(stage) + block + <span class=\"string\">'_branch'</span></span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\"># Retrieve Filters</span></span><br><span class=\"line\">    F1, F2, F3 = filters</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\"># Save the input value</span></span><br><span class=\"line\">    X_shortcut = X</span><br><span class=\"line\"> </span><br><span class=\"line\">    <span class=\"comment\">##### MAIN PATH #####</span></span><br><span class=\"line\">    <span class=\"comment\"># First component of main path </span></span><br><span class=\"line\">    X = Conv2D(F1, (<span class=\"number\">1</span>, <span class=\"number\">1</span>), strides = (s,s),padding=<span class=\"string\">'valid'</span>,name = conv_name_base + <span class=\"string\">'2a'</span>)(X)</span><br><span class=\"line\">    X = BatchNormalization(axis = <span class=\"number\">3</span>, name = bn_name_base + <span class=\"string\">'2a'</span>)(X)</span><br><span class=\"line\">    X = Activation(<span class=\"string\">'relu'</span>)(X)</span><br><span class=\"line\"> </span><br><span class=\"line\">    <span class=\"comment\"># Second component of main path (3 lines)</span></span><br><span class=\"line\">    X = Conv2D(F2,(f,f),strides=(<span class=\"number\">1</span>,<span class=\"number\">1</span>),padding=<span class=\"string\">'same'</span>,name=conv_name_base+<span class=\"string\">'2b'</span>)(X)</span><br><span class=\"line\">    X = BatchNormalization(axis=<span class=\"number\">3</span>,name=bn_name_base+<span class=\"string\">'2b'</span>)(X)</span><br><span class=\"line\">    X = Activation(<span class=\"string\">'relu'</span>)(X)</span><br><span class=\"line\"> </span><br><span class=\"line\">    <span class=\"comment\"># Third component of main path (2 lines)</span></span><br><span class=\"line\">    X = Conv2D(F3,(<span class=\"number\">1</span>,<span class=\"number\">1</span>),strides=(<span class=\"number\">1</span>,<span class=\"number\">1</span>),padding=<span class=\"string\">'valid'</span>,name=conv_name_base+<span class=\"string\">'2c'</span>)(X)</span><br><span class=\"line\">    X = BatchNormalization(axis=<span class=\"number\">3</span>,name=bn_name_base+<span class=\"string\">'2c'</span>)(X)</span><br><span class=\"line\"> </span><br><span class=\"line\">    <span class=\"comment\">##### SHORTCUT PATH #### (2 lines)</span></span><br><span class=\"line\">    X_shortcut = Conv2D(F3,(<span class=\"number\">1</span>,<span class=\"number\">1</span>),strides=(s,s),padding=<span class=\"string\">'valid'</span>,name=conv_name_base+<span class=\"string\">'1'</span>)(X_shortcut)</span><br><span class=\"line\">    X_shortcut = BatchNormalization(axis=<span class=\"number\">3</span>,name =bn_name_base+<span class=\"string\">'1'</span>)(X_shortcut)</span><br><span class=\"line\"> </span><br><span class=\"line\">    <span class=\"comment\"># Final step: Add shortcut value to main path, and pass it through a RELU activation (2 lines)</span></span><br><span class=\"line\">    X = Add()([X,X_shortcut])</span><br><span class=\"line\">    X = Activation(<span class=\"string\">'relu'</span>)(X)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> X</span><br></pre></td></tr></table></figure></p>\n<p>resnet50<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">ResNet50</span><span class=\"params\">(input_shape = <span class=\"params\">(<span class=\"number\">64</span>, <span class=\"number\">64</span>, <span class=\"number\">3</span>)</span>, classes = <span class=\"number\">30</span>)</span>:</span></span><br><span class=\"line\">    <span class=\"comment\"># Define the input as a tensor with shape input_shape</span></span><br><span class=\"line\">    X_input = Input(input_shape)</span><br><span class=\"line\"> </span><br><span class=\"line\">    <span class=\"comment\"># Zero-Padding</span></span><br><span class=\"line\">    X = ZeroPadding2D((<span class=\"number\">3</span>, <span class=\"number\">3</span>))(X_input)</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\"># Stage 1</span></span><br><span class=\"line\">    X = Conv2D(<span class=\"number\">64</span>, (<span class=\"number\">7</span>, <span class=\"number\">7</span>), strides = (<span class=\"number\">2</span>, <span class=\"number\">2</span>), name = <span class=\"string\">'conv1'</span>)(X)</span><br><span class=\"line\">    X = BatchNormalization(axis = <span class=\"number\">3</span>, name = <span class=\"string\">'bn_conv1'</span>)(X)</span><br><span class=\"line\">    X = Activation(<span class=\"string\">'relu'</span>)(X)</span><br><span class=\"line\">    X = MaxPooling2D((<span class=\"number\">3</span>, <span class=\"number\">3</span>), strides=(<span class=\"number\">2</span>, <span class=\"number\">2</span>))(X)</span><br><span class=\"line\"> </span><br><span class=\"line\">    <span class=\"comment\"># Stage 2</span></span><br><span class=\"line\">    X = convolutional_block(X, f = <span class=\"number\">3</span>, filters = [<span class=\"number\">64</span>, <span class=\"number\">64</span>, <span class=\"number\">256</span>], stage = <span class=\"number\">2</span>, block=<span class=\"string\">'a'</span>, s = <span class=\"number\">1</span>)</span><br><span class=\"line\">    X = identity_block(X, <span class=\"number\">3</span>, [<span class=\"number\">64</span>, <span class=\"number\">64</span>, <span class=\"number\">256</span>], stage=<span class=\"number\">2</span>, block=<span class=\"string\">'b'</span>)</span><br><span class=\"line\">    X = identity_block(X, <span class=\"number\">3</span>, [<span class=\"number\">64</span>, <span class=\"number\">64</span>, <span class=\"number\">256</span>], stage=<span class=\"number\">2</span>, block=<span class=\"string\">'c'</span>)</span><br><span class=\"line\"> </span><br><span class=\"line\">    <span class=\"comment\">### START CODE HERE ###</span></span><br><span class=\"line\"> </span><br><span class=\"line\">    <span class=\"comment\"># Stage 3 (4 lines)</span></span><br><span class=\"line\">    X = convolutional_block(X, f = <span class=\"number\">3</span>,filters= [<span class=\"number\">128</span>,<span class=\"number\">128</span>,<span class=\"number\">512</span>],stage=<span class=\"number\">3</span>,block=<span class=\"string\">'a'</span>,s=<span class=\"number\">2</span>)</span><br><span class=\"line\">    X = identity_block(X,<span class=\"number\">3</span>,[<span class=\"number\">128</span>,<span class=\"number\">128</span>,<span class=\"number\">512</span>],stage=<span class=\"number\">3</span>,block=<span class=\"string\">'b'</span>)</span><br><span class=\"line\">    X = identity_block(X,<span class=\"number\">3</span>,[<span class=\"number\">128</span>,<span class=\"number\">128</span>,<span class=\"number\">512</span>],stage=<span class=\"number\">3</span>,block=<span class=\"string\">'c'</span>)</span><br><span class=\"line\">    X = identity_block(X,<span class=\"number\">3</span>,[<span class=\"number\">128</span>,<span class=\"number\">128</span>,<span class=\"number\">512</span>],stage=<span class=\"number\">3</span>,block=<span class=\"string\">'d'</span>)</span><br><span class=\"line\"> </span><br><span class=\"line\">    <span class=\"comment\"># Stage 4 (6 lines)</span></span><br><span class=\"line\">    X = convolutional_block(X,f=<span class=\"number\">3</span>,filters=[<span class=\"number\">256</span>,<span class=\"number\">256</span>,<span class=\"number\">1024</span>],stage=<span class=\"number\">4</span>,block=<span class=\"string\">'a'</span>,s=<span class=\"number\">2</span>)</span><br><span class=\"line\">    X = identity_block(X,<span class=\"number\">3</span>,[<span class=\"number\">256</span>,<span class=\"number\">256</span>,<span class=\"number\">1024</span>],stage=<span class=\"number\">4</span>,block=<span class=\"string\">'b'</span>)</span><br><span class=\"line\">    X = identity_block(X,<span class=\"number\">3</span>,[<span class=\"number\">256</span>,<span class=\"number\">256</span>,<span class=\"number\">1024</span>],stage=<span class=\"number\">4</span>,block=<span class=\"string\">'c'</span>)</span><br><span class=\"line\">    X = identity_block(X,<span class=\"number\">3</span>,[<span class=\"number\">256</span>,<span class=\"number\">256</span>,<span class=\"number\">1024</span>],stage=<span class=\"number\">4</span>,block=<span class=\"string\">'d'</span>)</span><br><span class=\"line\">    X = identity_block(X,<span class=\"number\">3</span>,[<span class=\"number\">256</span>,<span class=\"number\">256</span>,<span class=\"number\">1024</span>],stage=<span class=\"number\">4</span>,block=<span class=\"string\">'e'</span>)</span><br><span class=\"line\">    X = identity_block(X,<span class=\"number\">3</span>,[<span class=\"number\">256</span>,<span class=\"number\">256</span>,<span class=\"number\">1024</span>],stage=<span class=\"number\">4</span>,block=<span class=\"string\">'f'</span>)</span><br><span class=\"line\"> </span><br><span class=\"line\">    <span class=\"comment\"># Stage 5 (3 lines)</span></span><br><span class=\"line\">    X = convolutional_block(X, f = <span class=\"number\">3</span>,filters= [<span class=\"number\">512</span>,<span class=\"number\">512</span>,<span class=\"number\">2048</span>],stage=<span class=\"number\">5</span>,block=<span class=\"string\">'a'</span>,s=<span class=\"number\">2</span>)</span><br><span class=\"line\">    X = identity_block(X,<span class=\"number\">3</span>,[<span class=\"number\">512</span>,<span class=\"number\">512</span>,<span class=\"number\">2048</span>],stage=<span class=\"number\">5</span>,block=<span class=\"string\">'b'</span>)</span><br><span class=\"line\">    X = identity_block(X,<span class=\"number\">3</span>,[<span class=\"number\">512</span>,<span class=\"number\">512</span>,<span class=\"number\">2048</span>],stage=<span class=\"number\">5</span>,block=<span class=\"string\">'c'</span>)</span><br><span class=\"line\"> </span><br><span class=\"line\">    <span class=\"comment\"># AVGPOOL (1 line). Use \"X = AveragePooling2D(...)(X)\"</span></span><br><span class=\"line\">    X = AveragePooling2D((<span class=\"number\">2</span>,<span class=\"number\">2</span>),strides=(<span class=\"number\">2</span>,<span class=\"number\">2</span>))(X)</span><br><span class=\"line\"> </span><br><span class=\"line\">    <span class=\"comment\"># output layer</span></span><br><span class=\"line\">    X = Flatten()(X)</span><br><span class=\"line\">    model = Model(inputs = X_input, outputs = X, name=<span class=\"string\">'ResNet50'</span>)</span><br><span class=\"line\"> </span><br><span class=\"line\">    <span class=\"keyword\">return</span> model</span><br></pre></td></tr></table></figure></p>\n<p><br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">base_model = ResNet50(input_shape=(<span class=\"number\">224</span>,<span class=\"number\">224</span>,<span class=\"number\">3</span>),classes=<span class=\"number\">30</span>) </span><br><span class=\"line\">base_model.load_weights(<span class=\"string\">'resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5'</span>)</span><br></pre></td></tr></table></figure></p>\n<p><br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/01_8.JPG\" alt=\"image\"></p>\n<h3 id=\"04-07-2018\"><a href=\"#04-07-2018\" class=\"headerlink\" title=\"04/07/2018\"></a>04/07/2018</h3><h4 id=\"Loading-pre-trained-model\"><a href=\"#Loading-pre-trained-model\" class=\"headerlink\" title=\"Loading pre-trained model\"></a>Loading pre-trained model</h4><p>kerasmodel.load_weightsmodel.load_weights(my_model_weights.h5, by_name=True) x= Dense(100, weights=oldModel.layers[1].get_weights())(x)</p>\n<p><br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">try</span>:</span><br><span class=\"line\">    base_model.load_weights(<span class=\"string\">'resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5'</span>,by_name=<span class=\"keyword\">True</span>)</span><br><span class=\"line\">    print(<span class=\"string\">\"load successful\"</span>)</span><br><span class=\"line\"><span class=\"keyword\">except</span>:</span><br><span class=\"line\">    print(<span class=\"string\">\"load failed\"</span>)</span><br></pre></td></tr></table></figure></p>\n<p><a href=\"https://github.com/Trouble404/Object-Detection-System-Based-on-CNN-and-Capsule-Network/blob/master/Faster-RCNN/Resnet50/resnet50_pre_load.ipynb\" target=\"_blank\" rel=\"noopener\">jupyter notebook</a></p>\n<h3 id=\"05-06-07-2018\"><a href=\"#05-06-07-2018\" class=\"headerlink\" title=\"05~06/07/2018\"></a>05~06/07/2018</h3><h4 id=\"construct-faster-rcnn-net\"><a href=\"#construct-faster-rcnn-net\" class=\"headerlink\" title=\"construct faster rcnn net\"></a>construct faster rcnn net</h4><p><strong>RoiPoolingConv</strong><br><br>ROI<br>ROIRegion of Interest<br>1Fast RCNN RoISelective Search<br>2Faster RCNNRPNRoIs<br>kerasLayer<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">RoiPoolingConv</span><span class=\"params\">(Layer)</span>:</span></span><br></pre></td></tr></table></figure></p>\n<p><a href=\"http://keras-cn.readthedocs.io/en/latest/layers/writting_layer/\" target=\"_blank\" rel=\"noopener\"></a></p>\n<p><br>**<a href=\"https://www.cnblogs.com/xuyuanyuan123/p/6674645.html\" target=\"_blank\" rel=\"noopener\">kwargs</a><br><a href=\"https://link.zhihu.com/?target=http%3A//python.jobbole.com/86787/\" target=\"_blank\" rel=\"noopener\">super</a>:<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"string\">'''ROI pooling layer for 2D inputs.</span></span><br><span class=\"line\"><span class=\"string\">    # Arguments</span></span><br><span class=\"line\"><span class=\"string\">        pool_size: int</span></span><br><span class=\"line\"><span class=\"string\">            Size of pooling region to use. pool_size = 7 will result in a 7x7 region.</span></span><br><span class=\"line\"><span class=\"string\">        num_rois: number of regions of interest to be used</span></span><br><span class=\"line\"><span class=\"string\">    '''</span></span><br><span class=\"line\"><span class=\"comment\">#  </span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self, pool_size, num_rois, **kwargs)</span>:</span></span><br><span class=\"line\"></span><br><span class=\"line\">        self.dim_ordering = K.image_dim_ordering()</span><br><span class=\"line\">        <span class=\"comment\"># print error when kernel not tensorflow or thoean</span></span><br><span class=\"line\">        <span class=\"keyword\">assert</span> self.dim_ordering <span class=\"keyword\">in</span> &#123;<span class=\"string\">'tf'</span>&#125;, <span class=\"string\">'dim_ordering must be in tf'</span></span><br><span class=\"line\"></span><br><span class=\"line\">        self.pool_size = pool_size</span><br><span class=\"line\">        self.num_rois = num_rois</span><br><span class=\"line\"></span><br><span class=\"line\">        super(RoiPoolingConv, self).__init__(**kwargs)</span><br></pre></td></tr></table></figure></p>\n<p>:<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">build</span><span class=\"params\">(self, input_shape)</span>:</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> self.dim_ordering == <span class=\"string\">'tf'</span>:</span><br><span class=\"line\">            self.nb_channels = input_shape[<span class=\"number\">0</span>][<span class=\"number\">3</span>]</span><br></pre></td></tr></table></figure></p>\n<p><br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">compute_output_shape</span><span class=\"params\">(self, input_shape)</span>:</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> self.dim_ordering == <span class=\"string\">'tf'</span>:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"keyword\">None</span>, self.num_rois, self.pool_size, self.pool_size, self.nb_channels</span><br></pre></td></tr></table></figure></p>\n<p>,, output:<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">call</span><span class=\"params\">(self, x, mask=None)</span>:</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">assert</span>(len(x) == <span class=\"number\">2</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">        img = x[<span class=\"number\">0</span>]</span><br><span class=\"line\">        rois = x[<span class=\"number\">1</span>]</span><br><span class=\"line\"></span><br><span class=\"line\">        input_shape = K.shape(img)</span><br><span class=\"line\"></span><br><span class=\"line\">        outputs = []</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">for</span> roi_idx <span class=\"keyword\">in</span> range(self.num_rois):</span><br><span class=\"line\"></span><br><span class=\"line\">            x = rois[<span class=\"number\">0</span>, roi_idx, <span class=\"number\">0</span>]</span><br><span class=\"line\">            y = rois[<span class=\"number\">0</span>, roi_idx, <span class=\"number\">1</span>]</span><br><span class=\"line\">            w = rois[<span class=\"number\">0</span>, roi_idx, <span class=\"number\">2</span>]</span><br><span class=\"line\">            h = rois[<span class=\"number\">0</span>, roi_idx, <span class=\"number\">3</span>]</span><br><span class=\"line\">            </span><br><span class=\"line\">            row_length = w / float(self.pool_size)</span><br><span class=\"line\">            col_length = h / float(self.pool_size)</span><br><span class=\"line\"></span><br><span class=\"line\">            num_pool_regions = self.pool_size</span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"keyword\">if</span> self.dim_ordering == <span class=\"string\">'tf'</span>:</span><br><span class=\"line\">                x = K.cast(x, <span class=\"string\">'int32'</span>)</span><br><span class=\"line\">                y = K.cast(y, <span class=\"string\">'int32'</span>)</span><br><span class=\"line\">                w = K.cast(w, <span class=\"string\">'int32'</span>)</span><br><span class=\"line\">                h = K.cast(h, <span class=\"string\">'int32'</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">                <span class=\"comment\"># resize porposal of feature map</span></span><br><span class=\"line\">                rs = tf.image.resize_images(img[:, y:y+h, x:x+w, :], (self.pool_size, self.pool_size))</span><br><span class=\"line\">                outputs.append(rs)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># outputsshape:(?, 7, 7, 512)</span></span><br><span class=\"line\">        final_output = K.concatenate(outputs, axis=<span class=\"number\">0</span>)</span><br><span class=\"line\">        final_output = K.reshape(final_output, (<span class=\"number\">1</span>, self.num_rois, self.pool_size, self.pool_size, self.nb_channels))</span><br><span class=\"line\">        <span class=\"comment\"># shape:(1, 32, 7, 7, 512)</span></span><br><span class=\"line\">        final_output = K.permute_dimensions(final_output, (<span class=\"number\">0</span>, <span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">4</span>))</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">return</span> final_output</span><br></pre></td></tr></table></figure></p>\n<p>batchvectorbatchRoIvectorchannel * w * hRoI Poolingboxw * h.</p>\n<p><strong>TimeDistributed </strong><br>FastRcnnROIpoolingRoiKerasTimeDistributed</p>\n<p>RelutensorTimeDistributedconv2DTimeDistributedROIROI</p>\n<p>Faster RCNNbboxnum_roisnum_roisTimeDistributedwrap</p>\n<p>conv  identity<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">conv_block_td</span><span class=\"params\">(input_tensor, kernel_size, filters, stage, block, input_shape, strides=<span class=\"params\">(<span class=\"number\">2</span>, <span class=\"number\">2</span>)</span>, trainable=True)</span>:</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># conv block time distributed</span></span><br><span class=\"line\"></span><br><span class=\"line\">    nb_filter1, nb_filter2, nb_filter3 = filters</span><br><span class=\"line\"></span><br><span class=\"line\">    bn_axis = <span class=\"number\">3</span></span><br><span class=\"line\"></span><br><span class=\"line\">    conv_name_base = <span class=\"string\">'res'</span> + str(stage) + block + <span class=\"string\">'_branch'</span></span><br><span class=\"line\">    bn_name_base = <span class=\"string\">'bn'</span> + str(stage) + block + <span class=\"string\">'_branch'</span></span><br><span class=\"line\"></span><br><span class=\"line\">    x = TimeDistributed(Convolution2D(nb_filter1, (<span class=\"number\">1</span>, <span class=\"number\">1</span>), strides=strides, trainable=trainable, kernel_initializer=<span class=\"string\">'normal'</span>), input_shape=input_shape, name=conv_name_base + <span class=\"string\">'2a'</span>)(input_tensor)</span><br><span class=\"line\">    x = TimeDistributed(BatchNormalization(axis=bn_axis), name=bn_name_base + <span class=\"string\">'2a'</span>)(x)</span><br><span class=\"line\">    x = Activation(<span class=\"string\">'relu'</span>)(x)</span><br><span class=\"line\"></span><br><span class=\"line\">    x = TimeDistributed(Convolution2D(nb_filter2, (kernel_size, kernel_size), padding=<span class=\"string\">'same'</span>, trainable=trainable, kernel_initializer=<span class=\"string\">'normal'</span>), name=conv_name_base + <span class=\"string\">'2b'</span>)(x)</span><br><span class=\"line\">    x = TimeDistributed(BatchNormalization(axis=bn_axis), name=bn_name_base + <span class=\"string\">'2b'</span>)(x)</span><br><span class=\"line\">    x = Activation(<span class=\"string\">'relu'</span>)(x)</span><br><span class=\"line\"></span><br><span class=\"line\">    x = TimeDistributed(Convolution2D(nb_filter3, (<span class=\"number\">1</span>, <span class=\"number\">1</span>), kernel_initializer=<span class=\"string\">'normal'</span>), name=conv_name_base + <span class=\"string\">'2c'</span>, trainable=trainable)(x)</span><br><span class=\"line\">    x = TimeDistributed(BatchNormalization(axis=bn_axis), name=bn_name_base + <span class=\"string\">'2c'</span>)(x)</span><br><span class=\"line\"></span><br><span class=\"line\">    shortcut = TimeDistributed(Convolution2D(nb_filter3, (<span class=\"number\">1</span>, <span class=\"number\">1</span>), strides=strides, trainable=trainable, kernel_initializer=<span class=\"string\">'normal'</span>), name=conv_name_base + <span class=\"string\">'1'</span>)(input_tensor)</span><br><span class=\"line\">    shortcut = TimeDistributed(BatchNormalization(axis=bn_axis), name=bn_name_base + <span class=\"string\">'1'</span>)(shortcut)</span><br><span class=\"line\"></span><br><span class=\"line\">    x = Add()([x, shortcut])</span><br><span class=\"line\">    x = Activation(<span class=\"string\">'relu'</span>)(x)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> x</span><br></pre></td></tr></table></figure></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">identity_block_td</span><span class=\"params\">(input_tensor, kernel_size, filters, stage, block, trainable=True)</span>:</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># identity block time distributed</span></span><br><span class=\"line\"></span><br><span class=\"line\">    nb_filter1, nb_filter2, nb_filter3 = filters</span><br><span class=\"line\"></span><br><span class=\"line\">    bn_axis = <span class=\"number\">3</span></span><br><span class=\"line\"></span><br><span class=\"line\">    conv_name_base = <span class=\"string\">'res'</span> + str(stage) + block + <span class=\"string\">'_branch'</span></span><br><span class=\"line\">    bn_name_base = <span class=\"string\">'bn'</span> + str(stage) + block + <span class=\"string\">'_branch'</span></span><br><span class=\"line\"></span><br><span class=\"line\">    x = TimeDistributed(Convolution2D(nb_filter1, (<span class=\"number\">1</span>, <span class=\"number\">1</span>), trainable=trainable, kernel_initializer=<span class=\"string\">'normal'</span>), name=conv_name_base + <span class=\"string\">'2a'</span>)(input_tensor)</span><br><span class=\"line\">    x = TimeDistributed(BatchNormalization(axis=bn_axis), name=bn_name_base + <span class=\"string\">'2a'</span>)(x)</span><br><span class=\"line\">    x = Activation(<span class=\"string\">'relu'</span>)(x)</span><br><span class=\"line\"></span><br><span class=\"line\">    x = TimeDistributed(Convolution2D(nb_filter2, (kernel_size, kernel_size), trainable=trainable, kernel_initializer=<span class=\"string\">'normal'</span>,padding=<span class=\"string\">'same'</span>), name=conv_name_base + <span class=\"string\">'2b'</span>)(x)</span><br><span class=\"line\">    x = TimeDistributed(BatchNormalization(axis=bn_axis), name=bn_name_base + <span class=\"string\">'2b'</span>)(x)</span><br><span class=\"line\">    x = Activation(<span class=\"string\">'relu'</span>)(x)</span><br><span class=\"line\"></span><br><span class=\"line\">    x = TimeDistributed(Convolution2D(nb_filter3, (<span class=\"number\">1</span>, <span class=\"number\">1</span>), trainable=trainable, kernel_initializer=<span class=\"string\">'normal'</span>), name=conv_name_base + <span class=\"string\">'2c'</span>)(x)</span><br><span class=\"line\">    x = TimeDistributed(BatchNormalization(axis=bn_axis), name=bn_name_base + <span class=\"string\">'2c'</span>)(x)</span><br><span class=\"line\"></span><br><span class=\"line\">    x = Add()([x, input_tensor])</span><br><span class=\"line\">    x = Activation(<span class=\"string\">'relu'</span>)(x)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">return</span> x</span><br></pre></td></tr></table></figure>\n<p>2DTimeDistributedDense</p>\n<p><strong>resnet50stage</strong><br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">classifier_layers</span><span class=\"params\">(x, input_shape, trainable=False)</span>:</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># Stage 5</span></span><br><span class=\"line\">    x = conv_block_td(x, <span class=\"number\">3</span>, [<span class=\"number\">512</span>, <span class=\"number\">512</span>, <span class=\"number\">2048</span>], stage=<span class=\"number\">5</span>, block=<span class=\"string\">'a'</span>, input_shape=input_shape, strides=(<span class=\"number\">2</span>, <span class=\"number\">2</span>), trainable=trainable)</span><br><span class=\"line\">    x = identity_block_td(x, <span class=\"number\">3</span>, [<span class=\"number\">512</span>, <span class=\"number\">512</span>, <span class=\"number\">2048</span>], stage=<span class=\"number\">5</span>, block=<span class=\"string\">'b'</span>, trainable=trainable)</span><br><span class=\"line\">    x = identity_block_td(x, <span class=\"number\">3</span>, [<span class=\"number\">512</span>, <span class=\"number\">512</span>, <span class=\"number\">2048</span>], stage=<span class=\"number\">5</span>, block=<span class=\"string\">'c'</span>, trainable=trainable)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># AVGPOOL</span></span><br><span class=\"line\">    x = TimeDistributed(AveragePooling2D((<span class=\"number\">7</span>, <span class=\"number\">7</span>)), name=<span class=\"string\">'avg_pool'</span>)(x)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">return</span> x</span><br></pre></td></tr></table></figure></p>\n<ul>\n<li>RoiPoolingConvshape(1, 32, 7, 7, 512)batch_size,</li>\n<li>TimeDistributed3D1323232</li>\n<li>out_classshape:(?, 32, 21); out_regrshape:(?, 32, 80)<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">classifier</span><span class=\"params\">(base_layers, input_rois, num_rois, nb_classes = <span class=\"number\">21</span>, trainable=False)</span>:</span></span><br><span class=\"line\"></span><br><span class=\"line\">    pooling_regions = <span class=\"number\">14</span></span><br><span class=\"line\">    input_shape = (num_rois,<span class=\"number\">14</span>,<span class=\"number\">14</span>,<span class=\"number\">1024</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    out_roi_pool = RoiPoolingConv(pooling_regions, num_rois)([base_layers, input_rois])</span><br><span class=\"line\">    out = classifier_layers(out_roi_pool, input_shape=input_shape, trainable=<span class=\"keyword\">True</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    out = TimeDistributed(Flatten())(out)</span><br><span class=\"line\"></span><br><span class=\"line\">    out_class = TimeDistributed(Dense(nb_classes, activation=<span class=\"string\">'softmax'</span>, kernel_initializer=<span class=\"string\">'zero'</span>), name=<span class=\"string\">'dense_class_&#123;&#125;'</span>.format(nb_classes))(out)</span><br><span class=\"line\">    <span class=\"comment\"># note: no regression target for bg class</span></span><br><span class=\"line\">    out_regr = TimeDistributed(Dense(<span class=\"number\">4</span> * (nb_classes<span class=\"number\">-1</span>), activation=<span class=\"string\">'linear'</span>, kernel_initializer=<span class=\"string\">'zero'</span>), name=<span class=\"string\">'dense_regress_&#123;&#125;'</span>.format(nb_classes))(out)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> [out_class, out_regr]</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<p><strong>RPN</strong></p>\n<ul>\n<li>x_class:sigmoidnum_anchors</li>\n<li>x_regr<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">rpn</span><span class=\"params\">(base_layers,num_anchors)</span>:</span></span><br><span class=\"line\"></span><br><span class=\"line\">    x = Convolution2D(<span class=\"number\">512</span>, (<span class=\"number\">3</span>, <span class=\"number\">3</span>), padding=<span class=\"string\">'same'</span>, activation=<span class=\"string\">'relu'</span>, kernel_initializer=<span class=\"string\">'normal'</span>, name=<span class=\"string\">'rpn_conv1'</span>)(base_layers)</span><br><span class=\"line\"></span><br><span class=\"line\">    x_class = Convolution2D(num_anchors, (<span class=\"number\">1</span>, <span class=\"number\">1</span>), activation=<span class=\"string\">'sigmoid'</span>, kernel_initializer=<span class=\"string\">'uniform'</span>, name=<span class=\"string\">'rpn_out_class'</span>)(x)</span><br><span class=\"line\">    x_regr = Convolution2D(num_anchors * <span class=\"number\">4</span>, (<span class=\"number\">1</span>, <span class=\"number\">1</span>), activation=<span class=\"string\">'linear'</span>, kernel_initializer=<span class=\"string\">'zero'</span>, name=<span class=\"string\">'rpn_out_regress'</span>)(x)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">return</span> [x_class, x_regr, base_layers]</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<p><strong>resnet</strong><br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">nn_base</span><span class=\"params\">(input_tensor=None, trainable=False)</span>:</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># Determine proper input shape</span></span><br><span class=\"line\"></span><br><span class=\"line\">    input_shape = (<span class=\"keyword\">None</span>, <span class=\"keyword\">None</span>, <span class=\"number\">3</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">if</span> input_tensor <span class=\"keyword\">is</span> <span class=\"keyword\">None</span>:</span><br><span class=\"line\">        img_input = Input(shape=input_shape)</span><br><span class=\"line\">    <span class=\"keyword\">else</span>:</span><br><span class=\"line\">        <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> K.is_keras_tensor(input_tensor):</span><br><span class=\"line\">            img_input = Input(tensor=input_tensor, shape=input_shape)</span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            img_input = input_tensor</span><br><span class=\"line\"></span><br><span class=\"line\">    bn_axis = <span class=\"number\">3</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># Zero-Padding</span></span><br><span class=\"line\">    x = ZeroPadding2D((<span class=\"number\">3</span>, <span class=\"number\">3</span>))(img_input)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># Stage 1</span></span><br><span class=\"line\">    x = Convolution2D(<span class=\"number\">64</span>, (<span class=\"number\">7</span>, <span class=\"number\">7</span>), strides=(<span class=\"number\">2</span>, <span class=\"number\">2</span>), name=<span class=\"string\">'conv1'</span>, trainable = trainable)(x)</span><br><span class=\"line\">    x = BatchNormalization(axis=bn_axis, name=<span class=\"string\">'bn_conv1'</span>)(x)</span><br><span class=\"line\">    x = Activation(<span class=\"string\">'relu'</span>)(x)</span><br><span class=\"line\">    x = MaxPooling2D((<span class=\"number\">3</span>, <span class=\"number\">3</span>), strides=(<span class=\"number\">2</span>, <span class=\"number\">2</span>))(x)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># Stage 2</span></span><br><span class=\"line\">    x = conv_block(x, <span class=\"number\">3</span>, [<span class=\"number\">64</span>, <span class=\"number\">64</span>, <span class=\"number\">256</span>], stage=<span class=\"number\">2</span>, block=<span class=\"string\">'a'</span>, strides=(<span class=\"number\">1</span>, <span class=\"number\">1</span>), trainable = trainable)</span><br><span class=\"line\">    x = identity_block(x, <span class=\"number\">3</span>, [<span class=\"number\">64</span>, <span class=\"number\">64</span>, <span class=\"number\">256</span>], stage=<span class=\"number\">2</span>, block=<span class=\"string\">'b'</span>, trainable = trainable)</span><br><span class=\"line\">    x = identity_block(x, <span class=\"number\">3</span>, [<span class=\"number\">64</span>, <span class=\"number\">64</span>, <span class=\"number\">256</span>], stage=<span class=\"number\">2</span>, block=<span class=\"string\">'c'</span>, trainable = trainable)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># Stage 3</span></span><br><span class=\"line\">    x = conv_block(x, <span class=\"number\">3</span>, [<span class=\"number\">128</span>, <span class=\"number\">128</span>, <span class=\"number\">512</span>], stage=<span class=\"number\">3</span>, block=<span class=\"string\">'a'</span>, trainable = trainable)</span><br><span class=\"line\">    x = identity_block(x, <span class=\"number\">3</span>, [<span class=\"number\">128</span>, <span class=\"number\">128</span>, <span class=\"number\">512</span>], stage=<span class=\"number\">3</span>, block=<span class=\"string\">'b'</span>, trainable = trainable)</span><br><span class=\"line\">    x = identity_block(x, <span class=\"number\">3</span>, [<span class=\"number\">128</span>, <span class=\"number\">128</span>, <span class=\"number\">512</span>], stage=<span class=\"number\">3</span>, block=<span class=\"string\">'c'</span>, trainable = trainable)</span><br><span class=\"line\">    x = identity_block(x, <span class=\"number\">3</span>, [<span class=\"number\">128</span>, <span class=\"number\">128</span>, <span class=\"number\">512</span>], stage=<span class=\"number\">3</span>, block=<span class=\"string\">'d'</span>, trainable = trainable)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># Stage 4</span></span><br><span class=\"line\">    x = conv_block(x, <span class=\"number\">3</span>, [<span class=\"number\">256</span>, <span class=\"number\">256</span>, <span class=\"number\">1024</span>], stage=<span class=\"number\">4</span>, block=<span class=\"string\">'a'</span>, trainable = trainable)</span><br><span class=\"line\">    x = identity_block(x, <span class=\"number\">3</span>, [<span class=\"number\">256</span>, <span class=\"number\">256</span>, <span class=\"number\">1024</span>], stage=<span class=\"number\">4</span>, block=<span class=\"string\">'b'</span>, trainable = trainable)</span><br><span class=\"line\">    x = identity_block(x, <span class=\"number\">3</span>, [<span class=\"number\">256</span>, <span class=\"number\">256</span>, <span class=\"number\">1024</span>], stage=<span class=\"number\">4</span>, block=<span class=\"string\">'c'</span>, trainable = trainable)</span><br><span class=\"line\">    x = identity_block(x, <span class=\"number\">3</span>, [<span class=\"number\">256</span>, <span class=\"number\">256</span>, <span class=\"number\">1024</span>], stage=<span class=\"number\">4</span>, block=<span class=\"string\">'d'</span>, trainable = trainable)</span><br><span class=\"line\">    x = identity_block(x, <span class=\"number\">3</span>, [<span class=\"number\">256</span>, <span class=\"number\">256</span>, <span class=\"number\">1024</span>], stage=<span class=\"number\">4</span>, block=<span class=\"string\">'e'</span>, trainable = trainable)</span><br><span class=\"line\">    x = identity_block(x, <span class=\"number\">3</span>, [<span class=\"number\">256</span>, <span class=\"number\">256</span>, <span class=\"number\">1024</span>], stage=<span class=\"number\">4</span>, block=<span class=\"string\">'f'</span>, trainable = trainable)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">return</span> x</span><br></pre></td></tr></table></figure></p>\n<p><strong></strong><br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># define the base network (resnet here)</span></span><br><span class=\"line\">shared_layers = nn.nn_base(img_input, trainable=<span class=\"keyword\">True</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># define the RPN, built on the base layers</span></span><br><span class=\"line\"><span class=\"comment\"># 9 types of anchors</span></span><br><span class=\"line\">num_anchors = len(C.anchor_box_scales) * len(C.anchor_box_ratios)</span><br><span class=\"line\">rpn = nn.rpn(shared_layers, num_anchors)</span><br><span class=\"line\"></span><br><span class=\"line\">classifier = nn.classifier(shared_layers, roi_input, C.num_rois, nb_classes=len(classes_count), trainable=<span class=\"keyword\">True</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">model_rpn = Model(img_input, rpn[:<span class=\"number\">2</span>])</span><br><span class=\"line\">model_classifier = Model([img_input, roi_input], classifier)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># this is a model that holds both the RPN and the classifier, used to load/save weights for the models</span></span><br><span class=\"line\">model_all = Model([img_input, roi_input], rpn[:<span class=\"number\">2</span>] + classifier)</span><br></pre></td></tr></table></figure></p>\n<p><a href=\"https://github.com/Trouble404/Object-Detection-System-Based-on-CNN-and-Capsule-Network/blob/master/Faster-RCNN/Resnet50/workflow_model.ipynb\" target=\"_blank\" rel=\"noopener\">jupyter notebook</a></p>\n<h3 id=\"09-07-2018\"><a href=\"#09-07-2018\" class=\"headerlink\" title=\"09/07/2018\"></a>09/07/2018</h3><h4 id=\"Loss-define\"><a href=\"#Loss-define\" class=\"headerlink\" title=\"Loss define\"></a>Loss define</h4><p><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/6_1.JPG\" alt=\"image\"></p>\n<p>(Multi-task Loss Function)Softmax Classification LossBounding Box Regression Loss</p>\n<p>$L({p_{i}},{t_{i}}) = \\frac{1}{N_{cls}}\\sum_{i}L_{cls}(p_{i},p_{i}^{\\ast}) + \\lambda\\frac{1}{N_{reg}}\\sum_{i}L_{reg}(t_{i},t_{i}^{\\ast})$</p>\n<p><strong>Softmax Classification</strong><br>RPN(cls)2k = 18conv5-3WH18conv5-39anchorsanchorscore(fg/bg)anchorSoftmaxreshape<br>$p_{i}$$p_{i}^{\\ast}$(label)anchor$p_{i}^{\\ast}$1$p_{i}^{\\ast}$0$L_{cls}$(log loss)</p>\n<p><strong>Bounding Box Regression</strong><br>RPN4k = 36$[x,y,w,h]$box(predicted box)$[x,y,w,h]$anchor$[x_{a},y_{a},w_{a},h_{a}]$ground truth$[x^{\\ast},y^{\\ast},w^{\\ast},h^{\\ast}]$anchor{$t$}ground truthanchor{$t^{\\ast}$}</p>\n<p>$t_{x}=\\frac{(x-x_{a})}{w_{a}}$ , $t_{y}=\\frac{(y-y_{a})}{h_{a}}$ , $t_{w}=log(\\frac{w}{w_{a}})$ , $t_{h}=log(\\frac{h}{h_{a}})$ (1)<br>$t_{x}^{\\ast}=\\frac{(x^{\\ast}-x_{a})}{w_{a}}$ , $t_{y}^{\\ast}=\\frac{(y^{\\ast}-y_{a})}{h_{a}}$ , $t_{w}^{\\ast}=log(\\frac{w^{\\ast}}{w_{a}})$ , $t_{h}^{\\ast}=log(\\frac{h^{\\ast}}{h_{a}})$ (2)</p>\n<p>{t}${t^{\\ast}}$${t}$${t^{\\ast}}$${t}$(1)</p>\n<p>Smooth L1:</p>\n<p>$$ Smooth_{L1}(x) =\\left{<br>\\begin{aligned}<br>0.5x^{2} \\ \\ |x| \\leqslant 1\\<br>|x| - 0.5 \\ \\ otherwise<br>\\end{aligned}<br>\\right.<br>$$</p>\n<p>$L_{reg} = Smooth_{L1}(t-t^{\\ast})$<br>Smooth L1L2L1<br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/7_1.JPG\" alt=\"iamge\"></p>\n<p>$p_{i}^{\\ast}L_{reg}$anchor$(p_{i}^{\\ast}=1)$anchorbboxground truth(IoU)bboxground truthwork(cls)(reg){p}{t}$N_{cls}$$N_{reg}$</p>\n<h3 id=\"10-07-2018\"><a href=\"#10-07-2018\" class=\"headerlink\" title=\"10/07/2018\"></a>10/07/2018</h3><h4 id=\"loss-code\"><a href=\"#loss-code\" class=\"headerlink\" title=\"loss code\"></a>loss code</h4><p>  generator to iteror, using next() to loop<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">yield</span> np.copy(x_img), [np.copy(y_rpn_cls), np.copy(y_rpn_regr)], img_data_aug</span><br></pre></td></tr></table></figure></p>\n<p>Rpn calculation:<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">img,rpn,img_aug = next(data_gen_train)</span><br></pre></td></tr></table></figure></p>\n<p> <img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/7_2.JPG\" alt=\"image\"></p>\n<p>def <br><br><br></p>\n<p>$L$  cls <br>$L({p_{i}},{t_{i}}) = \\frac{1}{N_{cls}}\\sum_{i}L_{cls}(p_{i},p_{i}^{\\ast})$</p>\n<p>$p_{i}$$p_{i}^{\\ast}$(label)anchor$p_{i}^{\\ast}$1$p_{i}^{\\ast}$0$L_{cls}$(log loss)</p>\n<p>  rpn loss cls:<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">rpn_loss_cls</span><span class=\"params\">(num_anchors)</span>:</span></span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">rpn_loss_cls_fixed_num</span><span class=\"params\">(y_true, y_pred)</span>:</span></span><br><span class=\"line\">            <span class=\"comment\"># binary_crossentropy -&gt; logloss</span></span><br><span class=\"line\">            <span class=\"comment\"># epsilon to increase robustness</span></span><br><span class=\"line\">\t\t<span class=\"keyword\">return</span> lambda_rpn_class * K.sum(y_true[:, :, :, :num_anchors] * K.binary_crossentropy(y_pred[:, :, :, :], y_true[:, :, :, num_anchors:])) / K.sum(epsilon + y_true[:, :, :, :num_anchors])</span><br><span class=\"line\">\t<span class=\"keyword\">return</span> rpn_loss_cls_fixed_num</span><br></pre></td></tr></table></figure></p>\n<p>$L$  reg <br>$L({p_{i}},{t_{i}}) =  \\lambda\\frac{1}{N_{reg}}\\sum_{i}L_{reg}(t_{i},t_{i}^{\\ast})$<br>Smooth L1:</p>\n<p>$$ Smooth_{L1}(x) =\\left{<br>\\begin{aligned}<br>0.5x^{2} \\ \\ |x| \\leqslant 1\\<br>|x| - 0.5 \\ \\ otherwise<br>\\end{aligned}<br>\\right.<br>$$<br>$L_{reg} = Smooth_{L1}(t-t^{\\ast})$</p>\n<p>  rpn loss reg:<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">rpn_loss_regr</span><span class=\"params\">(num_anchors)</span>:</span></span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">rpn_loss_regr_fixed_num</span><span class=\"params\">(y_true, y_pred)</span>:</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"comment\"># difference of ture value and predicted value</span></span><br><span class=\"line\">\t\tx = y_true[:, :, :, <span class=\"number\">4</span> * num_anchors:] - y_pred</span><br><span class=\"line\">\t\t<span class=\"comment\"># absulote value of difference</span></span><br><span class=\"line\">\t\tx_abs = K.abs(x)</span><br><span class=\"line\">\t\t<span class=\"comment\"># if absulote value less than 1, x_bool == 1, else x_bool = 0</span></span><br><span class=\"line\">\t\tx_bool = K.cast(K.less_equal(x_abs, <span class=\"number\">1.0</span>), tf.float32)</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">return</span> lambda_rpn_regr * K.sum(y_true[:, :, :, :<span class=\"number\">4</span> * num_anchors] * (x_bool * (<span class=\"number\">0.5</span> * x * x) + (<span class=\"number\">1</span> - x_bool) * (x_abs - <span class=\"number\">0.5</span>))) / K.sum(epsilon + y_true[:, :, :, :<span class=\"number\">4</span> * num_anchors])</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"keyword\">return</span> rpn_loss_regr_fixed_num</span><br></pre></td></tr></table></figure></p>\n<p>classlossclass_loss_clslossK.meanloss<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">class_loss_regr</span><span class=\"params\">(num_classes)</span>:</span></span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">class_loss_regr_fixed_num</span><span class=\"params\">(y_true, y_pred)</span>:</span></span><br><span class=\"line\">\t\tx = y_true[:, :, <span class=\"number\">4</span>*num_classes:] - y_pred</span><br><span class=\"line\">\t\tx_abs = K.abs(x)</span><br><span class=\"line\">\t\tx_bool = K.cast(K.less_equal(x_abs, <span class=\"number\">1.0</span>), <span class=\"string\">'float32'</span>)</span><br><span class=\"line\">\t\t<span class=\"keyword\">return</span> lambda_cls_regr * K.sum(y_true[:, :, :<span class=\"number\">4</span>*num_classes] * (x_bool * (<span class=\"number\">0.5</span> * x * x) + (<span class=\"number\">1</span> - x_bool) * (x_abs - <span class=\"number\">0.5</span>))) / K.sum(epsilon + y_true[:, :, :<span class=\"number\">4</span>*num_classes])</span><br><span class=\"line\">\t<span class=\"keyword\">return</span> class_loss_regr_fixed_num</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">class_loss_cls</span><span class=\"params\">(y_true, y_pred)</span>:</span></span><br><span class=\"line\">\t<span class=\"keyword\">return</span> lambda_cls_class * K.mean(categorical_crossentropy(y_true[<span class=\"number\">0</span>, :, :], y_pred[<span class=\"number\">0</span>, :, :]))</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"11-07-2018\"><a href=\"#11-07-2018\" class=\"headerlink\" title=\"11/07/2018\"></a>11/07/2018</h3><h4 id=\"Iridis\"><a href=\"#Iridis\" class=\"headerlink\" title=\"Iridis\"></a>Iridis</h4><h4 id=\"High-Performance-Computing-HPC\"><a href=\"#High-Performance-Computing-HPC\" class=\"headerlink\" title=\"High Performance Computing (HPC)\"></a>High Performance Computing (HPC)</h4><p><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/8_1.jpg\" alt=\"image\"><br><a href=\"https://www.southampton.ac.uk/isolutions/staff/high-performance-computing.page\" target=\"_blank\" rel=\"noopener\">Introduction</a></p>\n<p>Iridis 5 specifications</p>\n<ul>\n<li>#251 in hte world(Based on July 2018 TOPP500 list) with $R_{peak}\\sim1305.6\\ TFlops/s$</li>\n<li>464 2.0 GHz nodes with 40 cores per node, 192 GB memeory</li>\n<li>10 nodes with 4xGTX1080TI GPUs, 28 cores(hyper-threaded), 128 GB memeory</li>\n<li>10 nodes with 2xVolta Tesia GPUs, same as thandard compute</li>\n<li>2.2 PB disk with paraller file system (&gt;12GB\\s)</li>\n<li>5M Project delivered by OCF/IBM</li>\n</ul>\n<p><a href=\"https://mobaxterm.mobatek.net/\" target=\"_blank\" rel=\"noopener\">MobaXterm</a></p>\n<h4 id=\"create-my-own-conda-envieroment\"><a href=\"#create-my-own-conda-envieroment\" class=\"headerlink\" title=\"create my own conda envieroment\"></a>create my own conda envieroment</h4><p>Fllowing instroduction before</p>\n<h4 id=\"Slurm-command\"><a href=\"#Slurm-command\" class=\"headerlink\" title=\"Slurm command\"></a>Slurm command</h4><table>\n<thead>\n<tr>\n<th>Command</th>\n<th>Definition</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>sbatch</td>\n<td>Submits job scripts into system for execution (queued)</td>\n</tr>\n<tr>\n<td>scancel</td>\n<td>Cancels a job</td>\n</tr>\n<tr>\n<td>scontrol</td>\n<td>Used to display Slurm state, several options only available to root</td>\n</tr>\n<tr>\n<td>sinfo</td>\n<td>Display state of partitions and nodes</td>\n</tr>\n<tr>\n<td>squeue</td>\n<td>Display state of jobs</td>\n</tr>\n<tr>\n<td>salloc</td>\n<td>Submit a job for execution, or initiate job in real time</td>\n</tr>\n</tbody>\n</table>\n<p><strong> Bash script</strong><br><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#!/bin/bash</span></span><br><span class=\"line\"><span class=\"comment\">#SBATCH -J faster_rcnn </span></span><br><span class=\"line\"><span class=\"comment\">#SBATCH -o train_7.out</span></span><br><span class=\"line\"><span class=\"comment\">#SBATCH --ntasks=28</span></span><br><span class=\"line\"><span class=\"comment\">#SBATCH --nodes=1</span></span><br><span class=\"line\"><span class=\"comment\">#SBATCH --ntasks-per-node=8</span></span><br><span class=\"line\"><span class=\"comment\">#SBATCH --time=00:05:00</span></span><br><span class=\"line\"><span class=\"comment\">#SBATCH --gres=gpu:1</span></span><br><span class=\"line\"><span class=\"comment\">#SBATCH -p lyceum</span></span><br><span class=\"line\"></span><br><span class=\"line\">module load conda</span><br><span class=\"line\">module load cuda</span><br><span class=\"line\"><span class=\"built_in\">source</span> activate project</span><br><span class=\"line\">python test_frcnn.py</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"12-13-07-2018\"><a href=\"#12-13-07-2018\" class=\"headerlink\" title=\"12~13/07/2018\"></a>12~13/07/2018</h3><h4 id=\"change-plan\"><a href=\"#change-plan\" class=\"headerlink\" title=\"change plan\"></a>change plan</h4><p>faster r-cnncapsulefaster-rcnnfine turn</p>\n<p>1<br>ResNetIncRes V2ResNeXt  VGG </p>\n<p>2RPN<br>  RPN  Proposal </p>\n<p>3<br>   </p>\n<hr>\n<p>@1ION<br>Inside outside net: Detecting objects in context with skip pooling and recurrent neural networks<br></p>\n<p>1Inside Net<br> Inside  ROI  Scale  Feature Map<br> Skip-Pooling conv3-4-5-context <br> </p>\n<p>2Outside Net<br> Outside  ROI  Contextual<br> RNN  IRNN<br> </p>\n<hr>\n<p>@2 HyperNet<br>Hypernet: Towards accurate region proposal generation and joint object detection<br> Region Proposal <br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/8_2.JPG\" alt=\"image\"><br>   </p>\n<p>1Hyper Feature Extraction <br> Feature  Max Pooling  Deconv<br> Feature ConCat  LRN ION  L2 Norm</p>\n<p>2Region Proposal Generation<br> ConvNet RPN )<br> ROI Pooling Conv  FC  Position  ROI Pooling  13*13  bin Conv3*3*4 13*13*4  Cube FC  256d <br> Score+ BBox_Reg  Faster  Location OffSet<br> Overlap Greedy NMS  IOU 0.7 Image  1k  Region Top-200  Detetcion<br> Edge Box  Deep Box Deep Proposal </p>\n<p>3Object Detection<br> Fast RCNN<br>a FC  3<em>3</em>63<br>b DropOut  0.5  0.25<br> Proposal NMS  Box</p>\n<hr>\n<p>@3 MSCNN<br>A Unified Multi-scale Deep Convolutional Neural Network for Fast Object Detection<br>aScaleScaleFeature<br>ScaleScaleFeature</p>\n<p>b<br></p>\n<p>cScale-&gt;-&gt;Model<br> a b Trade-Off</p>\n<p>dScale-&gt;-&gt;-&gt;1Model</p>\n<p>eRCNNProposalCNN<br> aPatch</p>\n<p>fRPNCNN<br> b</p>\n<p>g<br> cover</p>\n<hr>\n<p>NMSsoft-nms<br>Repulsion loss overlapping  loss<br>faster rcnn</p>\n<h3 id=\"16-20-07-2018\"><a href=\"#16-20-07-2018\" class=\"headerlink\" title=\"16~20/07/2018\"></a>16~20/07/2018</h3><p></p>\n<h3 id=\"23-07-2018\"><a href=\"#23-07-2018\" class=\"headerlink\" title=\"23/07/2018\"></a>23/07/2018</h3><h4 id=\"fix-boxes-location-by-regrident\"><a href=\"#fix-boxes-location-by-regrident\" class=\"headerlink\" title=\"fix boxes location by regrident\"></a>fix boxes location by regrident</h4><p>regranchor</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"string\">\"\"\" fix boxes with grident</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">@param X: current cordinates of box</span></span><br><span class=\"line\"><span class=\"string\">@param T: coresspoding grident</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">@return: Fixed cordinates of box</span></span><br><span class=\"line\"><span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">apply_regr_np</span><span class=\"params\">(X, T)</span>:</span></span><br><span class=\"line\">\t<span class=\"keyword\">try</span>:</span><br><span class=\"line\">\t\tx = X[<span class=\"number\">0</span>, :, :]</span><br><span class=\"line\">\t\ty = X[<span class=\"number\">1</span>, :, :]</span><br><span class=\"line\">\t\tw = X[<span class=\"number\">2</span>, :, :]</span><br><span class=\"line\">\t\th = X[<span class=\"number\">3</span>, :, :]</span><br><span class=\"line\"></span><br><span class=\"line\">\t\ttx = T[<span class=\"number\">0</span>, :, :]</span><br><span class=\"line\">\t\tty = T[<span class=\"number\">1</span>, :, :]</span><br><span class=\"line\">\t\ttw = T[<span class=\"number\">2</span>, :, :]</span><br><span class=\"line\">\t\tth = T[<span class=\"number\">3</span>, :, :]</span><br></pre></td></tr></table></figure>\n<p>$t_{x}=\\frac{(x-x_{a})}{w_{a}}$ , $t_{y}=\\frac{(y-y_{a})}{h_{a}}$ , $t_{w}=log(\\frac{w}{w_{a}})$ , $t_{h}=log(\\frac{h}{h_{a}})$ (1)<br>$t_{x}^{\\ast}=\\frac{(x^{\\ast}-x_{a})}{w_{a}}$ , $t_{y}^{\\ast}=\\frac{(y^{\\ast}-y_{a})}{h_{a}}$ , $t_{w}^{\\ast}=log(\\frac{w^{\\ast}}{w_{a}})$ , $t_{h}^{\\ast}=log(\\frac{h^{\\ast}}{h_{a}})$ (2)</p>\n<p>{t}${t^{\\ast}}$${t}$${t^{\\ast}}$${t}$(1)</p>\n<p><br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">\t<span class=\"comment\"># centre cordinate</span></span><br><span class=\"line\">\tcx = x + w/<span class=\"number\">2.</span></span><br><span class=\"line\">\tcy = y + h/<span class=\"number\">2.</span></span><br><span class=\"line\">\t<span class=\"comment\"># fixed centre cordinate</span></span><br><span class=\"line\">\tcx1 = tx * w + cx</span><br><span class=\"line\">\tcy1 = ty * h + cy</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"comment\"># fixed wdith and height</span></span><br><span class=\"line\">\tw1 = np.exp(tw.astype(np.float64)) * w</span><br><span class=\"line\">\th1 = np.exp(th.astype(np.float64)) * h</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"comment\"># fixed left top corner's cordinate</span></span><br><span class=\"line\">\tx1 = cx1 - w1/<span class=\"number\">2.</span></span><br><span class=\"line\">\ty1 = cy1 - h1/<span class=\"number\">2.</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"comment\"># apporximate</span></span><br><span class=\"line\">\tx1 = np.round(x1)</span><br><span class=\"line\">\ty1 = np.round(y1)</span><br><span class=\"line\">\tw1 = np.round(w1)</span><br><span class=\"line\">\th1 = np.round(h1)</span><br><span class=\"line\">\t<span class=\"keyword\">return</span> np.stack([x1, y1, w1, h1])</span><br><span class=\"line\"><span class=\"keyword\">except</span> Exception <span class=\"keyword\">as</span> e:</span><br><span class=\"line\">\tprint(e)</span><br><span class=\"line\">\t<span class=\"keyword\">return</span> X</span><br></pre></td></tr></table></figure></p>\n<h4 id=\"NMS-no-max-suppression\"><a href=\"#NMS-no-max-suppression\" class=\"headerlink\" title=\"NMS no max suppression\"></a>NMS no max suppression</h4><p></p>\n<p><br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"string\">\"\"\" NMS , delete overlapping box</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">@param boxes: (n,4) box and coresspoding cordinates</span></span><br><span class=\"line\"><span class=\"string\">@param probs: (n,) box adn coresspding possibility</span></span><br><span class=\"line\"><span class=\"string\">@param overlap_thresh: treshold of delet box overlapping</span></span><br><span class=\"line\"><span class=\"string\">@param max_boxes: maximum keeping number of boxes</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">@return: boxes: boxes cordinates(x1,y1,x2,y2)</span></span><br><span class=\"line\"><span class=\"string\">@return: probs: coresspoding possibility</span></span><br><span class=\"line\"><span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">non_max_suppression_fast</span><span class=\"params\">(boxes, probs, overlap_thresh=<span class=\"number\">0.9</span>, max_boxes=<span class=\"number\">300</span>)</span>:</span></span><br></pre></td></tr></table></figure></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">if</span> len(boxes) == <span class=\"number\">0</span>:</span><br><span class=\"line\">   <span class=\"keyword\">return</span> []</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># grab the coordinates of the bounding boxes</span></span><br><span class=\"line\">x1 = boxes[:, <span class=\"number\">0</span>]</span><br><span class=\"line\">y1 = boxes[:, <span class=\"number\">1</span>]</span><br><span class=\"line\">x2 = boxes[:, <span class=\"number\">2</span>]</span><br><span class=\"line\">y2 = boxes[:, <span class=\"number\">3</span>]</span><br><span class=\"line\"></span><br><span class=\"line\">np.testing.assert_array_less(x1, x2)</span><br><span class=\"line\">np.testing.assert_array_less(y1, y2)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># if the bounding boxes integers, convert them to floats --</span></span><br><span class=\"line\"><span class=\"comment\"># this is important since we'll be doing a bunch of divisions</span></span><br><span class=\"line\"><span class=\"keyword\">if</span> boxes.dtype.kind == <span class=\"string\">\"i\"</span>:</span><br><span class=\"line\">\tboxes = boxes.astype(<span class=\"string\">\"float\"</span>)</span><br></pre></td></tr></table></figure>\n<p><br><br><br><br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># initialize the list of picked indexes\t</span></span><br><span class=\"line\">pick = []</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># calculate the areas</span></span><br><span class=\"line\">area = (x2 - x1) * (y2 - y1)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># sort the bounding boxes </span></span><br><span class=\"line\">idxs = np.argsort(probs)</span><br></pre></td></tr></table></figure></p>\n<p>pick<br><br>probs<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">while</span> len(idxs) &gt; <span class=\"number\">0</span>:</span><br><span class=\"line\"><span class=\"comment\"># grab the last index in the indexes list and add the</span></span><br><span class=\"line\"><span class=\"comment\"># index value to the list of picked indexes</span></span><br><span class=\"line\">last = len(idxs) - <span class=\"number\">1</span></span><br><span class=\"line\">i = idxs[last]</span><br><span class=\"line\">pick.append(i)</span><br></pre></td></tr></table></figure></p>\n<p>overlap_thresh</p>\n<p>idxs<br>overlap_thresh<br>max_boxes<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># find the intersection</span></span><br><span class=\"line\"></span><br><span class=\"line\">xx1_int = np.maximum(x1[i], x1[idxs[:last]])</span><br><span class=\"line\">yy1_int = np.maximum(y1[i], y1[idxs[:last]])</span><br><span class=\"line\">xx2_int = np.minimum(x2[i], x2[idxs[:last]])</span><br><span class=\"line\">yy2_int = np.minimum(y2[i], y2[idxs[:last]])</span><br><span class=\"line\"></span><br><span class=\"line\">ww_int = np.maximum(<span class=\"number\">0</span>, xx2_int - xx1_int)</span><br><span class=\"line\">hh_int = np.maximum(<span class=\"number\">0</span>, yy2_int - yy1_int)</span><br><span class=\"line\"></span><br><span class=\"line\">area_int = ww_int * hh_int</span><br></pre></td></tr></table></figure></p>\n<p>idxspick<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># find the union</span></span><br><span class=\"line\">area_union = area[i] + area[idxs[:last]] - area_int</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># compute the ratio of overlap</span></span><br><span class=\"line\">overlap = area_int/(area_union + <span class=\"number\">1e-6</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># delete all indexes from the index list that have</span></span><br><span class=\"line\">idxs = np.delete(idxs, np.concatenate(([last],np.where(overlap &gt; overlap_thresh)[<span class=\"number\">0</span>])))</span><br></pre></td></tr></table></figure></p>\n<p><br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">if</span> len(pick) &gt;= max_boxes:</span><br><span class=\"line\">   <span class=\"keyword\">break</span></span><br></pre></td></tr></table></figure></p>\n<p>[np.concatest]<br>max_boxes<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">boxes = boxes[pick].astype(<span class=\"string\">\"int\"</span>)</span><br><span class=\"line\">probs = probs[pick]</span><br><span class=\"line\"><span class=\"keyword\">return</span> boxes, probs</span><br></pre></td></tr></table></figure></p>\n<p>pick</p>\n<h3 id=\"24-07-2018\"><a href=\"#24-07-2018\" class=\"headerlink\" title=\"24/07/2018\"></a>24/07/2018</h3><h4 id=\"rpn-to-porposal-fixed\"><a href=\"#rpn-to-porposal-fixed\" class=\"headerlink\" title=\"rpn to porposal fixed\"></a>rpn to porposal fixed</h4><p>rpn<br><br>anchor_sizeanchor_ratio</p>\n<p></p>\n<p><br>1regr_layer[0, :, :, 4 * curr_layer:4 * curr_layer + 4]<br>2curr_layer</p>\n<p>anchorx,y,w,h</p>\n<p>regranchor</p>\n<p><br>0<br>x1,y1,x2,y2</p>\n<p>all_boxesn,4all_probsn,</p>\n<p><br>np.where() <br>np.delete(all_boxes, idxs, 0)</p>\n<p>9<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"string\">\"\"\" rpn to porposal</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">@param rpn_layer: porposal's coresspoding possibiliy, whether item exciseted</span></span><br><span class=\"line\"><span class=\"string\">@param regr_layer: porposal's coresspoding regrident</span></span><br><span class=\"line\"><span class=\"string\">@param C: Configuration</span></span><br><span class=\"line\"><span class=\"string\">@param dim_ordering: Dimensional organization</span></span><br><span class=\"line\"><span class=\"string\">@param use_regr=True: wether use regurident to fix proposal</span></span><br><span class=\"line\"><span class=\"string\">@param max_boxes=300: max boxes after apply this function</span></span><br><span class=\"line\"><span class=\"string\">@param overlap_thresh=0.9: threshold of overlapping</span></span><br><span class=\"line\"><span class=\"string\">@param C: Configuration</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">@return: max_boxes proposal with format (x1,y1,x2,y2)</span></span><br><span class=\"line\"><span class=\"string\">\"\"\"</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">rpn_to_roi</span><span class=\"params\">(rpn_layer, regr_layer, C, dim_ordering, use_regr=True, max_boxes=<span class=\"number\">300</span>,overlap_thresh=<span class=\"number\">0.9</span>)</span>:</span></span><br><span class=\"line\">\t<span class=\"comment\"># std_scaling default 4</span></span><br><span class=\"line\">\tregr_layer = regr_layer / C.std_scaling</span><br><span class=\"line\"></span><br><span class=\"line\">\tanchor_sizes = C.anchor_box_scales</span><br><span class=\"line\">\tanchor_ratios = C.anchor_box_ratios</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"keyword\">assert</span> rpn_layer.shape[<span class=\"number\">0</span>] == <span class=\"number\">1</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"comment\"># obtain img's width and height's matrix</span></span><br><span class=\"line\">\t(rows, cols) = rpn_layer.shape[<span class=\"number\">1</span>:<span class=\"number\">3</span>]</span><br><span class=\"line\"></span><br><span class=\"line\">\tcurr_layer = <span class=\"number\">0</span></span><br><span class=\"line\"></span><br><span class=\"line\">\tA = np.zeros((<span class=\"number\">4</span>, rpn_layer.shape[<span class=\"number\">1</span>], rpn_layer.shape[<span class=\"number\">2</span>], rpn_layer.shape[<span class=\"number\">3</span>]))</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"comment\"># anchor size is [128, 256, 512]</span></span><br><span class=\"line\">\t<span class=\"keyword\">for</span> anchor_size <span class=\"keyword\">in</span> anchor_sizes:</span><br><span class=\"line\">\t\t<span class=\"comment\"># anchor ratio is [1,2,1]</span></span><br><span class=\"line\">\t\t<span class=\"keyword\">for</span> anchor_ratio <span class=\"keyword\">in</span> anchor_ratios:</span><br><span class=\"line\">\t\t\t<span class=\"comment\"># rpn_stride = 16</span></span><br><span class=\"line\">\t\t\t<span class=\"comment\"># obatin anchor's weidth and height on feature map</span></span><br><span class=\"line\">\t\t\tanchor_x = (anchor_size * anchor_ratio[<span class=\"number\">0</span>])/C.rpn_stride</span><br><span class=\"line\">\t\t\tanchor_y = (anchor_size * anchor_ratio[<span class=\"number\">1</span>])/C.rpn_stride</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t<span class=\"comment\"># obtain current regrident</span></span><br><span class=\"line\">\t\t\t<span class=\"comment\"># when one dimentional obtain a value, the new varirant will decrease one dimenttion</span></span><br><span class=\"line\">\t\t\tregr = regr_layer[<span class=\"number\">0</span>, :, :, <span class=\"number\">4</span> * curr_layer:<span class=\"number\">4</span> * curr_layer + <span class=\"number\">4</span>]</span><br><span class=\"line\">\t\t\t<span class=\"comment\"># put depth to first bacause tensorflow as backend</span></span><br><span class=\"line\">\t\t\tregr = np.transpose(regr, (<span class=\"number\">2</span>, <span class=\"number\">0</span>, <span class=\"number\">1</span>))</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t<span class=\"comment\"># The rows of the output array X are copies of the vector x; columns of the output array Y are copies of the vector y</span></span><br><span class=\"line\">\t\t\t<span class=\"comment\"># each cordinartes of matrix cls and rows</span></span><br><span class=\"line\">\t\t\tX, Y = np.meshgrid(np.arange(cols),np. arange(rows))</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t<span class=\"comment\"># obtain anchors's (x,y,w,h)</span></span><br><span class=\"line\">\t\t\tA[<span class=\"number\">0</span>, :, :, curr_layer] = X - anchor_x/<span class=\"number\">2</span></span><br><span class=\"line\">\t\t\tA[<span class=\"number\">1</span>, :, :, curr_layer] = Y - anchor_y/<span class=\"number\">2</span></span><br><span class=\"line\">\t\t\tA[<span class=\"number\">2</span>, :, :, curr_layer] = anchor_x</span><br><span class=\"line\">\t\t\tA[<span class=\"number\">3</span>, :, :, curr_layer] = anchor_y</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t<span class=\"comment\"># fix boxes with grident</span></span><br><span class=\"line\">\t\t\t<span class=\"keyword\">if</span> use_regr:</span><br><span class=\"line\">\t\t\t\t<span class=\"comment\"># fixed corinates of box</span></span><br><span class=\"line\">\t\t\t\tA[:, :, :, curr_layer] = apply_regr_np(A[:, :, :, curr_layer], regr)</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t<span class=\"comment\"># fix unreasonable cordinates</span></span><br><span class=\"line\">\t\t\t<span class=\"comment\"># np.maximum(1,[]) will set the value less than 1 in [] to 1</span></span><br><span class=\"line\">\t\t\t<span class=\"comment\"># box's width and height can't less than 0</span></span><br><span class=\"line\">\t\t\tA[<span class=\"number\">2</span>, :, :, curr_layer] = np.maximum(<span class=\"number\">1</span>, A[<span class=\"number\">2</span>, :, :, curr_layer])</span><br><span class=\"line\">\t\t\tA[<span class=\"number\">3</span>, :, :, curr_layer] = np.maximum(<span class=\"number\">1</span>, A[<span class=\"number\">3</span>, :, :, curr_layer])</span><br><span class=\"line\">\t\t\t<span class=\"comment\"># fixed right bottom cordinates</span></span><br><span class=\"line\">\t\t\tA[<span class=\"number\">2</span>, :, :, curr_layer] += A[<span class=\"number\">0</span>, :, :, curr_layer]</span><br><span class=\"line\">\t\t\tA[<span class=\"number\">3</span>, :, :, curr_layer] += A[<span class=\"number\">1</span>, :, :, curr_layer]</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t<span class=\"comment\"># left top corner cordinates can't out image</span></span><br><span class=\"line\">\t\t\tA[<span class=\"number\">0</span>, :, :, curr_layer] = np.maximum(<span class=\"number\">0</span>, A[<span class=\"number\">0</span>, :, :, curr_layer])</span><br><span class=\"line\">\t\t\tA[<span class=\"number\">1</span>, :, :, curr_layer] = np.maximum(<span class=\"number\">0</span>, A[<span class=\"number\">1</span>, :, :, curr_layer])</span><br><span class=\"line\">\t\t\t<span class=\"comment\"># right bottom corner cordinates can't out img</span></span><br><span class=\"line\">\t\t\tA[<span class=\"number\">2</span>, :, :, curr_layer] = np.minimum(cols<span class=\"number\">-1</span>, A[<span class=\"number\">2</span>, :, :, curr_layer])</span><br><span class=\"line\">\t\t\tA[<span class=\"number\">3</span>, :, :, curr_layer] = np.minimum(rows<span class=\"number\">-1</span>, A[<span class=\"number\">3</span>, :, :, curr_layer])</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t<span class=\"comment\"># next layer</span></span><br><span class=\"line\">\t\t\tcurr_layer += <span class=\"number\">1</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"comment\"># obtain (n,4) object and coresspoding cordinate</span></span><br><span class=\"line\">\tall_boxes = np.reshape(A.transpose((<span class=\"number\">0</span>, <span class=\"number\">3</span>, <span class=\"number\">1</span>,<span class=\"number\">2</span>)), (<span class=\"number\">4</span>, <span class=\"number\">-1</span>)).transpose((<span class=\"number\">1</span>, <span class=\"number\">0</span>))</span><br><span class=\"line\">\t<span class=\"comment\"># obtain(n,) object and creoespdoing possibility</span></span><br><span class=\"line\">\tall_probs = rpn_layer.transpose((<span class=\"number\">0</span>, <span class=\"number\">3</span>, <span class=\"number\">1</span>, <span class=\"number\">2</span>)).reshape((<span class=\"number\">-1</span>))</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"comment\"># cordinates of left top and right bottom of box</span></span><br><span class=\"line\">\tx1 = all_boxes[:, <span class=\"number\">0</span>]</span><br><span class=\"line\">\ty1 = all_boxes[:, <span class=\"number\">1</span>]</span><br><span class=\"line\">\tx2 = all_boxes[:, <span class=\"number\">2</span>]</span><br><span class=\"line\">\ty2 = all_boxes[:, <span class=\"number\">3</span>]</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"comment\"># find where right cordinate bigger than left cordinate</span></span><br><span class=\"line\">\tidxs = np.where((x1 - x2 &gt;= <span class=\"number\">0</span>) | (y1 - y2 &gt;= <span class=\"number\">0</span>))</span><br><span class=\"line\">\t<span class=\"comment\"># delete thoese point at 0 dimentional -&gt; all boxes</span></span><br><span class=\"line\">\tall_boxes = np.delete(all_boxes, idxs, <span class=\"number\">0</span>)</span><br><span class=\"line\">\tall_probs = np.delete(all_probs, idxs, <span class=\"number\">0</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"comment\"># apply NMS to reduce overlapping boxes</span></span><br><span class=\"line\">\tresult = non_max_suppression_fast(all_boxes, all_probs, overlap_thresh=overlap_thresh, max_boxes=max_boxes)[<span class=\"number\">0</span>]</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"keyword\">return</span> result</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"25-07-2018\"><a href=\"#25-07-2018\" class=\"headerlink\" title=\"25/07/2018\"></a>25/07/2018</h3><h4 id=\"generate-classifiers-trainning-data\"><a href=\"#generate-classifiers-trainning-data\" class=\"headerlink\" title=\"generate classifiers trainning data\"></a>generate classifiers trainning data</h4><p>classifier,</p>\n<p><br>bboxes</p>\n<p>R, bboxes</p>\n<p>:<br><br><br></p>\n<p><br>1one-hot<br>y_class_num<br>coordslabelsloss<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">class_num = <span class=\"number\">2</span></span><br><span class=\"line\">class_label = <span class=\"number\">10</span> * [<span class=\"number\">0</span>]</span><br><span class=\"line\">print(class_label)</span><br><span class=\"line\">class_label[class_num] = <span class=\"number\">1</span></span><br><span class=\"line\">print(class_label)</span><br><span class=\"line\"></span><br><span class=\"line\">[<span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>]</span><br><span class=\"line\">[<span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">1</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>]</span><br></pre></td></tr></table></figure></p>\n<p></p>\n<p></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br><span class=\"line\">107</span><br><span class=\"line\">108</span><br><span class=\"line\">109</span><br><span class=\"line\">110</span><br><span class=\"line\">111</span><br><span class=\"line\">112</span><br><span class=\"line\">113</span><br><span class=\"line\">114</span><br><span class=\"line\">115</span><br><span class=\"line\">116</span><br><span class=\"line\">117</span><br><span class=\"line\">118</span><br><span class=\"line\">119</span><br><span class=\"line\">120</span><br><span class=\"line\">121</span><br><span class=\"line\">122</span><br><span class=\"line\">123</span><br><span class=\"line\">124</span><br><span class=\"line\">125</span><br><span class=\"line\">126</span><br><span class=\"line\">127</span><br><span class=\"line\">128</span><br><span class=\"line\">129</span><br><span class=\"line\">130</span><br><span class=\"line\">131</span><br><span class=\"line\">132</span><br><span class=\"line\">133</span><br><span class=\"line\">134</span><br><span class=\"line\">135</span><br><span class=\"line\">136</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"string\">\"\"\" generate classifier training data</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">@param R: porposal -&gt; boxes</span></span><br><span class=\"line\"><span class=\"string\">@param img_data: image data</span></span><br><span class=\"line\"><span class=\"string\">@param C: configuration</span></span><br><span class=\"line\"><span class=\"string\">@param class_mapping: classes and coresspoding numbers</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">@return: np.expand_dims(X, axis=0): boxes after filter</span></span><br><span class=\"line\"><span class=\"string\">@return: np.expand_dims(Y1, axis=0): boxes coresspoding class</span></span><br><span class=\"line\"><span class=\"string\">@return: np.expand_dims(Y2, axis=0): boxes coresspoding regurident</span></span><br><span class=\"line\"><span class=\"string\">@return IoUs: IOU</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">calc_iou</span><span class=\"params\">(R, img_data, C, class_mapping)</span>:</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"comment\"># obtain boxxes information from img data</span></span><br><span class=\"line\">\tbboxes = img_data[<span class=\"string\">'bboxes'</span>]</span><br><span class=\"line\">\t<span class=\"comment\"># obtain width and height of img</span></span><br><span class=\"line\">\t(width, height) = (img_data[<span class=\"string\">'width'</span>], img_data[<span class=\"string\">'height'</span>])</span><br><span class=\"line\">\t<span class=\"comment\"># get image dimensions for resizing</span></span><br><span class=\"line\">\t<span class=\"comment\"># Fix image's shortest edge to config setting: eg: 600</span></span><br><span class=\"line\">\t(resized_width, resized_height) = get_new_img_size(width, height, C.im_size)</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"comment\"># record parameters, bboxes cordinates on feature map</span></span><br><span class=\"line\">\tgta = np.zeros((len(bboxes), <span class=\"number\">4</span>))</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"comment\"># change bboxes's width and height because the img was rezised</span></span><br><span class=\"line\">\t<span class=\"keyword\">for</span> bbox_num, bbox <span class=\"keyword\">in</span> enumerate(bboxes):</span><br><span class=\"line\">\t\t<span class=\"comment\"># get the GT box coordinates, and resize to account for image resizing</span></span><br><span class=\"line\">\t\t<span class=\"comment\"># /C.rpn_stride mapping to feature map</span></span><br><span class=\"line\">\t\tgta[bbox_num, <span class=\"number\">0</span>] = int(round(bbox[<span class=\"string\">'x1'</span>] * (resized_width / float(width))/C.rpn_stride))</span><br><span class=\"line\">\t\tgta[bbox_num, <span class=\"number\">1</span>] = int(round(bbox[<span class=\"string\">'x2'</span>] * (resized_width / float(width))/C.rpn_stride))</span><br><span class=\"line\">\t\tgta[bbox_num, <span class=\"number\">2</span>] = int(round(bbox[<span class=\"string\">'y1'</span>] * (resized_height / float(height))/C.rpn_stride))</span><br><span class=\"line\">\t\tgta[bbox_num, <span class=\"number\">3</span>] = int(round(bbox[<span class=\"string\">'y2'</span>] * (resized_height / float(height))/C.rpn_stride))</span><br><span class=\"line\"></span><br><span class=\"line\">\tx_roi = []</span><br><span class=\"line\">\ty_class_num = []</span><br><span class=\"line\">\ty_class_regr_coords = []</span><br><span class=\"line\">\ty_class_regr_label = []</span><br><span class=\"line\">\tIoUs = [] <span class=\"comment\"># for debugging only</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"comment\"># for all given proposals -&gt; boxes</span></span><br><span class=\"line\">\t<span class=\"keyword\">for</span> ix <span class=\"keyword\">in</span> range(R.shape[<span class=\"number\">0</span>]):</span><br><span class=\"line\">\t\t<span class=\"comment\"># current boxes's cordinates</span></span><br><span class=\"line\">\t\t(x1, y1, x2, y2) = R[ix, :]</span><br><span class=\"line\">\t\tx1 = int(round(x1))</span><br><span class=\"line\">\t\ty1 = int(round(y1))</span><br><span class=\"line\">\t\tx2 = int(round(x2))</span><br><span class=\"line\">\t\ty2 = int(round(y2))</span><br><span class=\"line\"></span><br><span class=\"line\">\t\tbest_iou = <span class=\"number\">0.0</span></span><br><span class=\"line\">\t\tbest_bbox = <span class=\"number\">-1</span></span><br><span class=\"line\">\t\t<span class=\"comment\"># using current proposal to compare with given xml's boxes</span></span><br><span class=\"line\">\t\t<span class=\"keyword\">for</span> bbox_num <span class=\"keyword\">in</span> range(len(bboxes)):</span><br><span class=\"line\">\t\t\t<span class=\"comment\"># calculate current iou</span></span><br><span class=\"line\">\t\t\tcurr_iou = iou([gta[bbox_num, <span class=\"number\">0</span>], gta[bbox_num, <span class=\"number\">2</span>], gta[bbox_num, <span class=\"number\">1</span>], gta[bbox_num, <span class=\"number\">3</span>]], [x1, y1, x2, y2])</span><br><span class=\"line\">\t\t\t<span class=\"comment\"># update parameters</span></span><br><span class=\"line\">\t\t\t<span class=\"keyword\">if</span> curr_iou &gt; best_iou:</span><br><span class=\"line\">\t\t\t\tbest_iou = curr_iou</span><br><span class=\"line\">\t\t\t\tbest_bbox = bbox_num</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"comment\"># if iou to small, we don't put it in trainning because it should be backgroud</span></span><br><span class=\"line\">\t\t<span class=\"keyword\">if</span> best_iou &lt; C.classifier_min_overlap:</span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">continue</span></span><br><span class=\"line\">\t\t<span class=\"keyword\">else</span>:</span><br><span class=\"line\">\t\t\t<span class=\"comment\"># saveing left top cordinates, width and height</span></span><br><span class=\"line\">\t\t\tw = x2 - x1</span><br><span class=\"line\">\t\t\th = y2 - y1</span><br><span class=\"line\">\t\t\tx_roi.append([x1, y1, w, h])</span><br><span class=\"line\">\t\t\t<span class=\"comment\"># saving this bbox's iou</span></span><br><span class=\"line\">\t\t\tIoUs.append(best_iou)</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t<span class=\"comment\"># hard to classfier -&gt; set it to backgroud</span></span><br><span class=\"line\">\t\t\t<span class=\"keyword\">if</span> C.classifier_min_overlap &lt;= best_iou &lt; C.classifier_max_overlap:</span><br><span class=\"line\">\t\t\t\t<span class=\"comment\"># hard negative example</span></span><br><span class=\"line\">\t\t\t\tcls_name = <span class=\"string\">'bg'</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t<span class=\"comment\"># valid proposal</span></span><br><span class=\"line\">\t\t\t<span class=\"keyword\">elif</span> C.classifier_max_overlap &lt;= best_iou:</span><br><span class=\"line\">\t\t\t\t<span class=\"comment\"># coresspoding class name</span></span><br><span class=\"line\">\t\t\t\tcls_name = bboxes[best_bbox][<span class=\"string\">'class'</span>]</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t<span class=\"comment\"># calculate rpn graident with true cordinates given by xml file</span></span><br><span class=\"line\">\t\t\t\tcxg = (gta[best_bbox, <span class=\"number\">0</span>] + gta[best_bbox, <span class=\"number\">1</span>]) / <span class=\"number\">2.0</span></span><br><span class=\"line\">\t\t\t\tcyg = (gta[best_bbox, <span class=\"number\">2</span>] + gta[best_bbox, <span class=\"number\">3</span>]) / <span class=\"number\">2.0</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\tcx = x1 + w / <span class=\"number\">2.0</span></span><br><span class=\"line\">\t\t\t\tcy = y1 + h / <span class=\"number\">2.0</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\ttx = (cxg - cx) / float(w)</span><br><span class=\"line\">\t\t\t\tty = (cyg - cy) / float(h)</span><br><span class=\"line\">\t\t\t\ttw = np.log((gta[best_bbox, <span class=\"number\">1</span>] - gta[best_bbox, <span class=\"number\">0</span>]) / float(w))</span><br><span class=\"line\">\t\t\t\tth = np.log((gta[best_bbox, <span class=\"number\">3</span>] - gta[best_bbox, <span class=\"number\">2</span>]) / float(h))</span><br><span class=\"line\">\t\t\t<span class=\"keyword\">else</span>:</span><br><span class=\"line\">\t\t\t\tprint(<span class=\"string\">'roi = &#123;&#125;'</span>.format(best_iou))</span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">raise</span> RuntimeError</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"comment\"># class name's mapping number</span></span><br><span class=\"line\">\t\tclass_num = class_mapping[cls_name]</span><br><span class=\"line\">\t\t<span class=\"comment\"># list of calss label</span></span><br><span class=\"line\">\t\tclass_label = len(class_mapping) * [<span class=\"number\">0</span>]</span><br><span class=\"line\">\t\t<span class=\"comment\"># set class_num's coresspoding location to 1</span></span><br><span class=\"line\">\t\tclass_label[class_num] = <span class=\"number\">1</span></span><br><span class=\"line\">\t\t<span class=\"comment\"># privous is one-hot vector</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"comment\"># saving the one-hot vector</span></span><br><span class=\"line\">\t\ty_class_num.append(copy.deepcopy(class_label))</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"comment\"># coords used to saving calculated graident</span></span><br><span class=\"line\">\t\tcoords = [<span class=\"number\">0</span>] * <span class=\"number\">4</span> * (len(class_mapping) - <span class=\"number\">1</span>)</span><br><span class=\"line\">\t\t<span class=\"comment\"># labels used to decide whether adding to loss calculation</span></span><br><span class=\"line\">\t\tlabels = [<span class=\"number\">0</span>] * <span class=\"number\">4</span> * (len(class_mapping) - <span class=\"number\">1</span>)</span><br><span class=\"line\">\t\t<span class=\"keyword\">if</span> cls_name != <span class=\"string\">'bg'</span>:</span><br><span class=\"line\">\t\t\tlabel_pos = <span class=\"number\">4</span> * class_num</span><br><span class=\"line\">\t\t\tsx, sy, sw, sh = C.classifier_regr_std</span><br><span class=\"line\">\t\t\tcoords[label_pos:<span class=\"number\">4</span>+label_pos] = [sx*tx, sy*ty, sw*tw, sh*th]</span><br><span class=\"line\">\t\t\tlabels[label_pos:<span class=\"number\">4</span>+label_pos] = [<span class=\"number\">1</span>, <span class=\"number\">1</span>, <span class=\"number\">1</span>, <span class=\"number\">1</span>]</span><br><span class=\"line\">\t\t\ty_class_regr_coords.append(copy.deepcopy(coords))</span><br><span class=\"line\">\t\t\ty_class_regr_label.append(copy.deepcopy(labels))</span><br><span class=\"line\">\t\t<span class=\"keyword\">else</span>:</span><br><span class=\"line\">\t\t\ty_class_regr_coords.append(copy.deepcopy(coords))</span><br><span class=\"line\">\t\t\ty_class_regr_label.append(copy.deepcopy(labels))</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"comment\"># no bboxes</span></span><br><span class=\"line\">\t<span class=\"keyword\">if</span> len(x_roi) == <span class=\"number\">0</span>:</span><br><span class=\"line\">\t\t<span class=\"keyword\">return</span> <span class=\"keyword\">None</span>, <span class=\"keyword\">None</span>, <span class=\"keyword\">None</span>, <span class=\"keyword\">None</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"comment\"># matrix with [x1, y1, w, h]</span></span><br><span class=\"line\">\tX = np.array(x_roi)</span><br><span class=\"line\">\t<span class=\"comment\"># boxxes coresspoding class number</span></span><br><span class=\"line\">\tY1 = np.array(y_class_num)</span><br><span class=\"line\">\t<span class=\"comment\"># matrix of whether adding to calculation and coresspoding regrident</span></span><br><span class=\"line\">\tY2 = np.concatenate([np.array(y_class_regr_label),np.array(y_class_regr_coords)],axis=<span class=\"number\">1</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"comment\"># adding batch size dimention</span></span><br><span class=\"line\">\t<span class=\"keyword\">return</span> np.expand_dims(X, axis=<span class=\"number\">0</span>), np.expand_dims(Y1, axis=<span class=\"number\">0</span>), np.expand_dims(Y2, axis=<span class=\"number\">0</span>), IoUs</span><br></pre></td></tr></table></figure>\n<h3 id=\"26-27-07-2018\"><a href=\"#26-27-07-2018\" class=\"headerlink\" title=\"26~27/07/2018\"></a>26~27/07/2018</h3><h4 id=\"model-parameters\"><a href=\"#model-parameters\" class=\"headerlink\" title=\"model parameters\"></a>model parameters</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># rpn optimizer</span></span><br><span class=\"line\">optimizer = Adam(lr=<span class=\"number\">1e-5</span>)</span><br><span class=\"line\"><span class=\"comment\"># classifier optimizer</span></span><br><span class=\"line\">optimizer_classifier = Adam(lr=<span class=\"number\">1e-5</span>)</span><br><span class=\"line\"><span class=\"comment\"># defined loss apply, metrics used to print accury</span></span><br><span class=\"line\">model_rpn.compile(optimizer=optimizer, loss=[losses.rpn_loss_cls(num_anchors), losses.rpn_loss_regr(num_anchors)])</span><br><span class=\"line\">model_classifier.compile(optimizer=optimizer_classifier, loss=[losses.class_loss_cls, losses.class_loss_regr(len(classes_count)<span class=\"number\">-1</span>)], metrics=&#123;<span class=\"string\">'dense_class_&#123;&#125;'</span>.format(len(classes_count)): <span class=\"string\">'accuracy'</span>&#125;)</span><br><span class=\"line\"><span class=\"comment\"># for saving weight</span></span><br><span class=\"line\">model_all.compile(optimizer=<span class=\"string\">'sgd'</span>, loss=<span class=\"string\">'mae'</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># traing time of each epochs</span></span><br><span class=\"line\">epoch_length = <span class=\"number\">1000</span></span><br><span class=\"line\"><span class=\"comment\"># totoal epochs</span></span><br><span class=\"line\">num_epochs = <span class=\"number\">2000</span></span><br><span class=\"line\"><span class=\"comment\">#</span></span><br><span class=\"line\">iter_num = <span class=\"number\">0</span></span><br><span class=\"line\"><span class=\"comment\"># losses saving matrix</span></span><br><span class=\"line\">losses = np.zeros((epoch_length, <span class=\"number\">5</span>))</span><br><span class=\"line\">rpn_accuracy_rpn_monitor = []</span><br><span class=\"line\">rpn_accuracy_for_epoch = []</span><br><span class=\"line\">start_time = time.time()</span><br><span class=\"line\"><span class=\"comment\"># current total loss</span></span><br><span class=\"line\">best_loss = np.Inf</span><br><span class=\"line\"><span class=\"comment\"># sorted classing mapping</span></span><br><span class=\"line\">class_mapping_inv = &#123;v: k <span class=\"keyword\">for</span> k, v <span class=\"keyword\">in</span> class_mapping.items()&#125;</span><br></pre></td></tr></table></figure>\n<h4 id=\"Training-process\"><a href=\"#Training-process\" class=\"headerlink\" title=\"Training process\"></a>Training process</h4><p><br><strong>rpn</strong><br>RPN,XY</p>\n<p><strong>rpnclassifier:</strong><br><br><br><br>Y1[0, :, -1]0batch-1<br>neg_samples = neg_samples[0]<br>C.num_roisclassifierC.num_rois11</p>\n<p><strong>classifier:</strong><br>Lossaccury<br>loss_class[3]<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">classiferloss</span><br><span class=\"line\">[<span class=\"number\">1.4640709</span>, <span class=\"number\">1.0986123</span>, <span class=\"number\">0.36545864</span>, <span class=\"number\">0.15625</span>]</span><br></pre></td></tr></table></figure></p>\n<p>losslistnumpy<br>epochepochlossepochepoch.</p>\n<hr>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br><span class=\"line\">107</span><br><span class=\"line\">108</span><br><span class=\"line\">109</span><br><span class=\"line\">110</span><br><span class=\"line\">111</span><br><span class=\"line\">112</span><br><span class=\"line\">113</span><br><span class=\"line\">114</span><br><span class=\"line\">115</span><br><span class=\"line\">116</span><br><span class=\"line\">117</span><br><span class=\"line\">118</span><br><span class=\"line\">119</span><br><span class=\"line\">120</span><br><span class=\"line\">121</span><br><span class=\"line\">122</span><br><span class=\"line\">123</span><br><span class=\"line\">124</span><br><span class=\"line\">125</span><br><span class=\"line\">126</span><br><span class=\"line\">127</span><br><span class=\"line\">128</span><br><span class=\"line\">129</span><br><span class=\"line\">130</span><br><span class=\"line\">131</span><br><span class=\"line\">132</span><br><span class=\"line\">133</span><br><span class=\"line\">134</span><br><span class=\"line\">135</span><br><span class=\"line\">136</span><br><span class=\"line\">137</span><br><span class=\"line\">138</span><br><span class=\"line\">139</span><br><span class=\"line\">140</span><br><span class=\"line\">141</span><br><span class=\"line\">142</span><br><span class=\"line\">143</span><br><span class=\"line\">144</span><br><span class=\"line\">145</span><br><span class=\"line\">146</span><br><span class=\"line\">147</span><br><span class=\"line\">148</span><br><span class=\"line\">149</span><br><span class=\"line\">150</span><br><span class=\"line\">151</span><br><span class=\"line\">152</span><br><span class=\"line\">153</span><br><span class=\"line\">154</span><br><span class=\"line\">155</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># Training Process</span></span><br><span class=\"line\">print(<span class=\"string\">'Starting training'</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">for</span> epoch_num <span class=\"keyword\">in</span> range(num_epochs):</span><br><span class=\"line\">    <span class=\"comment\">#progbar is used to print % of processing</span></span><br><span class=\"line\">\tprogbar = generic_utils.Progbar(epoch_length)</span><br><span class=\"line\">    <span class=\"comment\"># print current process</span></span><br><span class=\"line\">\tprint(<span class=\"string\">'Epoch &#123;&#125;/&#123;&#125;'</span>.format(epoch_num + <span class=\"number\">1</span>, num_epochs))</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"keyword\">while</span> <span class=\"keyword\">True</span>:</span><br><span class=\"line\">\t\t<span class=\"keyword\">try</span>:</span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"comment\"># verbose True to print RPN situation, if can't generate boxes on positivate object, it will print error</span></span><br><span class=\"line\">\t\t\t<span class=\"keyword\">if</span> len(rpn_accuracy_rpn_monitor) == epoch_length <span class=\"keyword\">and</span> C.verbose:</span><br><span class=\"line\">                <span class=\"comment\"># postivate boxes / all boxes</span></span><br><span class=\"line\">\t\t\t\tmean_overlapping_bboxes = float(sum(rpn_accuracy_rpn_monitor))/len(rpn_accuracy_rpn_monitor)</span><br><span class=\"line\">\t\t\t\trpn_accuracy_rpn_monitor = []</span><br><span class=\"line\">\t\t\t\tprint(<span class=\"string\">'Average number of overlapping bounding boxes from RPN = &#123;&#125; for &#123;&#125; previous iterations'</span>.format(mean_overlapping_bboxes, epoch_length))</span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">if</span> mean_overlapping_bboxes == <span class=\"number\">0</span>:</span><br><span class=\"line\">\t\t\t\t\tprint(<span class=\"string\">'RPN is not producing bounding boxes that overlap the ground truth boxes. Check RPN settings or keep training.'</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"comment\"># obtain img, rpn information and img xml format</span></span><br><span class=\"line\">\t\t\tX, Y, img_data = next(data_gen_train)</span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"comment\"># train RPN net, X is img, Y is correspoding class type and graident</span></span><br><span class=\"line\">\t\t\tloss_rpn = model_rpn.train_on_batch(X, Y)</span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"comment\"># predict new Y from privious rpn model</span></span><br><span class=\"line\">\t\t\tP_rpn = model_rpn.predict_on_batch(X)</span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"comment\"># transform predicted rpn to cordinates of boxes</span></span><br><span class=\"line\">\t\t\tR = rpn_to_boxes.rpn_to_roi(P_rpn[<span class=\"number\">0</span>], P_rpn[<span class=\"number\">1</span>], C, K.image_dim_ordering(), use_regr=<span class=\"keyword\">True</span>, overlap_thresh=<span class=\"number\">0.7</span>, max_boxes=<span class=\"number\">300</span>)</span><br><span class=\"line\">\t\t\t<span class=\"comment\"># note: calc_iou converts from (x1,y1,x2,y2) to (x,y,w,h) format</span></span><br><span class=\"line\">            <span class=\"comment\"># X2: [x,y,w,h]</span></span><br><span class=\"line\">            <span class=\"comment\"># Y1: coresspoding class number -&gt; one hot vector</span></span><br><span class=\"line\">            <span class=\"comment\"># Y2: boxes coresspoding regrident</span></span><br><span class=\"line\">\t\t\tX2, Y1, Y2, IouS = rpn_to_classifier.calc_iou(R, img_data, C, class_mapping)</span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"comment\"># no box, stop this epoch</span></span><br><span class=\"line\">\t\t\t<span class=\"keyword\">if</span> X2 <span class=\"keyword\">is</span> <span class=\"keyword\">None</span>:</span><br><span class=\"line\">\t\t\t\trpn_accuracy_rpn_monitor.append(<span class=\"number\">0</span>)</span><br><span class=\"line\">\t\t\t\trpn_accuracy_for_epoch.append(<span class=\"number\">0</span>)</span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">continue</span></span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"comment\"># if last position of one-hot is 1 -&gt; is background</span></span><br><span class=\"line\">\t\t\tneg_samples = np.where(Y1[<span class=\"number\">0</span>, :, <span class=\"number\">-1</span>] == <span class=\"number\">1</span>)</span><br><span class=\"line\">            <span class=\"comment\"># else is postivate sample</span></span><br><span class=\"line\">\t\t\tpos_samples = np.where(Y1[<span class=\"number\">0</span>, :, <span class=\"number\">-1</span>] == <span class=\"number\">0</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"comment\"># obtain backgourd samples's coresspoding rows</span></span><br><span class=\"line\">\t\t\t<span class=\"keyword\">if</span> len(neg_samples) &gt; <span class=\"number\">0</span>:</span><br><span class=\"line\">\t\t\t\tneg_samples = neg_samples[<span class=\"number\">0</span>]</span><br><span class=\"line\">\t\t\t<span class=\"keyword\">else</span>:</span><br><span class=\"line\">\t\t\t\tneg_samples = []</span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"comment\"># obtain posivate samples's coresspoding rows</span></span><br><span class=\"line\">\t\t\t<span class=\"keyword\">if</span> len(pos_samples) &gt; <span class=\"number\">0</span>:</span><br><span class=\"line\">\t\t\t\tpos_samples = pos_samples[<span class=\"number\">0</span>]</span><br><span class=\"line\">\t\t\t<span class=\"keyword\">else</span>:</span><br><span class=\"line\">\t\t\t\tpos_samples = []</span><br><span class=\"line\">\t\t\t<span class=\"comment\"># saving posivate samples's number</span></span><br><span class=\"line\">\t\t\trpn_accuracy_rpn_monitor.append(len(pos_samples))</span><br><span class=\"line\">\t\t\trpn_accuracy_for_epoch.append((len(pos_samples)))</span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"comment\"># default 4 here</span></span><br><span class=\"line\">\t\t\t<span class=\"keyword\">if</span> C.num_rois &gt; <span class=\"number\">1</span>:</span><br><span class=\"line\">                <span class=\"comment\"># wehn postivate samples less than 2</span></span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">if</span> len(pos_samples) &lt; C.num_rois//<span class=\"number\">2</span>:</span><br><span class=\"line\">                    <span class=\"comment\"># chosse all samples</span></span><br><span class=\"line\">\t\t\t\t\tselected_pos_samples = pos_samples.tolist()</span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">else</span>:</span><br><span class=\"line\">                    <span class=\"comment\"># random choose 2 samples</span></span><br><span class=\"line\">\t\t\t\t\tselected_pos_samples = np.random.choice(pos_samples, C.num_rois//<span class=\"number\">2</span>, replace=<span class=\"keyword\">False</span>).tolist()</span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">try</span>:</span><br><span class=\"line\">                    <span class=\"comment\"># random choose num_rois - positave samples naegivate samples</span></span><br><span class=\"line\">\t\t\t\t\tselected_neg_samples = np.random.choice(neg_samples, C.num_rois - len(selected_pos_samples), replace=<span class=\"keyword\">False</span>).tolist()</span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">except</span>:</span><br><span class=\"line\">                    <span class=\"comment\"># if no enought neg samples, copy priouvs neg sample</span></span><br><span class=\"line\">\t\t\t\t\tselected_neg_samples = np.random.choice(neg_samples, C.num_rois - len(selected_pos_samples), replace=<span class=\"keyword\">True</span>).tolist()</span><br><span class=\"line\"></span><br><span class=\"line\">                <span class=\"comment\"># samples picked to classifier network</span></span><br><span class=\"line\">\t\t\t\tsel_samples = selected_pos_samples + selected_neg_samples</span><br><span class=\"line\">\t\t\t<span class=\"keyword\">else</span>:</span><br><span class=\"line\">\t\t\t\t<span class=\"comment\"># in the extreme case where num_rois = 1, we pick a random pos or neg sample</span></span><br><span class=\"line\">\t\t\t\tselected_pos_samples = pos_samples.tolist()</span><br><span class=\"line\">\t\t\t\tselected_neg_samples = neg_samples.tolist()</span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">if</span> np.random.randint(<span class=\"number\">0</span>, <span class=\"number\">2</span>):</span><br><span class=\"line\">\t\t\t\t\tsel_samples = random.choice(neg_samples)</span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">else</span>:</span><br><span class=\"line\">\t\t\t\t\tsel_samples = random.choice(pos_samples)</span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"comment\"># train classifier, img data, selceted samples' cordinates, mapping number of selected samples, coresspoding regreident</span></span><br><span class=\"line\">\t\t\tloss_class = model_classifier.train_on_batch([X, X2[:, sel_samples, :]], [Y1[:, sel_samples, :], Y2[:, sel_samples, :]])</span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"comment\"># in Keras, if loss part bigger than 1, it will return sum of part losses, each loss and accury</span></span><br><span class=\"line\">            <span class=\"comment\"># put each losses and accury into losses</span></span><br><span class=\"line\">\t\t\tlosses[iter_num, <span class=\"number\">0</span>] = loss_rpn[<span class=\"number\">1</span>]</span><br><span class=\"line\">\t\t\tlosses[iter_num, <span class=\"number\">1</span>] = loss_rpn[<span class=\"number\">2</span>]</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\tlosses[iter_num, <span class=\"number\">2</span>] = loss_class[<span class=\"number\">1</span>]</span><br><span class=\"line\">\t\t\tlosses[iter_num, <span class=\"number\">3</span>] = loss_class[<span class=\"number\">2</span>]</span><br><span class=\"line\">\t\t\tlosses[iter_num, <span class=\"number\">4</span>] = loss_class[<span class=\"number\">3</span>]</span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"comment\"># next iter</span></span><br><span class=\"line\">\t\t\titer_num += <span class=\"number\">1</span></span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"comment\"># display and update current mean value of losses</span></span><br><span class=\"line\">\t\t\tprogbar.update(iter_num, [(<span class=\"string\">'rpn_cls'</span>, np.mean(losses[:iter_num, <span class=\"number\">0</span>])), (<span class=\"string\">'rpn_regr'</span>, np.mean(losses[:iter_num, <span class=\"number\">1</span>])),</span><br><span class=\"line\">\t\t\t\t\t\t\t\t\t  (<span class=\"string\">'detector_cls'</span>, np.mean(losses[:iter_num, <span class=\"number\">2</span>])), (<span class=\"string\">'detector_regr'</span>, np.mean(losses[:iter_num, <span class=\"number\">3</span>]))])</span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"comment\"># reach epoch_length</span></span><br><span class=\"line\">\t\t\t<span class=\"keyword\">if</span> iter_num == epoch_length:</span><br><span class=\"line\">\t\t\t\tloss_rpn_cls = np.mean(losses[:, <span class=\"number\">0</span>])</span><br><span class=\"line\">\t\t\t\tloss_rpn_regr = np.mean(losses[:, <span class=\"number\">1</span>])</span><br><span class=\"line\">\t\t\t\tloss_class_cls = np.mean(losses[:, <span class=\"number\">2</span>])</span><br><span class=\"line\">\t\t\t\tloss_class_regr = np.mean(losses[:, <span class=\"number\">3</span>])</span><br><span class=\"line\">\t\t\t\tclass_acc = np.mean(losses[:, <span class=\"number\">4</span>])</span><br><span class=\"line\"></span><br><span class=\"line\">                <span class=\"comment\"># negativate samples / all samples</span></span><br><span class=\"line\">\t\t\t\tmean_overlapping_bboxes = float(sum(rpn_accuracy_for_epoch)) / len(rpn_accuracy_for_epoch)</span><br><span class=\"line\">                <span class=\"comment\"># reset</span></span><br><span class=\"line\">\t\t\t\trpn_accuracy_for_epoch = []</span><br><span class=\"line\"></span><br><span class=\"line\">                <span class=\"comment\"># print trainning loss and accrury</span></span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">if</span> C.verbose:</span><br><span class=\"line\">\t\t\t\t\tprint(<span class=\"string\">'Mean number of bounding boxes from RPN overlapping ground truth boxes: &#123;&#125;'</span>.format(mean_overlapping_bboxes))</span><br><span class=\"line\">\t\t\t\t\tprint(<span class=\"string\">'Classifier accuracy for bounding boxes from RPN: &#123;&#125;'</span>.format(class_acc))</span><br><span class=\"line\">\t\t\t\t\tprint(<span class=\"string\">'Loss RPN classifier: &#123;&#125;'</span>.format(loss_rpn_cls))</span><br><span class=\"line\">\t\t\t\t\tprint(<span class=\"string\">'Loss RPN regression: &#123;&#125;'</span>.format(loss_rpn_regr))</span><br><span class=\"line\">\t\t\t\t\tprint(<span class=\"string\">'Loss Detector classifier: &#123;&#125;'</span>.format(loss_class_cls))</span><br><span class=\"line\">\t\t\t\t\tprint(<span class=\"string\">'Loss Detector regression: &#123;&#125;'</span>.format(loss_class_regr))</span><br><span class=\"line\">                    <span class=\"comment\"># trainng time of one epoch</span></span><br><span class=\"line\">\t\t\t\t\tprint(<span class=\"string\">'Elapsed time: &#123;&#125;'</span>.format(time.time() - start_time))</span><br><span class=\"line\">                    </span><br><span class=\"line\">                <span class=\"comment\"># total loss</span></span><br><span class=\"line\">\t\t\t\tcurr_loss = loss_rpn_cls + loss_rpn_regr + loss_class_cls + loss_class_regr</span><br><span class=\"line\">                <span class=\"comment\"># reset</span></span><br><span class=\"line\">\t\t\t\titer_num = <span class=\"number\">0</span></span><br><span class=\"line\">                <span class=\"comment\"># reset time</span></span><br><span class=\"line\">\t\t\t\tstart_time = time.time()</span><br><span class=\"line\"></span><br><span class=\"line\">                <span class=\"comment\"># if obtain smaller total loss, save weight of current model</span></span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">if</span> curr_loss &lt; best_loss:</span><br><span class=\"line\">\t\t\t\t\t<span class=\"keyword\">if</span> C.verbose:</span><br><span class=\"line\">\t\t\t\t\t\tprint(<span class=\"string\">'Total loss decreased from &#123;&#125; to &#123;&#125;, saving weights'</span>.format(best_loss,curr_loss))</span><br><span class=\"line\">\t\t\t\t\tbest_loss = curr_loss</span><br><span class=\"line\">\t\t\t\t\tmodel_all.save_weights(C.model_path)</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">break</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">except</span> Exception <span class=\"keyword\">as</span> e:</span><br><span class=\"line\">\t\t\tprint(<span class=\"string\">'Exception: &#123;&#125;'</span>.format(e))</span><br><span class=\"line\">\t\t\t<span class=\"keyword\">continue</span></span><br><span class=\"line\"></span><br><span class=\"line\">print(<span class=\"string\">'Training complete, exiting.'</span>)</span><br></pre></td></tr></table></figure>\n<p><a href=\"https://github.com/Trouble404/Object-Detection-System-Based-on-CNN-and-Capsule-Network/blob/master/Faster-RCNN/workflow_train.ipynb\" target=\"_blank\" rel=\"noopener\">jupyter notebook</a></p>\n<h3 id=\"30-07-2018\"><a href=\"#30-07-2018\" class=\"headerlink\" title=\"30/07/2018\"></a>30/07/2018</h3><h4 id=\"Running-at-GPU-enviorment\"><a href=\"#Running-at-GPU-enviorment\" class=\"headerlink\" title=\"Running at GPU enviorment\"></a>Running at GPU enviorment</h4><p><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/9_1.JPG\" alt=\"image\"><br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/9_2.JPG\" alt=\"image\"><br>Meet error in GPU version tensorflow<br>No enough memory.</p>\n<p>Try to Running at Irius:</p>\n<p>Setting 3 differnet configration:<br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/9_3.JPG\" alt=\"image\"><br>at Prjoect1 file:<br>set epoch_length to number of training img<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">epoch_length = <span class=\"number\">11540</span></span><br><span class=\"line\">num_epochs = <span class=\"number\">100</span></span><br></pre></td></tr></table></figure></p>\n<p>Apply img enhance function<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">C.use_horizontal_flips = <span class=\"keyword\">True</span></span><br><span class=\"line\">C.use_vertical_flips = <span class=\"keyword\">True</span></span><br><span class=\"line\">C.rot_90 = <span class=\"keyword\">True</span></span><br></pre></td></tr></table></figure></p>\n<hr>\n<p>at Prjoect file:<br>set epoch_length to 1000, increase epoch<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">epoch_length = <span class=\"number\">1000</span></span><br><span class=\"line\">num_epochs = <span class=\"number\">2000</span></span><br></pre></td></tr></table></figure></p>\n<p>Apply img enhance and class balance function<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">C.use_horizontal_flips = <span class=\"keyword\">True</span></span><br><span class=\"line\">C.use_vertical_flips = <span class=\"keyword\">True</span></span><br><span class=\"line\">C.rot_90 = <span class=\"keyword\">True</span></span><br><span class=\"line\">C.balanced_classes = <span class=\"keyword\">True</span></span><br></pre></td></tr></table></figure></p>\n<hr>\n<p>at Prjoect3 file:<br>set epoch_length to 1000, increase epoch<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">epoch_length = <span class=\"number\">1000</span></span><br><span class=\"line\">num_epochs = <span class=\"number\">2000</span></span><br></pre></td></tr></table></figure></p>\n<p>Apply img enhance<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">C.use_horizontal_flips = <span class=\"keyword\">True</span></span><br><span class=\"line\">C.use_vertical_flips = <span class=\"keyword\">True</span></span><br><span class=\"line\">C.rot_90 = <span class=\"keyword\">True</span></span><br></pre></td></tr></table></figure></p>\n<hr>\n<h4 id=\"check-irdius-work\"><a href=\"#check-irdius-work\" class=\"headerlink\" title=\"check irdius work\"></a>check irdius work</h4><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">myqueue</span><br></pre></td></tr></table></figure>\n<p><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/9_4.JPG\" alt=\"image\"></p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ssh pink59</span><br><span class=\"line\">nvidia-smi</span><br></pre></td></tr></table></figure>\n<p><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/9_5.JPG\" alt=\"image\"></p>\n<h3 id=\"31-07-2018\"><a href=\"#31-07-2018\" class=\"headerlink\" title=\"31/07/2018\"></a>31/07/2018</h3><h4 id=\"obtain-trained-model-and-log-file\"><a href=\"#obtain-trained-model-and-log-file\" class=\"headerlink\" title=\"obtain trained model and log file\"></a>obtain trained model and log file</h4><p> Iriuds GPU24<br>LogBook.</p>\n<h4 id=\"plot-rpn-and-classfier-loss\"><a href=\"#plot-rpn-and-classfier-loss\" class=\"headerlink\" title=\"plot rpn and classfier loss\"></a>plot rpn and classfier loss</h4><p>epochrpn_cls, rpn_regr, detc_cls, detc_regr<br>List<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">obtain_each_batch</span><span class=\"params\">(filename)</span>:</span></span><br><span class=\"line\">    n = <span class=\"number\">0</span></span><br><span class=\"line\">    rpn_cls = []</span><br><span class=\"line\">    rpn_regr = []</span><br><span class=\"line\">    detector_cls = []</span><br><span class=\"line\">    detector_regr = []</span><br><span class=\"line\">    f = open(filename,<span class=\"string\">'r'</span>,buffering=<span class=\"number\">-1</span>)</span><br><span class=\"line\">    lines = f.readlines()</span><br><span class=\"line\">    <span class=\"keyword\">for</span> line <span class=\"keyword\">in</span> lines:</span><br><span class=\"line\">        n = n + <span class=\"number\">1</span></span><br><span class=\"line\">        match = re.match(<span class=\"string\">r'.* - rpn_cls: (.*) - rpn_regr: .*'</span>, line, re.M|re.I)</span><br><span class=\"line\">        <span class=\"keyword\">if</span> match <span class=\"keyword\">is</span> <span class=\"keyword\">None</span>:</span><br><span class=\"line\">            <span class=\"keyword\">continue</span></span><br><span class=\"line\">        <span class=\"keyword\">else</span>: </span><br><span class=\"line\">            rpn_cls.append(float(match.group(<span class=\"number\">1</span>)))</span><br><span class=\"line\"></span><br><span class=\"line\">        match = re.match(<span class=\"string\">r'.* - rpn_regr: (.*) - detector_cls: .*'</span>, line, re.M|re.I)</span><br><span class=\"line\">        <span class=\"keyword\">if</span> match <span class=\"keyword\">is</span> <span class=\"keyword\">None</span>:</span><br><span class=\"line\">            <span class=\"keyword\">continue</span></span><br><span class=\"line\">        <span class=\"keyword\">else</span>: </span><br><span class=\"line\">            rpn_regr.append(float(match.group(<span class=\"number\">1</span>)))            </span><br><span class=\"line\">            </span><br><span class=\"line\">        match = re.match(<span class=\"string\">r'.* - detector_cls: (.*) - detector_regr: .*'</span>, line, re.M|re.I)</span><br><span class=\"line\">        <span class=\"keyword\">if</span> match <span class=\"keyword\">is</span> <span class=\"keyword\">None</span>:</span><br><span class=\"line\">            <span class=\"keyword\">continue</span></span><br><span class=\"line\">        <span class=\"keyword\">else</span>: </span><br><span class=\"line\">            detector_cls.append(float(match.group(<span class=\"number\">1</span>))) </span><br><span class=\"line\">            </span><br><span class=\"line\">        match = re.match(<span class=\"string\">r'.* - detector_regr: (.*)\\n'</span>, line, re.M|re.I)</span><br><span class=\"line\">        <span class=\"keyword\">if</span> match <span class=\"keyword\">is</span> <span class=\"keyword\">None</span>:</span><br><span class=\"line\">            <span class=\"keyword\">continue</span></span><br><span class=\"line\">        <span class=\"keyword\">else</span>: </span><br><span class=\"line\">            det_regr = match.group(<span class=\"number\">1</span>)[<span class=\"number\">0</span>:<span class=\"number\">6</span>]</span><br><span class=\"line\">            detector_regr.append(float(det_regr))</span><br><span class=\"line\"></span><br><span class=\"line\">    f.close()</span><br><span class=\"line\">    print(n)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> rpn_cls, rpn_regr, detector_cls, detector_regr</span><br></pre></td></tr></table></figure></p>\n<p>epochaccury, loss of rpn cls, loss of rpn regr, loss of detc cls, loss of detc regr<br>list<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">obtain_batch</span><span class=\"params\">(filename)</span>:</span></span><br><span class=\"line\">    n = <span class=\"number\">0</span></span><br><span class=\"line\">    accuracy = []</span><br><span class=\"line\">    loss_rpn_cls = []</span><br><span class=\"line\">    loss_rpn_regr = []</span><br><span class=\"line\">    loss_detc_cls = []</span><br><span class=\"line\">    loss_detc_regr = []</span><br><span class=\"line\">    f = open(filename,<span class=\"string\">'r'</span>,buffering=<span class=\"number\">-1</span>)</span><br><span class=\"line\">    lines = f.readlines()</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"keyword\">for</span> line <span class=\"keyword\">in</span> lines:</span><br><span class=\"line\">        n = n + <span class=\"number\">1</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> <span class=\"string\">'Classifier accuracy for bounding boxes from RPN'</span> <span class=\"keyword\">in</span> line:</span><br><span class=\"line\">            result = re.findall(<span class=\"string\">r\"\\d+\\.?\\d*\"</span>,line)</span><br><span class=\"line\">            accuracy.append(float(result[<span class=\"number\">0</span>]))</span><br><span class=\"line\">            </span><br><span class=\"line\">        <span class=\"keyword\">if</span> <span class=\"string\">'Loss RPN classifier'</span> <span class=\"keyword\">in</span> line:</span><br><span class=\"line\">            result = re.findall(<span class=\"string\">r\"\\d+\\.?\\d*\"</span>,line)</span><br><span class=\"line\">            loss_rpn_cls.append(float(result[<span class=\"number\">0</span>]))       </span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">if</span> <span class=\"string\">'Loss RPN regression'</span> <span class=\"keyword\">in</span> line:</span><br><span class=\"line\">            result = re.findall(<span class=\"string\">r\"\\d+\\.?\\d*\"</span>,line)</span><br><span class=\"line\">            loss_rpn_regr.append(float(result[<span class=\"number\">0</span>]))</span><br><span class=\"line\">            </span><br><span class=\"line\">        <span class=\"keyword\">if</span> <span class=\"string\">'Loss Detector classifier'</span> <span class=\"keyword\">in</span> line:</span><br><span class=\"line\">            result = re.findall(<span class=\"string\">r\"\\d+\\.?\\d*\"</span>,line)</span><br><span class=\"line\">            loss_detc_cls.append(float(result[<span class=\"number\">0</span>]))</span><br><span class=\"line\">            </span><br><span class=\"line\">        <span class=\"keyword\">if</span> <span class=\"string\">'Loss Detector regression'</span> <span class=\"keyword\">in</span> line:</span><br><span class=\"line\">            result = re.findall(<span class=\"string\">r\"\\d+\\.?\\d*\"</span>,line)</span><br><span class=\"line\">            loss_detc_regr.append(float(result[<span class=\"number\">0</span>])) </span><br><span class=\"line\">            </span><br><span class=\"line\">    f.close()</span><br><span class=\"line\">    print(n)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> accuracy, loss_rpn_cls, loss_rpn_regr, loss_detc_cls, loss_detc_regr</span><br></pre></td></tr></table></figure></p>\n<h4 id=\"plot-epoch-loss-and-accury\"><a href=\"#plot-epoch-loss-and-accury\" class=\"headerlink\" title=\"plot epoch loss and accury\"></a>plot epoch loss and accury</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">filename = <span class=\"string\">r'F:\\desktop\\\\1000-no_balance\\train1.out'</span></span><br><span class=\"line\">aa,bb,cc,dd,ee = obtain_batch(filename)</span><br><span class=\"line\">x_cor = np.arange(<span class=\"number\">0</span>,len(aa),<span class=\"number\">1</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">plt.plot(x_cor,aa, c=<span class=\"string\">'b'</span>, label = <span class=\"string\">\"Accuracy\"</span>)</span><br><span class=\"line\">plt.plot(x_cor,bb, c=<span class=\"string\">'c'</span>, label = <span class=\"string\">\"Loss RPN classifier\"</span>)</span><br><span class=\"line\">plt.plot(x_cor,cc, c=<span class=\"string\">'g'</span>, label = <span class=\"string\">\"Loss RPN regression\"</span>)</span><br><span class=\"line\">plt.plot(x_cor,dd, c=<span class=\"string\">'k'</span>, label = <span class=\"string\">\"Loss Detector classifier\"</span>)</span><br><span class=\"line\">plt.plot(x_cor,ee, c=<span class=\"string\">'m'</span>, label = <span class=\"string\">\"Loss Detector regression\"</span>)</span><br><span class=\"line\">plt.ylabel(<span class=\"string\">\"Value of Accuracy and Loss\"</span>) </span><br><span class=\"line\">plt.xlabel(<span class=\"string\">\"Number of Epoch\"</span>)</span><br><span class=\"line\">plt.title(<span class=\"string\">'Loss and Accuracy for Totoal Epochs'</span>)  </span><br><span class=\"line\">plt.legend()</span><br><span class=\"line\">plt.ylim(<span class=\"number\">0</span>,<span class=\"number\">2</span>)</span><br><span class=\"line\"><span class=\"comment\">#plt.xlim(0,11540)</span></span><br><span class=\"line\">plt.savefig(<span class=\"string\">\"pic1.PNG\"</span>, dpi = <span class=\"number\">600</span>)</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n<p><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/pic1.PNG\" alt=\"image\"></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">filename = <span class=\"string\">r'F:\\desktop\\\\1000-no_balance\\train1.out'</span></span><br><span class=\"line\">a,b,c,d = obtain_each_batch(filename)</span><br><span class=\"line\">x_cor = np.arange(<span class=\"number\">0</span>,len(a),<span class=\"number\">1</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">plt.plot(x_cor,a, c=<span class=\"string\">'b'</span>, label = <span class=\"string\">\"rpn_cls\"</span>)</span><br><span class=\"line\">plt.plot(x_cor,b, c=<span class=\"string\">'c'</span>, label = <span class=\"string\">\"rpn_regr\"</span>)</span><br><span class=\"line\">plt.plot(x_cor,c, c=<span class=\"string\">'g'</span>, label = <span class=\"string\">\"detector_cls\"</span>)</span><br><span class=\"line\">plt.plot(x_cor,d, c=<span class=\"string\">'k'</span>, label = <span class=\"string\">\"detector_regr\"</span>)</span><br><span class=\"line\">plt.ylabel(<span class=\"string\">\"Value of Loss\"</span>) </span><br><span class=\"line\">plt.xlabel(<span class=\"string\">\"Epoch Length\"</span>)</span><br><span class=\"line\">plt.title(<span class=\"string\">'Loss for Lenght of Epoch'</span>)  </span><br><span class=\"line\">plt.legend()</span><br><span class=\"line\"><span class=\"comment\">#plt.ylim(0,2)</span></span><br><span class=\"line\">plt.xlim(<span class=\"number\">80787</span>,<span class=\"number\">92327</span>)</span><br><span class=\"line\">plt.savefig(<span class=\"string\">\"pic2.PNG\"</span>, dpi = <span class=\"number\">600</span>)</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n<p><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/pic2.PNG\" alt=\"image\"></p>\n<p><a href=\"https://github.com/Trouble404/Object-Detection-System-Based-on-CNN-and-Capsule-Network/blob/master/Plot/logbook_plot.ipynb\" target=\"_blank\" rel=\"noopener\">Jupyter notebook</a></p>\n<h2 id=\"August\"><a href=\"#August\" class=\"headerlink\" title=\"August\"></a>August</h2><h3 id=\"01-02-08-2018\"><a href=\"#01-02-08-2018\" class=\"headerlink\" title=\"01~02/08/2018\"></a>01~02/08/2018</h3><h4 id=\"test-network\"><a href=\"#test-network\" class=\"headerlink\" title=\"test network\"></a>test network</h4><p>train<br></p>\n<p><strong>rpn</strong><br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">shared_layers = nn.nn_base(img_input, trainable=<span class=\"keyword\">True</span>)</span><br><span class=\"line\">num_anchors = len(C.anchor_box_scales)*len(C.anchor_box_ratios)</span><br><span class=\"line\">rpn_layers = nn.rpn(shared_layers,num_anchors)</span><br></pre></td></tr></table></figure></p>\n<p><strong>classifier</strong><br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">classifier = nn.classifier(feature_map_input, roi_input, C.num_rois,nb_classes=len(class_mapping), trainable=<span class=\"keyword\">True</span>)</span><br></pre></td></tr></table></figure></p>\n<p><strong></strong><br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">C.model_path = <span class=\"string\">'gpu_resnet50_weights.h5'</span></span><br><span class=\"line\"><span class=\"keyword\">try</span>:</span><br><span class=\"line\">    print(<span class=\"string\">'Loading weights from &#123;&#125;'</span>.format(C.model_path))</span><br><span class=\"line\">    model_rpn.load_weights(C.model_path, by_name=<span class=\"keyword\">True</span>)</span><br><span class=\"line\">    model_classifier.load_weights(C.model_path, by_name=<span class=\"keyword\">True</span>)</span><br><span class=\"line\"><span class=\"keyword\">except</span>:</span><br><span class=\"line\">    print(<span class=\"string\">'can not load'</span>)</span><br></pre></td></tr></table></figure></p>\n<p><strong></strong><br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/test/test4.jpg\" alt=\"image\"><br></p>\n<ol>\n<li><p><br> <br> <br> <br> img</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">format_img_size</span><span class=\"params\">(img, C)</span>:</span></span><br><span class=\"line\">    (height,width,_) = img.shape</span><br><span class=\"line\">    <span class=\"keyword\">if</span> width &lt;= height:</span><br><span class=\"line\">        ratio = C.im_size/width</span><br><span class=\"line\">    <span class=\"keyword\">else</span>:</span><br><span class=\"line\">        ratio = C.im_size/height</span><br><span class=\"line\">    new_width, new_height = image_processing.get_new_img_size(width,height, C.im_size)</span><br><span class=\"line\">    img = cv2.resize(img, (new_width, new_height), interpolation=cv2.INTER_CUBIC)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> img, ratio</span><br></pre></td></tr></table></figure>\n</li>\n<li><p><br>  BGRRGBRESNET<br> np.float32<br> 1<br> <br> </p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">format_img_channels</span><span class=\"params\">(img, C)</span>:</span></span><br><span class=\"line\">\t<span class=\"string\">\"\"\" formats the image channels based on config \"\"\"</span></span><br><span class=\"line\">\timg = img[:, :, (<span class=\"number\">2</span>, <span class=\"number\">1</span>, <span class=\"number\">0</span>)]</span><br><span class=\"line\">\timg = img.astype(np.float32)</span><br><span class=\"line\">\timg[:, :, <span class=\"number\">0</span>] -= C.img_channel_mean[<span class=\"number\">0</span>]</span><br><span class=\"line\">\timg[:, :, <span class=\"number\">1</span>] -= C.img_channel_mean[<span class=\"number\">1</span>]</span><br><span class=\"line\">\timg[:, :, <span class=\"number\">2</span>] -= C.img_channel_mean[<span class=\"number\">2</span>]</span><br><span class=\"line\">\timg /= C.img_scaling_factor</span><br><span class=\"line\">\timg = np.transpose(img, (<span class=\"number\">2</span>, <span class=\"number\">0</span>, <span class=\"number\">1</span>))</span><br><span class=\"line\">\timg = np.expand_dims(img, axis=<span class=\"number\">0</span>)</span><br><span class=\"line\">\t<span class=\"keyword\">return</span> img</span><br></pre></td></tr></table></figure>\n</li>\n</ol>\n<p>tensorflow</p>\n<p><strong></strong><br>Y1:anchor<br>Y2:anchor<br>F:</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[Y1, Y2, F] = model_rpn.predict(X)</span><br></pre></td></tr></table></figure>\n<p>rpn16anchorrpn<br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/pic_anchors.png\" alt=\"image\"><br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/25_1.jpg\" alt=\"image\"></p>\n<p><strong>rpn:</strong><br>300(x1,y1,x2,y2)<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># transform predicted rpn to cordinates of boxes</span></span><br><span class=\"line\">R = rpn_to_boxes.rpn_to_roi(Y1, Y2, C, K.image_dim_ordering(), use_regr=<span class=\"keyword\">True</span>, overlap_thresh=<span class=\"number\">0.7</span>)</span><br></pre></td></tr></table></figure></p>\n<p><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/pic_rois.png\" alt=\"image\"></p>\n<p>(x1,y1,x2,y2)  (x,y,w,h)<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">R[:, <span class=\"number\">2</span>] -= R[:, <span class=\"number\">0</span>]</span><br><span class=\"line\">R[:, <span class=\"number\">3</span>] -= R[:, <span class=\"number\">1</span>]</span><br></pre></td></tr></table></figure></p>\n<p><strong></strong><br>C.num_rois<br>32300/32, 10<br>3232<br><br>3232<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># divided 32 bboxes as one group</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> jk <span class=\"keyword\">in</span> range(R.shape[<span class=\"number\">0</span>]//C.num_rois + <span class=\"number\">1</span>):</span><br><span class=\"line\">    <span class=\"comment\"># pick num_rios(32) bboxes one time, only pick to last bboxes in last group</span></span><br><span class=\"line\">    ROIs = np.expand_dims(R[C.num_rois*jk:C.num_rois*(jk+<span class=\"number\">1</span>), :], axis=<span class=\"number\">0</span>)</span><br><span class=\"line\">    <span class=\"comment\">#print(ROIs.shape)</span></span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\"># no proposals, out iter</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> ROIs.shape[<span class=\"number\">1</span>] == <span class=\"number\">0</span>:</span><br><span class=\"line\">        <span class=\"keyword\">break</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># when last time can't obtain num_rios(32) bboxes, adding bboxes with 0 to fill to 32 bboxes</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> jk == R.shape[<span class=\"number\">0</span>]//C.num_rois:</span><br><span class=\"line\">        <span class=\"comment\">#pad R</span></span><br><span class=\"line\">        curr_shape = ROIs.shape</span><br><span class=\"line\">        target_shape = (curr_shape[<span class=\"number\">0</span>],C.num_rois,curr_shape[<span class=\"number\">2</span>])</span><br><span class=\"line\">        ROIs_padded = np.zeros(target_shape).astype(ROIs.dtype)</span><br><span class=\"line\">        ROIs_padded[:, :curr_shape[<span class=\"number\">1</span>], :] = ROIs</span><br><span class=\"line\">        ROIs_padded[<span class=\"number\">0</span>, curr_shape[<span class=\"number\">1</span>]:, :] = ROIs[<span class=\"number\">0</span>, <span class=\"number\">0</span>, :]</span><br><span class=\"line\">        <span class=\"comment\"># 10 group with 320 bboxes</span></span><br><span class=\"line\">        ROIs = ROIs_padded</span><br></pre></td></tr></table></figure></p>\n<p></p>\n<p><strong></strong></p>\n<p><br>P_cls<br>P_regr<br>F:rpn<br>ROIS:<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[P_cls, P_regr] = model_classifier_only.predict([F, ROIs])</span><br></pre></td></tr></table></figure></p>\n<p><br><br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">for</span> ii <span class=\"keyword\">in</span> range(P_cls.shape[<span class=\"number\">1</span>]):</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># if smaller than setting threshold, we think this bbox invalid</span></span><br><span class=\"line\">    <span class=\"comment\"># and if this bbox's class is background, we don't need to care about it</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> np.max(P_cls[<span class=\"number\">0</span>, ii, :]) &lt; bbox_threshold <span class=\"keyword\">or</span> np.argmax(P_cls[<span class=\"number\">0</span>, ii, :]) == (P_cls.shape[<span class=\"number\">2</span>] - <span class=\"number\">1</span>):</span><br><span class=\"line\">        <span class=\"keyword\">continue</span></span><br></pre></td></tr></table></figure></p>\n<p><br>list<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># obatain max possibility's class name by class mapping</span></span><br><span class=\"line\">cls_name = class_mapping[np.argmax(P_cls[<span class=\"number\">0</span>, ii, :])]</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># saving bboxes and probs</span></span><br><span class=\"line\"><span class=\"keyword\">if</span> cls_name <span class=\"keyword\">not</span> <span class=\"keyword\">in</span> bboxes:</span><br><span class=\"line\">    bboxes[cls_name] = []</span><br><span class=\"line\">    probs[cls_name] = []</span><br></pre></td></tr></table></figure></p>\n<p><br><br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># obtain current cordinates of proposal</span></span><br><span class=\"line\">(x, y, w, h) = ROIs[<span class=\"number\">0</span>, ii, :]</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># obtain the position with max possibility</span></span><br><span class=\"line\">cls_num = np.argmax(P_cls[<span class=\"number\">0</span>, ii, :])</span><br></pre></td></tr></table></figure></p>\n<p><br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/pic_cls1.png\" alt=\"iamge\"></p>\n<p><br><br><br> C.rpn_stride<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">try</span>:</span><br><span class=\"line\">    <span class=\"comment\"># obtain privous position's bbox's regrient</span></span><br><span class=\"line\">    (tx, ty, tw, th) = P_regr[<span class=\"number\">0</span>, ii, <span class=\"number\">4</span>*cls_num:<span class=\"number\">4</span>*(cls_num+<span class=\"number\">1</span>)]</span><br><span class=\"line\">    <span class=\"comment\"># waiting test</span></span><br><span class=\"line\">    tx /= C.classifier_regr_std[<span class=\"number\">0</span>]</span><br><span class=\"line\">    ty /= C.classifier_regr_std[<span class=\"number\">1</span>]</span><br><span class=\"line\">    tw /= C.classifier_regr_std[<span class=\"number\">2</span>]</span><br><span class=\"line\">    th /= C.classifier_regr_std[<span class=\"number\">3</span>]</span><br><span class=\"line\">    <span class=\"comment\"># fix box with regreient</span></span><br><span class=\"line\">    x, y, w, h = rpn_to_boxes.apply_regr(x, y, w, h, tx, ty, tw, th)</span><br><span class=\"line\"><span class=\"keyword\">except</span>:</span><br><span class=\"line\">    <span class=\"keyword\">pass</span></span><br><span class=\"line\"><span class=\"comment\"># cordinates of current's box on real img</span></span><br><span class=\"line\">bboxes[cls_name].append([C.rpn_stride*x, C.rpn_stride*y, C.rpn_stride*(x+w), C.rpn_stride*(y+h)])</span><br><span class=\"line\"><span class=\"comment\"># coresspoding posbility</span></span><br><span class=\"line\">probs[cls_name].append(np.max(P_cls[<span class=\"number\">0</span>, ii, :]))</span><br></pre></td></tr></table></figure></p>\n<p><br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/pic_cls2.png\" alt=\"image\"></p>\n<p>bboxesbbox<br>No Max Supression<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># for all classes in current boxes</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> key <span class=\"keyword\">in</span> bboxes:</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># bboxes's cordinates</span></span><br><span class=\"line\">    bbox = np.array(bboxes[key])</span><br><span class=\"line\">    <span class=\"comment\"># apply NMX to merge some  overlapping boxes</span></span><br><span class=\"line\">    new_boxes, new_probs = rpn_to_boxes.non_max_suppression_fast(bbox, np.array(probs[key]), overlap_thresh=<span class=\"number\">0.5</span>)</span><br></pre></td></tr></table></figure></p>\n<p><br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/pic_cls3.png\" alt=\"image\"></p>\n<p><a href=\"https://github.com/Trouble404/Object-Detection-System-Based-on-CNN-and-Capsule-Network/blob/master/Faster-RCNN/test.ipynb\" target=\"_blank\" rel=\"noopener\">Jupyter notebook</a></p>\n<h4 id=\"result\"><a href=\"#result\" class=\"headerlink\" title=\"result\"></a>result</h4><p>Small img, only 8k</p>\n<div align=\"center\"><br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/test/test1.jpg\" height=\"200px\"><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/test/test1.png\" height=\"200px\"><br></div>\n\n<hr>\n<p>Overlapping img</p>\n<div align=\"center\"><br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/test/test2.jpg\" height=\"200px\"><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/test/test2.png\" height=\"200px\"><br></div>\n\n<hr>\n<p>Crowed People</p>\n<div align=\"center\"><br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/test/test3.jpg\" height=\"260px\"><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/test/test3.png\" height=\"260px\"><br></div>\n\n<hr>\n<p>cow and people</p>\n<div align=\"center\"><br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/test/test4.jpg\" height=\"260px\"><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/test/test4.png\" height=\"260px\"><br></div>\n\n<hr>\n<p>car and plane</p>\n<div align=\"center\"><br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/test/test5.jpg\" height=\"220px\"><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/test/test5.png\" height=\"220px\"><br></div>\n\n<hr>\n<p>Street img</p>\n<div align=\"center\"><br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/test/test6.jpg\" height=\"270px\"><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/test/test6.png\" height=\"270px\"><br></div>\n\n<hr>\n<p>Lots Dogs</p>\n<div align=\"center\"><br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/test/test7.jpg\" height=\"250px\"><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/test/test7.png\" height=\"250px\"><br></div>\n\n<hr>\n<p>Overlapping car and people</p>\n<div align=\"center\"><br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/test/test8.jpg\" height=\"290px\"><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/test/test8.png\" height=\"290px\"><br></div>\n\n<p></p>\n<h3 id=\"03-08-2018\"><a href=\"#03-08-2018\" class=\"headerlink\" title=\"03/08/2018\"></a>03/08/2018</h3><h4 id=\"evaluation\"><a href=\"#evaluation\" class=\"headerlink\" title=\"evaluation\"></a>evaluation</h4><p><strong>mAP</strong><br>mAPPrecisionRecallobject detectionobjectPrecisionRecall/ P-RAPmeanAPmAPmAP[0,1] </p>\n<p><strong>AP</strong>:PrecisionRecallsklearn.metrics.average_precision_score </p>\n<p>resizesklearn.metrics.average_precision_score</p>\n<hr>\n<p>rpntrain<br>feature map<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">num_features = <span class=\"number\">1024</span></span><br><span class=\"line\"></span><br><span class=\"line\">input_shape_img = (<span class=\"keyword\">None</span>, <span class=\"keyword\">None</span>, <span class=\"number\">3</span>)</span><br><span class=\"line\">input_shape_features = (<span class=\"keyword\">None</span>, <span class=\"keyword\">None</span>, num_features)</span><br><span class=\"line\"></span><br><span class=\"line\">img_input = Input(shape=input_shape_img)</span><br><span class=\"line\">roi_input = Input(shape=(C.num_rois, <span class=\"number\">4</span>))</span><br><span class=\"line\">feature_map_input = Input(shape=input_shape_features)</span><br><span class=\"line\"></span><br><span class=\"line\">classifier = nn.classifier(feature_map_input, roi_input, C.num_rois, nb_classes=len(class_mapping), trainable=<span class=\"keyword\">True</span>)</span><br></pre></td></tr></table></figure></p>\n<p></p>\n<p>VOC<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">train_imgs = []</span><br><span class=\"line\">test_imgs = []</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">for</span> each <span class=\"keyword\">in</span> all_imgs:</span><br><span class=\"line\">\t<span class=\"keyword\">if</span> each[<span class=\"string\">'imageset'</span>] == <span class=\"string\">'trainval'</span>:</span><br><span class=\"line\">\t\ttrain_imgs.append(each)</span><br><span class=\"line\">\t<span class=\"keyword\">if</span> each[<span class=\"string\">'imageset'</span>] == <span class=\"string\">'test'</span>:</span><br><span class=\"line\">\t\ttest_imgs.append(each)</span><br></pre></td></tr></table></figure></p>\n<p>xml<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">for</span> jk <span class=\"keyword\">in</span> range(new_boxes.shape[<span class=\"number\">0</span>]):</span><br><span class=\"line\">    (x1, y1, x2, y2) = new_boxes[jk, :]</span><br><span class=\"line\">    det = &#123;<span class=\"string\">'x1'</span>: x1, <span class=\"string\">'x2'</span>: x2, <span class=\"string\">'y1'</span>: y1, <span class=\"string\">'y2'</span>: y2, <span class=\"string\">'class'</span>: key, <span class=\"string\">'prob'</span>: new_probs[jk]&#125;</span><br><span class=\"line\">    all_dets.append(det)</span><br></pre></td></tr></table></figure></p>\n<p><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/014_1.JPG\" alt=\"image\"></p>\n<p><br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/014_2.JPG\" alt=\"image\"></p>\n<p>bbox_matchedFALSETrue<br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/14_3.JPG\" alt=\"image\"></p>\n<p>idx<br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/014_3.JPG\" alt=\"image\"></p>\n<p>iou0.5</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># process each bbox with hightest prob</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> box_idx <span class=\"keyword\">in</span> box_idx_sorted_by_prob:</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\"># obtain current box's cordinates, class and prob</span></span><br><span class=\"line\">    pred_box = pred[box_idx]</span><br><span class=\"line\">    pred_class = pred_box[<span class=\"string\">'class'</span>]</span><br><span class=\"line\">    pred_x1 = pred_box[<span class=\"string\">'x1'</span>]</span><br><span class=\"line\">    pred_x2 = pred_box[<span class=\"string\">'x2'</span>]</span><br><span class=\"line\">    pred_y1 = pred_box[<span class=\"string\">'y1'</span>]</span><br><span class=\"line\">    pred_y2 = pred_box[<span class=\"string\">'y2'</span>]</span><br><span class=\"line\">    pred_prob = pred_box[<span class=\"string\">'prob'</span>]</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\"># if not in P list, save current class infomration to it</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> pred_class <span class=\"keyword\">not</span> <span class=\"keyword\">in</span> P:</span><br><span class=\"line\">        P[pred_class] = []</span><br><span class=\"line\">        T[pred_class] = []</span><br><span class=\"line\">        <span class=\"comment\"># put porb to P</span></span><br><span class=\"line\">    P[pred_class].append(pred_prob)</span><br><span class=\"line\">    <span class=\"comment\"># used to check whether find current object</span></span><br><span class=\"line\">    found_match = <span class=\"keyword\">False</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># compare each real bbox</span></span><br><span class=\"line\">    <span class=\"comment\"># obtain real box's cordinates, class and prob</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> gt_box <span class=\"keyword\">in</span> gt:</span><br><span class=\"line\">        gt_class = gt_box[<span class=\"string\">'class'</span>]</span><br><span class=\"line\">        <span class=\"comment\"># bacause the image is rezied, so calculate the real cordinates</span></span><br><span class=\"line\">        gt_x1 = gt_box[<span class=\"string\">'x1'</span>]/fx</span><br><span class=\"line\">        gt_x2 = gt_box[<span class=\"string\">'x2'</span>]/fx</span><br><span class=\"line\">        gt_y1 = gt_box[<span class=\"string\">'y1'</span>]/fy</span><br><span class=\"line\">        gt_y2 = gt_box[<span class=\"string\">'y2'</span>]/fy</span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"comment\"># obtain box_matched - all false at beginning</span></span><br><span class=\"line\">        gt_seen = gt_box[<span class=\"string\">'bbox_matched'</span>]</span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"comment\"># ture class != predicted class</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> gt_class != pred_class:</span><br><span class=\"line\">            <span class=\"keyword\">continue</span></span><br><span class=\"line\">        <span class=\"comment\"># already matched</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> gt_seen:</span><br><span class=\"line\">            <span class=\"keyword\">continue</span></span><br><span class=\"line\">        <span class=\"comment\"># calculate iou of predicted bbox and real bbox </span></span><br><span class=\"line\">        iou = rpn_calculation.iou((pred_x1, pred_y1, pred_x2, pred_y2), (gt_x1, gt_y1, gt_x2, gt_y2))</span><br><span class=\"line\">        <span class=\"comment\"># if iou &gt; 0.5, we will set this prediction correct</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> iou &gt;= <span class=\"number\">0.5</span>:</span><br><span class=\"line\">            found_match = <span class=\"keyword\">True</span></span><br><span class=\"line\">            gt_box[<span class=\"string\">'bbox_matched'</span>] = <span class=\"keyword\">True</span></span><br><span class=\"line\">            <span class=\"keyword\">break</span></span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            <span class=\"keyword\">continue</span></span><br><span class=\"line\">    <span class=\"comment\"># 1 means this position's bbox correct match with orignal image</span></span><br><span class=\"line\">    T[pred_class].append(int(found_match))</span><br></pre></td></tr></table></figure>\n<p><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/014_4.JPG\" alt=\"image\"></p>\n<p>diffculttrue10<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># adding missing object compared to orignal image</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> gt_box <span class=\"keyword\">in</span> gt:</span><br><span class=\"line\">    <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> gt_box[<span class=\"string\">'bbox_matched'</span>] <span class=\"keyword\">and</span> <span class=\"keyword\">not</span> gt_box[<span class=\"string\">'difficult'</span>]:</span><br><span class=\"line\">        <span class=\"keyword\">if</span> gt_box[<span class=\"string\">'class'</span>] <span class=\"keyword\">not</span> <span class=\"keyword\">in</span> P:</span><br><span class=\"line\">            P[gt_box[<span class=\"string\">'class'</span>]] = []</span><br><span class=\"line\">            T[gt_box[<span class=\"string\">'class'</span>]] = []</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># T = 1 means there are object, P = 0 means we did't detected that</span></span><br><span class=\"line\">        T[gt_box[<span class=\"string\">'class'</span>]].append(<span class=\"number\">1</span>)</span><br><span class=\"line\">        P[gt_box[<span class=\"string\">'class'</span>]].append(<span class=\"number\">0</span>)</span><br></pre></td></tr></table></figure></p>\n<p><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/014_5.JPG\" alt=\"image\"></p>\n<p>average_precision_scoresklearnapmap<br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/014_6.JPG\" alt=\"image\"></p>\n<p><a href=\"https://github.com/Trouble404/Object-Detection-System-Based-on-CNN-and-Capsule-Network/blob/master/Faster-RCNN/map.ipynb\" target=\"_blank\" rel=\"noopener\">jupyter notebook</a></p>\n<h3 id=\"06-10-08-2018\"><a href=\"#06-10-08-2018\" class=\"headerlink\" title=\"06~10/08/2018\"></a>06~10/08/2018</h3><h4 id=\"adjust\"><a href=\"#adjust\" class=\"headerlink\" title=\"adjust\"></a>adjust</h4><p>ap<br><a href=\"https://github.com/Trouble404/Object-Detection-System-Based-on-CNN-and-Capsule-Network/blob/master/Faster-RCNN/map_all.ipynb\" target=\"_blank\" rel=\"noopener\">jupyter notebook</a></p>\n<h5 id=\"Project1-all-9-models\"><a href=\"#Project1-all-9-models\" class=\"headerlink\" title=\"Project1 all: 9 models:\"></a>Project1 all: 9 models:</h5><p>ALL_2 NO THRESHOLD OTHER WITH THRESHOLD MOST 0.8</p>\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">Classes</th>\n<th style=\"text-align:center\">ALL_1</th>\n<th style=\"text-align:center\">ALL_2</th>\n<th style=\"text-align:center\">ALL_3</th>\n<th style=\"text-align:center\">ALL_4</th>\n<th style=\"text-align:center\">ALL_5</th>\n<th style=\"text-align:center\">ALL_6</th>\n<th style=\"text-align:center\">ALL_7</th>\n<th style=\"text-align:center\">ALL_8</th>\n<th style=\"text-align:center\">ALL_9</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">motorbike</td>\n<td style=\"text-align:center\">0.2305</td>\n<td style=\"text-align:center\">0.2412</td>\n<td style=\"text-align:center\">0.2132</td>\n<td style=\"text-align:center\">0.2220</td>\n<td style=\"text-align:center\">0.2889</td>\n<td style=\"text-align:center\">0.2528</td>\n<td style=\"text-align:center\">0.2204</td>\n<td style=\"text-align:center\">0.2644</td>\n<td style=\"text-align:center\">0.2336</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">person</td>\n<td style=\"text-align:center\">0.6489</td>\n<td style=\"text-align:center\">0.6735</td>\n<td style=\"text-align:center\">0.7107</td>\n<td style=\"text-align:center\">0.6652</td>\n<td style=\"text-align:center\">0.7120</td>\n<td style=\"text-align:center\">0.7041</td>\n<td style=\"text-align:center\">0.7238</td>\n<td style=\"text-align:center\">0.7003</td>\n<td style=\"text-align:center\">0.7201</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">car</td>\n<td style=\"text-align:center\">0.1697</td>\n<td style=\"text-align:center\">0.1563</td>\n<td style=\"text-align:center\">0.2032</td>\n<td style=\"text-align:center\">0.2105</td>\n<td style=\"text-align:center\">0.2308</td>\n<td style=\"text-align:center\">0.2221</td>\n<td style=\"text-align:center\">0.2436</td>\n<td style=\"text-align:center\">0.2053</td>\n<td style=\"text-align:center\">0.2080</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">aeroplane</td>\n<td style=\"text-align:center\">0.7968</td>\n<td style=\"text-align:center\">0.7062</td>\n<td style=\"text-align:center\">0.7941</td>\n<td style=\"text-align:center\">0.6412</td>\n<td style=\"text-align:center\">0.7871</td>\n<td style=\"text-align:center\">0.7331</td>\n<td style=\"text-align:center\">0.7648</td>\n<td style=\"text-align:center\">0.7659</td>\n<td style=\"text-align:center\">0.6902</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">bottle</td>\n<td style=\"text-align:center\">0.2213</td>\n<td style=\"text-align:center\">0.2428</td>\n<td style=\"text-align:center\">0.2261</td>\n<td style=\"text-align:center\">0.1943</td>\n<td style=\"text-align:center\">0.2570</td>\n<td style=\"text-align:center\">0.2437</td>\n<td style=\"text-align:center\">0.2899</td>\n<td style=\"text-align:center\">0.1442</td>\n<td style=\"text-align:center\">0.2265</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">sheep</td>\n<td style=\"text-align:center\">0.6162</td>\n<td style=\"text-align:center\">0.5702</td>\n<td style=\"text-align:center\">0.6876</td>\n<td style=\"text-align:center\">0.6295</td>\n<td style=\"text-align:center\">0.6364</td>\n<td style=\"text-align:center\">0.5710</td>\n<td style=\"text-align:center\">0.6536</td>\n<td style=\"text-align:center\">0.6349</td>\n<td style=\"text-align:center\">0.6455</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">tvmonitor</td>\n<td style=\"text-align:center\">0.1582</td>\n<td style=\"text-align:center\">0.1601</td>\n<td style=\"text-align:center\">0.2231</td>\n<td style=\"text-align:center\">0.1748</td>\n<td style=\"text-align:center\">0.1551</td>\n<td style=\"text-align:center\">0.1603</td>\n<td style=\"text-align:center\">0.1317</td>\n<td style=\"text-align:center\">0.1584</td>\n<td style=\"text-align:center\">0.1678</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">boat</td>\n<td style=\"text-align:center\">0.3842</td>\n<td style=\"text-align:center\">0.2621</td>\n<td style=\"text-align:center\">0.2261</td>\n<td style=\"text-align:center\">0.1943</td>\n<td style=\"text-align:center\">0.3499</td>\n<td style=\"text-align:center\">0.2437</td>\n<td style=\"text-align:center\">0.2057</td>\n<td style=\"text-align:center\">0.2748</td>\n<td style=\"text-align:center\">0.3509</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">chair</td>\n<td style=\"text-align:center\">0.2811</td>\n<td style=\"text-align:center\">0.0563</td>\n<td style=\"text-align:center\">0.0891</td>\n<td style=\"text-align:center\">0.0621</td>\n<td style=\"text-align:center\">0.1353</td>\n<td style=\"text-align:center\">0.0865</td>\n<td style=\"text-align:center\">0.0907</td>\n<td style=\"text-align:center\">0.0854</td>\n<td style=\"text-align:center\">0.1282</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">bicycle</td>\n<td style=\"text-align:center\">0.1464</td>\n<td style=\"text-align:center\">0.1224</td>\n<td style=\"text-align:center\">0.1346</td>\n<td style=\"text-align:center\">0.1781</td>\n<td style=\"text-align:center\">0.1406</td>\n<td style=\"text-align:center\">0.1448</td>\n<td style=\"text-align:center\">0.1810</td>\n<td style=\"text-align:center\">0.1071</td>\n<td style=\"text-align:center\">0.1673</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">cat</td>\n<td style=\"text-align:center\">0.8901</td>\n<td style=\"text-align:center\">0.8565</td>\n<td style=\"text-align:center\">0.9103</td>\n<td style=\"text-align:center\">0.8417</td>\n<td style=\"text-align:center\">0.8289</td>\n<td style=\"text-align:center\">0.8274</td>\n<td style=\"text-align:center\">0.7572</td>\n<td style=\"text-align:center\">0.9143</td>\n<td style=\"text-align:center\">0.8118</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">pottedplant</td>\n<td style=\"text-align:center\">0.2075</td>\n<td style=\"text-align:center\">0.0926</td>\n<td style=\"text-align:center\">0.1790</td>\n<td style=\"text-align:center\">0.0532</td>\n<td style=\"text-align:center\">0.1517</td>\n<td style=\"text-align:center\">0.1150</td>\n<td style=\"text-align:center\">0.1080</td>\n<td style=\"text-align:center\">0.1022</td>\n<td style=\"text-align:center\">0.0939</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">horse</td>\n<td style=\"text-align:center\">0.1185</td>\n<td style=\"text-align:center\">0.0588</td>\n<td style=\"text-align:center\">0.0726</td>\n<td style=\"text-align:center\">0.0489</td>\n<td style=\"text-align:center\">0.0696</td>\n<td style=\"text-align:center\">0.0695</td>\n<td style=\"text-align:center\">0.0637</td>\n<td style=\"text-align:center\">0.0651</td>\n<td style=\"text-align:center\">0.0640</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">sofa</td>\n<td style=\"text-align:center\">0.2797</td>\n<td style=\"text-align:center\">0.2309</td>\n<td style=\"text-align:center\">0.2852</td>\n<td style=\"text-align:center\">0.2966</td>\n<td style=\"text-align:center\">0.3855</td>\n<td style=\"text-align:center\">0.4817</td>\n<td style=\"text-align:center\">0.3659</td>\n<td style=\"text-align:center\">0.3132</td>\n<td style=\"text-align:center\">0.3090</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">dog</td>\n<td style=\"text-align:center\">0.5359</td>\n<td style=\"text-align:center\">0.5077</td>\n<td style=\"text-align:center\">0.5578</td>\n<td style=\"text-align:center\">0.4413</td>\n<td style=\"text-align:center\">0.4832</td>\n<td style=\"text-align:center\">0.5793</td>\n<td style=\"text-align:center\">0.5687</td>\n<td style=\"text-align:center\">0.4910</td>\n<td style=\"text-align:center\">0.4598</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">cow</td>\n<td style=\"text-align:center\">0.7582</td>\n<td style=\"text-align:center\">0.6229</td>\n<td style=\"text-align:center\">0.7295</td>\n<td style=\"text-align:center\">0.5420</td>\n<td style=\"text-align:center\">0.5379</td>\n<td style=\"text-align:center\">0.5312</td>\n<td style=\"text-align:center\">0.5147</td>\n<td style=\"text-align:center\">0.5706</td>\n<td style=\"text-align:center\">0.6503</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">diningtable</td>\n<td style=\"text-align:center\">0.3979</td>\n<td style=\"text-align:center\">0.2734</td>\n<td style=\"text-align:center\">0.3739</td>\n<td style=\"text-align:center\">0.2963</td>\n<td style=\"text-align:center\">0.4715</td>\n<td style=\"text-align:center\">0.4987</td>\n<td style=\"text-align:center\">0.3895</td>\n<td style=\"text-align:center\">0.4983</td>\n<td style=\"text-align:center\">0.4666</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">bus</td>\n<td style=\"text-align:center\">0.6203</td>\n<td style=\"text-align:center\">0.5572</td>\n<td style=\"text-align:center\">0.6468</td>\n<td style=\"text-align:center\">0.6032</td>\n<td style=\"text-align:center\">0.6320</td>\n<td style=\"text-align:center\">0.6096</td>\n<td style=\"text-align:center\">0.7169</td>\n<td style=\"text-align:center\">0.5938</td>\n<td style=\"text-align:center\">0.5485</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">bird</td>\n<td style=\"text-align:center\">0.6164</td>\n<td style=\"text-align:center\">0.6662</td>\n<td style=\"text-align:center\">0.5692</td>\n<td style=\"text-align:center\">0.5751</td>\n<td style=\"text-align:center\">0.5407</td>\n<td style=\"text-align:center\">0.4125</td>\n<td style=\"text-align:center\">0.4925</td>\n<td style=\"text-align:center\">0.4347</td>\n<td style=\"text-align:center\">0.5208</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">train</td>\n<td style=\"text-align:center\">0.8655</td>\n<td style=\"text-align:center\">0.6916</td>\n<td style=\"text-align:center\">0.7141</td>\n<td style=\"text-align:center\">0.7166</td>\n<td style=\"text-align:center\">0.7643</td>\n<td style=\"text-align:center\">0.8107</td>\n<td style=\"text-align:center\">0.7100</td>\n<td style=\"text-align:center\">0.7194</td>\n<td style=\"text-align:center\">0.6263</td>\n</tr>\n<tr>\n<td style=\"text-align:center\"><strong>mAP</strong></td>\n<td style=\"text-align:center\"><strong>0.4472</strong></td>\n<td style=\"text-align:center\"><strong>0.3874</strong></td>\n<td style=\"text-align:center\"><strong>0.4341</strong></td>\n<td style=\"text-align:center\"><strong>0.3859</strong></td>\n<td style=\"text-align:center\"><strong>0.4279</strong></td>\n<td style=\"text-align:center\"><strong>0.4141</strong></td>\n<td style=\"text-align:center\"><strong>0.4096</strong></td>\n<td style=\"text-align:center\"><strong>0.4022</strong></td>\n<td style=\"text-align:center\"><strong>0.4045</strong></td>\n</tr>\n</tbody>\n</table>\n<hr>\n<h5 id=\"Project1-epoch-lenght-1000-epoch-1041-7-models\"><a href=\"#Project1-epoch-lenght-1000-epoch-1041-7-models\" class=\"headerlink\" title=\"Project1 epoch_lenght=1000, epoch:1041 : 7 models:\"></a>Project1 epoch_lenght=1000, epoch:1041 : 7 models:</h5><p>ALL WITH THRESHOLD MOST 0.51</p>\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">Classes</th>\n<th style=\"text-align:center\">ALL_1</th>\n<th style=\"text-align:center\">ALL_2</th>\n<th style=\"text-align:center\">ALL_3</th>\n<th style=\"text-align:center\">ALL_4</th>\n<th style=\"text-align:center\">ALL_5</th>\n<th style=\"text-align:center\">ALL_6</th>\n<th style=\"text-align:center\">ALL_7</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">motorbike</td>\n<td style=\"text-align:center\">0.2433</td>\n<td style=\"text-align:center\">0.2128</td>\n<td style=\"text-align:center\">0.2232</td>\n<td style=\"text-align:center\">0.2262</td>\n<td style=\"text-align:center\">0.2286</td>\n<td style=\"text-align:center\">0.2393</td>\n<td style=\"text-align:center\">0.2279</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">person</td>\n<td style=\"text-align:center\">0.6560</td>\n<td style=\"text-align:center\">0.6537</td>\n<td style=\"text-align:center\">0.6742</td>\n<td style=\"text-align:center\">0.6952</td>\n<td style=\"text-align:center\">0.6852</td>\n<td style=\"text-align:center\">0.6719</td>\n<td style=\"text-align:center\">0.6636</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">car</td>\n<td style=\"text-align:center\">0.1562</td>\n<td style=\"text-align:center\">0.1905</td>\n<td style=\"text-align:center\">0.1479</td>\n<td style=\"text-align:center\">0.2024</td>\n<td style=\"text-align:center\">0.2010</td>\n<td style=\"text-align:center\">0.1379</td>\n<td style=\"text-align:center\">0.1583</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">aeroplane</td>\n<td style=\"text-align:center\">0.7359</td>\n<td style=\"text-align:center\">0.6837</td>\n<td style=\"text-align:center\">0.6729</td>\n<td style=\"text-align:center\">0.6687</td>\n<td style=\"text-align:center\">0.6957</td>\n<td style=\"text-align:center\">0.7339</td>\n<td style=\"text-align:center\">0.6391</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">bottle</td>\n<td style=\"text-align:center\">0.1913</td>\n<td style=\"text-align:center\">0.1937</td>\n<td style=\"text-align:center\">0.2635</td>\n<td style=\"text-align:center\">0.1843</td>\n<td style=\"text-align:center\">0.2570</td>\n<td style=\"text-align:center\">0.1632</td>\n<td style=\"text-align:center\">0.1863</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">sheep</td>\n<td style=\"text-align:center\">0.5429</td>\n<td style=\"text-align:center\">0.5579</td>\n<td style=\"text-align:center\">0.6219</td>\n<td style=\"text-align:center\">0.5355</td>\n<td style=\"text-align:center\">0.5881</td>\n<td style=\"text-align:center\">0.5441</td>\n<td style=\"text-align:center\">0.5824</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">tvmonitor</td>\n<td style=\"text-align:center\">0.1295</td>\n<td style=\"text-align:center\">0.1601</td>\n<td style=\"text-align:center\">0.1368</td>\n<td style=\"text-align:center\">0.1407</td>\n<td style=\"text-align:center\">0.1147</td>\n<td style=\"text-align:center\">0.1349</td>\n<td style=\"text-align:center\">0.1154</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">boat</td>\n<td style=\"text-align:center\">0.1913</td>\n<td style=\"text-align:center\">0.2880</td>\n<td style=\"text-align:center\">0.2635</td>\n<td style=\"text-align:center\">0.3433</td>\n<td style=\"text-align:center\">0.3335</td>\n<td style=\"text-align:center\">0.3422</td>\n<td style=\"text-align:center\">0.3069</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">chair</td>\n<td style=\"text-align:center\">0.0587</td>\n<td style=\"text-align:center\">0.0657</td>\n<td style=\"text-align:center\">0.0342</td>\n<td style=\"text-align:center\">0.0680</td>\n<td style=\"text-align:center\">0.0695</td>\n<td style=\"text-align:center\">0.0752</td>\n<td style=\"text-align:center\">0.0760</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">bicycle</td>\n<td style=\"text-align:center\">0.1013</td>\n<td style=\"text-align:center\">0.1485</td>\n<td style=\"text-align:center\">0.1225</td>\n<td style=\"text-align:center\">0.1871</td>\n<td style=\"text-align:center\">0.1685</td>\n<td style=\"text-align:center\">0.1037</td>\n<td style=\"text-align:center\">0.1490</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">cat</td>\n<td style=\"text-align:center\">0.8737</td>\n<td style=\"text-align:center\">0.8557</td>\n<td style=\"text-align:center\">0.8007</td>\n<td style=\"text-align:center\">0.7982</td>\n<td style=\"text-align:center\">0.8045</td>\n<td style=\"text-align:center\">0.8067</td>\n<td style=\"text-align:center\">0.7732</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">pottedplant</td>\n<td style=\"text-align:center\">0.0694</td>\n<td style=\"text-align:center\">0.1059</td>\n<td style=\"text-align:center\">0.0748</td>\n<td style=\"text-align:center\">0.0878</td>\n<td style=\"text-align:center\">0.0893</td>\n<td style=\"text-align:center\">0.0690</td>\n<td style=\"text-align:center\">0.0865</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">horse</td>\n<td style=\"text-align:center\">0.0556</td>\n<td style=\"text-align:center\">0.0561</td>\n<td style=\"text-align:center\">0.0581</td>\n<td style=\"text-align:center\">0.0770</td>\n<td style=\"text-align:center\">0.0575</td>\n<td style=\"text-align:center\">0.0539</td>\n<td style=\"text-align:center\">0.0522</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">sofa</td>\n<td style=\"text-align:center\">0.2177</td>\n<td style=\"text-align:center\">0.2917</td>\n<td style=\"text-align:center\">0.1699</td>\n<td style=\"text-align:center\">0.1940</td>\n<td style=\"text-align:center\">0.3177</td>\n<td style=\"text-align:center\">0.1863</td>\n<td style=\"text-align:center\">0.1857</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">dog</td>\n<td style=\"text-align:center\">0.6269</td>\n<td style=\"text-align:center\">0.4989</td>\n<td style=\"text-align:center\">0.5015</td>\n<td style=\"text-align:center\">0.5333</td>\n<td style=\"text-align:center\">0.4914</td>\n<td style=\"text-align:center\">0.5572</td>\n<td style=\"text-align:center\">0.4747</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">cow</td>\n<td style=\"text-align:center\">0.5216</td>\n<td style=\"text-align:center\">0.6229</td>\n<td style=\"text-align:center\">0.5283</td>\n<td style=\"text-align:center\">0.6426</td>\n<td style=\"text-align:center\">0.4358</td>\n<td style=\"text-align:center\">0.4227</td>\n<td style=\"text-align:center\">0.4589</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">diningtable</td>\n<td style=\"text-align:center\">0.3076</td>\n<td style=\"text-align:center\">0.3889</td>\n<td style=\"text-align:center\">0.3283</td>\n<td style=\"text-align:center\">0.2404</td>\n<td style=\"text-align:center\">0.4219</td>\n<td style=\"text-align:center\">0.4153</td>\n<td style=\"text-align:center\">0.2627</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">bus</td>\n<td style=\"text-align:center\">0.5865</td>\n<td style=\"text-align:center\">0.5222</td>\n<td style=\"text-align:center\">0.6312</td>\n<td style=\"text-align:center\">0.5853</td>\n<td style=\"text-align:center\">0.5042</td>\n<td style=\"text-align:center\">0.4882</td>\n<td style=\"text-align:center\">0.5576</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">bird</td>\n<td style=\"text-align:center\">0.5339</td>\n<td style=\"text-align:center\">0.5039</td>\n<td style=\"text-align:center\">0.5150</td>\n<td style=\"text-align:center\">0.5152</td>\n<td style=\"text-align:center\">0.5838</td>\n<td style=\"text-align:center\">0.3890</td>\n<td style=\"text-align:center\">0.4680</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">train</td>\n<td style=\"text-align:center\">0.4994</td>\n<td style=\"text-align:center\">0.6541</td>\n<td style=\"text-align:center\">0.6702</td>\n<td style=\"text-align:center\">0.6920</td>\n<td style=\"text-align:center\">0.5959</td>\n<td style=\"text-align:center\">0.5893</td>\n<td style=\"text-align:center\">0.6861</td>\n</tr>\n<tr>\n<td style=\"text-align:center\"><strong>mAP</strong></td>\n<td style=\"text-align:center\"><strong>0.3699</strong></td>\n<td style=\"text-align:center\"><strong>0.3765</strong></td>\n<td style=\"text-align:center\"><strong>0.3748</strong></td>\n<td style=\"text-align:center\"><strong>0.3814</strong></td>\n<td style=\"text-align:center\"><strong>0.3786</strong></td>\n<td style=\"text-align:center\"><strong>0.3562</strong></td>\n<td style=\"text-align:center\"><strong>0.3555</strong></td>\n</tr>\n</tbody>\n</table>\n<p>classap</p>\n<p><strong>VOC2007</strong><br>VOC2007OpenCVBUG<br><br>VOC2012VOC2007Irius</p>\n<p>20AP10002Wclass_balance</p>\n<p>class balance</p>\n<p>imagenet</p>\n<p></p>\n<h3 id=\"13-08-2018\"><a href=\"#13-08-2018\" class=\"headerlink\" title=\"13/08/2018\"></a>13/08/2018</h3><h4 id=\"soft-NMS\"><a href=\"#soft-NMS\" class=\"headerlink\" title=\"soft-NMS\"></a>soft-NMS</h4><p>BSMBDBMNtaverage precision, AP</p>\n<p>M<br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/018_1.JPG\" alt=\"image\"><br>NMSSoft-NMSNMS<strong>MM</strong>PASCAL VOC  MS-COCOSoft-NMSSoft-NMS</p>\n<p><br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/018_2.JPG\" alt=\"image\"></p>\n<p><br>NMS<br>$$ s_{i}=\\left{<br>\\begin{aligned}<br>s_{i}, \\ \\ \\ \\ iou(M,b_{i}) &lt;  N_{t} \\<br>0, \\ \\ \\ \\ iou(M,b_{i}) \\geq  N_{t}<br>\\end{aligned}<br>\\right.<br>$$</p>\n<p>SOFT NMS<br>$$ s_{i}=\\left{<br>\\begin{aligned}<br>s_{i}, \\ \\ \\ \\ iou(M,b_{i}) &lt;  N_{t} \\<br>1-iou(M,b_{i}), \\ \\ \\ \\ iou(M,b_{i}) \\geq  N_{t}<br>\\end{aligned}<br>\\right.<br>$$</p>\n<p>MNtMM</p>\n<p>Ntsoft-NMS</p>\n<p>Gaussian penalty:<br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/018_3.JPG\" alt=\"image\"></p>\n<p><br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br><span class=\"line\">107</span><br><span class=\"line\">108</span><br><span class=\"line\">109</span><br><span class=\"line\">110</span><br><span class=\"line\">111</span><br><span class=\"line\">112</span><br><span class=\"line\">113</span><br><span class=\"line\">114</span><br><span class=\"line\">115</span><br><span class=\"line\">116</span><br><span class=\"line\">117</span><br><span class=\"line\">118</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"string\">\"\"\" NMS , delete overlapping box</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">@param boxes: (n,4) box and coresspoding cordinates</span></span><br><span class=\"line\"><span class=\"string\">@param probs: (n,) box adn coresspding possibility</span></span><br><span class=\"line\"><span class=\"string\">@param overlap_thresh: treshold of delet box overlapping</span></span><br><span class=\"line\"><span class=\"string\">@param max_boxes: maximum keeping number of boxes</span></span><br><span class=\"line\"><span class=\"string\">@param method: 1 for linear soft NMS, 2 for gaussian soft NMS</span></span><br><span class=\"line\"><span class=\"string\">@param sigma: parameter of gaussian soft NMS</span></span><br><span class=\"line\"><span class=\"string\">prob_thresh: threshold of probs after soft NMS</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">@return: boxes: boxes cordinates(x1,y1,x2,y2)</span></span><br><span class=\"line\"><span class=\"string\">@return: probs: coresspoding possibility</span></span><br><span class=\"line\"><span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">soft_nms</span><span class=\"params\">(boxes, probs, overlap_thresh=<span class=\"number\">0.9</span>, max_boxes=<span class=\"number\">300</span>, method = <span class=\"number\">1</span>, sigma=<span class=\"number\">0.5</span>, prob_thresh=<span class=\"number\">0.49</span>)</span>:</span></span><br><span class=\"line\">    <span class=\"comment\"># number of input boxes</span></span><br><span class=\"line\">    N = boxes.shape[<span class=\"number\">0</span>]</span><br><span class=\"line\">    <span class=\"comment\"># if the bounding boxes integers, convert them to floats --</span></span><br><span class=\"line\">    <span class=\"comment\"># this is important since we'll be doing a bunch of divisions</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> boxes.dtype.kind == <span class=\"string\">\"i\"</span>:</span><br><span class=\"line\">        boxes = boxes.astype(<span class=\"string\">\"float\"</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># iterate all boxes</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(N):</span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"comment\"># obtain current boxes' cordinates and probs</span></span><br><span class=\"line\">        maxscore = probs[i]</span><br><span class=\"line\">        maxpos = i</span><br><span class=\"line\"></span><br><span class=\"line\">        tx1 = boxes[i,<span class=\"number\">0</span>]</span><br><span class=\"line\">        ty1 = boxes[i,<span class=\"number\">1</span>]</span><br><span class=\"line\">        tx2 = boxes[i,<span class=\"number\">2</span>]</span><br><span class=\"line\">        ty2 = boxes[i,<span class=\"number\">3</span>]</span><br><span class=\"line\">        ts = probs[i]</span><br><span class=\"line\"></span><br><span class=\"line\">        pos = i + <span class=\"number\">1</span></span><br><span class=\"line\">        <span class=\"comment\"># get max box</span></span><br><span class=\"line\">        <span class=\"keyword\">while</span> pos &lt; N:</span><br><span class=\"line\">            <span class=\"keyword\">if</span> maxscore &lt; probs[pos]:</span><br><span class=\"line\">                maxscore = probs[pos]</span><br><span class=\"line\">                maxpos = pos</span><br><span class=\"line\">            pos = pos + <span class=\"number\">1</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># add max box as a detection </span></span><br><span class=\"line\">        boxes[i,<span class=\"number\">0</span>] = boxes[maxpos,<span class=\"number\">0</span>]</span><br><span class=\"line\">        boxes[i,<span class=\"number\">1</span>] = boxes[maxpos,<span class=\"number\">1</span>]</span><br><span class=\"line\">        boxes[i,<span class=\"number\">2</span>] = boxes[maxpos,<span class=\"number\">2</span>]</span><br><span class=\"line\">        boxes[i,<span class=\"number\">3</span>] = boxes[maxpos,<span class=\"number\">3</span>]</span><br><span class=\"line\">        probs[i] = probs[maxpos]</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># swap ith box with position of max box</span></span><br><span class=\"line\">        boxes[maxpos,<span class=\"number\">0</span>] = tx1</span><br><span class=\"line\">        boxes[maxpos,<span class=\"number\">1</span>] = ty1</span><br><span class=\"line\">        boxes[maxpos,<span class=\"number\">2</span>] = tx2</span><br><span class=\"line\">        boxes[maxpos,<span class=\"number\">3</span>] = ty2</span><br><span class=\"line\">        probs[maxpos] = ts</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># cordinates of max box</span></span><br><span class=\"line\">        tx1 = boxes[i,<span class=\"number\">0</span>]</span><br><span class=\"line\">        ty1 = boxes[i,<span class=\"number\">1</span>]</span><br><span class=\"line\">        tx2 = boxes[i,<span class=\"number\">2</span>]</span><br><span class=\"line\">        ty2 = boxes[i,<span class=\"number\">3</span>]</span><br><span class=\"line\">        ts = probs[i]</span><br><span class=\"line\"></span><br><span class=\"line\">        pos = i + <span class=\"number\">1</span></span><br><span class=\"line\">        <span class=\"comment\"># NMS iterations, note that N changes if detection boxes fall below threshold</span></span><br><span class=\"line\">        <span class=\"keyword\">while</span> pos &lt; N:</span><br><span class=\"line\">            x1 = boxes[pos, <span class=\"number\">0</span>]</span><br><span class=\"line\">            y1 = boxes[pos, <span class=\"number\">1</span>]</span><br><span class=\"line\">            x2 = boxes[pos, <span class=\"number\">2</span>]</span><br><span class=\"line\">            y2 = boxes[pos, <span class=\"number\">3</span>]</span><br><span class=\"line\">            s = probs[pos]</span><br><span class=\"line\">            </span><br><span class=\"line\">            <span class=\"comment\"># calculate the areas, +1 for robatness</span></span><br><span class=\"line\">            area = (x2 - x1 + <span class=\"number\">1</span>) * (y2 - y1 + <span class=\"number\">1</span>)</span><br><span class=\"line\">            iw = (min(tx2, x2) - max(tx1, x1) + <span class=\"number\">1</span>)</span><br><span class=\"line\">            <span class=\"comment\"># # confirm left top cordinates less than top right</span></span><br><span class=\"line\">            <span class=\"keyword\">if</span> iw &gt; <span class=\"number\">0</span>:</span><br><span class=\"line\">                ih = (min(ty2, y2) - max(ty1, y1) + <span class=\"number\">1</span>)</span><br><span class=\"line\">                <span class=\"comment\"># confirm left top cordinates less than top right</span></span><br><span class=\"line\">                <span class=\"keyword\">if</span> ih &gt; <span class=\"number\">0</span>:</span><br><span class=\"line\">                    <span class=\"comment\"># find the union</span></span><br><span class=\"line\">                    ua = float((tx2 - tx1 + <span class=\"number\">1</span>) * (ty2 - ty1 + <span class=\"number\">1</span>) + area - iw * ih)</span><br><span class=\"line\">                    <span class=\"comment\">#iou between max box and detection box</span></span><br><span class=\"line\">                    ov = iw * ih / ua</span><br><span class=\"line\"></span><br><span class=\"line\">                    <span class=\"keyword\">if</span> method == <span class=\"number\">1</span>: <span class=\"comment\"># linear</span></span><br><span class=\"line\">                        <span class=\"keyword\">if</span> ov &gt; overlap_thresh: </span><br><span class=\"line\">                            weight = <span class=\"number\">1</span> - ov</span><br><span class=\"line\">                        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">                            weight = <span class=\"number\">1</span></span><br><span class=\"line\">                    <span class=\"keyword\">elif</span> method == <span class=\"number\">2</span>: <span class=\"comment\"># gaussian</span></span><br><span class=\"line\">                        weight = np.exp(-(ov * ov)/sigma)</span><br><span class=\"line\">                    <span class=\"keyword\">else</span>: <span class=\"comment\"># original NMS</span></span><br><span class=\"line\">                        <span class=\"keyword\">if</span> ov &gt; overlap_thresh: </span><br><span class=\"line\">                            weight = <span class=\"number\">0</span></span><br><span class=\"line\">                        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">                            weight = <span class=\"number\">1</span></span><br><span class=\"line\"></span><br><span class=\"line\">                    <span class=\"comment\"># obtain adjusted probs</span></span><br><span class=\"line\">                    probs[pos] = weight*probs[pos]</span><br><span class=\"line\"></span><br><span class=\"line\">   </span><br><span class=\"line\">                    <span class=\"comment\"># if box score falls below threshold, discard the box by swapping with last box</span></span><br><span class=\"line\">                    <span class=\"comment\"># update N</span></span><br><span class=\"line\">                    <span class=\"keyword\">if</span> probs[pos] &lt; prob_thresh:</span><br><span class=\"line\">                        boxes[pos,<span class=\"number\">0</span>] = boxes[N<span class=\"number\">-1</span>, <span class=\"number\">0</span>]</span><br><span class=\"line\">                        boxes[pos,<span class=\"number\">1</span>] = boxes[N<span class=\"number\">-1</span>, <span class=\"number\">1</span>]</span><br><span class=\"line\">                        boxes[pos,<span class=\"number\">2</span>] = boxes[N<span class=\"number\">-1</span>, <span class=\"number\">2</span>]</span><br><span class=\"line\">                        boxes[pos,<span class=\"number\">3</span>] = boxes[N<span class=\"number\">-1</span>, <span class=\"number\">3</span>]</span><br><span class=\"line\">                        probs[pos] = probs[N<span class=\"number\">-1</span>]</span><br><span class=\"line\">                        N = N - <span class=\"number\">1</span></span><br><span class=\"line\">                        pos = pos - <span class=\"number\">1</span></span><br><span class=\"line\"></span><br><span class=\"line\">            pos = pos + <span class=\"number\">1</span></span><br><span class=\"line\">    <span class=\"comment\"># keep is the idx of current keeping objects, front ith objectes</span></span><br><span class=\"line\">    keep = [i <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(N)]</span><br><span class=\"line\">    <span class=\"keyword\">return</span> boxes[keep], probs[keep]</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"14-08-2018\"><a href=\"#14-08-2018\" class=\"headerlink\" title=\"14/08/2018\"></a>14/08/2018</h3><h4 id=\"OVERLAPPING-OBJECT-DETECTION\"><a href=\"#OVERLAPPING-OBJECT-DETECTION\" class=\"headerlink\" title=\"OVERLAPPING OBJECT DETECTION\"></a>OVERLAPPING OBJECT DETECTION</h4><h3 id=\"15-08-2018\"><a href=\"#15-08-2018\" class=\"headerlink\" title=\"15/08/2018\"></a>15/08/2018</h3><h3 id=\"16-08-2018\"><a href=\"#16-08-2018\" class=\"headerlink\" title=\"16/08/2018\"></a>16/08/2018</h3><h3 id=\"17-08-2018\"><a href=\"#17-08-2018\" class=\"headerlink\" title=\"17/08/2018\"></a>17/08/2018</h3><h2 id=\"September\"><a href=\"#September\" class=\"headerlink\" title=\"September\"></a>September</h2>"}],"PostAsset":[],"PostCategory":[{"post_id":"ck9jwdnfx0001jlrcgips8q0o","category_id":"ck9jwdnh6000cjlrcwlvmu5ot","_id":"ck9jwdnht000wjlrcle22eby6"},{"post_id":"ck9jwdng10003jlrc7j1i0314","category_id":"ck9jwdnha000ejlrc7w8nwbju","_id":"ck9jwdnhu000yjlrc3igx1rdt"},{"post_id":"ck9jwdng30005jlrcozi1jzkz","category_id":"ck9jwdnha000gjlrcmtigkx3m","_id":"ck9jwdnhu0010jlrc07epd8im"},{"post_id":"ck9jwdng40006jlrcowjfpwux","category_id":"ck9jwdnhb000ijlrcrmjbet7n","_id":"ck9jwdnhu0012jlrcqcsp73m4"},{"post_id":"ck9jwdng50007jlrcdxl2eea8","category_id":"ck9jwdnhb000ijlrcrmjbet7n","_id":"ck9jwdnhu0014jlrcc39pz5hc"},{"post_id":"ck9jwdng50008jlrcbfxepafd","category_id":"ck9jwdnhd000mjlrcleeolcba","_id":"ck9jwdnhu0016jlrc5up5xlbz"},{"post_id":"ck9jwdng60009jlrclgm4jkxk","category_id":"ck9jwdnhd000mjlrcleeolcba","_id":"ck9jwdnhv0018jlrcuk2prea4"},{"post_id":"ck9jwdng7000ajlrckzg7vut2","category_id":"ck9jwdnhd000mjlrcleeolcba","_id":"ck9jwdnhv001ajlrcw2qfd7gj"},{"post_id":"ck9jwdng9000bjlrcae01whra","category_id":"ck9jwdnhd000mjlrcleeolcba","_id":"ck9jwdnhv001cjlrc8pwuopr6"},{"post_id":"ck9jwdnmb001ejlrcuz1ek2ga","category_id":"ck9jwdnmm001fjlrcfjvwudkl","_id":"ck9jwdnn0001ijlrckl3ya8dm"}],"PostTag":[{"post_id":"ck9jwdnfx0001jlrcgips8q0o","tag_id":"ck9jwdnh9000djlrcg0y6kj4x","_id":"ck9jwdnht000vjlrcz7j76a0i"},{"post_id":"ck9jwdng10003jlrc7j1i0314","tag_id":"ck9jwdnha000fjlrcl9ysio6f","_id":"ck9jwdnhu000xjlrcgy1toe22"},{"post_id":"ck9jwdng30005jlrcozi1jzkz","tag_id":"ck9jwdnhb000hjlrcf921d18d","_id":"ck9jwdnhu000zjlrct76cqnj3"},{"post_id":"ck9jwdng40006jlrcowjfpwux","tag_id":"ck9jwdnhb000jjlrcaepavhro","_id":"ck9jwdnhu0011jlrco914igle"},{"post_id":"ck9jwdng50007jlrcdxl2eea8","tag_id":"ck9jwdnhc000ljlrcz97ycyuo","_id":"ck9jwdnhu0013jlrcqkt2554y"},{"post_id":"ck9jwdng50007jlrcdxl2eea8","tag_id":"ck9jwdnhe000njlrcuov4rg19","_id":"ck9jwdnhu0015jlrcds6pc41m"},{"post_id":"ck9jwdng50008jlrcbfxepafd","tag_id":"ck9jwdnhe000pjlrc6067xyo6","_id":"ck9jwdnhv0017jlrcpuhe1ehm"},{"post_id":"ck9jwdng60009jlrclgm4jkxk","tag_id":"ck9jwdnhe000pjlrc6067xyo6","_id":"ck9jwdnhv0019jlrclfknsnv7"},{"post_id":"ck9jwdng7000ajlrckzg7vut2","tag_id":"ck9jwdnhe000pjlrc6067xyo6","_id":"ck9jwdnhv001bjlrcrxwsij7o"},{"post_id":"ck9jwdng9000bjlrcae01whra","tag_id":"ck9jwdnhe000pjlrc6067xyo6","_id":"ck9jwdnhv001djlrcei4c0ku6"},{"post_id":"ck9jwdnmb001ejlrcuz1ek2ga","tag_id":"ck9jwdnhe000pjlrc6067xyo6","_id":"ck9jwdnn0001hjlrcalhwjwbk"},{"post_id":"ck9jwdnmb001ejlrcuz1ek2ga","tag_id":"ck9jwdnmm001gjlrcodx80cmr","_id":"ck9jwdnn1001jjlrcpxrnaqgx"}],"Tag":[{"name":"A","_id":"ck9jwdnh9000djlrcg0y6kj4x"},{"name":"data analysis","_id":"ck9jwdnha000fjlrcl9ysio6f"},{"name":"Linux","_id":"ck9jwdnhb000hjlrcf921d18d"},{"name":"MXnet","_id":"ck9jwdnhb000jjlrcaepavhro"},{"name":"Machine Learning","_id":"ck9jwdnhc000ljlrcz97ycyuo"},{"name":"NBA","_id":"ck9jwdnhe000njlrcuov4rg19"},{"name":"Deep Learning","_id":"ck9jwdnhe000pjlrc6067xyo6"},{"name":"Object Detection","_id":"ck9jwdnmm001gjlrcodx80cmr"}]}}