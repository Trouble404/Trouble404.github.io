{"meta":{"version":1,"warehouse":"2.2.0"},"models":{"Asset":[{"_id":"source/CNAME","path":"CNAME","modified":1,"renderable":0},{"_id":"source/about/resume_cn.pdf","path":"about/resume_cn.pdf","modified":1,"renderable":0},{"_id":"source/about/resume_en.pdf","path":"about/resume_en.pdf","modified":1,"renderable":0},{"_id":"themes/next/source/css/main.styl","path":"css/main.styl","modified":1,"renderable":1},{"_id":"themes/next/source/images/algolia_logo.svg","path":"images/algolia_logo.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc-sa.svg","path":"images/cc-by-nc-sa.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc-nd.svg","path":"images/cc-by-nc-nd.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc.svg","path":"images/cc-by-nc.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by-nd.svg","path":"images/cc-by-nd.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by.svg","path":"images/cc-by.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by-sa.svg","path":"images/cc-by-sa.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-zero.svg","path":"images/cc-zero.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/favicon.ico","path":"images/favicon.ico","modified":1,"renderable":1},{"_id":"themes/next/source/images/loading.gif","path":"images/loading.gif","modified":1,"renderable":1},{"_id":"themes/next/source/images/placeholder.gif","path":"images/placeholder.gif","modified":1,"renderable":1},{"_id":"themes/next/source/images/quote-l.svg","path":"images/quote-l.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/quote-r.svg","path":"images/quote-r.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/searchicon.png","path":"images/searchicon.png","modified":1,"renderable":1},{"_id":"source/uploads/avatar.jpg","path":"uploads/avatar.jpg","modified":1,"renderable":0},{"_id":"themes/next/source/images/alipay-reward-image.jpg","path":"images/alipay-reward-image.jpg","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/affix.js","path":"js/src/affix.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/algolia-search.js","path":"js/src/algolia-search.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/bootstrap.js","path":"js/src/bootstrap.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/exturl.js","path":"js/src/exturl.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/hook-duoshuo.js","path":"js/src/hook-duoshuo.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/js.cookie.js","path":"js/src/js.cookie.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/motion.js","path":"js/src/motion.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/post-details.js","path":"js/src/post-details.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/scrollspy.js","path":"js/src/scrollspy.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/scroll-cookie.js","path":"js/src/scroll-cookie.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/utils.js","path":"js/src/utils.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.css","path":"lib/algolia-instant-search/instantsearch.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/canvas-nest/canvas-nest.min.js","path":"lib/canvas-nest/canvas-nest.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/canvas-ribbon/canvas-ribbon.js","path":"lib/canvas-ribbon/canvas-ribbon.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fastclick/LICENSE","path":"lib/fastclick/LICENSE","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fastclick/README.md","path":"lib/fastclick/README.md","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fastclick/bower.json","path":"lib/fastclick/bower.json","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/HELP-US-OUT.txt","path":"lib/font-awesome/HELP-US-OUT.txt","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/bower.json","path":"lib/font-awesome/bower.json","modified":1,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/CONTRIBUTING.md","path":"lib/jquery_lazyload/CONTRIBUTING.md","modified":1,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/README.md","path":"lib/jquery_lazyload/README.md","modified":1,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/bower.json","path":"lib/jquery_lazyload/bower.json","modified":1,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/jquery.lazyload.js","path":"lib/jquery_lazyload/jquery.lazyload.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/jquery.scrollstop.js","path":"lib/jquery_lazyload/jquery.scrollstop.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-barber-shop.min.css","path":"lib/pace/pace-theme-barber-shop.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-big-counter.min.css","path":"lib/pace/pace-theme-big-counter.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-bounce.min.css","path":"lib/pace/pace-theme-bounce.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-center-atom.min.css","path":"lib/pace/pace-theme-center-atom.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-center-circle.min.css","path":"lib/pace/pace-theme-center-circle.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-center-radar.min.css","path":"lib/pace/pace-theme-center-radar.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-center-simple.min.css","path":"lib/pace/pace-theme-center-simple.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-corner-indicator.min.css","path":"lib/pace/pace-theme-corner-indicator.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-fill-left.min.css","path":"lib/pace/pace-theme-fill-left.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-flash.min.css","path":"lib/pace/pace-theme-flash.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-loading-bar.min.css","path":"lib/pace/pace-theme-loading-bar.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-minimal.min.css","path":"lib/pace/pace-theme-minimal.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-mac-osx.min.css","path":"lib/pace/pace-theme-mac-osx.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace.min.js","path":"lib/pace/pace.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/three/canvas_lines.min.js","path":"lib/three/canvas_lines.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/three/three-waves.min.js","path":"lib/three/three-waves.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/three/canvas_sphere.min.js","path":"lib/three/canvas_sphere.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/velocity/bower.json","path":"lib/velocity/bower.json","modified":1,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.ui.js","path":"lib/velocity/velocity.ui.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.min.js","path":"lib/velocity/velocity.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.ui.min.js","path":"lib/velocity/velocity.ui.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/images/wechat-reward-image.jpg","path":"images/wechat-reward-image.jpg","modified":1,"renderable":1},{"_id":"themes/next/source/lib/jquery/index.js","path":"lib/jquery/index.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/han.css","path":"lib/Han/dist/han.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/han.min.css","path":"lib/Han/dist/han.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/han.min.js","path":"lib/Han/dist/han.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/schemes/pisces.js","path":"js/src/schemes/pisces.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_loading.gif","path":"lib/fancybox/source/fancybox_loading.gif","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/blank.gif","path":"lib/fancybox/source/blank.gif","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_loading@2x.gif","path":"lib/fancybox/source/fancybox_loading@2x.gif","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_overlay.png","path":"lib/fancybox/source/fancybox_overlay.png","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_sprite.png","path":"lib/fancybox/source/fancybox_sprite.png","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_sprite@2x.png","path":"lib/fancybox/source/fancybox_sprite@2x.png","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.css","path":"lib/fancybox/source/jquery.fancybox.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.js","path":"lib/fancybox/source/jquery.fancybox.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.pack.js","path":"lib/fancybox/source/jquery.fancybox.pack.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fastclick/lib/fastclick.js","path":"lib/fastclick/lib/fastclick.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fastclick/lib/fastclick.min.js","path":"lib/fastclick/lib/fastclick.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css","path":"lib/font-awesome/css/font-awesome.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css.map","path":"lib/font-awesome/css/font-awesome.css.map","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.min.css","path":"lib/font-awesome/css/font-awesome.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.min.js","path":"lib/ua-parser-js/dist/ua-parser.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.pack.js","path":"lib/ua-parser-js/dist/ua-parser.pack.js","modified":1,"renderable":1},{"_id":"themes/next/source/uploads/avatar.jpg","path":"uploads/avatar.jpg","modified":1,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/han.js","path":"lib/Han/dist/han.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff","path":"lib/font-awesome/fonts/fontawesome-webfont.woff","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff2","path":"lib/font-awesome/fonts/fontawesome-webfont.woff2","modified":1,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.js","path":"lib/velocity/velocity.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/font/han-space.otf","path":"lib/Han/dist/font/han-space.otf","modified":1,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/font/han-space.woff","path":"lib/Han/dist/font/han-space.woff","modified":1,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/font/han.otf","path":"lib/Han/dist/font/han.otf","modified":1,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/font/han.woff","path":"lib/Han/dist/font/han.woff","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-buttons.css","path":"lib/fancybox/source/helpers/jquery.fancybox-buttons.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/fancybox_buttons.png","path":"lib/fancybox/source/helpers/fancybox_buttons.png","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-buttons.js","path":"lib/fancybox/source/helpers/jquery.fancybox-buttons.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-media.js","path":"lib/fancybox/source/helpers/jquery.fancybox-media.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-thumbs.css","path":"lib/fancybox/source/helpers/jquery.fancybox-thumbs.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-thumbs.js","path":"lib/fancybox/source/helpers/jquery.fancybox-thumbs.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/FontAwesome.otf","path":"lib/font-awesome/fonts/FontAwesome.otf","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.eot","path":"lib/font-awesome/fonts/fontawesome-webfont.eot","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.ttf","path":"lib/font-awesome/fonts/fontawesome-webfont.ttf","modified":1,"renderable":1},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.js","path":"lib/algolia-instant-search/instantsearch.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/three/three.min.js","path":"lib/three/three.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.svg","path":"lib/font-awesome/fonts/fontawesome-webfont.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/avatar.gif","path":"images/avatar.gif","modified":1,"renderable":1}],"Cache":[{"_id":"source/CNAME","hash":"ab075e42f34723bce8bab60d8b6da686e2c40faf","modified":1588077535395},{"_id":"themes/next/LICENSE","hash":"ec44503d7e617144909e54533754f0147845f0c5","modified":1588077535403},{"_id":"themes/next/README.en.md","hash":"fd7a00ae9026fb4f87dd7eed9ce049d0db447140","modified":1588077535403},{"_id":"themes/next/README.md","hash":"06aaf1241e9e1619956c86d8b1397a643840a9d1","modified":1588077535403},{"_id":"themes/next/_config.yml","hash":"fac953669b7b971521ea844872323176b9c5a314","modified":1588077535403},{"_id":"themes/next/bower.json","hash":"63c38f50fb54b25bf5101f566189f9e5b3a6ef0e","modified":1588077535403},{"_id":"themes/next/gulpfile.coffee","hash":"412defab3d93d404b7c26aaa0279e2e586e97454","modified":1588077535403},{"_id":"themes/next/package.json","hash":"85a77bafb3d1e958b82e52528b7a95fcd59efda9","modified":1588077535407},{"_id":"source/_posts/Apri.md","hash":"e46a2ac1405e03c0b0d787209dac231c41999ba2","modified":1588077535395},{"_id":"source/_posts/IT salary in USA.md","hash":"3f8d0ab495504c4f653af17315725ed4245f52b9","modified":1588077535395},{"_id":"source/_posts/LInux.md","hash":"4fa5d3d0d1ea3313f0c8b77a8b9c6be4472e7e89","modified":1588077535395},{"_id":"source/_posts/MXnet_Config.md","hash":"f38c40598865394833052d8c16f17c83cc36e0b1","modified":1588077535395},{"_id":"source/_posts/Machine Learning in NBA.md","hash":"81f70ba31ae801b420f2df13adb2ebb7cb836edf","modified":1588077535395},{"_id":"source/_posts/MalongTEch-Learning-Semantic-Soft-Segmentation.md","hash":"657334901cdc361ad6572b66ad99ff92cf6f2bac","modified":1588077535395},{"_id":"source/_posts/MalongTech-Learning-3D-Image-Segmentation.md","hash":"25f0f6b65a92dc0e2c930c73a24218a3782d3df6","modified":1588077535395},{"_id":"source/_posts/MalongTech-Learning-Image-Segmentation.md","hash":"c6af03a0cc401ed7843b9dd1be51bf2d5c876800","modified":1588077535395},{"_id":"source/_posts/MalongTech-Learning-Object-Detection.md","hash":"fbc943b5b82a8ee28c933a628c59b15031886a46","modified":1588077535395},{"_id":"source/about/index.md","hash":"04a7b4058187f12ffeffcc55f78492c489ed1f0c","modified":1588077535395},{"_id":"source/about/resume_cn.pdf","hash":"87a665f33c05dfe969edf2001c4474a3f9ee5195","modified":1588077535395},{"_id":"source/about/resume_en.pdf","hash":"10b6e617530d73b17a81c9f855d1656adb3673f1","modified":1588077535395},{"_id":"source/categories/index.md","hash":"36e51eebd296314d61d0461e591e4bd96a0f42eb","modified":1588077535395},{"_id":"source/tags/index.md","hash":"5bd05991aa1304e46cb2e7fd82e953747c177986","modified":1588077535395},{"_id":"themes/next/languages/de.yml","hash":"4be3e7d296d5592e0d111dfa6cbbff02602c972d","modified":1588077535403},{"_id":"themes/next/languages/default.yml","hash":"d912814caac150da1611c96843371a87714e52f9","modified":1588077535403},{"_id":"themes/next/languages/en.yml","hash":"b3ee45143bc014578db6b8ac0573f7c7b143a743","modified":1588077535403},{"_id":"themes/next/languages/fr-FR.yml","hash":"0d5bd8bbbeafb72506124ed35e7509debc753612","modified":1588077535403},{"_id":"themes/next/languages/id.yml","hash":"c0848e93bf33a1333ff232905b6b392b1e056dd1","modified":1588077535403},{"_id":"themes/next/languages/ja.yml","hash":"1a608dc799c0f9c36b626bac6fe3404acb45b86d","modified":1588077535403},{"_id":"themes/next/languages/ko.yml","hash":"5c811514aef401317a9ec38b95679d6d2ef0ad42","modified":1588077535403},{"_id":"themes/next/languages/pt-BR.yml","hash":"cc8b5a67ec87b0d5aec6e253bab67ec3cfe3069c","modified":1588077535403},{"_id":"themes/next/languages/pt.yml","hash":"943475a7d681f37ede579cd62da9c50568ca0f8d","modified":1588077535403},{"_id":"themes/next/languages/zh-Hans.yml","hash":"3111ce4cc5f30868b3628f9f805d2aef3b75d1c1","modified":1588077535403},{"_id":"themes/next/languages/zh-hk.yml","hash":"b58c0d85daa4d62b0c9753a59de0739aa0120735","modified":1588077535403},{"_id":"themes/next/languages/ru.yml","hash":"84d41a111e497236b2c1fa16e9b91668a1f37037","modified":1588077535403},{"_id":"themes/next/languages/zh-tw.yml","hash":"8ce0a32411de111ae39d08e4bc936767dacdeb08","modified":1588077535403},{"_id":"themes/next/layout/_layout.swig","hash":"b88585f9e1b7071f6670b20b77b656edd087ccc9","modified":1588077535407},{"_id":"themes/next/layout/archive.swig","hash":"c2be7c95af6205c7501a261f2fc9702c57107f89","modified":1588077535407},{"_id":"themes/next/layout/category.swig","hash":"3cbb3f72429647411f9e85f2544bdf0e3ad2e6b2","modified":1588077535407},{"_id":"themes/next/layout/post.swig","hash":"182a99b1f6db0350106c6bb480fede0bbdb7e40f","modified":1588077535407},{"_id":"themes/next/layout/index.swig","hash":"4bf29f44ca9519a005671f2f2a79a48a148b435b","modified":1588077535407},{"_id":"themes/next/layout/schedule.swig","hash":"87ad6055df01fa2e63e51887d34a2d8f0fbd2f5a","modified":1588077535407},{"_id":"themes/next/layout/tag.swig","hash":"34e1c016cbdf94a31f9c5d494854ff46b2a182e9","modified":1588077535407},{"_id":"themes/next/layout/page.swig","hash":"dbff0302b4bfabb51556a197bf65190eb30361f0","modified":1588077535407},{"_id":"themes/next/scripts/merge-configs.js","hash":"3ce1be32bb77ee19da25e8dae7dc04e2afc46ca1","modified":1588077535407},{"_id":"themes/next/scripts/merge.js","hash":"39b84b937b2a9608b94e5872349a47200e1800ff","modified":1588077535407},{"_id":"themes/next/test/helpers.js","hash":"f25e7f3265eb5a6e1ccbb5e5012fa9bebf134105","modified":1588077535427},{"_id":"themes/next/test/intern.js","hash":"db90b1063356727d72be0d77054fdc32fa882a66","modified":1588077535427},{"_id":"themes/next/test/.jshintrc","hash":"c9fca43ae0d99718e45a6f5ce736a18ba5fc8fb6","modified":1588077535427},{"_id":"themes/next/source/fonts/.gitkeep","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1588077535411},{"_id":"source/_posts/LogBook.md","hash":"e54572cc145d33a4f633564f26bc3f86cffb249d","modified":1588077535395},{"_id":"themes/next/layout/_custom/header.swig","hash":"ba8ab5a0280b953aa97435ff8946cbcbb2755a27","modified":1588077535407},{"_id":"themes/next/layout/_custom/sidebar.swig","hash":"ba8ab5a0280b953aa97435ff8946cbcbb2755a27","modified":1588077535407},{"_id":"themes/next/layout/_macro/post-copyright.swig","hash":"0f59a51b5cea3e8a7c078db486626cddc2978622","modified":1588077535407},{"_id":"themes/next/layout/_macro/post-collapse.swig","hash":"8c56dd26157cbc580ae41d97ac34b90ab48ced3f","modified":1588077535407},{"_id":"themes/next/layout/_macro/reward.swig","hash":"5b1e91c2f6f88fbecd426cd0727e7b7854c6cc1d","modified":1588077535407},{"_id":"themes/next/layout/_macro/wechat-subscriber.swig","hash":"e2e4eae391476da994045ed4c7faf5e05aca2cd7","modified":1588077535407},{"_id":"themes/next/layout/_partials/comments.swig","hash":"e748c83209ca737be156f8179807d12bc652c1f2","modified":1588077535407},{"_id":"themes/next/layout/_macro/sidebar.swig","hash":"f10ca698e8ea0c31ff72a6cffa832c3cd703a133","modified":1588077535407},{"_id":"themes/next/layout/_partials/footer.swig","hash":"683616f4a80796051e2346d80acf838a589450b0","modified":1588077535407},{"_id":"themes/next/layout/_macro/post.swig","hash":"b6b86a199f5a3692ec492123fe8cb53a8ca08169","modified":1588077535407},{"_id":"themes/next/layout/_partials/page-header.swig","hash":"77c61e0baea3544df361b7338c3cd13dc84dde22","modified":1588077535407},{"_id":"themes/next/layout/_partials/pagination.swig","hash":"1634fb887842698e01ff6e632597fe03c75d2d01","modified":1588077535407},{"_id":"themes/next/layout/_partials/head.swig","hash":"476e6c2452732c2741f518004d336bc348e710d7","modified":1588077535407},{"_id":"themes/next/layout/_partials/search.swig","hash":"b4ebe4a52a3b51efe549dd1cdee846103664f5eb","modified":1588077535407},{"_id":"themes/next/layout/_scripts/boostrap.swig","hash":"c0f5a0955f69ca4ed9ee64a2d5f8aa75064935ad","modified":1588077535407},{"_id":"themes/next/layout/_scripts/commons.swig","hash":"931808ad9b8d8390c0dcf9bdeb0954eeb9185d68","modified":1588077535407},{"_id":"themes/next/layout/_scripts/vendors.swig","hash":"53c894e6f3573c662dc4e4f7b5a6f1a32f1a8c94","modified":1588077535407},{"_id":"themes/next/layout/_partials/header.swig","hash":"d6bf1d1554d91eaf1bfc40ba8905ae81673e5f45","modified":1588077535407},{"_id":"themes/next/layout/_third-party/duoshuo-hot-articles.swig","hash":"ba75672183d94f1de7c8bd0eeee497a58c70e889","modified":1588077535407},{"_id":"themes/next/layout/_third-party/exturl.swig","hash":"8301c9600bb3e47f7fb98b0e0332ef3c51bb1688","modified":1588077535407},{"_id":"themes/next/layout/_third-party/mathjax.swig","hash":"a0bd3388587fd943baae0d84ca779a707fbcad89","modified":1588077535407},{"_id":"themes/next/layout/_third-party/rating.swig","hash":"554ec568e9d2c71e4a624a8de3cb5929050811d6","modified":1588077535407},{"_id":"themes/next/layout/_third-party/schedule.swig","hash":"db15d7e1552aa2d2386a6b8a33b3b3a40bf9e43d","modified":1588077535407},{"_id":"themes/next/layout/_third-party/scroll-cookie.swig","hash":"9a188938d46931d5f3882a140aa1c48b3a893f0c","modified":1588077535407},{"_id":"themes/next/scripts/tags/button.js","hash":"aaf71be6b483fca7a65cd6296c2cf1c2271c26a6","modified":1588077535407},{"_id":"themes/next/scripts/tags/center-quote.js","hash":"99b66949f18398689b904907af23c013be1b978f","modified":1588077535407},{"_id":"themes/next/scripts/tags/full-image.js","hash":"c9f833158c66bd72f627a0559cf96550e867aa72","modified":1588077535407},{"_id":"themes/next/scripts/tags/exturl.js","hash":"5022c0ba9f1d13192677cf1fd66005c57c3d0f53","modified":1588077535407},{"_id":"themes/next/scripts/tags/group-pictures.js","hash":"ac681b0d0d8d39ba3817336c0270c6787c2b6b70","modified":1588077535407},{"_id":"themes/next/scripts/tags/label.js","hash":"6f00952d70aadece844ce7fd27adc52816cc7374","modified":1588077535407},{"_id":"themes/next/scripts/tags/lazy-image.js","hash":"bcba2ff25cd7850ce6da322d8bd85a8dd00b5ceb","modified":1588077535407},{"_id":"themes/next/scripts/tags/tabs.js","hash":"aa7fc94a5ec27737458d9fe1a75c0db7593352fd","modified":1588077535407},{"_id":"themes/next/scripts/tags/note.js","hash":"f7eae135f35cdab23728e9d0d88b76e00715faa0","modified":1588077535407},{"_id":"themes/next/source/css/main.styl","hash":"a91dbb7ef799f0a171b5e726c801139efe545176","modified":1588077535411},{"_id":"themes/next/source/images/algolia_logo.svg","hash":"45eeea0b5fba833e21e38ea10ed5ab385ceb4f01","modified":1588077535411},{"_id":"themes/next/source/images/cc-by-nc-sa.svg","hash":"6f55543d1fb9cbc436c101d24f802dec7b41efc3","modified":1588077535419},{"_id":"themes/next/source/images/cc-by-nc-nd.svg","hash":"bc3588c9b2d7c68830524783120ff6cf957cf668","modified":1588077535419},{"_id":"themes/next/source/images/cc-by-nc.svg","hash":"6f076713fb9bf934aa2c1046bdf2cf2e37bc1eab","modified":1588077535419},{"_id":"themes/next/source/images/cc-by-nd.svg","hash":"42cd73da328077ccc92f859bb8f3cf621b3484f8","modified":1588077535419},{"_id":"themes/next/source/images/cc-by.svg","hash":"e92a33c32d1dac8ed94849b2b4e6456e887efe70","modified":1588077535419},{"_id":"themes/next/source/images/cc-by-sa.svg","hash":"70c1535f43e54e5ff35ca81419e77e4c0c301398","modified":1588077535419},{"_id":"themes/next/source/images/cc-zero.svg","hash":"9bfb52b2f63527a7049247bf00d44e6dc1170e7d","modified":1588077535419},{"_id":"themes/next/source/images/favicon.ico","hash":"17c483c2915dbd1c84cc42d124a7199bdc7ade1f","modified":1588077535419},{"_id":"themes/next/source/images/loading.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1588077535419},{"_id":"themes/next/source/images/placeholder.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1588077535419},{"_id":"themes/next/source/images/quote-l.svg","hash":"cd108d6f44351cadf8e6742565217f88818a0458","modified":1588077535419},{"_id":"themes/next/source/images/quote-r.svg","hash":"2a2a250b32a87c69dcc1b1976c74b747bedbfb41","modified":1588077535419},{"_id":"themes/next/source/images/searchicon.png","hash":"67727a6a969be0b2659b908518fa6706eed307b8","modified":1588077535419},{"_id":"themes/next/layout/_scripts/schemes/muse.swig","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1588077535407},{"_id":"themes/next/layout/_scripts/schemes/mist.swig","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1588077535407},{"_id":"themes/next/source/css/_mixins/Mist.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1588077535411},{"_id":"themes/next/source/css/_mixins/Muse.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1588077535411},{"_id":"themes/next/source/css/_mixins/custom.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1588077535411},{"_id":"themes/next/source/css/_variables/Muse.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1588077535411},{"_id":"themes/next/source/css/_variables/custom.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1588077535411},{"_id":"themes/next/source/images/alipay-reward-image.jpg","hash":"ee38a3c5ac61f24467a668ba5e7246053b7b1b8c","modified":1588077535411},{"_id":"source/uploads/avatar.jpg","hash":"e3e7311f9624008234681ff57e411471dd80f379","modified":1588077535395},{"_id":"themes/next/layout/_partials/head/external-fonts.swig","hash":"f5e487b0d213ca0bd94aa30bc23b240d65081627","modified":1588077535407},{"_id":"themes/next/layout/_partials/head/custom-head.swig","hash":"a223919d2e1bf17ca4d6abb2c86f2efca9883dc1","modified":1588077535407},{"_id":"themes/next/layout/_partials/search/swiftype.swig","hash":"a8c7f9ca7c605d039a1f3bf4e4d3183700a3dd62","modified":1588077535407},{"_id":"themes/next/layout/_partials/search/tinysou.swig","hash":"b25002a83cbd2ca0c4a5df87ad5bff26477c0457","modified":1588077535407},{"_id":"themes/next/layout/_partials/search/localsearch.swig","hash":"b2f0d247b213e4cf8de47af6a304d98070cc7256","modified":1588077535407},{"_id":"themes/next/layout/_partials/share/add-this.swig","hash":"9e3d133ac5bcc6cb51702c83b2611a49811abad1","modified":1588077535407},{"_id":"themes/next/layout/_partials/share/baidushare.swig","hash":"5a4f01331d79c6f0eac119e92e2861ff61f1b35a","modified":1588077535407},{"_id":"themes/next/layout/_partials/share/jiathis.swig","hash":"12684840de632eb16e53ffa863166306a756fd4f","modified":1588077535407},{"_id":"themes/next/layout/_partials/share/duoshuo_share.swig","hash":"d4fbffd7fa8f2090eb32a871872665d90a885fac","modified":1588077535407},{"_id":"themes/next/layout/_scripts/schemes/gemini.swig","hash":"a9a3995b9615adfb8d6b127c78c6771627bee19a","modified":1588077535407},{"_id":"themes/next/layout/_scripts/schemes/pisces.swig","hash":"a9a3995b9615adfb8d6b127c78c6771627bee19a","modified":1588077535407},{"_id":"themes/next/layout/_scripts/pages/post-details.swig","hash":"9b84ab576982b2c3bb0291da49143bc77fba3cc6","modified":1588077535407},{"_id":"themes/next/layout/_third-party/analytics/application-insights.swig","hash":"71397a5823e8ec8aad3b68aace13150623b3e19d","modified":1588077535407},{"_id":"themes/next/layout/_third-party/analytics/busuanzi-counter.swig","hash":"16cb23818909f57dac1a5ada66869971c33d7bd8","modified":1588077535407},{"_id":"themes/next/layout/_third-party/analytics/baidu-analytics.swig","hash":"753d262911c27baf663fcaf199267133528656af","modified":1588077535407},{"_id":"themes/next/layout/_third-party/analytics/cnzz-analytics.swig","hash":"a10b7f19d7b5725527514622899df413a34a89db","modified":1588077535407},{"_id":"themes/next/layout/_third-party/analytics/facebook-sdk.swig","hash":"7d94845f96197d9d84a405fa5d4ede75fb81b225","modified":1588077535407},{"_id":"themes/next/layout/_third-party/analytics/google-analytics.swig","hash":"b1e13df83fb2b1d5d513b30b7aa6158b0837daab","modified":1588077535407},{"_id":"themes/next/layout/_third-party/analytics/index.swig","hash":"441f1a1b4e2f652d3b975995bd9d44ff4866f057","modified":1588077535407},{"_id":"themes/next/layout/_third-party/analytics/tencent-mta.swig","hash":"5a8027328f060f965b3014060bebec1d7cf149c1","modified":1588077535407},{"_id":"themes/next/layout/_third-party/analytics/tencent-analytics.swig","hash":"8a399df90dadba5ad4e781445b58f4765aeb701e","modified":1588077535407},{"_id":"themes/next/layout/_third-party/analytics/lean-analytics.swig","hash":"e6d10ee4fb70b3ae1cd37e9e36e000306734aa2e","modified":1588077535407},{"_id":"themes/next/layout/_third-party/comments/changyan.swig","hash":"4c501ea0b9c494181eb3c607c5526a5754e7fbd8","modified":1588077535407},{"_id":"themes/next/layout/_third-party/comments/disqus.swig","hash":"b83a51bbe0f1e2ded9819070840b0ea145f003a6","modified":1588077535407},{"_id":"themes/next/layout/_third-party/analytics/vkontakte-api.swig","hash":"f9a1647a8f1866deeb94052d1f87a5df99cb1e70","modified":1588077535407},{"_id":"themes/next/layout/_third-party/comments/duoshuo.swig","hash":"1600f340e0225361580c44890568dc07dbcf2c89","modified":1588077535407},{"_id":"themes/next/layout/_third-party/comments/hypercomments.swig","hash":"af7f3e43cbdc4f88c13f101f0f341af96ace3383","modified":1588077535407},{"_id":"themes/next/layout/_third-party/comments/livere.swig","hash":"9246162d4bc7e949ce1d12d135cbbaf5dc3024ec","modified":1588077535407},{"_id":"themes/next/layout/_third-party/comments/index.swig","hash":"be2aaeb8f05979e2ba501248480d5294256d61f2","modified":1588077535407},{"_id":"themes/next/layout/_third-party/comments/youyan.swig","hash":"7e65ff8fe586cd655b0e9d1ad2912663ff9bd36c","modified":1588077535407},{"_id":"themes/next/layout/_third-party/search/index.swig","hash":"34599633658f3b0ffb487728b7766e1c7b551f5a","modified":1588077535407},{"_id":"themes/next/layout/_third-party/search/localsearch.swig","hash":"93479642fd076a1257fecc25fcf5d20ccdefe509","modified":1588077535407},{"_id":"themes/next/layout/_third-party/seo/baidu-push.swig","hash":"d8c98938719284fa06492c114d99a1904652a555","modified":1588077535407},{"_id":"themes/next/layout/_third-party/search/tinysou.swig","hash":"fe95dd3d166634c466e19aa756e65ad6e8254d3e","modified":1588077535407},{"_id":"themes/next/source/css/_custom/custom.styl","hash":"3403fdd8efde1a0afd11ae8a5a97673f5903087f","modified":1588077535411},{"_id":"themes/next/source/css/_mixins/Gemini.styl","hash":"07f7da320689f828f6e36a6123807964a45157a0","modified":1588077535411},{"_id":"themes/next/source/css/_mixins/Pisces.styl","hash":"bf009e85212749405c27d89b49f401911355ecc7","modified":1588077535411},{"_id":"themes/next/source/css/_mixins/base.styl","hash":"0e55cbd93852dc3f8ccb44df74d35d9918f847e0","modified":1588077535411},{"_id":"themes/next/source/css/_variables/Gemini.styl","hash":"2a4e844dec690365774c2f6e8984706fee39ea63","modified":1588077535411},{"_id":"themes/next/source/css/_variables/Mist.styl","hash":"e55265c8a8a6ae0c3c08e3509de92ee62c3cb5f6","modified":1588077535411},{"_id":"themes/next/source/css/_variables/base.styl","hash":"a627633d3bb70b8501572b18037def478beb7017","modified":1588077535411},{"_id":"themes/next/source/css/_variables/Pisces.styl","hash":"89f88b9c9a191dd980f799fc36b83b63290d3ac9","modified":1588077535411},{"_id":"themes/next/source/js/src/affix.js","hash":"1b509c3b5b290a6f4607f0f06461a0c33acb69b1","modified":1588077535419},{"_id":"themes/next/source/js/src/algolia-search.js","hash":"cb431b54ba9c692165a1f5a12e4c564a560f8058","modified":1588077535419},{"_id":"themes/next/source/js/src/bootstrap.js","hash":"96c8b5fe1999de1b3a46730d9812787dfcd65884","modified":1588077535419},{"_id":"themes/next/source/js/src/exturl.js","hash":"a2a0f0de07e46211f74942a468f42ee270aa555c","modified":1588077535419},{"_id":"themes/next/source/js/src/hook-duoshuo.js","hash":"b35a7dc47b634197b93487cea8671a40a9fdffce","modified":1588077535419},{"_id":"themes/next/source/js/src/js.cookie.js","hash":"1512c751d219577d338ac0780fb2bbd9075d5298","modified":1588077535419},{"_id":"themes/next/source/js/src/motion.js","hash":"dda8c76fce91d7f140c06de2583ba806810f12c2","modified":1588077535419},{"_id":"themes/next/source/js/src/post-details.js","hash":"50fa390554f0fb467d8eb84ac8eff2cffb13fe67","modified":1588077535419},{"_id":"themes/next/source/js/src/scrollspy.js","hash":"b7657be25fc52ec67c75ab5481bdcb483573338b","modified":1588077535419},{"_id":"themes/next/source/js/src/scroll-cookie.js","hash":"02cf91514e41200bc9df5d8bdbeb58575ec06074","modified":1588077535419},{"_id":"themes/next/source/js/src/utils.js","hash":"f90c7611dc665b5e321cb81c0bd689445bab438a","modified":1588077535419},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.css","hash":"90ef19edc982645b118b095615838d9c5eaba0de","modified":1588077535419},{"_id":"themes/next/source/lib/canvas-nest/canvas-nest.min.js","hash":"0387e75e23b1db108a755073fe52a0d03eb391a7","modified":1588077535419},{"_id":"themes/next/source/lib/canvas-ribbon/canvas-ribbon.js","hash":"82fee688910efc644d3d1c3305c6ae28ba3f38f9","modified":1588077535419},{"_id":"themes/next/source/lib/fancybox/.bower.json","hash":"cc40a9b11e52348e554c84e4a5c058056f6b7aeb","modified":1588077535419},{"_id":"themes/next/source/lib/fancybox/.gitattributes","hash":"2db21acfbd457452462f71cc4048a943ee61b8e0","modified":1588077535419},{"_id":"themes/next/source/lib/jquery/.bower.json","hash":"865d6c1328ab209a4376b9d2b7a7824369565f28","modified":1588077535423},{"_id":"themes/next/source/lib/fastclick/LICENSE","hash":"6f474ea75c42442da7bbcf2e9143ce98258efd8d","modified":1588077535419},{"_id":"themes/next/source/lib/fastclick/.bower.json","hash":"bf3eef9d647cd7c9b62feda3bc708c6cdd7c0877","modified":1588077535419},{"_id":"themes/next/source/lib/fastclick/README.md","hash":"68a9b9d53126405b0fa5f3324f1fb96dbcc547aa","modified":1588077535419},{"_id":"themes/next/source/lib/fastclick/bower.json","hash":"a9b3ee1e4db71a0e4ea6d5bed292d176dd68b261","modified":1588077535419},{"_id":"themes/next/source/lib/font-awesome/.bower.json","hash":"b4aefc910578d76b267e86dfffdd5121c8db9aec","modified":1588077535419},{"_id":"themes/next/source/lib/font-awesome/.gitignore","hash":"03ddbf76c1dd1afb93eed0b670d2eee747472ef1","modified":1588077535419},{"_id":"themes/next/source/lib/font-awesome/.npmignore","hash":"c31ff06a740955e44edd4403902e653ccabfd4db","modified":1588077535419},{"_id":"themes/next/source/lib/font-awesome/HELP-US-OUT.txt","hash":"ee33b2798b1e714b904d663436c6b3521011d1fa","modified":1588077535419},{"_id":"themes/next/source/lib/font-awesome/bower.json","hash":"71e7183634dc1b9449f590f15ebd7201add22ca7","modified":1588077535419},{"_id":"themes/next/source/lib/jquery_lazyload/.bower.json","hash":"90fa628f156d8045357ff11eaf32e61abacf10e8","modified":1588077535423},{"_id":"themes/next/source/lib/jquery_lazyload/CONTRIBUTING.md","hash":"4ded6fee668544778e97e38c2b211fc56c848e77","modified":1588077535423},{"_id":"themes/next/source/lib/jquery_lazyload/README.md","hash":"b930297cb98b8e1dbd5abe9bc1ed9d5935d18ce8","modified":1588077535423},{"_id":"themes/next/source/lib/jquery_lazyload/bower.json","hash":"e0acf1db27b0cc16128a59c46db1db406b5c4c58","modified":1588077535423},{"_id":"themes/next/source/lib/jquery_lazyload/jquery.lazyload.js","hash":"f4a570908f6c89c6edfb1c74959e733eaadea4f2","modified":1588077535423},{"_id":"themes/next/source/lib/jquery_lazyload/jquery.scrollstop.js","hash":"bf773ad48a0b9aa77681a89d7569eefc0f7b7b18","modified":1588077535423},{"_id":"themes/next/source/lib/pace/pace-theme-barber-shop.min.css","hash":"ee0d51446cb4ffe1bb96bd7bc8c8e046dddfcf46","modified":1588077535423},{"_id":"themes/next/source/lib/pace/pace-theme-big-counter.min.css","hash":"5b561dc328af4c4d512e20a76fe964d113a32ba8","modified":1588077535423},{"_id":"themes/next/source/lib/pace/pace-theme-bounce.min.css","hash":"f6bdb9a785b7979dd8ec5c60e278af955ef1e585","modified":1588077535423},{"_id":"themes/next/source/lib/pace/pace-theme-center-atom.min.css","hash":"dcf79c24fe5350fb73d8038573a104e73639e9d3","modified":1588077535423},{"_id":"themes/next/source/lib/pace/pace-theme-center-circle.min.css","hash":"a4066769c78affbfbc5e30a600e2c7862cd532e0","modified":1588077535423},{"_id":"themes/next/source/lib/pace/pace-theme-center-radar.min.css","hash":"ab7cba998bf4c03b13df342bf43647fa4f419783","modified":1588077535423},{"_id":"themes/next/source/lib/pace/pace-theme-center-simple.min.css","hash":"67f44c947548bd4d77e7590d3f59e236cbf9e98a","modified":1588077535423},{"_id":"themes/next/source/lib/pace/pace-theme-corner-indicator.min.css","hash":"b3c64c973f31884e3d8145989476707333406b9a","modified":1588077535423},{"_id":"themes/next/source/lib/pace/pace-theme-fill-left.min.css","hash":"0bec1e235a4a2cccda3f993b205424e1441a44ae","modified":1588077535423},{"_id":"themes/next/source/lib/pace/pace-theme-flash.min.css","hash":"13ace22c40312d7bbd8d9c1e50eff897a7a497d8","modified":1588077535423},{"_id":"themes/next/source/lib/pace/pace-theme-loading-bar.min.css","hash":"7ee28875dfc1230d76c537f6605766e8d4011e9f","modified":1588077535423},{"_id":"themes/next/source/lib/pace/pace-theme-minimal.min.css","hash":"9cd783cceb8a191f3c8b5d81f7a430ecc3e489d3","modified":1588077535423},{"_id":"themes/next/source/lib/pace/pace-theme-mac-osx.min.css","hash":"9f2e7b51b084da407863826b25265b31150b3821","modified":1588077535423},{"_id":"themes/next/source/lib/pace/pace.min.js","hash":"8aaa675f577d5501f5f22d5ccb07c2b76310b690","modified":1588077535423},{"_id":"themes/next/source/lib/three/canvas_lines.min.js","hash":"2d9a9f38c493fdf7c0b833bb9184b6a1645c11b2","modified":1588077535423},{"_id":"themes/next/source/lib/three/three-waves.min.js","hash":"8148492dd49aa876d32bb7d5b728d3f5bf6f5074","modified":1588077535423},{"_id":"themes/next/source/lib/three/canvas_sphere.min.js","hash":"46a50b91c98b639c9a2b9265c5a1e66a5c656881","modified":1588077535423},{"_id":"themes/next/source/lib/velocity/.bower.json","hash":"63da5e80ebb61bb66a2794d5936315ca44231f0c","modified":1588077535423},{"_id":"themes/next/source/lib/velocity/bower.json","hash":"92d92860418c4216aa59eb4cb4a556290a7ad9c3","modified":1588077535423},{"_id":"themes/next/source/lib/velocity/velocity.ui.js","hash":"dbbfb50f6502f6b81dcc9fee7b31f1e812da3464","modified":1588077535423},{"_id":"themes/next/source/lib/velocity/velocity.min.js","hash":"bf172816a9c57f9040e3d19c24e181a142daf92b","modified":1588077535423},{"_id":"themes/next/source/lib/velocity/velocity.ui.min.js","hash":"dde584994ac13dc601836e86f4cf490e418d9723","modified":1588077535423},{"_id":"themes/next/source/images/wechat-reward-image.jpg","hash":"a81a23bcba5d49564c5afe2210957a86a8175431","modified":1588077535419},{"_id":"themes/next/source/lib/jquery/index.js","hash":"17a740d68a1c330876c198b6a4d9319f379f3af2","modified":1588077535423},{"_id":"themes/next/layout/_third-party/search/algolia-search/assets.swig","hash":"218cc936ba3518a3591b2c9eda46bc701edf7710","modified":1588077535407},{"_id":"themes/next/layout/_third-party/search/algolia-search/dom.swig","hash":"2530de0f3125a912756f6c0e9090cd012134a4c5","modified":1588077535407},{"_id":"themes/next/source/css/_schemes/Muse/_layout.styl","hash":"31127dcbf4c7b4ada53ffbf1638b5fe325b7cbc0","modified":1588077535411},{"_id":"themes/next/source/css/_schemes/Muse/_menu.styl","hash":"f23ac53ab901c48859dd29eee6e386b60ff956ba","modified":1588077535411},{"_id":"themes/next/source/css/_schemes/Muse/_logo.styl","hash":"748dbfbf9c08e719ddc775958003c64b00d39dab","modified":1588077535411},{"_id":"themes/next/source/css/_schemes/Muse/_search.styl","hash":"09c965022c13b84ed8a661fee8ac2a6d550495ae","modified":1588077535411},{"_id":"themes/next/source/css/_schemes/Mist/_base.styl","hash":"25d5e45a355ee2093f3b8b8eeac125ebf3905026","modified":1588077535411},{"_id":"themes/next/source/css/_schemes/Muse/index.styl","hash":"5dbc0d0c897e46760e5dbee416530d485c747bba","modified":1588077535411},{"_id":"themes/next/source/css/_schemes/Mist/_logo.styl","hash":"b1025c421406d2c24cc92a02ae28c1915b01e240","modified":1588077535411},{"_id":"themes/next/source/css/_schemes/Mist/_header.styl","hash":"d0bfd1bef988c76f7d7dd72d88af6f0908a8b0db","modified":1588077535411},{"_id":"themes/next/source/css/_schemes/Mist/_menu.styl","hash":"26666c1f472bf5f3fb9bc62081cca22b4de15ccb","modified":1588077535411},{"_id":"themes/next/source/css/_schemes/Mist/_posts-expanded.styl","hash":"ce272226a1570f5f7c70243b751a5b0fe1671a88","modified":1588077535411},{"_id":"themes/next/source/css/_schemes/Mist/_search.styl","hash":"09c965022c13b84ed8a661fee8ac2a6d550495ae","modified":1588077535411},{"_id":"themes/next/source/css/_schemes/Mist/index.styl","hash":"9b913b73d31d21f057f97115ffab93cfa578b884","modified":1588077535411},{"_id":"themes/next/source/css/_schemes/Gemini/index.styl","hash":"a509016ac0227a1903d7f0ca3a825cf9ac7fde33","modified":1588077535411},{"_id":"themes/next/source/css/_common/outline/outline.styl","hash":"12662536c7a07fff548abe94171f34b768dd610f","modified":1588077535407},{"_id":"themes/next/source/css/_schemes/Pisces/_brand.styl","hash":"bce344d3a665b4c55230d2a91eac2ad16d6f32fd","modified":1588077535411},{"_id":"themes/next/source/css/_schemes/Pisces/_layout.styl","hash":"416988dca389e6e2fdfa51fa7f4ee07eb53f82fb","modified":1588077535411},{"_id":"themes/next/source/css/_schemes/Pisces/_menu.styl","hash":"4642e30010af8b2b037f5b43146b10a934941958","modified":1588077535411},{"_id":"themes/next/source/css/_schemes/Pisces/_posts.styl","hash":"1f6e2ce674735269599acc6d77b3ea18d31967fc","modified":1588077535411},{"_id":"themes/next/source/css/_schemes/Pisces/_sidebar.styl","hash":"c48d4a561d047b3705924949b3ab7b57bee94ecd","modified":1588077535411},{"_id":"themes/next/source/css/_schemes/Pisces/index.styl","hash":"86197902dfd3bededba10ba62b8f9f22e0420bde","modified":1588077535411},{"_id":"themes/next/source/css/_common/components/back-to-top-sidebar.styl","hash":"b0e2a0e27a32f72cb283fe4b33d010d485113379","modified":1588077535407},{"_id":"themes/next/source/css/_common/components/back-to-top.styl","hash":"237d185ac62ec9877e300947fa0109c44fb8db19","modified":1588077535407},{"_id":"themes/next/source/css/_common/components/buttons.styl","hash":"22828f5141c0cecb9ef25a110e194cdfa3a36423","modified":1588077535407},{"_id":"themes/next/source/css/_common/components/comments.styl","hash":"ff4489cd582f518bba6909a301ac1292a38b4e96","modified":1588077535407},{"_id":"themes/next/source/css/_common/components/components.styl","hash":"7ad4081466b397e2a6204141bb7768b7c01bd93c","modified":1588077535407},{"_id":"themes/next/source/css/_common/components/pagination.styl","hash":"88559b13ce94311405b170a0506ded91273beceb","modified":1588077535407},{"_id":"themes/next/source/css/_common/components/tag-cloud.styl","hash":"6eb4bcc3056bd279d000607e8b4dad50d368ca69","modified":1588077535407},{"_id":"themes/next/source/css/_common/scaffolding/base.styl","hash":"eec22651977ea25b5e65e8cb1b4906eef69ec588","modified":1588077535407},{"_id":"themes/next/source/css/_common/scaffolding/helpers.styl","hash":"1da5c800d025345f212a3bf1be035060f4e5e6ed","modified":1588077535407},{"_id":"themes/next/source/css/_common/scaffolding/mobile.styl","hash":"9a45ed506274f655b11995c408cc566b16dada79","modified":1588077535407},{"_id":"themes/next/source/css/_common/scaffolding/normalize.styl","hash":"3f40e8a9fe8e7bd5cfc4cf4cbbbcb9539462e973","modified":1588077535411},{"_id":"themes/next/source/css/_common/scaffolding/scaffolding.styl","hash":"a17e2b871a335f290afb392a08f94fd35f59c715","modified":1588077535411},{"_id":"themes/next/source/css/_common/scaffolding/tables.styl","hash":"ea9069645696f86c5df64208490876fe150c8cae","modified":1588077535411},{"_id":"themes/next/source/lib/Han/dist/han.css","hash":"6c26cdb36687d4f0a11dabf5290a909c3506be5c","modified":1588077535419},{"_id":"themes/next/source/lib/Han/dist/han.min.css","hash":"6d586bfcfb7ae48f1b12f76eec82d3ad31947501","modified":1588077535419},{"_id":"themes/next/source/lib/Han/dist/han.min.js","hash":"16b03db23a52623348f37c04544f2792032c1fb6","modified":1588077535419},{"_id":"themes/next/source/js/src/schemes/pisces.js","hash":"c4358416f0a116d7f4037542fa3b385947e80908","modified":1588077535419},{"_id":"themes/next/source/lib/fancybox/source/fancybox_loading.gif","hash":"1a755fb2599f3a313cc6cfdb14df043f8c14a99c","modified":1588077535419},{"_id":"themes/next/source/lib/fancybox/source/blank.gif","hash":"2daeaa8b5f19f0bc209d976c02bd6acb51b00b0a","modified":1588077535419},{"_id":"themes/next/source/lib/fancybox/source/fancybox_loading@2x.gif","hash":"273b123496a42ba45c3416adb027cd99745058b0","modified":1588077535419},{"_id":"themes/next/source/lib/fancybox/source/fancybox_overlay.png","hash":"b3a4ee645ba494f52840ef8412015ba0f465dbe0","modified":1588077535419},{"_id":"themes/next/source/lib/fancybox/source/fancybox_sprite.png","hash":"17df19f97628e77be09c352bf27425faea248251","modified":1588077535419},{"_id":"themes/next/source/lib/fancybox/source/fancybox_sprite@2x.png","hash":"30c58913f327e28f466a00f4c1ac8001b560aed8","modified":1588077535419},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.css","hash":"5f163444617b6cf267342f06ac166a237bb62df9","modified":1588077535419},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.js","hash":"1cf3d47b5ccb7cb6e9019c64f2a88d03a64853e4","modified":1588077535419},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.pack.js","hash":"53360764b429c212f424399384417ccc233bb3be","modified":1588077535419},{"_id":"themes/next/source/lib/fastclick/lib/fastclick.js","hash":"1d6aeda0480d0e4cb6198edf7719d601d4ae2ccc","modified":1588077535419},{"_id":"themes/next/source/lib/fastclick/lib/fastclick.min.js","hash":"2cae0f5a6c5d6f3cb993015e6863f9483fc4de18","modified":1588077535419},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css","hash":"3655f1fdf1e584c4d8e8d39026093ca306a5a341","modified":1588077535423},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css.map","hash":"1573904b82807abbb32c97a3632c6c6808eaac50","modified":1588077535423},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.min.css","hash":"88af80502c44cd52ca81ffe7dc7276b7eccb06cf","modified":1588077535423},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.min.js","hash":"41ea797c68dbcff2f6fb3aba1d1043a22e7cc0f6","modified":1588077535423},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.pack.js","hash":"a817b6c158cbc5bab3582713de9fe18a18a80552","modified":1588077535423},{"_id":"themes/next/source/uploads/avatar.jpg","hash":"e3e7311f9624008234681ff57e411471dd80f379","modified":1588077535427},{"_id":"themes/next/source/lib/Han/dist/han.js","hash":"4ac683b2bc8531c84d98f51b86957be0e6f830f3","modified":1588077535419},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff","hash":"28b782240b3e76db824e12c02754a9731a167527","modified":1588077535423},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff2","hash":"d6f48cba7d076fb6f2fd6ba993a75b9dc1ecbf0c","modified":1588077535423},{"_id":"themes/next/source/lib/velocity/velocity.js","hash":"4237c6e9d59da349639de20e559e87c2c0218cfd","modified":1588077535423},{"_id":"themes/next/source/css/_schemes/Muse/sidebar/sidebar-blogroll.styl","hash":"1f09be9bb38411f0629b58c3b23873589a6dbcaa","modified":1588077535411},{"_id":"themes/next/source/css/_schemes/Mist/outline/outline.styl","hash":"a07aa12cc36ac5c819670c2a3c17d07ed7a08986","modified":1588077535411},{"_id":"themes/next/source/css/_schemes/Mist/sidebar/sidebar-blogroll.styl","hash":"1f09be9bb38411f0629b58c3b23873589a6dbcaa","modified":1588077535411},{"_id":"themes/next/source/css/_common/components/footer/footer.styl","hash":"4c4ef6e997d0c6e21de39c2daa0c768e12c8c6fa","modified":1588077535407},{"_id":"themes/next/source/css/_common/components/header/header.styl","hash":"53cde051e0337f4bf42fb8d6d7a79fa3fa6d4ef2","modified":1588077535407},{"_id":"themes/next/source/css/_common/components/header/headerband.styl","hash":"d63e0cacc53dd375fcc113465a4328c59ff5f2c1","modified":1588077535407},{"_id":"themes/next/source/css/_common/components/header/menu.styl","hash":"1a0d059799a298fe17c49a44298d32cebde93785","modified":1588077535407},{"_id":"themes/next/source/css/_common/components/header/site-meta.styl","hash":"0656e753f182c9f47fef7304c847b7587a85ef0d","modified":1588077535407},{"_id":"themes/next/source/css/_common/components/header/site-nav.styl","hash":"1727702eac5d326b5c81a667944a245016668231","modified":1588077535407},{"_id":"themes/next/source/css/_common/components/highlight/diff.styl","hash":"167986d0f649516671ddf7193eebba7b421cd115","modified":1588077535407},{"_id":"themes/next/source/css/_common/components/highlight/highlight.styl","hash":"b3b783511bbd94af7e941abf8ff411885db7395b","modified":1588077535407},{"_id":"themes/next/source/css/_common/components/highlight/theme.styl","hash":"7fe4d4d656e86276c17cb4e48a560cb6a4def703","modified":1588077535407},{"_id":"themes/next/source/css/_common/components/pages/archive.styl","hash":"104b5c79cd891506e0beaf938b083685f1da8637","modified":1588077535407},{"_id":"themes/next/source/css/_common/components/pages/categories.styl","hash":"7fb593f90d74a99c21840679933b9ef6fdc16a61","modified":1588077535407},{"_id":"themes/next/source/css/_common/components/pages/pages.styl","hash":"f9760ecf186954cee3ba4a149be334e9ba296b89","modified":1588077535407},{"_id":"themes/next/source/css/_common/components/pages/post-detail.styl","hash":"4e3838d7ac81d9ad133960f0f7ed58a44a015285","modified":1588077535407},{"_id":"themes/next/source/css/_common/components/pages/schedule.styl","hash":"8cf318644acc8b4978537c263290363e21c7f5af","modified":1588077535407},{"_id":"themes/next/source/css/_common/components/post/post-button.styl","hash":"4783f85872bc7e218c1522a5c1c68cd27a5922db","modified":1588077535407},{"_id":"themes/next/source/css/_common/components/post/post-collapse.styl","hash":"875cbe88d5c7f6248990e2beb97c9828920e7e24","modified":1588077535407},{"_id":"themes/next/source/css/_common/components/post/post-copyright.styl","hash":"caf263d1928496688c0e1419801eafd7e6919ce5","modified":1588077535407},{"_id":"themes/next/source/css/_common/components/post/post-eof.styl","hash":"a200c0a1c5a895ac9dc41e0641a5dfcd766be99b","modified":1588077535407},{"_id":"themes/next/source/css/_common/components/post/post-expand.styl","hash":"58f9e6aba94733244a87d2ba5966c5a009486509","modified":1588077535407},{"_id":"themes/next/source/css/_common/components/post/post-gallery.styl","hash":"cd9e214e502697f2f2db84eb721bac57a49b0fce","modified":1588077535407},{"_id":"themes/next/source/css/_common/components/post/post-meta.styl","hash":"658accf8e196721f295003da66941e6d1f7b81b0","modified":1588077535407},{"_id":"themes/next/source/css/_common/components/post/post-nav.styl","hash":"27deb3d3a243d30022055dac7dad851024099a8b","modified":1588077535407},{"_id":"themes/next/source/css/_common/components/post/post-reward.styl","hash":"f363a544aa800a2a5ed97c40887fe9743f67b03b","modified":1588077535407},{"_id":"themes/next/source/css/_common/components/post/post-rtl.styl","hash":"b2495ae5e04dcca610aacadc47881d9e716cd440","modified":1588077535407},{"_id":"themes/next/source/css/_common/components/post/post-tags.styl","hash":"5a982d8ef3b3623ea5f59e63728990f5623c1b57","modified":1588077535407},{"_id":"themes/next/source/css/_common/components/post/post-title.styl","hash":"350469437b20ecfd6f3ca45e400478f8e3f71cfb","modified":1588077535407},{"_id":"themes/next/source/css/_common/components/post/post-type.styl","hash":"01567edaea6978628aa5521a122a85434c418bfd","modified":1588077535407},{"_id":"themes/next/source/css/_common/components/post/post-widgets.styl","hash":"2cb09973d29a8e34e2a3425ac6e0938296970d8e","modified":1588077535407},{"_id":"themes/next/source/css/_common/components/post/post-wordcount.styl","hash":"268c9704481fdb0b4d1e646196386143990fe235","modified":1588077535407},{"_id":"themes/next/source/css/_common/components/post/post.styl","hash":"efc65bba7f2423439e9bca7d32ef7728c21e5c97","modified":1588077535407},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-author-links.styl","hash":"5f3510419161ec22ca88cce6a181ddad61de9e86","modified":1588077535407},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-author.styl","hash":"761eba9811b050b25d548cc0854de4824b41eb08","modified":1588077535407},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-blogroll.styl","hash":"ac060861b27b764bc4012fc362a25a332df4045a","modified":1588077535407},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-dimmer.styl","hash":"11c22f0fb3f6beb13e5a425ec064a4ff974c13b7","modified":1588077535407},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-feed-link.styl","hash":"61f8cea3c01acd600e90e1bc2a07def405503748","modified":1588077535407},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-nav.styl","hash":"1153bb71edf253765145559674390e16dd67c633","modified":1588077535407},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-toc.styl","hash":"28a8737c090fbffd188d73a00b42e90b9ee57df2","modified":1588077535407},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-toggle.styl","hash":"a1521d48bb06d8d703753f52a198baa197af7da2","modified":1588077535407},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar.styl","hash":"8e00d9a0bdf35ffc0a7fa387fa294b953c2d28fc","modified":1588077535407},{"_id":"themes/next/source/css/_common/components/sidebar/site-state.styl","hash":"e71652d3216e289c8548b1ea2357822c1476a425","modified":1588077535407},{"_id":"themes/next/source/css/_common/components/tags/blockquote-center.styl","hash":"2fe76476432b31993338cb45cdb3b29a518b6379","modified":1588077535407},{"_id":"themes/next/source/css/_common/components/tags/exturl.styl","hash":"a3bdd71237afc112b2aa255f278cab6baeb25351","modified":1588077535407},{"_id":"themes/next/source/css/_common/components/tags/full-image.styl","hash":"3159b55f35c40bd08e55b00148c523760a708c51","modified":1588077535407},{"_id":"themes/next/source/css/_common/components/tags/group-pictures.styl","hash":"2ad1a2a9bbf6742d1b0762c4c623b68113d1e0fe","modified":1588077535407},{"_id":"themes/next/source/css/_common/components/tags/label.styl","hash":"2ab1322fe52ab5aafd49e68f5bd890e8380ee927","modified":1588077535407},{"_id":"themes/next/source/css/_common/components/tags/note-modern.styl","hash":"962b654f8f7cbd18a298126a403d236ed4540516","modified":1588077535407},{"_id":"themes/next/source/css/_common/components/tags/note.styl","hash":"9a409b798decdefdaf7a23f0b11004a8c27e82f3","modified":1588077535407},{"_id":"themes/next/source/css/_common/components/tags/tabs.styl","hash":"154a87a32d2fead480d5e909c37f6c476671c5e6","modified":1588077535407},{"_id":"themes/next/source/css/_common/components/tags/tags.styl","hash":"b80604868e4f5cf20fccafd7ee415c20c804f700","modified":1588077535407},{"_id":"themes/next/source/css/_common/components/third-party/algolia-search.styl","hash":"bba4f3bdb7517cd85376df3e1209b570c0548c69","modified":1588077535407},{"_id":"themes/next/source/css/_common/components/third-party/baidushare.styl","hash":"5dbeed535d63a50265d96b396a5440f9bb31e4ba","modified":1588077535407},{"_id":"themes/next/source/css/_common/components/third-party/busuanzi-counter.styl","hash":"a6e7d698702c2e383dde3fde2abde27951679084","modified":1588077535407},{"_id":"themes/next/source/css/_common/components/third-party/duoshuo.styl","hash":"717cc7f82be9cc151e23a7678601ff2fd3a7fa1d","modified":1588077535407},{"_id":"themes/next/source/css/_common/components/third-party/han.styl","hash":"10599e16414a8b7a76c4e79e6617b5fe3d4d1adf","modified":1588077535407},{"_id":"themes/next/source/css/_common/components/third-party/localsearch.styl","hash":"16087276945fa038f199692e3eabb1c52b8ea633","modified":1588077535407},{"_id":"themes/next/source/css/_common/components/third-party/jiathis.styl","hash":"15975ba7456b96916b1dbac448a1a0d2c38b8f3d","modified":1588077535407},{"_id":"themes/next/source/css/_common/components/third-party/third-party.styl","hash":"37e406ec42b7a53c72395bdbaa434270019e7179","modified":1588077535407},{"_id":"themes/next/source/lib/Han/dist/font/han-space.otf","hash":"07436f011b44051f61b8329c99de4bec64e86f4b","modified":1588077535419},{"_id":"themes/next/source/lib/Han/dist/font/han-space.woff","hash":"7a635062b10bf5662ae1d218ba0980171005d060","modified":1588077535419},{"_id":"themes/next/source/lib/Han/dist/font/han.otf","hash":"f1f6bb8f461f5672e000380195d3d2358a28494c","modified":1588077535419},{"_id":"themes/next/source/lib/Han/dist/font/han.woff","hash":"f38ff9b2eecaa17b50b66aa2dae87e9e7436d195","modified":1588077535419},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-buttons.css","hash":"1a9d8e5c22b371fcc69d4dbbb823d9c39f04c0c8","modified":1588077535419},{"_id":"themes/next/source/lib/fancybox/source/helpers/fancybox_buttons.png","hash":"e385b139516c6813dcd64b8fc431c364ceafe5f3","modified":1588077535419},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-buttons.js","hash":"91e41741c2e93f732c82aaacec4cfc6e3f3ec876","modified":1588077535419},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-media.js","hash":"3bdf69ed2469e4fb57f5a95f17300eef891ff90d","modified":1588077535419},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-thumbs.css","hash":"4ac329c16a5277592fc12a37cca3d72ca4ec292f","modified":1588077535419},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-thumbs.js","hash":"53e194f4a72e649c04fb586dd57762b8c022800b","modified":1588077535419},{"_id":"themes/next/source/lib/font-awesome/fonts/FontAwesome.otf","hash":"048707bc52ac4b6563aaa383bfe8660a0ddc908c","modified":1588077535423},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.eot","hash":"d980c2ce873dc43af460d4d572d441304499f400","modified":1588077535423},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.ttf","hash":"13b1eab65a983c7a73bc7997c479d66943f7c6cb","modified":1588077535423},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.js","hash":"90a1b22129efc172e2dfcceeeb76bff58bc3192f","modified":1588077535419},{"_id":"themes/next/source/lib/three/three.min.js","hash":"26273b1cb4914850a89529b48091dc584f2c57b8","modified":1588077535423},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.svg","hash":"b5483b11f8ba213e733b5b8af9927a04fec996f6","modified":1588077535423},{"_id":"themes/next/source/images/avatar.gif","hash":"5b72c7a6939173933d8b2da234f4fa7ebfe42382","modified":1588077535419},{"_id":"public/search.xml","hash":"b3e2879e02fd6dbbcba34ceeb7eff3d942e3a2a8","modified":1588077567742},{"_id":"public/about/index.html","hash":"495686e4ea087d6c7678b1a30ff8a9f87da7c72d","modified":1588077567747},{"_id":"public/categories/index.html","hash":"3ebdad01980253b897ce69bc540f59a5b65e8f24","modified":1588077567747},{"_id":"public/tags/index.html","hash":"089d2ac294f5ab405ac65a20264dae6cc0cc454c","modified":1588077567747},{"_id":"public/2018/11/30/MalongTech-Learning-3D-Image-Segmentation/index.html","hash":"a3c1df3b20705bf0b6e58e585df32f83a9c0aa54","modified":1588077567747},{"_id":"public/2018/11/29/MalongTEch-Learning-Semantic-Soft-Segmentation/index.html","hash":"9a6f344a783a793b954c97aca16928efa4d3a9ae","modified":1588077567747},{"_id":"public/2018/11/26/MalongTech-Learning-Image-Segmentation/index.html","hash":"8a94758e3d5f60459dc5fa1126b911b8cde476e0","modified":1588077567747},{"_id":"public/2018/11/12/MalongTech-Learning-Object-Detection/index.html","hash":"1cf847daa4021139faeb707f9de7deda81bfa28a","modified":1588077567747},{"_id":"public/2018/11/12/LInux/index.html","hash":"9f806b2a70945c64fa31fd86022d4c1bc9bf3826","modified":1588077567747},{"_id":"public/2018/05/25/LogBook/index.html","hash":"4b2dd40906e4c3fffd17f071fdd16db7c288c540","modified":1588077567748},{"_id":"public/2018/04/30/Apri/index.html","hash":"d81760a163345028acf06b60d735dc5acdb97b0a","modified":1588077567748},{"_id":"public/2018/01/31/Machine Learning in NBA/index.html","hash":"57f82560722a8c8316f6ad87abf97a6864d67625","modified":1588077567748},{"_id":"public/2018/01/28/MXnet_Config/index.html","hash":"b23df582e6fbd00415b05d32f4b28b376963ddcb","modified":1588077567748},{"_id":"public/2018/01/28/IT salary in USA/index.html","hash":"5b7c219cfedf22c57aef87191e475a82103fb264","modified":1588077567748},{"_id":"public/archives/index.html","hash":"1f8f5d8a923c6d30fc13155f386be1ec24111865","modified":1588077567748},{"_id":"public/archives/2018/index.html","hash":"179a744cf317b7c4103ddb670a23a16ef17ee42b","modified":1588077567748},{"_id":"public/archives/2018/01/index.html","hash":"77fb4a4b0c4fe60ff6c5b0c5c69159b32732d626","modified":1588077567748},{"_id":"public/archives/2018/04/index.html","hash":"716d281645980a17d03796c92c385227bfd6c87e","modified":1588077567748},{"_id":"public/archives/2018/05/index.html","hash":"de1a23066fa46886b8f010ce267489ea4e1d4442","modified":1588077567748},{"_id":"public/archives/2018/11/index.html","hash":"35111fec922b29b7466e5f729f425882b3f717c1","modified":1588077567749},{"_id":"public/index.html","hash":"338254a39e47b1974542263aad1470452cb68686","modified":1588077567749},{"_id":"public/categories/My-Life/index.html","hash":"c4aa6d9b05083781e4779a4d1121c17c34e298b5","modified":1588077567749},{"_id":"public/categories/Data-Visualisation/index.html","hash":"c5594dd8f32d04d032ab7c3374b65c09e002729b","modified":1588077567749},{"_id":"public/categories/System/index.html","hash":"09261c058d217eb8f3e286efb0a8fb47acc4c914","modified":1588077567749},{"_id":"public/categories/Machine-Learning/index.html","hash":"756a2c225e1a66f5ac05ebaabd2e1ce44607260d","modified":1588077567749},{"_id":"public/categories/实习/index.html","hash":"cd30cd8677340f83440a9e682e87acf6aae1faed","modified":1588077567749},{"_id":"public/categories/Msc-Project/index.html","hash":"d8b5b172885c200e20f3c153851eb2d773f284dd","modified":1588077567749},{"_id":"public/tags/友人A/index.html","hash":"b306a5a49f06f6093eaa5be2b85f555f4ef42775","modified":1588077567749},{"_id":"public/tags/data-analysis/index.html","hash":"3747074801c435e18487a5cc7bc135459b170677","modified":1588077567749},{"_id":"public/tags/Linux/index.html","hash":"4a2259247cfb079dff25911be215f5f3fbe773e2","modified":1588077567749},{"_id":"public/tags/MXnet/index.html","hash":"32c63b6d5f91711b7a8d544bdd4e66163711730d","modified":1588077567749},{"_id":"public/tags/Machine-Learning/index.html","hash":"01475b7e1c0b4a5f5c5b7ed8f33b1912224298b3","modified":1588077567750},{"_id":"public/tags/NBA/index.html","hash":"85c0d9f68eb724c2684ce089610cf1b40f2d6b94","modified":1588077567750},{"_id":"public/tags/Deep-Learning/index.html","hash":"444c0b1ce2b61f87fb2b7bf760d723045a9fe473","modified":1588077567750},{"_id":"public/tags/Object-Detection/index.html","hash":"bb9d816bc739df783a85463eaee14cd6e8a944d5","modified":1588077567750},{"_id":"public/CNAME","hash":"ab075e42f34723bce8bab60d8b6da686e2c40faf","modified":1588077567756},{"_id":"public/about/resume_en.pdf","hash":"10b6e617530d73b17a81c9f855d1656adb3673f1","modified":1588077567756},{"_id":"public/about/resume_cn.pdf","hash":"87a665f33c05dfe969edf2001c4474a3f9ee5195","modified":1588077567756},{"_id":"public/images/algolia_logo.svg","hash":"45eeea0b5fba833e21e38ea10ed5ab385ceb4f01","modified":1588077567756},{"_id":"public/images/cc-by-nc-sa.svg","hash":"6f55543d1fb9cbc436c101d24f802dec7b41efc3","modified":1588077567756},{"_id":"public/images/cc-by-nc-nd.svg","hash":"bc3588c9b2d7c68830524783120ff6cf957cf668","modified":1588077567756},{"_id":"public/images/cc-by-nc.svg","hash":"6f076713fb9bf934aa2c1046bdf2cf2e37bc1eab","modified":1588077567756},{"_id":"public/images/cc-by-nd.svg","hash":"42cd73da328077ccc92f859bb8f3cf621b3484f8","modified":1588077567756},{"_id":"public/images/cc-by.svg","hash":"e92a33c32d1dac8ed94849b2b4e6456e887efe70","modified":1588077567756},{"_id":"public/images/cc-by-sa.svg","hash":"70c1535f43e54e5ff35ca81419e77e4c0c301398","modified":1588077567756},{"_id":"public/images/cc-zero.svg","hash":"9bfb52b2f63527a7049247bf00d44e6dc1170e7d","modified":1588077567757},{"_id":"public/images/favicon.ico","hash":"17c483c2915dbd1c84cc42d124a7199bdc7ade1f","modified":1588077567757},{"_id":"public/images/loading.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1588077567757},{"_id":"public/images/placeholder.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1588077567757},{"_id":"public/images/quote-l.svg","hash":"cd108d6f44351cadf8e6742565217f88818a0458","modified":1588077567757},{"_id":"public/images/quote-r.svg","hash":"2a2a250b32a87c69dcc1b1976c74b747bedbfb41","modified":1588077567757},{"_id":"public/images/searchicon.png","hash":"67727a6a969be0b2659b908518fa6706eed307b8","modified":1588077567757},{"_id":"public/lib/fastclick/LICENSE","hash":"6f474ea75c42442da7bbcf2e9143ce98258efd8d","modified":1588077567757},{"_id":"public/lib/font-awesome/HELP-US-OUT.txt","hash":"ee33b2798b1e714b904d663436c6b3521011d1fa","modified":1588077567757},{"_id":"public/lib/fancybox/source/fancybox_loading.gif","hash":"1a755fb2599f3a313cc6cfdb14df043f8c14a99c","modified":1588077567757},{"_id":"public/lib/fancybox/source/blank.gif","hash":"2daeaa8b5f19f0bc209d976c02bd6acb51b00b0a","modified":1588077567757},{"_id":"public/lib/fancybox/source/fancybox_loading@2x.gif","hash":"273b123496a42ba45c3416adb027cd99745058b0","modified":1588077567757},{"_id":"public/lib/fancybox/source/fancybox_overlay.png","hash":"b3a4ee645ba494f52840ef8412015ba0f465dbe0","modified":1588077567757},{"_id":"public/lib/fancybox/source/fancybox_sprite.png","hash":"17df19f97628e77be09c352bf27425faea248251","modified":1588077567757},{"_id":"public/lib/fancybox/source/fancybox_sprite@2x.png","hash":"30c58913f327e28f466a00f4c1ac8001b560aed8","modified":1588077567757},{"_id":"public/lib/font-awesome/css/font-awesome.css.map","hash":"1573904b82807abbb32c97a3632c6c6808eaac50","modified":1588077567757},{"_id":"public/lib/Han/dist/font/han-space.otf","hash":"07436f011b44051f61b8329c99de4bec64e86f4b","modified":1588077567758},{"_id":"public/lib/Han/dist/font/han-space.woff","hash":"7a635062b10bf5662ae1d218ba0980171005d060","modified":1588077567758},{"_id":"public/lib/Han/dist/font/han.otf","hash":"f1f6bb8f461f5672e000380195d3d2358a28494c","modified":1588077567758},{"_id":"public/lib/Han/dist/font/han.woff","hash":"f38ff9b2eecaa17b50b66aa2dae87e9e7436d195","modified":1588077567758},{"_id":"public/lib/fancybox/source/helpers/fancybox_buttons.png","hash":"e385b139516c6813dcd64b8fc431c364ceafe5f3","modified":1588077567758},{"_id":"public/images/alipay-reward-image.jpg","hash":"ee38a3c5ac61f24467a668ba5e7246053b7b1b8c","modified":1588077568154},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.woff","hash":"28b782240b3e76db824e12c02754a9731a167527","modified":1588077568154},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.woff2","hash":"d6f48cba7d076fb6f2fd6ba993a75b9dc1ecbf0c","modified":1588077568154},{"_id":"public/js/src/affix.js","hash":"978e0422b5bf1b560236d8d10ebc1adcf66392e3","modified":1588077568158},{"_id":"public/js/src/bootstrap.js","hash":"6117f97b4984b8e33f21c726132da64ba678e4ed","modified":1588077568158},{"_id":"public/js/src/exturl.js","hash":"e42e2aaab7bf4c19a0c8e779140e079c6aa5c0b1","modified":1588077568158},{"_id":"public/js/src/algolia-search.js","hash":"b172f697ed339a24b1e80261075232978d164c35","modified":1588077568158},{"_id":"public/js/src/hook-duoshuo.js","hash":"a6119070c0119f33e08b29da7d2cce2635eb40a0","modified":1588077568158},{"_id":"public/js/src/motion.js","hash":"dc0365b2fb315a8b43d3ef19b59d3a82a366fcc1","modified":1588077568159},{"_id":"public/js/src/js.cookie.js","hash":"9b37973a90fd50e71ea91682265715e45ae82c75","modified":1588077568159},{"_id":"public/js/src/post-details.js","hash":"0693695a9512641daff63d99da772625a058ab18","modified":1588077568159},{"_id":"public/js/src/scrollspy.js","hash":"fe4da1b9fe73518226446f5f27d2831e4426fc35","modified":1588077568159},{"_id":"public/js/src/scroll-cookie.js","hash":"09dc828cbf5f31158ff6250d2bf7c3cde6365c67","modified":1588077568159},{"_id":"public/js/src/utils.js","hash":"2917c39c75b14b6dab7e1c46ab4d87b4df9fcd5d","modified":1588077568159},{"_id":"public/lib/algolia-instant-search/instantsearch.min.css","hash":"90ef19edc982645b118b095615838d9c5eaba0de","modified":1588077568159},{"_id":"public/lib/canvas-nest/canvas-nest.min.js","hash":"0387e75e23b1db108a755073fe52a0d03eb391a7","modified":1588077568159},{"_id":"public/lib/canvas-ribbon/canvas-ribbon.js","hash":"7fd2f3e2773555392ef40df40cae3bedb884f17a","modified":1588077568159},{"_id":"public/lib/fastclick/bower.json","hash":"4dcecf83afddba148464d5339c93f6d0aa9f42e9","modified":1588077568159},{"_id":"public/lib/font-awesome/bower.json","hash":"64394a2a9aa00f8e321d8daa5e51a420f0e96dad","modified":1588077568159},{"_id":"public/lib/jquery_lazyload/bower.json","hash":"ae3c3b61e6e7f9e1d7e3585ad854380ecc04cf53","modified":1588077568159},{"_id":"public/lib/jquery_lazyload/jquery.lazyload.js","hash":"481fd478650e12b67c201a0ea41e92743f8b45a3","modified":1588077568159},{"_id":"public/lib/jquery_lazyload/jquery.scrollstop.js","hash":"0e9a81785a011c98be5ea821a8ed7d411818cfd1","modified":1588077568159},{"_id":"public/lib/pace/pace-theme-barber-shop.min.css","hash":"ee0d51446cb4ffe1bb96bd7bc8c8e046dddfcf46","modified":1588077568159},{"_id":"public/lib/pace/pace-theme-big-counter.min.css","hash":"5b561dc328af4c4d512e20a76fe964d113a32ba8","modified":1588077568160},{"_id":"public/lib/pace/pace-theme-bounce.min.css","hash":"f6bdb9a785b7979dd8ec5c60e278af955ef1e585","modified":1588077568160},{"_id":"public/lib/pace/pace-theme-center-atom.min.css","hash":"dcf79c24fe5350fb73d8038573a104e73639e9d3","modified":1588077568160},{"_id":"public/lib/pace/pace-theme-center-circle.min.css","hash":"a4066769c78affbfbc5e30a600e2c7862cd532e0","modified":1588077568160},{"_id":"public/lib/pace/pace-theme-center-radar.min.css","hash":"ab7cba998bf4c03b13df342bf43647fa4f419783","modified":1588077568160},{"_id":"public/lib/pace/pace-theme-center-simple.min.css","hash":"67f44c947548bd4d77e7590d3f59e236cbf9e98a","modified":1588077568160},{"_id":"public/lib/pace/pace-theme-corner-indicator.min.css","hash":"b3c64c973f31884e3d8145989476707333406b9a","modified":1588077568160},{"_id":"public/lib/pace/pace-theme-fill-left.min.css","hash":"0bec1e235a4a2cccda3f993b205424e1441a44ae","modified":1588077568160},{"_id":"public/lib/pace/pace-theme-flash.min.css","hash":"13ace22c40312d7bbd8d9c1e50eff897a7a497d8","modified":1588077568160},{"_id":"public/lib/pace/pace-theme-loading-bar.min.css","hash":"7ee28875dfc1230d76c537f6605766e8d4011e9f","modified":1588077568160},{"_id":"public/lib/pace/pace-theme-minimal.min.css","hash":"9cd783cceb8a191f3c8b5d81f7a430ecc3e489d3","modified":1588077568160},{"_id":"public/lib/pace/pace-theme-mac-osx.min.css","hash":"9f2e7b51b084da407863826b25265b31150b3821","modified":1588077568160},{"_id":"public/lib/pace/pace.min.js","hash":"9944dfb7814b911090e96446cea4d36e2b487234","modified":1588077568160},{"_id":"public/lib/velocity/bower.json","hash":"0ef14e7ccdfba5db6eb3f8fc6aa3b47282c36409","modified":1588077568160},{"_id":"public/lib/velocity/velocity.ui.min.js","hash":"ed5e534cd680a25d8d14429af824f38a2c7d9908","modified":1588077568160},{"_id":"public/js/src/schemes/pisces.js","hash":"79da92119bc246fe05d1626ac98426a83ec90a94","modified":1588077568160},{"_id":"public/lib/fancybox/source/jquery.fancybox.css","hash":"5f163444617b6cf267342f06ac166a237bb62df9","modified":1588077568161},{"_id":"public/lib/fastclick/lib/fastclick.min.js","hash":"2cae0f5a6c5d6f3cb993015e6863f9483fc4de18","modified":1588077568161},{"_id":"public/lib/ua-parser-js/dist/ua-parser.pack.js","hash":"214dad442a92d36af77ed0ca1d9092b16687f02f","modified":1588077568161},{"_id":"public/lib/ua-parser-js/dist/ua-parser.min.js","hash":"38628e75e4412cc6f11074e03e1c6d257aae495b","modified":1588077568161},{"_id":"public/lib/fancybox/source/helpers/jquery.fancybox-buttons.css","hash":"1a9d8e5c22b371fcc69d4dbbb823d9c39f04c0c8","modified":1588077568162},{"_id":"public/lib/fancybox/source/helpers/jquery.fancybox-buttons.js","hash":"91e41741c2e93f732c82aaacec4cfc6e3f3ec876","modified":1588077568162},{"_id":"public/lib/fancybox/source/helpers/jquery.fancybox-media.js","hash":"3bdf69ed2469e4fb57f5a95f17300eef891ff90d","modified":1588077568162},{"_id":"public/lib/fancybox/source/helpers/jquery.fancybox-thumbs.css","hash":"4ac329c16a5277592fc12a37cca3d72ca4ec292f","modified":1588077568162},{"_id":"public/lib/fancybox/source/helpers/jquery.fancybox-thumbs.js","hash":"53e194f4a72e649c04fb586dd57762b8c022800b","modified":1588077568162},{"_id":"public/lib/fastclick/README.html","hash":"da3c74d484c73cc7df565e8abbfa4d6a5a18d4da","modified":1588077568162},{"_id":"public/lib/jquery_lazyload/CONTRIBUTING.html","hash":"a6358170d346af13b1452ac157b60505bec7015c","modified":1588077568162},{"_id":"public/lib/jquery_lazyload/README.html","hash":"bde24335f6bc09d8801c0dcd7274f71b466552bd","modified":1588077568162},{"_id":"public/css/main.css","hash":"b1aae4aec9d11681d892254a39b87fc33d44e186","modified":1588077568162},{"_id":"public/lib/three/canvas_lines.min.js","hash":"dce4a3b65f8bf958f973690caa7ec4952f353b0c","modified":1588077568162},{"_id":"public/lib/three/three-waves.min.js","hash":"d968cba6b3a50b3626a02d67b544f349d83b147c","modified":1588077568162},{"_id":"public/lib/three/canvas_sphere.min.js","hash":"d8ea241a53c135a650f7335d2b6982b899fd58a9","modified":1588077568162},{"_id":"public/lib/velocity/velocity.ui.js","hash":"6a1d101eab3de87527bb54fcc8c7b36b79d8f0df","modified":1588077568163},{"_id":"public/lib/velocity/velocity.min.js","hash":"2f1afadc12e4cf59ef3b405308d21baa97e739c6","modified":1588077568163},{"_id":"public/lib/Han/dist/han.css","hash":"bd40da3fba8735df5850956814e312bd7b3193d7","modified":1588077568163},{"_id":"public/lib/Han/dist/han.min.css","hash":"a0c9e32549a8b8cf327ab9227b037f323cdb60ee","modified":1588077568163},{"_id":"public/lib/jquery/index.js","hash":"41b4bfbaa96be6d1440db6e78004ade1c134e276","modified":1588077568163},{"_id":"public/lib/Han/dist/han.min.js","hash":"f559c68a25065a14f47da954a7617d87263e409d","modified":1588077568163},{"_id":"public/lib/fancybox/source/jquery.fancybox.pack.js","hash":"53360764b429c212f424399384417ccc233bb3be","modified":1588077568163},{"_id":"public/lib/fancybox/source/jquery.fancybox.js","hash":"1cf3d47b5ccb7cb6e9019c64f2a88d03a64853e4","modified":1588077568163},{"_id":"public/lib/fastclick/lib/fastclick.js","hash":"06cef196733a710e77ad7e386ced6963f092dc55","modified":1588077568163},{"_id":"public/lib/font-awesome/css/font-awesome.css","hash":"0140952c64e3f2b74ef64e050f2fe86eab6624c8","modified":1588077568163},{"_id":"public/lib/font-awesome/css/font-awesome.min.css","hash":"512c7d79033e3028a9be61b540cf1a6870c896f8","modified":1588077568163},{"_id":"public/lib/Han/dist/han.js","hash":"e345397e0585c9fed1449e614ec13e0224acf2ab","modified":1588077568163},{"_id":"public/lib/velocity/velocity.js","hash":"9f08181baea0cc0e906703b7e5df9111b9ef3373","modified":1588077568164},{"_id":"public/lib/algolia-instant-search/instantsearch.min.js","hash":"9ccc6f8144f54e86df9a3fd33a18368d81cf3a4f","modified":1588077568164},{"_id":"public/lib/three/three.min.js","hash":"73f4cdc17e51a72b9bf5b9291f65386d615c483b","modified":1588077568164},{"_id":"public/lib/font-awesome/fonts/FontAwesome.otf","hash":"048707bc52ac4b6563aaa383bfe8660a0ddc908c","modified":1588077568164},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.eot","hash":"d980c2ce873dc43af460d4d572d441304499f400","modified":1588077568164},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.ttf","hash":"13b1eab65a983c7a73bc7997c479d66943f7c6cb","modified":1588077568165},{"_id":"public/images/wechat-reward-image.jpg","hash":"a81a23bcba5d49564c5afe2210957a86a8175431","modified":1588077568204},{"_id":"public/uploads/avatar.jpg","hash":"e3e7311f9624008234681ff57e411471dd80f379","modified":1588077568208},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.svg","hash":"b5483b11f8ba213e733b5b8af9927a04fec996f6","modified":1588077568211},{"_id":"public/images/avatar.gif","hash":"5b72c7a6939173933d8b2da234f4fa7ebfe42382","modified":1588077568229}],"Category":[{"name":"My Life","_id":"ck9jwdnh6000cjlrcwlvmu5ot"},{"name":"Data Visualisation","_id":"ck9jwdnha000ejlrc7w8nwbju"},{"name":"System","_id":"ck9jwdnha000gjlrcmtigkx3m"},{"name":"Machine Learning","_id":"ck9jwdnhb000ijlrcrmjbet7n"},{"name":"实习","_id":"ck9jwdnhd000mjlrcleeolcba"},{"name":"Msc Project","_id":"ck9jwdnmm001fjlrcfjvwudkl"}],"Data":[],"Page":[{"title":"About KAMISAMA","date":"2018-04-25T20:34:58.000Z","comments":0,"_content":"\n---\n# 爱生活  爱 LeBron James\n\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/about/LBJ1.gif)\n\n---\n\n---\n# 简历/Resume\n\n[中文简历](resume_cn.pdf)\n[Resume_En](resume_en.pdf)\n\n---\n","source":"about/index.md","raw":"---\ntitle: About KAMISAMA\ndate: 2018-04-26 04:34:58\ncomments: false\n---\n\n---\n# 爱生活  爱 LeBron James\n\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/about/LBJ1.gif)\n\n---\n\n---\n# 简历/Resume\n\n[中文简历](resume_cn.pdf)\n[Resume_En](resume_en.pdf)\n\n---\n","updated":"2020-04-28T12:38:55.395Z","path":"about/index.html","layout":"page","_id":"ck9jwdnfv0000jlrcrr2nd8x5","content":"<hr>\n<h1 id=\"爱生活-爱-LeBron-James\"><a href=\"#爱生活-爱-LeBron-James\" class=\"headerlink\" title=\"爱生活  爱 LeBron James\"></a>爱生活  爱 LeBron James</h1><p><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/about/LBJ1.gif\" alt=\"image\"></p>\n<hr>\n<hr>\n<h1 id=\"简历-Resume\"><a href=\"#简历-Resume\" class=\"headerlink\" title=\"简历/Resume\"></a>简历/Resume</h1><p><a href=\"resume_cn.pdf\">中文简历</a><br><a href=\"resume_en.pdf\">Resume_En</a></p>\n<hr>\n","site":{"data":{}},"excerpt":"","more":"<hr>\n<h1 id=\"爱生活-爱-LeBron-James\"><a href=\"#爱生活-爱-LeBron-James\" class=\"headerlink\" title=\"爱生活  爱 LeBron James\"></a>爱生活  爱 LeBron James</h1><p><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/about/LBJ1.gif\" alt=\"image\"></p>\n<hr>\n<hr>\n<h1 id=\"简历-Resume\"><a href=\"#简历-Resume\" class=\"headerlink\" title=\"简历/Resume\"></a>简历/Resume</h1><p><a href=\"resume_cn.pdf\">中文简历</a><br><a href=\"resume_en.pdf\">Resume_En</a></p>\n<hr>\n"},{"title":"Categories","date":"2018-04-25T19:21:32.000Z","type":"categories","comments":0,"_content":"","source":"categories/index.md","raw":"---\ntitle: Categories\ndate: 2018-04-26 03:21:32\ntype: \"categories\"\ncomments: false\n---\n","updated":"2020-04-28T12:38:55.395Z","path":"categories/index.html","layout":"page","_id":"ck9jwdng10002jlrcoib9utu0","content":"","site":{"data":{}},"excerpt":"","more":""},{"title":"Tags","date":"2018-04-25T19:03:03.000Z","type":"tags","comments":0,"_content":"","source":"tags/index.md","raw":"---\ntitle: Tags\ndate: 2018-04-26 03:03:03\ntype: \"tags\"\ncomments: false\n---\n","updated":"2020-04-28T12:38:55.395Z","path":"tags/index.html","layout":"page","_id":"ck9jwdng20004jlrcjb5bj9ob","content":"","site":{"data":{}},"excerpt":"","more":""}],"Post":[{"title":"四月は君の嘘","date":"2018-04-30T12:04:14.000Z","_content":"\n## YouTube Source\n[【四月是你的謊言AMV】你還會記得嗎？四月は君の嘘 Your Lie in April](https://www.youtube.com/watch?v=-fFqguvUmwM)\n\n<iframe width=\"731\" height=\"411\" src=\"https://www.youtube.com/embed/-fFqguvUmwM\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>\n\n## BiliBili Source\n[【四月是你的谎言】你还会记得吗](https://www.bilibili.com/video/av2113169/)\n\n## 4月的最后一天了呢\n<center>\n与你相遇的四月就要来了「**君と出会った四月が来ているから**」\n没有你的四月就要来了「**君のいない四月は来ないで**」\n\n</center>\n\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/YourLieInApril/1.png)\n<!-- more -->\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/YourLieInApril/2.png)\n\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/YourLieInApril/3.png)\n\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/YourLieInApril/4.png)\n\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/YourLieInApril/5.png)\n\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/YourLieInApril/6.png)\n\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/YourLieInApril/7.png)\n\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/YourLieInApril/8.png)\n\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/YourLieInApril/9.png)","source":"_posts/Apri.md","raw":"---\ntitle: 四月は君の嘘\ndate: 2018-04-30 20:04:14\ntags: [友人A]\ncategories: My Life\n---\n\n## YouTube Source\n[【四月是你的謊言AMV】你還會記得嗎？四月は君の嘘 Your Lie in April](https://www.youtube.com/watch?v=-fFqguvUmwM)\n\n<iframe width=\"731\" height=\"411\" src=\"https://www.youtube.com/embed/-fFqguvUmwM\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>\n\n## BiliBili Source\n[【四月是你的谎言】你还会记得吗](https://www.bilibili.com/video/av2113169/)\n\n## 4月的最后一天了呢\n<center>\n与你相遇的四月就要来了「**君と出会った四月が来ているから**」\n没有你的四月就要来了「**君のいない四月は来ないで**」\n\n</center>\n\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/YourLieInApril/1.png)\n<!-- more -->\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/YourLieInApril/2.png)\n\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/YourLieInApril/3.png)\n\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/YourLieInApril/4.png)\n\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/YourLieInApril/5.png)\n\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/YourLieInApril/6.png)\n\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/YourLieInApril/7.png)\n\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/YourLieInApril/8.png)\n\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/YourLieInApril/9.png)","slug":"Apri","published":1,"updated":"2020-04-28T12:38:55.395Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck9jwdnfx0001jlrcgips8q0o","content":"<h2 id=\"YouTube-Source\"><a href=\"#YouTube-Source\" class=\"headerlink\" title=\"YouTube Source\"></a>YouTube Source</h2><p><a href=\"https://www.youtube.com/watch?v=-fFqguvUmwM\" target=\"_blank\" rel=\"noopener\">【四月是你的謊言AMV】你還會記得嗎？四月は君の嘘 Your Lie in April</a></p>\n<iframe width=\"731\" height=\"411\" src=\"https://www.youtube.com/embed/-fFqguvUmwM\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>\n\n<h2 id=\"BiliBili-Source\"><a href=\"#BiliBili-Source\" class=\"headerlink\" title=\"BiliBili Source\"></a>BiliBili Source</h2><p><a href=\"https://www.bilibili.com/video/av2113169/\" target=\"_blank\" rel=\"noopener\">【四月是你的谎言】你还会记得吗</a></p>\n<h2 id=\"4月的最后一天了呢\"><a href=\"#4月的最后一天了呢\" class=\"headerlink\" title=\"4月的最后一天了呢\"></a>4月的最后一天了呢</h2><center><br>与你相遇的四月就要来了「<strong>君と出会った四月が来ているから</strong>」<br>没有你的四月就要来了「<strong>君のいない四月は来ないで</strong>」<br><br></center>\n\n<p><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/YourLieInApril/1.png\" alt=\"image\"><br><a id=\"more\"></a><br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/YourLieInApril/2.png\" alt=\"image\"></p>\n<p><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/YourLieInApril/3.png\" alt=\"image\"></p>\n<p><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/YourLieInApril/4.png\" alt=\"image\"></p>\n<p><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/YourLieInApril/5.png\" alt=\"image\"></p>\n<p><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/YourLieInApril/6.png\" alt=\"image\"></p>\n<p><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/YourLieInApril/7.png\" alt=\"image\"></p>\n<p><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/YourLieInApril/8.png\" alt=\"image\"></p>\n<p><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/YourLieInApril/9.png\" alt=\"image\"></p>\n","site":{"data":{}},"excerpt":"<h2 id=\"YouTube-Source\"><a href=\"#YouTube-Source\" class=\"headerlink\" title=\"YouTube Source\"></a>YouTube Source</h2><p><a href=\"https://www.youtube.com/watch?v=-fFqguvUmwM\" target=\"_blank\" rel=\"noopener\">【四月是你的謊言AMV】你還會記得嗎？四月は君の嘘 Your Lie in April</a></p>\n<iframe width=\"731\" height=\"411\" src=\"https://www.youtube.com/embed/-fFqguvUmwM\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>\n\n<h2 id=\"BiliBili-Source\"><a href=\"#BiliBili-Source\" class=\"headerlink\" title=\"BiliBili Source\"></a>BiliBili Source</h2><p><a href=\"https://www.bilibili.com/video/av2113169/\" target=\"_blank\" rel=\"noopener\">【四月是你的谎言】你还会记得吗</a></p>\n<h2 id=\"4月的最后一天了呢\"><a href=\"#4月的最后一天了呢\" class=\"headerlink\" title=\"4月的最后一天了呢\"></a>4月的最后一天了呢</h2><center><br>与你相遇的四月就要来了「<strong>君と出会った四月が来ているから</strong>」<br>没有你的四月就要来了「<strong>君のいない四月は来ないで</strong>」<br><br></center>\n\n<p><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/YourLieInApril/1.png\" alt=\"image\"><br>","more":"<br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/YourLieInApril/2.png\" alt=\"image\"></p>\n<p><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/YourLieInApril/3.png\" alt=\"image\"></p>\n<p><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/YourLieInApril/4.png\" alt=\"image\"></p>\n<p><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/YourLieInApril/5.png\" alt=\"image\"></p>\n<p><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/YourLieInApril/6.png\" alt=\"image\"></p>\n<p><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/YourLieInApril/7.png\" alt=\"image\"></p>\n<p><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/YourLieInApril/8.png\" alt=\"image\"></p>\n<p><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/YourLieInApril/9.png\" alt=\"image\"></p>"},{"title":"IT salary in USA","date":"2018-01-28T00:23:32.000Z","_content":"\n## Result\n**Chart with Anlysis**\n\n**Website: SALARY DISTRIBUTION OF DIFFERENT JOBS IN USA**\n\n![image](https://raw.githubusercontent.com/Trouble404/IT-Salary-in-USA/master/readme_add_pic/page1.PNG)<!-- more -->\n![image](https://raw.githubusercontent.com/Trouble404/IT-Salary-in-USA/master/readme_add_pic/page2.PNG)\n![image](https://raw.githubusercontent.com/Trouble404/IT-Salary-in-USA/master/readme_add_pic/page3.PNG)\n---\n\n---\n\n## Data process\n### Datasets used to anlysis\n*    U.S. Technology Jobs on Dice.com  Found in [Kaggle](https://www.kaggle.com/PromptCloudHQ/us-technology-jobs-on-dicecom/data)\n*    US jobs on Monster.com  Found in [Kaggle](https://www.kaggle.com/PromptCloudHQ/us-jobs-on-monstercom)\n### Data Clean\n*    Obtain **salay** and coresspoding **states** information in US jos on Monster.com  [clean process](https://github.com/Trouble404/IT-Salary-in-USA/blob/master/chart/chart1/chart1_data.ipynb)\n*    Obtain **salay** and coresspoding **cities** information in US jos on Monster.com  [clean process](https://github.com/Trouble404/IT-Salary-in-USA/blob/master/chart/chart2/chart2_data.ipynb)\n*    Obtain **skills** information in U.S Technology Jobs on Dice.com  [clean process](https://github.com/Trouble404/IT-Salary-in-USA/blob/master/chart/chart3/chart3_data.ipynb)\n\n---\n\n## Chart Plotting\n### Chart 1: Map chart of average salay of 3 types jobs\n[Tableau address](https://public.tableau.com/profile/j.zhang#!/vizhome/chart1_23/dashborad1)\n\n<img src=\"https://raw.githubusercontent.com/Trouble404/IT-Salary-in-USA/master/chart/chart1/Chart1.png\" width=\"1000px\" />\n\n### Chart 2: Bubble chart of average salay of 3 types jobs\n[Tableau address](https://public.tableau.com/profile/j.zhang#!/vizhome/chart2_10/dasbord1)\n\n<img src=\"https://raw.githubusercontent.com/Trouble404/IT-Salary-in-USA/master/chart/chart2/Chart2.png\" width=\"500px\" />\n\n### Chart 3 : Cloud words chart of skills used in IT-jobs\n[Cloud words tool](https://timdream.org/wordcloud/#wikipedia:Cloud)\n\n<img src=\"https://raw.githubusercontent.com/Trouble404/IT-Salary-in-USA/master/chart/chart3/Chart3.png\" width=\"500px\" />\n\n---\n","source":"_posts/IT salary in USA.md","raw":"---\ntitle: IT salary in USA\ndate: 2018-01-28 08:23:32\ntags: [data analysis]\ncategories: Data Visualisation\n---\n\n## Result\n**Chart with Anlysis**\n\n**Website: SALARY DISTRIBUTION OF DIFFERENT JOBS IN USA**\n\n![image](https://raw.githubusercontent.com/Trouble404/IT-Salary-in-USA/master/readme_add_pic/page1.PNG)<!-- more -->\n![image](https://raw.githubusercontent.com/Trouble404/IT-Salary-in-USA/master/readme_add_pic/page2.PNG)\n![image](https://raw.githubusercontent.com/Trouble404/IT-Salary-in-USA/master/readme_add_pic/page3.PNG)\n---\n\n---\n\n## Data process\n### Datasets used to anlysis\n*    U.S. Technology Jobs on Dice.com  Found in [Kaggle](https://www.kaggle.com/PromptCloudHQ/us-technology-jobs-on-dicecom/data)\n*    US jobs on Monster.com  Found in [Kaggle](https://www.kaggle.com/PromptCloudHQ/us-jobs-on-monstercom)\n### Data Clean\n*    Obtain **salay** and coresspoding **states** information in US jos on Monster.com  [clean process](https://github.com/Trouble404/IT-Salary-in-USA/blob/master/chart/chart1/chart1_data.ipynb)\n*    Obtain **salay** and coresspoding **cities** information in US jos on Monster.com  [clean process](https://github.com/Trouble404/IT-Salary-in-USA/blob/master/chart/chart2/chart2_data.ipynb)\n*    Obtain **skills** information in U.S Technology Jobs on Dice.com  [clean process](https://github.com/Trouble404/IT-Salary-in-USA/blob/master/chart/chart3/chart3_data.ipynb)\n\n---\n\n## Chart Plotting\n### Chart 1: Map chart of average salay of 3 types jobs\n[Tableau address](https://public.tableau.com/profile/j.zhang#!/vizhome/chart1_23/dashborad1)\n\n<img src=\"https://raw.githubusercontent.com/Trouble404/IT-Salary-in-USA/master/chart/chart1/Chart1.png\" width=\"1000px\" />\n\n### Chart 2: Bubble chart of average salay of 3 types jobs\n[Tableau address](https://public.tableau.com/profile/j.zhang#!/vizhome/chart2_10/dasbord1)\n\n<img src=\"https://raw.githubusercontent.com/Trouble404/IT-Salary-in-USA/master/chart/chart2/Chart2.png\" width=\"500px\" />\n\n### Chart 3 : Cloud words chart of skills used in IT-jobs\n[Cloud words tool](https://timdream.org/wordcloud/#wikipedia:Cloud)\n\n<img src=\"https://raw.githubusercontent.com/Trouble404/IT-Salary-in-USA/master/chart/chart3/Chart3.png\" width=\"500px\" />\n\n---\n","slug":"IT salary in USA","published":1,"updated":"2020-04-28T12:38:55.395Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck9jwdng10003jlrc7j1i0314","content":"<h2 id=\"Result\"><a href=\"#Result\" class=\"headerlink\" title=\"Result\"></a>Result</h2><p><strong>Chart with Anlysis</strong></p>\n<p><strong>Website: SALARY DISTRIBUTION OF DIFFERENT JOBS IN USA</strong></p>\n<p><img src=\"https://raw.githubusercontent.com/Trouble404/IT-Salary-in-USA/master/readme_add_pic/page1.PNG\" alt=\"image\"><a id=\"more\"></a><br><img src=\"https://raw.githubusercontent.com/Trouble404/IT-Salary-in-USA/master/readme_add_pic/page2.PNG\" alt=\"image\"></p>\n<h2 id><a href=\"#\" class=\"headerlink\" title></a><img src=\"https://raw.githubusercontent.com/Trouble404/IT-Salary-in-USA/master/readme_add_pic/page3.PNG\" alt=\"image\"></h2><hr>\n<h2 id=\"Data-process\"><a href=\"#Data-process\" class=\"headerlink\" title=\"Data process\"></a>Data process</h2><h3 id=\"Datasets-used-to-anlysis\"><a href=\"#Datasets-used-to-anlysis\" class=\"headerlink\" title=\"Datasets used to anlysis\"></a>Datasets used to anlysis</h3><ul>\n<li>U.S. Technology Jobs on Dice.com  Found in <a href=\"https://www.kaggle.com/PromptCloudHQ/us-technology-jobs-on-dicecom/data\" target=\"_blank\" rel=\"noopener\">Kaggle</a></li>\n<li>US jobs on Monster.com  Found in <a href=\"https://www.kaggle.com/PromptCloudHQ/us-jobs-on-monstercom\" target=\"_blank\" rel=\"noopener\">Kaggle</a><h3 id=\"Data-Clean\"><a href=\"#Data-Clean\" class=\"headerlink\" title=\"Data Clean\"></a>Data Clean</h3></li>\n<li>Obtain <strong>salay</strong> and coresspoding <strong>states</strong> information in US jos on Monster.com  <a href=\"https://github.com/Trouble404/IT-Salary-in-USA/blob/master/chart/chart1/chart1_data.ipynb\" target=\"_blank\" rel=\"noopener\">clean process</a></li>\n<li>Obtain <strong>salay</strong> and coresspoding <strong>cities</strong> information in US jos on Monster.com  <a href=\"https://github.com/Trouble404/IT-Salary-in-USA/blob/master/chart/chart2/chart2_data.ipynb\" target=\"_blank\" rel=\"noopener\">clean process</a></li>\n<li>Obtain <strong>skills</strong> information in U.S Technology Jobs on Dice.com  <a href=\"https://github.com/Trouble404/IT-Salary-in-USA/blob/master/chart/chart3/chart3_data.ipynb\" target=\"_blank\" rel=\"noopener\">clean process</a></li>\n</ul>\n<hr>\n<h2 id=\"Chart-Plotting\"><a href=\"#Chart-Plotting\" class=\"headerlink\" title=\"Chart Plotting\"></a>Chart Plotting</h2><h3 id=\"Chart-1-Map-chart-of-average-salay-of-3-types-jobs\"><a href=\"#Chart-1-Map-chart-of-average-salay-of-3-types-jobs\" class=\"headerlink\" title=\"Chart 1: Map chart of average salay of 3 types jobs\"></a>Chart 1: Map chart of average salay of 3 types jobs</h3><p><a href=\"https://public.tableau.com/profile/j.zhang#!/vizhome/chart1_23/dashborad1\" target=\"_blank\" rel=\"noopener\">Tableau address</a></p>\n<p><img src=\"https://raw.githubusercontent.com/Trouble404/IT-Salary-in-USA/master/chart/chart1/Chart1.png\" width=\"1000px\"></p>\n<h3 id=\"Chart-2-Bubble-chart-of-average-salay-of-3-types-jobs\"><a href=\"#Chart-2-Bubble-chart-of-average-salay-of-3-types-jobs\" class=\"headerlink\" title=\"Chart 2: Bubble chart of average salay of 3 types jobs\"></a>Chart 2: Bubble chart of average salay of 3 types jobs</h3><p><a href=\"https://public.tableau.com/profile/j.zhang#!/vizhome/chart2_10/dasbord1\" target=\"_blank\" rel=\"noopener\">Tableau address</a></p>\n<p><img src=\"https://raw.githubusercontent.com/Trouble404/IT-Salary-in-USA/master/chart/chart2/Chart2.png\" width=\"500px\"></p>\n<h3 id=\"Chart-3-Cloud-words-chart-of-skills-used-in-IT-jobs\"><a href=\"#Chart-3-Cloud-words-chart-of-skills-used-in-IT-jobs\" class=\"headerlink\" title=\"Chart 3 : Cloud words chart of skills used in IT-jobs\"></a>Chart 3 : Cloud words chart of skills used in IT-jobs</h3><p><a href=\"https://timdream.org/wordcloud/#wikipedia:Cloud\" target=\"_blank\" rel=\"noopener\">Cloud words tool</a></p>\n<p><img src=\"https://raw.githubusercontent.com/Trouble404/IT-Salary-in-USA/master/chart/chart3/Chart3.png\" width=\"500px\"></p>\n<hr>\n","site":{"data":{}},"excerpt":"<h2 id=\"Result\"><a href=\"#Result\" class=\"headerlink\" title=\"Result\"></a>Result</h2><p><strong>Chart with Anlysis</strong></p>\n<p><strong>Website: SALARY DISTRIBUTION OF DIFFERENT JOBS IN USA</strong></p>\n<p><img src=\"https://raw.githubusercontent.com/Trouble404/IT-Salary-in-USA/master/readme_add_pic/page1.PNG\" alt=\"image\">","more":"<br><img src=\"https://raw.githubusercontent.com/Trouble404/IT-Salary-in-USA/master/readme_add_pic/page2.PNG\" alt=\"image\"></p>\n<h2 id><a href=\"#\" class=\"headerlink\" title></a><img src=\"https://raw.githubusercontent.com/Trouble404/IT-Salary-in-USA/master/readme_add_pic/page3.PNG\" alt=\"image\"></h2><hr>\n<h2 id=\"Data-process\"><a href=\"#Data-process\" class=\"headerlink\" title=\"Data process\"></a>Data process</h2><h3 id=\"Datasets-used-to-anlysis\"><a href=\"#Datasets-used-to-anlysis\" class=\"headerlink\" title=\"Datasets used to anlysis\"></a>Datasets used to anlysis</h3><ul>\n<li>U.S. Technology Jobs on Dice.com  Found in <a href=\"https://www.kaggle.com/PromptCloudHQ/us-technology-jobs-on-dicecom/data\" target=\"_blank\" rel=\"noopener\">Kaggle</a></li>\n<li>US jobs on Monster.com  Found in <a href=\"https://www.kaggle.com/PromptCloudHQ/us-jobs-on-monstercom\" target=\"_blank\" rel=\"noopener\">Kaggle</a><h3 id=\"Data-Clean\"><a href=\"#Data-Clean\" class=\"headerlink\" title=\"Data Clean\"></a>Data Clean</h3></li>\n<li>Obtain <strong>salay</strong> and coresspoding <strong>states</strong> information in US jos on Monster.com  <a href=\"https://github.com/Trouble404/IT-Salary-in-USA/blob/master/chart/chart1/chart1_data.ipynb\" target=\"_blank\" rel=\"noopener\">clean process</a></li>\n<li>Obtain <strong>salay</strong> and coresspoding <strong>cities</strong> information in US jos on Monster.com  <a href=\"https://github.com/Trouble404/IT-Salary-in-USA/blob/master/chart/chart2/chart2_data.ipynb\" target=\"_blank\" rel=\"noopener\">clean process</a></li>\n<li>Obtain <strong>skills</strong> information in U.S Technology Jobs on Dice.com  <a href=\"https://github.com/Trouble404/IT-Salary-in-USA/blob/master/chart/chart3/chart3_data.ipynb\" target=\"_blank\" rel=\"noopener\">clean process</a></li>\n</ul>\n<hr>\n<h2 id=\"Chart-Plotting\"><a href=\"#Chart-Plotting\" class=\"headerlink\" title=\"Chart Plotting\"></a>Chart Plotting</h2><h3 id=\"Chart-1-Map-chart-of-average-salay-of-3-types-jobs\"><a href=\"#Chart-1-Map-chart-of-average-salay-of-3-types-jobs\" class=\"headerlink\" title=\"Chart 1: Map chart of average salay of 3 types jobs\"></a>Chart 1: Map chart of average salay of 3 types jobs</h3><p><a href=\"https://public.tableau.com/profile/j.zhang#!/vizhome/chart1_23/dashborad1\" target=\"_blank\" rel=\"noopener\">Tableau address</a></p>\n<p><img src=\"https://raw.githubusercontent.com/Trouble404/IT-Salary-in-USA/master/chart/chart1/Chart1.png\" width=\"1000px\"></p>\n<h3 id=\"Chart-2-Bubble-chart-of-average-salay-of-3-types-jobs\"><a href=\"#Chart-2-Bubble-chart-of-average-salay-of-3-types-jobs\" class=\"headerlink\" title=\"Chart 2: Bubble chart of average salay of 3 types jobs\"></a>Chart 2: Bubble chart of average salay of 3 types jobs</h3><p><a href=\"https://public.tableau.com/profile/j.zhang#!/vizhome/chart2_10/dasbord1\" target=\"_blank\" rel=\"noopener\">Tableau address</a></p>\n<p><img src=\"https://raw.githubusercontent.com/Trouble404/IT-Salary-in-USA/master/chart/chart2/Chart2.png\" width=\"500px\"></p>\n<h3 id=\"Chart-3-Cloud-words-chart-of-skills-used-in-IT-jobs\"><a href=\"#Chart-3-Cloud-words-chart-of-skills-used-in-IT-jobs\" class=\"headerlink\" title=\"Chart 3 : Cloud words chart of skills used in IT-jobs\"></a>Chart 3 : Cloud words chart of skills used in IT-jobs</h3><p><a href=\"https://timdream.org/wordcloud/#wikipedia:Cloud\" target=\"_blank\" rel=\"noopener\">Cloud words tool</a></p>\n<p><img src=\"https://raw.githubusercontent.com/Trouble404/IT-Salary-in-USA/master/chart/chart3/Chart3.png\" width=\"500px\"></p>\n<hr>"},{"title":"Ubuntu","date":"2018-11-12T01:04:14.000Z","_content":"\n## Ubuntu 配置\n\n### Anaconda3\n[官网下载安装包](https://www.anaconda.com/download/#linux)  \n**For Linux Installer**<!-- more -->\n\n打开命令行\n1. /path/filename 替换为安装包路径\n```\nsha256sum /path/filename\n```\n\n2. 安装\n```\nbash ~/path/filename\n```\n\n3. 安装过程中出现说明以及选择的地方选择YES\n\n4. 修改环境变量\n\n\n```\nvim ~/.bashrc\n```\n按\"i\"进入编辑模式，在最后一行添加\n```\nexport PATH=~/anaconda3/bin:$PATH\n```\n然后重启环境变量\n```\nsource ~/.bashrc\n```\n\n5. 配置完成，命令行输入\n```\nanaconda-navigator\n```\n6. 启动\n\n### Anaconda环境管理\n**断开VPN!!!**\n\n1. 创建新环境```\nconda create -n pytorch python=3.5\n```\n\n2. 启动环境```\nsource activate pytorch\n```\n\n3. 安装Pytorch以及torchvision [具体版本命令网址](https://pytorch.org/)```\nconda install pytorch-cpu torchvision-cpu -c pytorch\n```\n\n4. 关联环境到Jupyter-Notebook \n```\nconda install nb_conda\n```\n\n### 切换国内源\n1. 升级pip>10.0\n```\npip install -i https://pypi.tuna.tsinghua.edu.cn/simple pip -U\n```\n2. 设置\n```\npip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple\n```\n\n3. Anaconda 镜像\n```\nconda config --add channels 'https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/'\nconda config --set show_channel_urls yes\n```\n\n\n\n\n\n\n\n\n\n","source":"_posts/LInux.md","raw":"---\ntitle: Ubuntu\ndate: 2018-11-12 09:04:14\ntags: [Linux]\ncategories: System\n---\n\n## Ubuntu 配置\n\n### Anaconda3\n[官网下载安装包](https://www.anaconda.com/download/#linux)  \n**For Linux Installer**<!-- more -->\n\n打开命令行\n1. /path/filename 替换为安装包路径\n```\nsha256sum /path/filename\n```\n\n2. 安装\n```\nbash ~/path/filename\n```\n\n3. 安装过程中出现说明以及选择的地方选择YES\n\n4. 修改环境变量\n\n\n```\nvim ~/.bashrc\n```\n按\"i\"进入编辑模式，在最后一行添加\n```\nexport PATH=~/anaconda3/bin:$PATH\n```\n然后重启环境变量\n```\nsource ~/.bashrc\n```\n\n5. 配置完成，命令行输入\n```\nanaconda-navigator\n```\n6. 启动\n\n### Anaconda环境管理\n**断开VPN!!!**\n\n1. 创建新环境```\nconda create -n pytorch python=3.5\n```\n\n2. 启动环境```\nsource activate pytorch\n```\n\n3. 安装Pytorch以及torchvision [具体版本命令网址](https://pytorch.org/)```\nconda install pytorch-cpu torchvision-cpu -c pytorch\n```\n\n4. 关联环境到Jupyter-Notebook \n```\nconda install nb_conda\n```\n\n### 切换国内源\n1. 升级pip>10.0\n```\npip install -i https://pypi.tuna.tsinghua.edu.cn/simple pip -U\n```\n2. 设置\n```\npip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple\n```\n\n3. Anaconda 镜像\n```\nconda config --add channels 'https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/'\nconda config --set show_channel_urls yes\n```\n\n\n\n\n\n\n\n\n\n","slug":"LInux","published":1,"updated":"2020-04-28T12:38:55.395Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck9jwdng30005jlrcozi1jzkz","content":"<h2 id=\"Ubuntu-配置\"><a href=\"#Ubuntu-配置\" class=\"headerlink\" title=\"Ubuntu 配置\"></a>Ubuntu 配置</h2><h3 id=\"Anaconda3\"><a href=\"#Anaconda3\" class=\"headerlink\" title=\"Anaconda3\"></a>Anaconda3</h3><p><a href=\"https://www.anaconda.com/download/#linux\" target=\"_blank\" rel=\"noopener\">官网下载安装包</a><br><strong>For Linux Installer</strong><a id=\"more\"></a></p>\n<p>打开命令行</p>\n<ol>\n<li><p>/path/filename 替换为安装包路径</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sha256sum /path/filename</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>安装</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">bash ~/path/filename</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>安装过程中出现说明以及选择的地方选择YES</p>\n</li>\n<li><p>修改环境变量</p>\n</li>\n</ol>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">vim ~/.bashrc</span><br></pre></td></tr></table></figure>\n<p>按”i”进入编辑模式，在最后一行添加<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">export PATH=~/anaconda3/bin:$PATH</span><br></pre></td></tr></table></figure></p>\n<p>然后重启环境变量<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">source ~/.bashrc</span><br></pre></td></tr></table></figure></p>\n<ol start=\"5\">\n<li><p>配置完成，命令行输入</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">anaconda-navigator</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>启动</p>\n</li>\n</ol>\n<h3 id=\"Anaconda环境管理\"><a href=\"#Anaconda环境管理\" class=\"headerlink\" title=\"Anaconda环境管理\"></a>Anaconda环境管理</h3><p><strong>断开VPN!!!</strong></p>\n<ol>\n<li><p>创建新环境<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">conda create -n pytorch python=3.5</span><br></pre></td></tr></table></figure></p>\n</li>\n<li><p>启动环境<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">source activate pytorch</span><br></pre></td></tr></table></figure></p>\n</li>\n<li><p>安装Pytorch以及torchvision <a href=\"https://pytorch.org/\" target=\"_blank\" rel=\"noopener\">具体版本命令网址</a><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">conda install pytorch-cpu torchvision-cpu -c pytorch</span><br></pre></td></tr></table></figure></p>\n</li>\n<li><p>关联环境到Jupyter-Notebook </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">conda install nb_conda</span><br></pre></td></tr></table></figure>\n</li>\n</ol>\n<h3 id=\"切换国内源\"><a href=\"#切换国内源\" class=\"headerlink\" title=\"切换国内源\"></a>切换国内源</h3><ol>\n<li><p>升级pip&gt;10.0</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">pip install -i https://pypi.tuna.tsinghua.edu.cn/simple pip -U</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>设置</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>Anaconda 镜像</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">conda config --add channels &apos;https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/&apos;</span><br><span class=\"line\">conda config --set show_channel_urls yes</span><br></pre></td></tr></table></figure>\n</li>\n</ol>\n","site":{"data":{}},"excerpt":"<h2 id=\"Ubuntu-配置\"><a href=\"#Ubuntu-配置\" class=\"headerlink\" title=\"Ubuntu 配置\"></a>Ubuntu 配置</h2><h3 id=\"Anaconda3\"><a href=\"#Anaconda3\" class=\"headerlink\" title=\"Anaconda3\"></a>Anaconda3</h3><p><a href=\"https://www.anaconda.com/download/#linux\" target=\"_blank\" rel=\"noopener\">官网下载安装包</a><br><strong>For Linux Installer</strong>","more":"</p>\n<p>打开命令行</p>\n<ol>\n<li><p>/path/filename 替换为安装包路径</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sha256sum /path/filename</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>安装</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">bash ~/path/filename</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>安装过程中出现说明以及选择的地方选择YES</p>\n</li>\n<li><p>修改环境变量</p>\n</li>\n</ol>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">vim ~/.bashrc</span><br></pre></td></tr></table></figure>\n<p>按”i”进入编辑模式，在最后一行添加<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">export PATH=~/anaconda3/bin:$PATH</span><br></pre></td></tr></table></figure></p>\n<p>然后重启环境变量<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">source ~/.bashrc</span><br></pre></td></tr></table></figure></p>\n<ol start=\"5\">\n<li><p>配置完成，命令行输入</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">anaconda-navigator</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>启动</p>\n</li>\n</ol>\n<h3 id=\"Anaconda环境管理\"><a href=\"#Anaconda环境管理\" class=\"headerlink\" title=\"Anaconda环境管理\"></a>Anaconda环境管理</h3><p><strong>断开VPN!!!</strong></p>\n<ol>\n<li><p>创建新环境<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">conda create -n pytorch python=3.5</span><br></pre></td></tr></table></figure></p>\n</li>\n<li><p>启动环境<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">source activate pytorch</span><br></pre></td></tr></table></figure></p>\n</li>\n<li><p>安装Pytorch以及torchvision <a href=\"https://pytorch.org/\" target=\"_blank\" rel=\"noopener\">具体版本命令网址</a><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">conda install pytorch-cpu torchvision-cpu -c pytorch</span><br></pre></td></tr></table></figure></p>\n</li>\n<li><p>关联环境到Jupyter-Notebook </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">conda install nb_conda</span><br></pre></td></tr></table></figure>\n</li>\n</ol>\n<h3 id=\"切换国内源\"><a href=\"#切换国内源\" class=\"headerlink\" title=\"切换国内源\"></a>切换国内源</h3><ol>\n<li><p>升级pip&gt;10.0</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">pip install -i https://pypi.tuna.tsinghua.edu.cn/simple pip -U</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>设置</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>Anaconda 镜像</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">conda config --add channels &apos;https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/&apos;</span><br><span class=\"line\">conda config --set show_channel_urls yes</span><br></pre></td></tr></table></figure>\n</li>\n</ol>"},{"title":"MXnet 配置","date":"2018-01-28T12:04:14.000Z","_content":"\n\n# MXnet GPU version configuration (Windows 10. GTX960M)\n## Tools\n* [Microsoft Visual Studio 2015](https://www.visualstudio.com/zh-hans/vs/older-downloads/)\n* [CUDA 9.0](http://docs.nvidia.com/cuda/cuda-installation-guide-microsoft-windows/)\n* [cuDNN7](https://developer.nvidia.com/cudnn)\n* [CMake](https://cmake.org/)\n* [OpenCV3.0](https://sourceforge.net/projects/opencvlibrary/files/opencv-win/3.0.0/opencv-3.0.0.exe/download)\n* [OpenBLAS](https://sourceforge.net/projects/openblas/files/v0.2.14/)\n* [Anaconda](https://www.anaconda.com/download/)\n* [Graphviz](https://graphviz.gitlab.io/_pages/Download/Download_windows.html)\n<!-- more -->\n---\n\n## Methods\n* Step 1: Install VS2015\n\n* Step 2: Install CUDA 9.0\n\n* Step 3: Install cuDNN7 解压后把cudnn目录下的bin目录加到PATH环境变量里\n\n* Step 4: Install Opencv 下载并解压，然后创建环境变量OpenCV_DIR，把opencv/build目录添加进去,把\\opencv\\build\\x64\\vc12\\bin和\\opencv\\build\\x86\\vc12\\bin添加到PATH路径\n\n* Step 5: Install openBLAS 需要下载mingw64_dll.zip和OpenBLAS-v0.2.14-Win64-int64.zip两个文件. 创建环境变量 OpenBLAS_HOME，把openBLAS根目录加进去,把DLL所在目录需要添加到环境变量path中. 创建 \"C:\\Program files (x86)\\OpenBLAS\\\" 复制相关文件进去\n\n* Step 6: Install Anaconda 把安装路径添加到PATH里去\n\n* Step 7: Install MXnet 创建MXnet 文件夹 然后使用命令行CD至该文件夹\n  {% codeblock %}\n  git clone --recursive https://github.com/dmlc/mxnet\n  {% endcodeblock %}\n\n  在根目录创建build文件夹  (补充操作 目前未使用： 打开make文件夹的config.mk 文件 修改USE_CUDNN = 0 to USE_CUDNN = 1, 修改USE_BLAS = openBLAS )\n\n* Step 8: Install Cmake Configure and Genreate VS工程 **mxnet.sln**  Configure的配置选择如下图\n![image](https://raw.githubusercontent.com/Trouble404/Kaggle-Dog-breed-Identification/master/readme_pic_add/cmake.PNG)\n\n* Step 9: 使用VS2015 打开mxnet.sln 切换成release模式 64位 然后启动编译 编译完成后，在mxnet_build\\Release目录下生成了**libmxnet.dll**文件\n\n* Step 10: Install graphviz library 添加安装路径到环境变量path\n\n* Step 11: 使用Anaconda的命令行 新建一个虚拟环境\n  {% codeblock %}\n  conda create  --name MXNet python=2.7\n  {% endcodeblock %}\n\n  MXNET目前不太适配python3, 激活环境 \n  {% codeblock %}\n  activate MXNet\n  {% endcodeblock %}\n\n* Step 12: cd 至mxnet文件夹的python文件夹里， 拷贝如图的各个文件到该文件夹 \n\n  ![iamge](https://raw.githubusercontent.com/Trouble404/Kaggle-Dog-breed-Identification/master/readme_pic_add/dll.PNG)\n\n  然后使用\n  {% codeblock %}\n  python setup.py install\n  {% endcodeblock %}\n\n\n\n* Step 13: 再次拷贝上图的文件到Anaconda的MXNet虚拟环境的 Lib\\site-packages\\mxnet-版本名.egg\\mxnet 中 并且添加此路径到环境变量path中\n\n* Step 14: \n  {% codeblock %}\n  conda install nb_conda\n  {% endcodeblock %}\n\n  激活这个虚拟环境到juptyer notebook里面  打开jupyter book就可以进行测试了\n\n![iamge](https://raw.githubusercontent.com/Trouble404/Kaggle-Dog-breed-Identification/master/readme_pic_add/test.png)\n\n\n","source":"_posts/MXnet_Config.md","raw":"---\ntitle: MXnet 配置\ndate: 2018-01-28 20:04:14\ntags: [MXnet]\ncategories: Machine Learning\n---\n\n\n# MXnet GPU version configuration (Windows 10. GTX960M)\n## Tools\n* [Microsoft Visual Studio 2015](https://www.visualstudio.com/zh-hans/vs/older-downloads/)\n* [CUDA 9.0](http://docs.nvidia.com/cuda/cuda-installation-guide-microsoft-windows/)\n* [cuDNN7](https://developer.nvidia.com/cudnn)\n* [CMake](https://cmake.org/)\n* [OpenCV3.0](https://sourceforge.net/projects/opencvlibrary/files/opencv-win/3.0.0/opencv-3.0.0.exe/download)\n* [OpenBLAS](https://sourceforge.net/projects/openblas/files/v0.2.14/)\n* [Anaconda](https://www.anaconda.com/download/)\n* [Graphviz](https://graphviz.gitlab.io/_pages/Download/Download_windows.html)\n<!-- more -->\n---\n\n## Methods\n* Step 1: Install VS2015\n\n* Step 2: Install CUDA 9.0\n\n* Step 3: Install cuDNN7 解压后把cudnn目录下的bin目录加到PATH环境变量里\n\n* Step 4: Install Opencv 下载并解压，然后创建环境变量OpenCV_DIR，把opencv/build目录添加进去,把\\opencv\\build\\x64\\vc12\\bin和\\opencv\\build\\x86\\vc12\\bin添加到PATH路径\n\n* Step 5: Install openBLAS 需要下载mingw64_dll.zip和OpenBLAS-v0.2.14-Win64-int64.zip两个文件. 创建环境变量 OpenBLAS_HOME，把openBLAS根目录加进去,把DLL所在目录需要添加到环境变量path中. 创建 \"C:\\Program files (x86)\\OpenBLAS\\\" 复制相关文件进去\n\n* Step 6: Install Anaconda 把安装路径添加到PATH里去\n\n* Step 7: Install MXnet 创建MXnet 文件夹 然后使用命令行CD至该文件夹\n  {% codeblock %}\n  git clone --recursive https://github.com/dmlc/mxnet\n  {% endcodeblock %}\n\n  在根目录创建build文件夹  (补充操作 目前未使用： 打开make文件夹的config.mk 文件 修改USE_CUDNN = 0 to USE_CUDNN = 1, 修改USE_BLAS = openBLAS )\n\n* Step 8: Install Cmake Configure and Genreate VS工程 **mxnet.sln**  Configure的配置选择如下图\n![image](https://raw.githubusercontent.com/Trouble404/Kaggle-Dog-breed-Identification/master/readme_pic_add/cmake.PNG)\n\n* Step 9: 使用VS2015 打开mxnet.sln 切换成release模式 64位 然后启动编译 编译完成后，在mxnet_build\\Release目录下生成了**libmxnet.dll**文件\n\n* Step 10: Install graphviz library 添加安装路径到环境变量path\n\n* Step 11: 使用Anaconda的命令行 新建一个虚拟环境\n  {% codeblock %}\n  conda create  --name MXNet python=2.7\n  {% endcodeblock %}\n\n  MXNET目前不太适配python3, 激活环境 \n  {% codeblock %}\n  activate MXNet\n  {% endcodeblock %}\n\n* Step 12: cd 至mxnet文件夹的python文件夹里， 拷贝如图的各个文件到该文件夹 \n\n  ![iamge](https://raw.githubusercontent.com/Trouble404/Kaggle-Dog-breed-Identification/master/readme_pic_add/dll.PNG)\n\n  然后使用\n  {% codeblock %}\n  python setup.py install\n  {% endcodeblock %}\n\n\n\n* Step 13: 再次拷贝上图的文件到Anaconda的MXNet虚拟环境的 Lib\\site-packages\\mxnet-版本名.egg\\mxnet 中 并且添加此路径到环境变量path中\n\n* Step 14: \n  {% codeblock %}\n  conda install nb_conda\n  {% endcodeblock %}\n\n  激活这个虚拟环境到juptyer notebook里面  打开jupyter book就可以进行测试了\n\n![iamge](https://raw.githubusercontent.com/Trouble404/Kaggle-Dog-breed-Identification/master/readme_pic_add/test.png)\n\n\n","slug":"MXnet_Config","published":1,"updated":"2020-04-28T12:38:55.395Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck9jwdng40006jlrcowjfpwux","content":"<h1 id=\"MXnet-GPU-version-configuration-Windows-10-GTX960M\"><a href=\"#MXnet-GPU-version-configuration-Windows-10-GTX960M\" class=\"headerlink\" title=\"MXnet GPU version configuration (Windows 10. GTX960M)\"></a>MXnet GPU version configuration (Windows 10. GTX960M)</h1><h2 id=\"Tools\"><a href=\"#Tools\" class=\"headerlink\" title=\"Tools\"></a>Tools</h2><ul>\n<li><a href=\"https://www.visualstudio.com/zh-hans/vs/older-downloads/\" target=\"_blank\" rel=\"noopener\">Microsoft Visual Studio 2015</a></li>\n<li><a href=\"http://docs.nvidia.com/cuda/cuda-installation-guide-microsoft-windows/\" target=\"_blank\" rel=\"noopener\">CUDA 9.0</a></li>\n<li><a href=\"https://developer.nvidia.com/cudnn\" target=\"_blank\" rel=\"noopener\">cuDNN7</a></li>\n<li><a href=\"https://cmake.org/\" target=\"_blank\" rel=\"noopener\">CMake</a></li>\n<li><a href=\"https://sourceforge.net/projects/opencvlibrary/files/opencv-win/3.0.0/opencv-3.0.0.exe/download\" target=\"_blank\" rel=\"noopener\">OpenCV3.0</a></li>\n<li><a href=\"https://sourceforge.net/projects/openblas/files/v0.2.14/\" target=\"_blank\" rel=\"noopener\">OpenBLAS</a></li>\n<li><a href=\"https://www.anaconda.com/download/\" target=\"_blank\" rel=\"noopener\">Anaconda</a></li>\n<li><a href=\"https://graphviz.gitlab.io/_pages/Download/Download_windows.html\" target=\"_blank\" rel=\"noopener\">Graphviz</a><a id=\"more\"></a>\n</li>\n</ul>\n<hr>\n<h2 id=\"Methods\"><a href=\"#Methods\" class=\"headerlink\" title=\"Methods\"></a>Methods</h2><ul>\n<li><p>Step 1: Install VS2015</p>\n</li>\n<li><p>Step 2: Install CUDA 9.0</p>\n</li>\n<li><p>Step 3: Install cuDNN7 解压后把cudnn目录下的bin目录加到PATH环境变量里</p>\n</li>\n<li><p>Step 4: Install Opencv 下载并解压，然后创建环境变量OpenCV_DIR，把opencv/build目录添加进去,把\\opencv\\build\\x64\\vc12\\bin和\\opencv\\build\\x86\\vc12\\bin添加到PATH路径</p>\n</li>\n<li><p>Step 5: Install openBLAS 需要下载mingw64_dll.zip和OpenBLAS-v0.2.14-Win64-int64.zip两个文件. 创建环境变量 OpenBLAS_HOME，把openBLAS根目录加进去,把DLL所在目录需要添加到环境变量path中. 创建 “C:\\Program files (x86)\\OpenBLAS\\” 复制相关文件进去</p>\n</li>\n<li><p>Step 6: Install Anaconda 把安装路径添加到PATH里去</p>\n</li>\n<li><p>Step 7: Install MXnet 创建MXnet 文件夹 然后使用命令行CD至该文件夹</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git clone --recursive https://github.com/dmlc/mxnet</span><br></pre></td></tr></table></figure>\n<p>在根目录创建build文件夹  (补充操作 目前未使用： 打开make文件夹的config.mk 文件 修改USE_CUDNN = 0 to USE_CUDNN = 1, 修改USE_BLAS = openBLAS )</p>\n</li>\n<li><p>Step 8: Install Cmake Configure and Genreate VS工程 <strong>mxnet.sln</strong>  Configure的配置选择如下图<br><img src=\"https://raw.githubusercontent.com/Trouble404/Kaggle-Dog-breed-Identification/master/readme_pic_add/cmake.PNG\" alt=\"image\"></p>\n</li>\n<li><p>Step 9: 使用VS2015 打开mxnet.sln 切换成release模式 64位 然后启动编译 编译完成后，在mxnet_build\\Release目录下生成了<strong>libmxnet.dll</strong>文件</p>\n</li>\n<li><p>Step 10: Install graphviz library 添加安装路径到环境变量path</p>\n</li>\n<li><p>Step 11: 使用Anaconda的命令行 新建一个虚拟环境</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">conda create  --name MXNet python=2.7</span><br></pre></td></tr></table></figure>\n<p>MXNET目前不太适配python3, 激活环境 </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">activate MXNet</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>Step 12: cd 至mxnet文件夹的python文件夹里， 拷贝如图的各个文件到该文件夹 </p>\n<p><img src=\"https://raw.githubusercontent.com/Trouble404/Kaggle-Dog-breed-Identification/master/readme_pic_add/dll.PNG\" alt=\"iamge\"></p>\n<p>然后使用</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">python setup.py install</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<ul>\n<li><p>Step 13: 再次拷贝上图的文件到Anaconda的MXNet虚拟环境的 Lib\\site-packages\\mxnet-版本名.egg\\mxnet 中 并且添加此路径到环境变量path中</p>\n</li>\n<li><p>Step 14: </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">conda install nb_conda</span><br></pre></td></tr></table></figure>\n<p>激活这个虚拟环境到juptyer notebook里面  打开jupyter book就可以进行测试了</p>\n</li>\n</ul>\n<p><img src=\"https://raw.githubusercontent.com/Trouble404/Kaggle-Dog-breed-Identification/master/readme_pic_add/test.png\" alt=\"iamge\"></p>\n","site":{"data":{}},"excerpt":"<h1 id=\"MXnet-GPU-version-configuration-Windows-10-GTX960M\"><a href=\"#MXnet-GPU-version-configuration-Windows-10-GTX960M\" class=\"headerlink\" title=\"MXnet GPU version configuration (Windows 10. GTX960M)\"></a>MXnet GPU version configuration (Windows 10. GTX960M)</h1><h2 id=\"Tools\"><a href=\"#Tools\" class=\"headerlink\" title=\"Tools\"></a>Tools</h2><ul>\n<li><a href=\"https://www.visualstudio.com/zh-hans/vs/older-downloads/\" target=\"_blank\" rel=\"noopener\">Microsoft Visual Studio 2015</a></li>\n<li><a href=\"http://docs.nvidia.com/cuda/cuda-installation-guide-microsoft-windows/\" target=\"_blank\" rel=\"noopener\">CUDA 9.0</a></li>\n<li><a href=\"https://developer.nvidia.com/cudnn\" target=\"_blank\" rel=\"noopener\">cuDNN7</a></li>\n<li><a href=\"https://cmake.org/\" target=\"_blank\" rel=\"noopener\">CMake</a></li>\n<li><a href=\"https://sourceforge.net/projects/opencvlibrary/files/opencv-win/3.0.0/opencv-3.0.0.exe/download\" target=\"_blank\" rel=\"noopener\">OpenCV3.0</a></li>\n<li><a href=\"https://sourceforge.net/projects/openblas/files/v0.2.14/\" target=\"_blank\" rel=\"noopener\">OpenBLAS</a></li>\n<li><a href=\"https://www.anaconda.com/download/\" target=\"_blank\" rel=\"noopener\">Anaconda</a></li>\n<li><a href=\"https://graphviz.gitlab.io/_pages/Download/Download_windows.html\" target=\"_blank\" rel=\"noopener\">Graphviz</a>","more":"</li>\n</ul>\n<hr>\n<h2 id=\"Methods\"><a href=\"#Methods\" class=\"headerlink\" title=\"Methods\"></a>Methods</h2><ul>\n<li><p>Step 1: Install VS2015</p>\n</li>\n<li><p>Step 2: Install CUDA 9.0</p>\n</li>\n<li><p>Step 3: Install cuDNN7 解压后把cudnn目录下的bin目录加到PATH环境变量里</p>\n</li>\n<li><p>Step 4: Install Opencv 下载并解压，然后创建环境变量OpenCV_DIR，把opencv/build目录添加进去,把\\opencv\\build\\x64\\vc12\\bin和\\opencv\\build\\x86\\vc12\\bin添加到PATH路径</p>\n</li>\n<li><p>Step 5: Install openBLAS 需要下载mingw64_dll.zip和OpenBLAS-v0.2.14-Win64-int64.zip两个文件. 创建环境变量 OpenBLAS_HOME，把openBLAS根目录加进去,把DLL所在目录需要添加到环境变量path中. 创建 “C:\\Program files (x86)\\OpenBLAS\\” 复制相关文件进去</p>\n</li>\n<li><p>Step 6: Install Anaconda 把安装路径添加到PATH里去</p>\n</li>\n<li><p>Step 7: Install MXnet 创建MXnet 文件夹 然后使用命令行CD至该文件夹</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git clone --recursive https://github.com/dmlc/mxnet</span><br></pre></td></tr></table></figure>\n<p>在根目录创建build文件夹  (补充操作 目前未使用： 打开make文件夹的config.mk 文件 修改USE_CUDNN = 0 to USE_CUDNN = 1, 修改USE_BLAS = openBLAS )</p>\n</li>\n<li><p>Step 8: Install Cmake Configure and Genreate VS工程 <strong>mxnet.sln</strong>  Configure的配置选择如下图<br><img src=\"https://raw.githubusercontent.com/Trouble404/Kaggle-Dog-breed-Identification/master/readme_pic_add/cmake.PNG\" alt=\"image\"></p>\n</li>\n<li><p>Step 9: 使用VS2015 打开mxnet.sln 切换成release模式 64位 然后启动编译 编译完成后，在mxnet_build\\Release目录下生成了<strong>libmxnet.dll</strong>文件</p>\n</li>\n<li><p>Step 10: Install graphviz library 添加安装路径到环境变量path</p>\n</li>\n<li><p>Step 11: 使用Anaconda的命令行 新建一个虚拟环境</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">conda create  --name MXNet python=2.7</span><br></pre></td></tr></table></figure>\n<p>MXNET目前不太适配python3, 激活环境 </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">activate MXNet</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>Step 12: cd 至mxnet文件夹的python文件夹里， 拷贝如图的各个文件到该文件夹 </p>\n<p><img src=\"https://raw.githubusercontent.com/Trouble404/Kaggle-Dog-breed-Identification/master/readme_pic_add/dll.PNG\" alt=\"iamge\"></p>\n<p>然后使用</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">python setup.py install</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<ul>\n<li><p>Step 13: 再次拷贝上图的文件到Anaconda的MXNet虚拟环境的 Lib\\site-packages\\mxnet-版本名.egg\\mxnet 中 并且添加此路径到环境变量path中</p>\n</li>\n<li><p>Step 14: </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">conda install nb_conda</span><br></pre></td></tr></table></figure>\n<p>激活这个虚拟环境到juptyer notebook里面  打开jupyter book就可以进行测试了</p>\n</li>\n</ul>\n<p><img src=\"https://raw.githubusercontent.com/Trouble404/Kaggle-Dog-breed-Identification/master/readme_pic_add/test.png\" alt=\"iamge\"></p>"},{"title":"Using Machine Learning in NBA","date":"2018-01-31T04:23:32.000Z","_content":"\n## Fan-map plotting\n*   Obtain the game data in 2000~2017 in NBA\n*   Obtain the follwers of 2015~2017 NBA rookies by [twittR](https://www.rdocumentation.org/packages/twitteR/versions/1.1.9) package\n*   Obtain location information and plot the fan-map by [Tableau](https://www.tableau.com/zh-cn)\n<!-- more -->\n---\n\n## Machine learning apply\n### all-star players in 2017 prediction\n*    [PCA 主成份分析](https://github.com/Trouble404/NBA-with-Machine-Learning/blob/master/nerual_network/pca/nba%20all%20star/pca%20of%20nba%20all%20players_00-17.ipynb) in all-star player prediction\n*  [Keras](https://keras-cn.readthedocs.io/en/latest/)是一个高层神经网络API，Keras由纯Python编写而成并基[Tensorflow](https://github.com/tensorflow/tensorflow)后端\n*    [All-star players in 2017 prediction](https://github.com/Trouble404/NBA-with-Machine-Learning/blob/master/nerual_network/prediction/nba_all_star_prediction/nba%20all%20star%20prediction.ipynb)\n*    [Tensorboard](https://www.tensorflow.org/get_started/summaries_and_tensorboard) is used to compare and choose better model.\n![image](https://raw.githubusercontent.com/Trouble404/NBA-with-Machine-Learning/master/readme_add_pic/full-list.PNG)\n\n### Best rookies in 2017 prediction\n*    [PCA 主成份分析](https://github.com/Trouble404/NBA-with-Machine-Learning/blob/master/nerual_network/pca/nba%20rookies%20best/pca%20process.ipynb) in best rookies prediction\n* [Keras](https://keras-cn.readthedocs.io/en/latest/)是一个高层神经网络API，Keras由纯Python编写而成并基[Tensorflow](https://github.com/tensorflow/tensorflow)后端\n*    [Best rookies in 2017 prediction](https://github.com/Trouble404/NBA-with-Machine-Learning/blob/master/nerual_network/prediction/rookies_best/normal%20prediction/Best%205%20rookie.ipynb)\n*    [First and Second rookies in 2017 prediction](https://github.com/Trouble404/NBA-with-Machine-Learning/blob/master/nerual_network/prediction/rookies_first_second/first_second_rookie-tensorboard.ipynb) **Muti-classes classification application**\n*    [Tensorboard](https://www.tensorflow.org/get_started/summaries_and_tensorboard) is used to compare and choose better model.\n![image](https://raw.githubusercontent.com/Trouble404/NBA-with-Machine-Learning/master/readme_add_pic/firstrookies.PNG)\n\n### Potential all-star players in 2015~16 prediction\n*    [PCA 主成份分析](https://github.com/Trouble404/NBA-with-Machine-Learning/blob/master/nerual_network/pca/nba%20rookies%20all%20star/pca%20process.ipynb) in potential rookies prediction\n* [Keras](https://keras-cn.readthedocs.io/en/latest/)是一个高层神经网络API，Keras由纯Python编写而成并基[Tensorflow](https://github.com/tensorflow/tensorflow)后端\n*    [Potential rookies in 2015~16 prediction](https://github.com/Trouble404/NBA-with-Machine-Learning/blob/master/nerual_network/prediction/rookies_all_star_prediction/All%20star%20rookie%20without%20pca.ipynb)\n*    [Tensorboard](https://www.tensorflow.org/get_started/summaries_and_tensorboard) is used to compare and choose better model.\n![image](https://raw.githubusercontent.com/Trouble404/NBA-with-Machine-Learning/master/readme_add_pic/rookies-allstar.PNG)\n\n---\n\n## Result\n**Website: [2017 NBA ALL-STAR AND BEST ROOKIES PREDICTION WITH FAN-MAP](https://d2v4olxsjbfep7.cloudfront.net/panels.html)**\n---\n\nexample: \n*  Neural network model\n\n![image](https://raw.githubusercontent.com/Trouble404/NBA-with-Machine-Learning/master/readme_add_pic/model.png)\n\n*  2D PCA (linear unseparable)\n\n![image](https://raw.githubusercontent.com/Trouble404/NBA-with-Machine-Learning/master/readme_add_pic/pca.PNG)\n\n\n*  Tensorboard check\n\n![image](https://raw.githubusercontent.com/Trouble404/NBA-with-Machine-Learning/master/readme_add_pic/tensor_board.PNG)\n","source":"_posts/Machine Learning in NBA.md","raw":"---\ntitle: Using Machine Learning in NBA\ndate: 2018-01-31 12:23:32\ntags: [Machine Learning, NBA]\ncategories: Machine Learning\n---\n\n## Fan-map plotting\n*   Obtain the game data in 2000~2017 in NBA\n*   Obtain the follwers of 2015~2017 NBA rookies by [twittR](https://www.rdocumentation.org/packages/twitteR/versions/1.1.9) package\n*   Obtain location information and plot the fan-map by [Tableau](https://www.tableau.com/zh-cn)\n<!-- more -->\n---\n\n## Machine learning apply\n### all-star players in 2017 prediction\n*    [PCA 主成份分析](https://github.com/Trouble404/NBA-with-Machine-Learning/blob/master/nerual_network/pca/nba%20all%20star/pca%20of%20nba%20all%20players_00-17.ipynb) in all-star player prediction\n*  [Keras](https://keras-cn.readthedocs.io/en/latest/)是一个高层神经网络API，Keras由纯Python编写而成并基[Tensorflow](https://github.com/tensorflow/tensorflow)后端\n*    [All-star players in 2017 prediction](https://github.com/Trouble404/NBA-with-Machine-Learning/blob/master/nerual_network/prediction/nba_all_star_prediction/nba%20all%20star%20prediction.ipynb)\n*    [Tensorboard](https://www.tensorflow.org/get_started/summaries_and_tensorboard) is used to compare and choose better model.\n![image](https://raw.githubusercontent.com/Trouble404/NBA-with-Machine-Learning/master/readme_add_pic/full-list.PNG)\n\n### Best rookies in 2017 prediction\n*    [PCA 主成份分析](https://github.com/Trouble404/NBA-with-Machine-Learning/blob/master/nerual_network/pca/nba%20rookies%20best/pca%20process.ipynb) in best rookies prediction\n* [Keras](https://keras-cn.readthedocs.io/en/latest/)是一个高层神经网络API，Keras由纯Python编写而成并基[Tensorflow](https://github.com/tensorflow/tensorflow)后端\n*    [Best rookies in 2017 prediction](https://github.com/Trouble404/NBA-with-Machine-Learning/blob/master/nerual_network/prediction/rookies_best/normal%20prediction/Best%205%20rookie.ipynb)\n*    [First and Second rookies in 2017 prediction](https://github.com/Trouble404/NBA-with-Machine-Learning/blob/master/nerual_network/prediction/rookies_first_second/first_second_rookie-tensorboard.ipynb) **Muti-classes classification application**\n*    [Tensorboard](https://www.tensorflow.org/get_started/summaries_and_tensorboard) is used to compare and choose better model.\n![image](https://raw.githubusercontent.com/Trouble404/NBA-with-Machine-Learning/master/readme_add_pic/firstrookies.PNG)\n\n### Potential all-star players in 2015~16 prediction\n*    [PCA 主成份分析](https://github.com/Trouble404/NBA-with-Machine-Learning/blob/master/nerual_network/pca/nba%20rookies%20all%20star/pca%20process.ipynb) in potential rookies prediction\n* [Keras](https://keras-cn.readthedocs.io/en/latest/)是一个高层神经网络API，Keras由纯Python编写而成并基[Tensorflow](https://github.com/tensorflow/tensorflow)后端\n*    [Potential rookies in 2015~16 prediction](https://github.com/Trouble404/NBA-with-Machine-Learning/blob/master/nerual_network/prediction/rookies_all_star_prediction/All%20star%20rookie%20without%20pca.ipynb)\n*    [Tensorboard](https://www.tensorflow.org/get_started/summaries_and_tensorboard) is used to compare and choose better model.\n![image](https://raw.githubusercontent.com/Trouble404/NBA-with-Machine-Learning/master/readme_add_pic/rookies-allstar.PNG)\n\n---\n\n## Result\n**Website: [2017 NBA ALL-STAR AND BEST ROOKIES PREDICTION WITH FAN-MAP](https://d2v4olxsjbfep7.cloudfront.net/panels.html)**\n---\n\nexample: \n*  Neural network model\n\n![image](https://raw.githubusercontent.com/Trouble404/NBA-with-Machine-Learning/master/readme_add_pic/model.png)\n\n*  2D PCA (linear unseparable)\n\n![image](https://raw.githubusercontent.com/Trouble404/NBA-with-Machine-Learning/master/readme_add_pic/pca.PNG)\n\n\n*  Tensorboard check\n\n![image](https://raw.githubusercontent.com/Trouble404/NBA-with-Machine-Learning/master/readme_add_pic/tensor_board.PNG)\n","slug":"Machine Learning in NBA","published":1,"updated":"2020-04-28T12:38:55.395Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck9jwdng50007jlrcdxl2eea8","content":"<h2 id=\"Fan-map-plotting\"><a href=\"#Fan-map-plotting\" class=\"headerlink\" title=\"Fan-map plotting\"></a>Fan-map plotting</h2><ul>\n<li>Obtain the game data in 2000~2017 in NBA</li>\n<li>Obtain the follwers of 2015~2017 NBA rookies by <a href=\"https://www.rdocumentation.org/packages/twitteR/versions/1.1.9\" target=\"_blank\" rel=\"noopener\">twittR</a> package</li>\n<li>Obtain location information and plot the fan-map by <a href=\"https://www.tableau.com/zh-cn\" target=\"_blank\" rel=\"noopener\">Tableau</a><a id=\"more\"></a>\n</li>\n</ul>\n<hr>\n<h2 id=\"Machine-learning-apply\"><a href=\"#Machine-learning-apply\" class=\"headerlink\" title=\"Machine learning apply\"></a>Machine learning apply</h2><h3 id=\"all-star-players-in-2017-prediction\"><a href=\"#all-star-players-in-2017-prediction\" class=\"headerlink\" title=\"all-star players in 2017 prediction\"></a>all-star players in 2017 prediction</h3><ul>\n<li><a href=\"https://github.com/Trouble404/NBA-with-Machine-Learning/blob/master/nerual_network/pca/nba%20all%20star/pca%20of%20nba%20all%20players_00-17.ipynb\" target=\"_blank\" rel=\"noopener\">PCA 主成份分析</a> in all-star player prediction</li>\n<li><a href=\"https://keras-cn.readthedocs.io/en/latest/\" target=\"_blank\" rel=\"noopener\">Keras</a>是一个高层神经网络API，Keras由纯Python编写而成并基<a href=\"https://github.com/tensorflow/tensorflow\" target=\"_blank\" rel=\"noopener\">Tensorflow</a>后端</li>\n<li><a href=\"https://github.com/Trouble404/NBA-with-Machine-Learning/blob/master/nerual_network/prediction/nba_all_star_prediction/nba%20all%20star%20prediction.ipynb\" target=\"_blank\" rel=\"noopener\">All-star players in 2017 prediction</a></li>\n<li><a href=\"https://www.tensorflow.org/get_started/summaries_and_tensorboard\" target=\"_blank\" rel=\"noopener\">Tensorboard</a> is used to compare and choose better model.<br><img src=\"https://raw.githubusercontent.com/Trouble404/NBA-with-Machine-Learning/master/readme_add_pic/full-list.PNG\" alt=\"image\"></li>\n</ul>\n<h3 id=\"Best-rookies-in-2017-prediction\"><a href=\"#Best-rookies-in-2017-prediction\" class=\"headerlink\" title=\"Best rookies in 2017 prediction\"></a>Best rookies in 2017 prediction</h3><ul>\n<li><a href=\"https://github.com/Trouble404/NBA-with-Machine-Learning/blob/master/nerual_network/pca/nba%20rookies%20best/pca%20process.ipynb\" target=\"_blank\" rel=\"noopener\">PCA 主成份分析</a> in best rookies prediction</li>\n<li><a href=\"https://keras-cn.readthedocs.io/en/latest/\" target=\"_blank\" rel=\"noopener\">Keras</a>是一个高层神经网络API，Keras由纯Python编写而成并基<a href=\"https://github.com/tensorflow/tensorflow\" target=\"_blank\" rel=\"noopener\">Tensorflow</a>后端</li>\n<li><a href=\"https://github.com/Trouble404/NBA-with-Machine-Learning/blob/master/nerual_network/prediction/rookies_best/normal%20prediction/Best%205%20rookie.ipynb\" target=\"_blank\" rel=\"noopener\">Best rookies in 2017 prediction</a></li>\n<li><a href=\"https://github.com/Trouble404/NBA-with-Machine-Learning/blob/master/nerual_network/prediction/rookies_first_second/first_second_rookie-tensorboard.ipynb\" target=\"_blank\" rel=\"noopener\">First and Second rookies in 2017 prediction</a> <strong>Muti-classes classification application</strong></li>\n<li><a href=\"https://www.tensorflow.org/get_started/summaries_and_tensorboard\" target=\"_blank\" rel=\"noopener\">Tensorboard</a> is used to compare and choose better model.<br><img src=\"https://raw.githubusercontent.com/Trouble404/NBA-with-Machine-Learning/master/readme_add_pic/firstrookies.PNG\" alt=\"image\"></li>\n</ul>\n<h3 id=\"Potential-all-star-players-in-2015-16-prediction\"><a href=\"#Potential-all-star-players-in-2015-16-prediction\" class=\"headerlink\" title=\"Potential all-star players in 2015~16 prediction\"></a>Potential all-star players in 2015~16 prediction</h3><ul>\n<li><a href=\"https://github.com/Trouble404/NBA-with-Machine-Learning/blob/master/nerual_network/pca/nba%20rookies%20all%20star/pca%20process.ipynb\" target=\"_blank\" rel=\"noopener\">PCA 主成份分析</a> in potential rookies prediction</li>\n<li><a href=\"https://keras-cn.readthedocs.io/en/latest/\" target=\"_blank\" rel=\"noopener\">Keras</a>是一个高层神经网络API，Keras由纯Python编写而成并基<a href=\"https://github.com/tensorflow/tensorflow\" target=\"_blank\" rel=\"noopener\">Tensorflow</a>后端</li>\n<li><a href=\"https://github.com/Trouble404/NBA-with-Machine-Learning/blob/master/nerual_network/prediction/rookies_all_star_prediction/All%20star%20rookie%20without%20pca.ipynb\" target=\"_blank\" rel=\"noopener\">Potential rookies in 2015~16 prediction</a></li>\n<li><a href=\"https://www.tensorflow.org/get_started/summaries_and_tensorboard\" target=\"_blank\" rel=\"noopener\">Tensorboard</a> is used to compare and choose better model.<br><img src=\"https://raw.githubusercontent.com/Trouble404/NBA-with-Machine-Learning/master/readme_add_pic/rookies-allstar.PNG\" alt=\"image\"></li>\n</ul>\n<hr>\n<h2 id=\"Result\"><a href=\"#Result\" class=\"headerlink\" title=\"Result\"></a>Result</h2><h2 id=\"Website-2017-NBA-ALL-STAR-AND-BEST-ROOKIES-PREDICTION-WITH-FAN-MAP\"><a href=\"#Website-2017-NBA-ALL-STAR-AND-BEST-ROOKIES-PREDICTION-WITH-FAN-MAP\" class=\"headerlink\" title=\"Website: 2017 NBA ALL-STAR AND BEST ROOKIES PREDICTION WITH FAN-MAP\"></a><strong>Website: <a href=\"https://d2v4olxsjbfep7.cloudfront.net/panels.html\" target=\"_blank\" rel=\"noopener\">2017 NBA ALL-STAR AND BEST ROOKIES PREDICTION WITH FAN-MAP</a></strong></h2><p>example: </p>\n<ul>\n<li>Neural network model</li>\n</ul>\n<p><img src=\"https://raw.githubusercontent.com/Trouble404/NBA-with-Machine-Learning/master/readme_add_pic/model.png\" alt=\"image\"></p>\n<ul>\n<li>2D PCA (linear unseparable)</li>\n</ul>\n<p><img src=\"https://raw.githubusercontent.com/Trouble404/NBA-with-Machine-Learning/master/readme_add_pic/pca.PNG\" alt=\"image\"></p>\n<ul>\n<li>Tensorboard check</li>\n</ul>\n<p><img src=\"https://raw.githubusercontent.com/Trouble404/NBA-with-Machine-Learning/master/readme_add_pic/tensor_board.PNG\" alt=\"image\"></p>\n","site":{"data":{}},"excerpt":"<h2 id=\"Fan-map-plotting\"><a href=\"#Fan-map-plotting\" class=\"headerlink\" title=\"Fan-map plotting\"></a>Fan-map plotting</h2><ul>\n<li>Obtain the game data in 2000~2017 in NBA</li>\n<li>Obtain the follwers of 2015~2017 NBA rookies by <a href=\"https://www.rdocumentation.org/packages/twitteR/versions/1.1.9\" target=\"_blank\" rel=\"noopener\">twittR</a> package</li>\n<li>Obtain location information and plot the fan-map by <a href=\"https://www.tableau.com/zh-cn\" target=\"_blank\" rel=\"noopener\">Tableau</a>","more":"</li>\n</ul>\n<hr>\n<h2 id=\"Machine-learning-apply\"><a href=\"#Machine-learning-apply\" class=\"headerlink\" title=\"Machine learning apply\"></a>Machine learning apply</h2><h3 id=\"all-star-players-in-2017-prediction\"><a href=\"#all-star-players-in-2017-prediction\" class=\"headerlink\" title=\"all-star players in 2017 prediction\"></a>all-star players in 2017 prediction</h3><ul>\n<li><a href=\"https://github.com/Trouble404/NBA-with-Machine-Learning/blob/master/nerual_network/pca/nba%20all%20star/pca%20of%20nba%20all%20players_00-17.ipynb\" target=\"_blank\" rel=\"noopener\">PCA 主成份分析</a> in all-star player prediction</li>\n<li><a href=\"https://keras-cn.readthedocs.io/en/latest/\" target=\"_blank\" rel=\"noopener\">Keras</a>是一个高层神经网络API，Keras由纯Python编写而成并基<a href=\"https://github.com/tensorflow/tensorflow\" target=\"_blank\" rel=\"noopener\">Tensorflow</a>后端</li>\n<li><a href=\"https://github.com/Trouble404/NBA-with-Machine-Learning/blob/master/nerual_network/prediction/nba_all_star_prediction/nba%20all%20star%20prediction.ipynb\" target=\"_blank\" rel=\"noopener\">All-star players in 2017 prediction</a></li>\n<li><a href=\"https://www.tensorflow.org/get_started/summaries_and_tensorboard\" target=\"_blank\" rel=\"noopener\">Tensorboard</a> is used to compare and choose better model.<br><img src=\"https://raw.githubusercontent.com/Trouble404/NBA-with-Machine-Learning/master/readme_add_pic/full-list.PNG\" alt=\"image\"></li>\n</ul>\n<h3 id=\"Best-rookies-in-2017-prediction\"><a href=\"#Best-rookies-in-2017-prediction\" class=\"headerlink\" title=\"Best rookies in 2017 prediction\"></a>Best rookies in 2017 prediction</h3><ul>\n<li><a href=\"https://github.com/Trouble404/NBA-with-Machine-Learning/blob/master/nerual_network/pca/nba%20rookies%20best/pca%20process.ipynb\" target=\"_blank\" rel=\"noopener\">PCA 主成份分析</a> in best rookies prediction</li>\n<li><a href=\"https://keras-cn.readthedocs.io/en/latest/\" target=\"_blank\" rel=\"noopener\">Keras</a>是一个高层神经网络API，Keras由纯Python编写而成并基<a href=\"https://github.com/tensorflow/tensorflow\" target=\"_blank\" rel=\"noopener\">Tensorflow</a>后端</li>\n<li><a href=\"https://github.com/Trouble404/NBA-with-Machine-Learning/blob/master/nerual_network/prediction/rookies_best/normal%20prediction/Best%205%20rookie.ipynb\" target=\"_blank\" rel=\"noopener\">Best rookies in 2017 prediction</a></li>\n<li><a href=\"https://github.com/Trouble404/NBA-with-Machine-Learning/blob/master/nerual_network/prediction/rookies_first_second/first_second_rookie-tensorboard.ipynb\" target=\"_blank\" rel=\"noopener\">First and Second rookies in 2017 prediction</a> <strong>Muti-classes classification application</strong></li>\n<li><a href=\"https://www.tensorflow.org/get_started/summaries_and_tensorboard\" target=\"_blank\" rel=\"noopener\">Tensorboard</a> is used to compare and choose better model.<br><img src=\"https://raw.githubusercontent.com/Trouble404/NBA-with-Machine-Learning/master/readme_add_pic/firstrookies.PNG\" alt=\"image\"></li>\n</ul>\n<h3 id=\"Potential-all-star-players-in-2015-16-prediction\"><a href=\"#Potential-all-star-players-in-2015-16-prediction\" class=\"headerlink\" title=\"Potential all-star players in 2015~16 prediction\"></a>Potential all-star players in 2015~16 prediction</h3><ul>\n<li><a href=\"https://github.com/Trouble404/NBA-with-Machine-Learning/blob/master/nerual_network/pca/nba%20rookies%20all%20star/pca%20process.ipynb\" target=\"_blank\" rel=\"noopener\">PCA 主成份分析</a> in potential rookies prediction</li>\n<li><a href=\"https://keras-cn.readthedocs.io/en/latest/\" target=\"_blank\" rel=\"noopener\">Keras</a>是一个高层神经网络API，Keras由纯Python编写而成并基<a href=\"https://github.com/tensorflow/tensorflow\" target=\"_blank\" rel=\"noopener\">Tensorflow</a>后端</li>\n<li><a href=\"https://github.com/Trouble404/NBA-with-Machine-Learning/blob/master/nerual_network/prediction/rookies_all_star_prediction/All%20star%20rookie%20without%20pca.ipynb\" target=\"_blank\" rel=\"noopener\">Potential rookies in 2015~16 prediction</a></li>\n<li><a href=\"https://www.tensorflow.org/get_started/summaries_and_tensorboard\" target=\"_blank\" rel=\"noopener\">Tensorboard</a> is used to compare and choose better model.<br><img src=\"https://raw.githubusercontent.com/Trouble404/NBA-with-Machine-Learning/master/readme_add_pic/rookies-allstar.PNG\" alt=\"image\"></li>\n</ul>\n<hr>\n<h2 id=\"Result\"><a href=\"#Result\" class=\"headerlink\" title=\"Result\"></a>Result</h2><h2 id=\"Website-2017-NBA-ALL-STAR-AND-BEST-ROOKIES-PREDICTION-WITH-FAN-MAP\"><a href=\"#Website-2017-NBA-ALL-STAR-AND-BEST-ROOKIES-PREDICTION-WITH-FAN-MAP\" class=\"headerlink\" title=\"Website: 2017 NBA ALL-STAR AND BEST ROOKIES PREDICTION WITH FAN-MAP\"></a><strong>Website: <a href=\"https://d2v4olxsjbfep7.cloudfront.net/panels.html\" target=\"_blank\" rel=\"noopener\">2017 NBA ALL-STAR AND BEST ROOKIES PREDICTION WITH FAN-MAP</a></strong></h2><p>example: </p>\n<ul>\n<li>Neural network model</li>\n</ul>\n<p><img src=\"https://raw.githubusercontent.com/Trouble404/NBA-with-Machine-Learning/master/readme_add_pic/model.png\" alt=\"image\"></p>\n<ul>\n<li>2D PCA (linear unseparable)</li>\n</ul>\n<p><img src=\"https://raw.githubusercontent.com/Trouble404/NBA-with-Machine-Learning/master/readme_add_pic/pca.PNG\" alt=\"image\"></p>\n<ul>\n<li>Tensorboard check</li>\n</ul>\n<p><img src=\"https://raw.githubusercontent.com/Trouble404/NBA-with-Machine-Learning/master/readme_add_pic/tensor_board.PNG\" alt=\"image\"></p>"},{"title":"【转】论文阅读 - Semantic Soft Segmentation","date":"2018-11-29T03:00:00.000Z","_content":"\n# 论文阅读 - Semantic Soft Segmentation\n\n<center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Semantic-soft-Segmentation/1.png)</center>\n\n---\n\n题目：[Semantic Soft Segmentation - SIGGRAPH2018](http://people.inf.ethz.ch/aksoyy/papers/TOG18-sss.pdf)\n\n作者：[Yagiz Aksoy](http://people.inf.ethz.ch/aksoyy/), [Tae-Hyun Oh](http://taehyunoh.com/), [Sylvain Paris](http://people.csail.mit.edu/sparis/), [Marc Pollefeys](https://www.inf.ethz.ch/personal/marc.pollefeys/) and [Wojciech Matusik](http://people.csail.mit.edu/wojciech/)\n\n团队：MIT CSAIL, Adobe Research<!-- more -->\n\n---\n\n[[Paper - Semantic Soft Segmentation - SIGGRAPH2018]](http://people.inf.ethz.ch/aksoyy/papers/TOG18-sss.pdf)\n\n[[Supplementary Material - Semantic Soft Segmentation - SIGGRAPH2018]](http://people.inf.ethz.ch/aksoyy/papers/TOG18-sss-supp.pdf)\n\n[[HomePage]](http://people.inf.ethz.ch/aksoyy/sss/)\n\n[[Github - SIGGRAPH18SSS - Semantic feature generator- 特征提取源码]](https://github.com/iyah4888/SIGGRAPH18SSS)\n\n[[Github - Spectral segmentation implementation - 分割源码]](https://github.com/yaksoy/SemanticSoftSegmentation)\n\n[[YouTube - Video]](https://youtu.be/QYIQbfnS9jA)\n\n语义软分割(Semantic Soft Segments)，旨在精确表示图像不同区域间的软过渡. 类似与磁力套索(magnetic lasso) 和魔术棒(magic wand) 的功能.\n\n从谱分割(spectral segmentation) 角度来解决 soft segmentation 问题，提出的图结构(Graph Structure)，既考虑了图片的纹理和颜色特征，也利用了由深度神经网络生成的更高层的语义信息. 根据仔细构建的 Laplacian 矩阵的特征分解(eigendecomposition) 自动的生成 soft segments.\n\n出发点：\n1. 能够分割图片中的不同物体，同时精确表示出分割物体间的过渡情况.\n2. 自动完成分割，不用手工操作.\n\nSemantic Soft Segmentation，自动将图像分解为不同的层，以覆盖场景的物体对象，并通过软过渡(soft transitions) 来分离不同的物体对象.\n\n相关研究方向：\n\n*   Soft segmentation - 将图像分解为两个或多个分割，每个像素可能属于不止一个分割部分.\n*   Natural image matting - 估计用于定义的前景区域中每个像素的不透明度. 一般输入是 trimap，其分别定义了不透明的前景，透明的背景以及未知透明度的区域.\n*   Targeted edit propagation\n*   Semantic segmentation - 语义分割\n\n## 技术路线\n\n**问题描述**：\n给定输入图片，自动生成其 soft 分割结果，即，分解为表示了场景内物体的不同层，包括物体的透明度和物体间的软过渡.\n每一层的各个像素由一个透明度值alpha表示. alpha=0 表示完全不透明(fully opaque)，alpha=1 表示完全透明(fully transparent)，alpha 值在 0-1 之间，则表示部分不透明度.\n\n$$(R,G,B)_{input} = \\sum_{i} \\alpha_{i}(R,G,B)_{i}$$\n$$\\sum_{i}\\alpha_{i}=1$$\n\n输入图片的 RGB 像素可以表示为每一层中的像素值与对应的 alpha 值的加权和.\n\n<center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Semantic-soft-Segmentation/2.png)</center>\n\n### 1\\. 低层特征构建 - Nonlocal Color Affinity\n\n构建低层次的仿射关系项，以表示基于颜色的像素间较大范围的关联性特征. Nonloal Color Affinity可以提升分解恢复过程中isolated的区域的效果。\n<center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Semantic-soft-Segmentation/3.png)</center>\n\n主要构建过程：\n1. 采用 SLIC(超像素分割) 生成 2500 个超像素;\n2. 估计每个超像素和对应于图像 20% 尺寸半径内所有超像素的仿射关系.\n\n### 2\\. 高层特征构建 - High-Level Semantic Affinity\n\n虽然 nonlocal color affinity 添加了像素间大范围间的相互作用关系，但仍是低层特征.\n这里构建高层语义仿射关系项，以使得属于同一场景物体的像素尽可能的接近，不同场景物体的像素间的关系远离.\n\n<center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Semantic-soft-Segmentation/4.png)</center>\n\n### 3\\. 图像层创建 - Creating the Layers\n\n通过对 Laplacian 矩阵进行特征分解，提取特征向量，并对特征向量进行两步稀疏处理，来创建图像层.\n\n1. 构建 Laplacian 矩阵\n3. 受约束的稀疏化(Constrained sparsification)\n3. 松弛的稀疏化(Relaxed sparsification)\n\n<center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Semantic-soft-Segmentation/5.png)</center>\n\n### 4\\. 语义特征向量 - Semantic Feature Vectors\n在高层特征构建时，相同物体的像素的特征向量相似，不同物体的像素的特征向量不同.\n特征向量是采用语义分割的深度网络模型训练和生成的.\n\n这里采用了 DeepLab-ResNet-101 作为特征提取器，但网络训练是采用的是度量学习方法，最大化不同物体的特征间的 L2 距离(稍微修改了 N-Pair loss).\n\n<center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Semantic-soft-Segmentation/6.png)</center>\n\n### 5\\. 个人看法\n使用了底层特征和高层特征(包括deep learning)产生的语义特征构建的拉普拉斯矩阵的特征分解创建了精细的图层来聚类区分最大可能的前景和背景。分割的效果特别不错，不过计算量特别的庞大，3~4分钟处理一张图片。并且比较依赖图像中的颜色信息，对颜色相近的物体的效果不是特别的好。 在影视方面有不错的前景，也可能可以考虑用来帮助标注人员产生不错的分割图并且进行进一步的标注。","source":"_posts/MalongTEch-Learning-Semantic-Soft-Segmentation.md","raw":"---\ntitle: 【转】论文阅读 - Semantic Soft Segmentation\ndate: 2018-11-29 11:00:00\ntags: [Deep Learning]\ncategories: 实习\n---\n\n# 论文阅读 - Semantic Soft Segmentation\n\n<center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Semantic-soft-Segmentation/1.png)</center>\n\n---\n\n题目：[Semantic Soft Segmentation - SIGGRAPH2018](http://people.inf.ethz.ch/aksoyy/papers/TOG18-sss.pdf)\n\n作者：[Yagiz Aksoy](http://people.inf.ethz.ch/aksoyy/), [Tae-Hyun Oh](http://taehyunoh.com/), [Sylvain Paris](http://people.csail.mit.edu/sparis/), [Marc Pollefeys](https://www.inf.ethz.ch/personal/marc.pollefeys/) and [Wojciech Matusik](http://people.csail.mit.edu/wojciech/)\n\n团队：MIT CSAIL, Adobe Research<!-- more -->\n\n---\n\n[[Paper - Semantic Soft Segmentation - SIGGRAPH2018]](http://people.inf.ethz.ch/aksoyy/papers/TOG18-sss.pdf)\n\n[[Supplementary Material - Semantic Soft Segmentation - SIGGRAPH2018]](http://people.inf.ethz.ch/aksoyy/papers/TOG18-sss-supp.pdf)\n\n[[HomePage]](http://people.inf.ethz.ch/aksoyy/sss/)\n\n[[Github - SIGGRAPH18SSS - Semantic feature generator- 特征提取源码]](https://github.com/iyah4888/SIGGRAPH18SSS)\n\n[[Github - Spectral segmentation implementation - 分割源码]](https://github.com/yaksoy/SemanticSoftSegmentation)\n\n[[YouTube - Video]](https://youtu.be/QYIQbfnS9jA)\n\n语义软分割(Semantic Soft Segments)，旨在精确表示图像不同区域间的软过渡. 类似与磁力套索(magnetic lasso) 和魔术棒(magic wand) 的功能.\n\n从谱分割(spectral segmentation) 角度来解决 soft segmentation 问题，提出的图结构(Graph Structure)，既考虑了图片的纹理和颜色特征，也利用了由深度神经网络生成的更高层的语义信息. 根据仔细构建的 Laplacian 矩阵的特征分解(eigendecomposition) 自动的生成 soft segments.\n\n出发点：\n1. 能够分割图片中的不同物体，同时精确表示出分割物体间的过渡情况.\n2. 自动完成分割，不用手工操作.\n\nSemantic Soft Segmentation，自动将图像分解为不同的层，以覆盖场景的物体对象，并通过软过渡(soft transitions) 来分离不同的物体对象.\n\n相关研究方向：\n\n*   Soft segmentation - 将图像分解为两个或多个分割，每个像素可能属于不止一个分割部分.\n*   Natural image matting - 估计用于定义的前景区域中每个像素的不透明度. 一般输入是 trimap，其分别定义了不透明的前景，透明的背景以及未知透明度的区域.\n*   Targeted edit propagation\n*   Semantic segmentation - 语义分割\n\n## 技术路线\n\n**问题描述**：\n给定输入图片，自动生成其 soft 分割结果，即，分解为表示了场景内物体的不同层，包括物体的透明度和物体间的软过渡.\n每一层的各个像素由一个透明度值alpha表示. alpha=0 表示完全不透明(fully opaque)，alpha=1 表示完全透明(fully transparent)，alpha 值在 0-1 之间，则表示部分不透明度.\n\n$$(R,G,B)_{input} = \\sum_{i} \\alpha_{i}(R,G,B)_{i}$$\n$$\\sum_{i}\\alpha_{i}=1$$\n\n输入图片的 RGB 像素可以表示为每一层中的像素值与对应的 alpha 值的加权和.\n\n<center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Semantic-soft-Segmentation/2.png)</center>\n\n### 1\\. 低层特征构建 - Nonlocal Color Affinity\n\n构建低层次的仿射关系项，以表示基于颜色的像素间较大范围的关联性特征. Nonloal Color Affinity可以提升分解恢复过程中isolated的区域的效果。\n<center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Semantic-soft-Segmentation/3.png)</center>\n\n主要构建过程：\n1. 采用 SLIC(超像素分割) 生成 2500 个超像素;\n2. 估计每个超像素和对应于图像 20% 尺寸半径内所有超像素的仿射关系.\n\n### 2\\. 高层特征构建 - High-Level Semantic Affinity\n\n虽然 nonlocal color affinity 添加了像素间大范围间的相互作用关系，但仍是低层特征.\n这里构建高层语义仿射关系项，以使得属于同一场景物体的像素尽可能的接近，不同场景物体的像素间的关系远离.\n\n<center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Semantic-soft-Segmentation/4.png)</center>\n\n### 3\\. 图像层创建 - Creating the Layers\n\n通过对 Laplacian 矩阵进行特征分解，提取特征向量，并对特征向量进行两步稀疏处理，来创建图像层.\n\n1. 构建 Laplacian 矩阵\n3. 受约束的稀疏化(Constrained sparsification)\n3. 松弛的稀疏化(Relaxed sparsification)\n\n<center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Semantic-soft-Segmentation/5.png)</center>\n\n### 4\\. 语义特征向量 - Semantic Feature Vectors\n在高层特征构建时，相同物体的像素的特征向量相似，不同物体的像素的特征向量不同.\n特征向量是采用语义分割的深度网络模型训练和生成的.\n\n这里采用了 DeepLab-ResNet-101 作为特征提取器，但网络训练是采用的是度量学习方法，最大化不同物体的特征间的 L2 距离(稍微修改了 N-Pair loss).\n\n<center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Semantic-soft-Segmentation/6.png)</center>\n\n### 5\\. 个人看法\n使用了底层特征和高层特征(包括deep learning)产生的语义特征构建的拉普拉斯矩阵的特征分解创建了精细的图层来聚类区分最大可能的前景和背景。分割的效果特别不错，不过计算量特别的庞大，3~4分钟处理一张图片。并且比较依赖图像中的颜色信息，对颜色相近的物体的效果不是特别的好。 在影视方面有不错的前景，也可能可以考虑用来帮助标注人员产生不错的分割图并且进行进一步的标注。","slug":"MalongTEch-Learning-Semantic-Soft-Segmentation","published":1,"updated":"2020-04-28T12:38:55.395Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck9jwdng50008jlrcbfxepafd","content":"<h1 id=\"论文阅读-Semantic-Soft-Segmentation\"><a href=\"#论文阅读-Semantic-Soft-Segmentation\" class=\"headerlink\" title=\"论文阅读 - Semantic Soft Segmentation\"></a>论文阅读 - Semantic Soft Segmentation</h1><center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Semantic-soft-Segmentation/1.png\" alt=\"image\"></center>\n\n<hr>\n<p>题目：<a href=\"http://people.inf.ethz.ch/aksoyy/papers/TOG18-sss.pdf\" target=\"_blank\" rel=\"noopener\">Semantic Soft Segmentation - SIGGRAPH2018</a></p>\n<p>作者：<a href=\"http://people.inf.ethz.ch/aksoyy/\" target=\"_blank\" rel=\"noopener\">Yagiz Aksoy</a>, <a href=\"http://taehyunoh.com/\" target=\"_blank\" rel=\"noopener\">Tae-Hyun Oh</a>, <a href=\"http://people.csail.mit.edu/sparis/\" target=\"_blank\" rel=\"noopener\">Sylvain Paris</a>, <a href=\"https://www.inf.ethz.ch/personal/marc.pollefeys/\" target=\"_blank\" rel=\"noopener\">Marc Pollefeys</a> and <a href=\"http://people.csail.mit.edu/wojciech/\" target=\"_blank\" rel=\"noopener\">Wojciech Matusik</a></p>\n<p>团队：MIT CSAIL, Adobe Research<a id=\"more\"></a></p>\n<hr>\n<p><a href=\"http://people.inf.ethz.ch/aksoyy/papers/TOG18-sss.pdf\" target=\"_blank\" rel=\"noopener\">[Paper - Semantic Soft Segmentation - SIGGRAPH2018]</a></p>\n<p><a href=\"http://people.inf.ethz.ch/aksoyy/papers/TOG18-sss-supp.pdf\" target=\"_blank\" rel=\"noopener\">[Supplementary Material - Semantic Soft Segmentation - SIGGRAPH2018]</a></p>\n<p><a href=\"http://people.inf.ethz.ch/aksoyy/sss/\" target=\"_blank\" rel=\"noopener\">[HomePage]</a></p>\n<p><a href=\"https://github.com/iyah4888/SIGGRAPH18SSS\" target=\"_blank\" rel=\"noopener\">[Github - SIGGRAPH18SSS - Semantic feature generator- 特征提取源码]</a></p>\n<p><a href=\"https://github.com/yaksoy/SemanticSoftSegmentation\" target=\"_blank\" rel=\"noopener\">[Github - Spectral segmentation implementation - 分割源码]</a></p>\n<p><a href=\"https://youtu.be/QYIQbfnS9jA\" target=\"_blank\" rel=\"noopener\">[YouTube - Video]</a></p>\n<p>语义软分割(Semantic Soft Segments)，旨在精确表示图像不同区域间的软过渡. 类似与磁力套索(magnetic lasso) 和魔术棒(magic wand) 的功能.</p>\n<p>从谱分割(spectral segmentation) 角度来解决 soft segmentation 问题，提出的图结构(Graph Structure)，既考虑了图片的纹理和颜色特征，也利用了由深度神经网络生成的更高层的语义信息. 根据仔细构建的 Laplacian 矩阵的特征分解(eigendecomposition) 自动的生成 soft segments.</p>\n<p>出发点：</p>\n<ol>\n<li>能够分割图片中的不同物体，同时精确表示出分割物体间的过渡情况.</li>\n<li>自动完成分割，不用手工操作.</li>\n</ol>\n<p>Semantic Soft Segmentation，自动将图像分解为不同的层，以覆盖场景的物体对象，并通过软过渡(soft transitions) 来分离不同的物体对象.</p>\n<p>相关研究方向：</p>\n<ul>\n<li>Soft segmentation - 将图像分解为两个或多个分割，每个像素可能属于不止一个分割部分.</li>\n<li>Natural image matting - 估计用于定义的前景区域中每个像素的不透明度. 一般输入是 trimap，其分别定义了不透明的前景，透明的背景以及未知透明度的区域.</li>\n<li>Targeted edit propagation</li>\n<li>Semantic segmentation - 语义分割</li>\n</ul>\n<h2 id=\"技术路线\"><a href=\"#技术路线\" class=\"headerlink\" title=\"技术路线\"></a>技术路线</h2><p><strong>问题描述</strong>：<br>给定输入图片，自动生成其 soft 分割结果，即，分解为表示了场景内物体的不同层，包括物体的透明度和物体间的软过渡.<br>每一层的各个像素由一个透明度值alpha表示. alpha=0 表示完全不透明(fully opaque)，alpha=1 表示完全透明(fully transparent)，alpha 值在 0-1 之间，则表示部分不透明度.</p>\n<p>$$(R,G,B)<em>{input} = \\sum</em>{i} \\alpha_{i}(R,G,B)<em>{i}$$<br>$$\\sum</em>{i}\\alpha_{i}=1$$</p>\n<p>输入图片的 RGB 像素可以表示为每一层中的像素值与对应的 alpha 值的加权和.</p>\n<center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Semantic-soft-Segmentation/2.png\" alt=\"image\"></center>\n\n<h3 id=\"1-低层特征构建-Nonlocal-Color-Affinity\"><a href=\"#1-低层特征构建-Nonlocal-Color-Affinity\" class=\"headerlink\" title=\"1. 低层特征构建 - Nonlocal Color Affinity\"></a>1. 低层特征构建 - Nonlocal Color Affinity</h3><p>构建低层次的仿射关系项，以表示基于颜色的像素间较大范围的关联性特征. Nonloal Color Affinity可以提升分解恢复过程中isolated的区域的效果。</p>\n<center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Semantic-soft-Segmentation/3.png\" alt=\"image\"></center>\n\n<p>主要构建过程：</p>\n<ol>\n<li>采用 SLIC(超像素分割) 生成 2500 个超像素;</li>\n<li>估计每个超像素和对应于图像 20% 尺寸半径内所有超像素的仿射关系.</li>\n</ol>\n<h3 id=\"2-高层特征构建-High-Level-Semantic-Affinity\"><a href=\"#2-高层特征构建-High-Level-Semantic-Affinity\" class=\"headerlink\" title=\"2. 高层特征构建 - High-Level Semantic Affinity\"></a>2. 高层特征构建 - High-Level Semantic Affinity</h3><p>虽然 nonlocal color affinity 添加了像素间大范围间的相互作用关系，但仍是低层特征.<br>这里构建高层语义仿射关系项，以使得属于同一场景物体的像素尽可能的接近，不同场景物体的像素间的关系远离.</p>\n<center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Semantic-soft-Segmentation/4.png\" alt=\"image\"></center>\n\n<h3 id=\"3-图像层创建-Creating-the-Layers\"><a href=\"#3-图像层创建-Creating-the-Layers\" class=\"headerlink\" title=\"3. 图像层创建 - Creating the Layers\"></a>3. 图像层创建 - Creating the Layers</h3><p>通过对 Laplacian 矩阵进行特征分解，提取特征向量，并对特征向量进行两步稀疏处理，来创建图像层.</p>\n<ol>\n<li>构建 Laplacian 矩阵</li>\n<li>受约束的稀疏化(Constrained sparsification)</li>\n<li>松弛的稀疏化(Relaxed sparsification)</li>\n</ol>\n<center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Semantic-soft-Segmentation/5.png\" alt=\"image\"></center>\n\n<h3 id=\"4-语义特征向量-Semantic-Feature-Vectors\"><a href=\"#4-语义特征向量-Semantic-Feature-Vectors\" class=\"headerlink\" title=\"4. 语义特征向量 - Semantic Feature Vectors\"></a>4. 语义特征向量 - Semantic Feature Vectors</h3><p>在高层特征构建时，相同物体的像素的特征向量相似，不同物体的像素的特征向量不同.<br>特征向量是采用语义分割的深度网络模型训练和生成的.</p>\n<p>这里采用了 DeepLab-ResNet-101 作为特征提取器，但网络训练是采用的是度量学习方法，最大化不同物体的特征间的 L2 距离(稍微修改了 N-Pair loss).</p>\n<center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Semantic-soft-Segmentation/6.png\" alt=\"image\"></center>\n\n<h3 id=\"5-个人看法\"><a href=\"#5-个人看法\" class=\"headerlink\" title=\"5. 个人看法\"></a>5. 个人看法</h3><p>使用了底层特征和高层特征(包括deep learning)产生的语义特征构建的拉普拉斯矩阵的特征分解创建了精细的图层来聚类区分最大可能的前景和背景。分割的效果特别不错，不过计算量特别的庞大，3~4分钟处理一张图片。并且比较依赖图像中的颜色信息，对颜色相近的物体的效果不是特别的好。 在影视方面有不错的前景，也可能可以考虑用来帮助标注人员产生不错的分割图并且进行进一步的标注。</p>\n","site":{"data":{}},"excerpt":"<h1 id=\"论文阅读-Semantic-Soft-Segmentation\"><a href=\"#论文阅读-Semantic-Soft-Segmentation\" class=\"headerlink\" title=\"论文阅读 - Semantic Soft Segmentation\"></a>论文阅读 - Semantic Soft Segmentation</h1><center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Semantic-soft-Segmentation/1.png\" alt=\"image\"></center>\n\n<hr>\n<p>题目：<a href=\"http://people.inf.ethz.ch/aksoyy/papers/TOG18-sss.pdf\" target=\"_blank\" rel=\"noopener\">Semantic Soft Segmentation - SIGGRAPH2018</a></p>\n<p>作者：<a href=\"http://people.inf.ethz.ch/aksoyy/\" target=\"_blank\" rel=\"noopener\">Yagiz Aksoy</a>, <a href=\"http://taehyunoh.com/\" target=\"_blank\" rel=\"noopener\">Tae-Hyun Oh</a>, <a href=\"http://people.csail.mit.edu/sparis/\" target=\"_blank\" rel=\"noopener\">Sylvain Paris</a>, <a href=\"https://www.inf.ethz.ch/personal/marc.pollefeys/\" target=\"_blank\" rel=\"noopener\">Marc Pollefeys</a> and <a href=\"http://people.csail.mit.edu/wojciech/\" target=\"_blank\" rel=\"noopener\">Wojciech Matusik</a></p>\n<p>团队：MIT CSAIL, Adobe Research","more":"</p>\n<hr>\n<p><a href=\"http://people.inf.ethz.ch/aksoyy/papers/TOG18-sss.pdf\" target=\"_blank\" rel=\"noopener\">[Paper - Semantic Soft Segmentation - SIGGRAPH2018]</a></p>\n<p><a href=\"http://people.inf.ethz.ch/aksoyy/papers/TOG18-sss-supp.pdf\" target=\"_blank\" rel=\"noopener\">[Supplementary Material - Semantic Soft Segmentation - SIGGRAPH2018]</a></p>\n<p><a href=\"http://people.inf.ethz.ch/aksoyy/sss/\" target=\"_blank\" rel=\"noopener\">[HomePage]</a></p>\n<p><a href=\"https://github.com/iyah4888/SIGGRAPH18SSS\" target=\"_blank\" rel=\"noopener\">[Github - SIGGRAPH18SSS - Semantic feature generator- 特征提取源码]</a></p>\n<p><a href=\"https://github.com/yaksoy/SemanticSoftSegmentation\" target=\"_blank\" rel=\"noopener\">[Github - Spectral segmentation implementation - 分割源码]</a></p>\n<p><a href=\"https://youtu.be/QYIQbfnS9jA\" target=\"_blank\" rel=\"noopener\">[YouTube - Video]</a></p>\n<p>语义软分割(Semantic Soft Segments)，旨在精确表示图像不同区域间的软过渡. 类似与磁力套索(magnetic lasso) 和魔术棒(magic wand) 的功能.</p>\n<p>从谱分割(spectral segmentation) 角度来解决 soft segmentation 问题，提出的图结构(Graph Structure)，既考虑了图片的纹理和颜色特征，也利用了由深度神经网络生成的更高层的语义信息. 根据仔细构建的 Laplacian 矩阵的特征分解(eigendecomposition) 自动的生成 soft segments.</p>\n<p>出发点：</p>\n<ol>\n<li>能够分割图片中的不同物体，同时精确表示出分割物体间的过渡情况.</li>\n<li>自动完成分割，不用手工操作.</li>\n</ol>\n<p>Semantic Soft Segmentation，自动将图像分解为不同的层，以覆盖场景的物体对象，并通过软过渡(soft transitions) 来分离不同的物体对象.</p>\n<p>相关研究方向：</p>\n<ul>\n<li>Soft segmentation - 将图像分解为两个或多个分割，每个像素可能属于不止一个分割部分.</li>\n<li>Natural image matting - 估计用于定义的前景区域中每个像素的不透明度. 一般输入是 trimap，其分别定义了不透明的前景，透明的背景以及未知透明度的区域.</li>\n<li>Targeted edit propagation</li>\n<li>Semantic segmentation - 语义分割</li>\n</ul>\n<h2 id=\"技术路线\"><a href=\"#技术路线\" class=\"headerlink\" title=\"技术路线\"></a>技术路线</h2><p><strong>问题描述</strong>：<br>给定输入图片，自动生成其 soft 分割结果，即，分解为表示了场景内物体的不同层，包括物体的透明度和物体间的软过渡.<br>每一层的各个像素由一个透明度值alpha表示. alpha=0 表示完全不透明(fully opaque)，alpha=1 表示完全透明(fully transparent)，alpha 值在 0-1 之间，则表示部分不透明度.</p>\n<p>$$(R,G,B)<em>{input} = \\sum</em>{i} \\alpha_{i}(R,G,B)<em>{i}$$<br>$$\\sum</em>{i}\\alpha_{i}=1$$</p>\n<p>输入图片的 RGB 像素可以表示为每一层中的像素值与对应的 alpha 值的加权和.</p>\n<center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Semantic-soft-Segmentation/2.png\" alt=\"image\"></center>\n\n<h3 id=\"1-低层特征构建-Nonlocal-Color-Affinity\"><a href=\"#1-低层特征构建-Nonlocal-Color-Affinity\" class=\"headerlink\" title=\"1. 低层特征构建 - Nonlocal Color Affinity\"></a>1. 低层特征构建 - Nonlocal Color Affinity</h3><p>构建低层次的仿射关系项，以表示基于颜色的像素间较大范围的关联性特征. Nonloal Color Affinity可以提升分解恢复过程中isolated的区域的效果。</p>\n<center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Semantic-soft-Segmentation/3.png\" alt=\"image\"></center>\n\n<p>主要构建过程：</p>\n<ol>\n<li>采用 SLIC(超像素分割) 生成 2500 个超像素;</li>\n<li>估计每个超像素和对应于图像 20% 尺寸半径内所有超像素的仿射关系.</li>\n</ol>\n<h3 id=\"2-高层特征构建-High-Level-Semantic-Affinity\"><a href=\"#2-高层特征构建-High-Level-Semantic-Affinity\" class=\"headerlink\" title=\"2. 高层特征构建 - High-Level Semantic Affinity\"></a>2. 高层特征构建 - High-Level Semantic Affinity</h3><p>虽然 nonlocal color affinity 添加了像素间大范围间的相互作用关系，但仍是低层特征.<br>这里构建高层语义仿射关系项，以使得属于同一场景物体的像素尽可能的接近，不同场景物体的像素间的关系远离.</p>\n<center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Semantic-soft-Segmentation/4.png\" alt=\"image\"></center>\n\n<h3 id=\"3-图像层创建-Creating-the-Layers\"><a href=\"#3-图像层创建-Creating-the-Layers\" class=\"headerlink\" title=\"3. 图像层创建 - Creating the Layers\"></a>3. 图像层创建 - Creating the Layers</h3><p>通过对 Laplacian 矩阵进行特征分解，提取特征向量，并对特征向量进行两步稀疏处理，来创建图像层.</p>\n<ol>\n<li>构建 Laplacian 矩阵</li>\n<li>受约束的稀疏化(Constrained sparsification)</li>\n<li>松弛的稀疏化(Relaxed sparsification)</li>\n</ol>\n<center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Semantic-soft-Segmentation/5.png\" alt=\"image\"></center>\n\n<h3 id=\"4-语义特征向量-Semantic-Feature-Vectors\"><a href=\"#4-语义特征向量-Semantic-Feature-Vectors\" class=\"headerlink\" title=\"4. 语义特征向量 - Semantic Feature Vectors\"></a>4. 语义特征向量 - Semantic Feature Vectors</h3><p>在高层特征构建时，相同物体的像素的特征向量相似，不同物体的像素的特征向量不同.<br>特征向量是采用语义分割的深度网络模型训练和生成的.</p>\n<p>这里采用了 DeepLab-ResNet-101 作为特征提取器，但网络训练是采用的是度量学习方法，最大化不同物体的特征间的 L2 距离(稍微修改了 N-Pair loss).</p>\n<center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Semantic-soft-Segmentation/6.png\" alt=\"image\"></center>\n\n<h3 id=\"5-个人看法\"><a href=\"#5-个人看法\" class=\"headerlink\" title=\"5. 个人看法\"></a>5. 个人看法</h3><p>使用了底层特征和高层特征(包括deep learning)产生的语义特征构建的拉普拉斯矩阵的特征分解创建了精细的图层来聚类区分最大可能的前景和背景。分割的效果特别不错，不过计算量特别的庞大，3~4分钟处理一张图片。并且比较依赖图像中的颜色信息，对颜色相近的物体的效果不是特别的好。 在影视方面有不错的前景，也可能可以考虑用来帮助标注人员产生不错的分割图并且进行进一步的标注。</p>"},{"title":"MalongTech 学习内容:图像分割-3D or 医学领域","date":"2018-11-30T03:00:00.000Z","_content":"\n# 医学相关的图像分割技术\n\n<center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/3D-Segmentation/1.png)</center>\n\n| 医学2D | 医学3D|\n| ---------- | -----------|\n| A Novel Domain Adaptation Framework for Medical Image Segmentation   | 3D U-Net: Learning Dense Volumetric Segmentation from Sparrse Annotation   |\n| DeepMedic for Brain Tumor Segmentation | Joint Sequence Learning and Cross-Modality Convolution for 3D Biomedical Segmentation  |\n|  | Simultaneous Super-Resolution and Cross-Modality Synthesis of 3D Medical Image using Weekly-Supervised Joint Convolutional Sparse Coding |\n<!-- more -->\n\n---\n\n## A Novel Domain Adaptation Framework for Medical Image Segmentation\n\n[论文地址](https://arxiv.org/pdf/1810.05732.pdf)\n\n**脑部肿瘤切割**\n* 难点在于难以精确定位肿瘤(肿瘤形状各异，分布广泛)\n* 核磁共振4种模态(Modality)\n  1. 自旋晶格驰像(T1)\n  2. T1对比(T2)\n  3. 自旋松弛(T2)\n  4. 流体衰减反转恢复(FLAIR)\n\n**论文创新点**:\n* A biophysics based domain adaptation method(物理肿瘤生长模型加上对抗网络生成逼真的MR图像补充数据集)\n* An automatic method to 分割健康的组织(白质灰质，脑脊液)，通过健康的组织轮廓辅助脑补图像切割\n\n**方法**\n* **Data Augmentation:** 使用基于生物学的肿瘤生长模型(PDE)模拟合成的肿瘤，在使用一个辅助的神经网络修正模拟肿瘤到correct intensities distribution对比真实的MR图像(会通过强加循环一致性限制分布情况)\n* \n* **Extened segmentation:** 扩展分割到健康的薄壁组织。先使用(in-house diffeomorphic registration code)微分同胚技术(是一种光滑可逆的变换，在MRI图像配准中可以保证形变后的拓扑结构保持不变，同时避免出现不合理的物理现象)处理过的数据进行训练。<center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/3D-Segmentation/2.png)</center>\n\n  然后再使用DNN去分割健康的组织(神经胶质，脑脊液，灰质和白质)，结果如上图。 这样可以增加健康组织的轮廓作为重要的训练信息并且改进了原来的类不平衡的问题。\n  \n  具体的步骤分为:\n    * Affine registration of each atlas image to the brats image\n    * Diffeomorphic registration of each atlas image to the BraTS image(把健康组织的信息匹配BraTS的数据集结构)\n    * Majority voting to fuse labels of all deformed atlases to get the final healthy tissuse segmentation\n  \n* **模型**\n\n  1. 3D U-Net(会在下面讨论)\n\n    使用3D进行第一阶段的检测，获得肿瘤的初始位置。\n  2. U-Net(在前一章节讲过)\n\n    使用2D的U-Net以及domain adaptation results获得最终的分割结果\n    <center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/3D-Segmentation/3.png)</center>\n    \n* **缺点**\n\n  这个框架只支持2D的domain trasformations，所以对3D的数据只能进行切片并且最后是用的2D的神经网络，这样没有3D的网络来的efficient以及精确。\n\n---\n\n## 3D U-Net: Learning Dense Volumetric Segmentation from Sparse Annotation\n\n[论文地址](https://arxiv.org/pdf/1606.06650.pdf)\n\n在生物医学领域，3D数据是很多的，一层一层转化为2D数据去标注训是不现实的(及其耗时)，而且用整个3D体积的全部数据去训练既低效又极有可能过拟合(相邻切片的数据是非常相近的)。这篇文章提出的3D Unet只需要少部分2D的标注切片就可以生成密集的立体的分割。此网络主要有两个不同的作用方式:\n* Semi-automated setup: 在一个少量稀疏标注的数据集上训练并在此数据集的图像上预测其他未标注的地方。\n* Fully-automated setup: 在representative的稀疏标注的数据集训练，然后用来切割新的图像。\n<center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/3D-Segmentation/4.png)</center>\n\n**改进**\n* 在2D U-Net的基础上，仍然使用encoder去分析整个图片, 但是扩展了decoder来产生full-resolution的切割。\n* 使用3D数据作为输入，因此网络改用3D convolutions, 3D max pooling 和 3D up-convolutional。\n* 避免使用bottlenecks结构因为输入的数据不会很多，避免丢失重要的信息。\n\n**网络**\n<center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/3D-Segmentation/5.png)</center>\n\n* Encoder部分: 每层包含两个$3 \\times 3 \\times 3$卷积和ReLU, 然后是一个strides为2的$2 \\times 2 \\times 2$的最大池化层。\n* Decoder部分: 每层包含strides为2的$2 \\times 2 \\times 2$的upconvolution以及两个$3 \\times 3 \\times 3$卷积和ReLU\n* 这里作者还用了Batch Normalization 防止梯度爆炸，并且在BN后增加了缩放和平移$x_{new}=\\alpha \\cdot x + \\beta$, 其中两个超参是学习出来的。\n* 能在稀疏标注的训练集训练的原因是使用了weighted softmanx loss function, 把unlabeled pixels的权重置为0让网络只学习有标注的部分并且提高了泛化能力。\n\n---\n\n## Joint Sequence Learning and Cross-Modality Convolution for 3D Biomedical Segmentation\n\n[论文地址](https://arxiv.org/pdf/1704.07754.pdf)\n\n<center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/3D-Segmentation/9.png)</center>\n\n目前对于定位肿瘤的难点在于：\n1. 肿瘤形状各异，例如神经胶质瘤与胶质母细胞瘤形状不同\n2. 肿瘤分布广泛，可能分布于大脑的任何区域\n\n这篇文章提出来一个思路就是交叉形态卷积的方法做一个 encoder-decoder 的网络结构，然后同时用LSTM对2D的切片序列建模。\n\nMRI也是跟CT一样断层扫描的过程且包含4种模态(Modality)，就是它一层一层，一层扫出来的就是一个2D的图片，然后多层累计起来就是3D的，但是其实切割是要切割出3D的脑部肿瘤位置，这样就需要把2D的变成3D的，把2D的切片之间的关系通过LSTM描述出来，最后把多模态卷积和LSTM网络结合在一起，达到3D切割。\n\n\n**模型**\n<center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/3D-Segmentation/6.png)</center>\n\n这个方法的framework大概是这样的，从左到右看。\n\n*   首先每一个脑部的MRI数据，他都是通过四种模态切出来的，这里用四种不同的颜色来表示，相当于每一个slice就是我说的那个2D的图片。\n\n*   切完之后他会把四个模态，就是到图b这个阶段了，四个模态交叉在一起做一个multi-modal的encoder，这个encoder就是用一个神经网络来实现的。\n\n*   四个模态encode到一起之后，在这一步就用神经网络把四个模态下的脑部切割出来了，这是2D的情况下。\n\n*   然后再加上convolution LSTM把2D的切割、2D和2D之间的dependency描述出来之后就形成了3D的切割，然后再做一下decoder，展现成最后这种形式。在最中间有一个切割出来的东西，其他没被切割到的background。\n\n**方法**\n<center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/3D-Segmentation/7.png)</center>\n\n1. **MME(Multi-Modal Encoder)**\n\n  类似于SegNet里的编码器结构, 因为数据集比较小，因此网络也简化了，使用四个卷积核，通过batch-normalization，然后加一个非线性变换，在后面有四个最大池化层。\n  \n2. **MRF(Multi-Resolution Fusion)**\n    <center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/3D-Segmentation/8.png)</center>\n    结合多尺度多模态的信息，通过在不同的尺度的encoder和decoder中进行feature multiplication代替级联因此不会增加特征映射的大小。\n    \n3. **CMC(Cross-Modality COnvolution)**\n\n  CMC可以把空间信息以及不同模态的关系结合到一起。\n  \n  四个模态的数据进入到这个卷积网络之后，就会把相同channel下的每一个模态stack在一起形成一个block, 就是每个channel里面有 C 个slice，就是说它是一个立体结构了，一个的长宽是H、W，高是C的这种。四个模态弄到一起就是C×4×H×W。然后通过一个三维的卷积，卷积的大小里有个C×4，也就是用4×1×1的一个卷积核，做卷积之后得到每一层的切割出来的特征。\n\n4. **Slice Sequence Learning**\n\n  使用一个端到端的切片序列学习框架去建模切片之间的相关性。这个convolution LSTM跟普通的LSTM有一个区别，就是把原来的矩阵相乘替换为一个卷积操作，就是普通的乘法变成卷积层，这样它就能够在把之前状态的空间信息保留着。其实它的目的就是，卷积LSTM会描述一个2D切割边缘的趋势，比如说这一张中切片它的形态是这样的，然后到下一张它会有一个轻微的变化，要把这种变化描述出来。\n  \n  此外不同切片的convLSTM的权重是共享的因此需要更新的参数不会随着序列增加。\n  \n5. **Decoder**\n\n  包含上采样以及soft-max进行分类\n  \n**训练**\n1. **Single Slice Training**:\n\n  使用了median frequency balancing调节cross-entropy loss的权重来缓解数据不平衡的情况(98%的都是健康区域)\n  $$\\alpha_{c}=\\frac{median\\_freq}{freq(c)}$$\n  \n2. **Two-Phase Training**:\n\n  * 第一阶段：只采样包含肿瘤的切片并且使用median frequency balancing的方法进行训练。\n  * 第二阶段：去除median frequency策略并且调低学习率，在真实的肿瘤分布概率下进行训练\n\n**结果**\n<center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/3D-Segmentation/10.png)</center>","source":"_posts/MalongTech-Learning-3D-Image-Segmentation.md","raw":"---\ntitle: MalongTech 学习内容:图像分割-3D or 医学领域\ndate: 2018-11-30 11:00:00\ntags: [Deep Learning]\ncategories: 实习\n---\n\n# 医学相关的图像分割技术\n\n<center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/3D-Segmentation/1.png)</center>\n\n| 医学2D | 医学3D|\n| ---------- | -----------|\n| A Novel Domain Adaptation Framework for Medical Image Segmentation   | 3D U-Net: Learning Dense Volumetric Segmentation from Sparrse Annotation   |\n| DeepMedic for Brain Tumor Segmentation | Joint Sequence Learning and Cross-Modality Convolution for 3D Biomedical Segmentation  |\n|  | Simultaneous Super-Resolution and Cross-Modality Synthesis of 3D Medical Image using Weekly-Supervised Joint Convolutional Sparse Coding |\n<!-- more -->\n\n---\n\n## A Novel Domain Adaptation Framework for Medical Image Segmentation\n\n[论文地址](https://arxiv.org/pdf/1810.05732.pdf)\n\n**脑部肿瘤切割**\n* 难点在于难以精确定位肿瘤(肿瘤形状各异，分布广泛)\n* 核磁共振4种模态(Modality)\n  1. 自旋晶格驰像(T1)\n  2. T1对比(T2)\n  3. 自旋松弛(T2)\n  4. 流体衰减反转恢复(FLAIR)\n\n**论文创新点**:\n* A biophysics based domain adaptation method(物理肿瘤生长模型加上对抗网络生成逼真的MR图像补充数据集)\n* An automatic method to 分割健康的组织(白质灰质，脑脊液)，通过健康的组织轮廓辅助脑补图像切割\n\n**方法**\n* **Data Augmentation:** 使用基于生物学的肿瘤生长模型(PDE)模拟合成的肿瘤，在使用一个辅助的神经网络修正模拟肿瘤到correct intensities distribution对比真实的MR图像(会通过强加循环一致性限制分布情况)\n* \n* **Extened segmentation:** 扩展分割到健康的薄壁组织。先使用(in-house diffeomorphic registration code)微分同胚技术(是一种光滑可逆的变换，在MRI图像配准中可以保证形变后的拓扑结构保持不变，同时避免出现不合理的物理现象)处理过的数据进行训练。<center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/3D-Segmentation/2.png)</center>\n\n  然后再使用DNN去分割健康的组织(神经胶质，脑脊液，灰质和白质)，结果如上图。 这样可以增加健康组织的轮廓作为重要的训练信息并且改进了原来的类不平衡的问题。\n  \n  具体的步骤分为:\n    * Affine registration of each atlas image to the brats image\n    * Diffeomorphic registration of each atlas image to the BraTS image(把健康组织的信息匹配BraTS的数据集结构)\n    * Majority voting to fuse labels of all deformed atlases to get the final healthy tissuse segmentation\n  \n* **模型**\n\n  1. 3D U-Net(会在下面讨论)\n\n    使用3D进行第一阶段的检测，获得肿瘤的初始位置。\n  2. U-Net(在前一章节讲过)\n\n    使用2D的U-Net以及domain adaptation results获得最终的分割结果\n    <center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/3D-Segmentation/3.png)</center>\n    \n* **缺点**\n\n  这个框架只支持2D的domain trasformations，所以对3D的数据只能进行切片并且最后是用的2D的神经网络，这样没有3D的网络来的efficient以及精确。\n\n---\n\n## 3D U-Net: Learning Dense Volumetric Segmentation from Sparse Annotation\n\n[论文地址](https://arxiv.org/pdf/1606.06650.pdf)\n\n在生物医学领域，3D数据是很多的，一层一层转化为2D数据去标注训是不现实的(及其耗时)，而且用整个3D体积的全部数据去训练既低效又极有可能过拟合(相邻切片的数据是非常相近的)。这篇文章提出的3D Unet只需要少部分2D的标注切片就可以生成密集的立体的分割。此网络主要有两个不同的作用方式:\n* Semi-automated setup: 在一个少量稀疏标注的数据集上训练并在此数据集的图像上预测其他未标注的地方。\n* Fully-automated setup: 在representative的稀疏标注的数据集训练，然后用来切割新的图像。\n<center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/3D-Segmentation/4.png)</center>\n\n**改进**\n* 在2D U-Net的基础上，仍然使用encoder去分析整个图片, 但是扩展了decoder来产生full-resolution的切割。\n* 使用3D数据作为输入，因此网络改用3D convolutions, 3D max pooling 和 3D up-convolutional。\n* 避免使用bottlenecks结构因为输入的数据不会很多，避免丢失重要的信息。\n\n**网络**\n<center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/3D-Segmentation/5.png)</center>\n\n* Encoder部分: 每层包含两个$3 \\times 3 \\times 3$卷积和ReLU, 然后是一个strides为2的$2 \\times 2 \\times 2$的最大池化层。\n* Decoder部分: 每层包含strides为2的$2 \\times 2 \\times 2$的upconvolution以及两个$3 \\times 3 \\times 3$卷积和ReLU\n* 这里作者还用了Batch Normalization 防止梯度爆炸，并且在BN后增加了缩放和平移$x_{new}=\\alpha \\cdot x + \\beta$, 其中两个超参是学习出来的。\n* 能在稀疏标注的训练集训练的原因是使用了weighted softmanx loss function, 把unlabeled pixels的权重置为0让网络只学习有标注的部分并且提高了泛化能力。\n\n---\n\n## Joint Sequence Learning and Cross-Modality Convolution for 3D Biomedical Segmentation\n\n[论文地址](https://arxiv.org/pdf/1704.07754.pdf)\n\n<center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/3D-Segmentation/9.png)</center>\n\n目前对于定位肿瘤的难点在于：\n1. 肿瘤形状各异，例如神经胶质瘤与胶质母细胞瘤形状不同\n2. 肿瘤分布广泛，可能分布于大脑的任何区域\n\n这篇文章提出来一个思路就是交叉形态卷积的方法做一个 encoder-decoder 的网络结构，然后同时用LSTM对2D的切片序列建模。\n\nMRI也是跟CT一样断层扫描的过程且包含4种模态(Modality)，就是它一层一层，一层扫出来的就是一个2D的图片，然后多层累计起来就是3D的，但是其实切割是要切割出3D的脑部肿瘤位置，这样就需要把2D的变成3D的，把2D的切片之间的关系通过LSTM描述出来，最后把多模态卷积和LSTM网络结合在一起，达到3D切割。\n\n\n**模型**\n<center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/3D-Segmentation/6.png)</center>\n\n这个方法的framework大概是这样的，从左到右看。\n\n*   首先每一个脑部的MRI数据，他都是通过四种模态切出来的，这里用四种不同的颜色来表示，相当于每一个slice就是我说的那个2D的图片。\n\n*   切完之后他会把四个模态，就是到图b这个阶段了，四个模态交叉在一起做一个multi-modal的encoder，这个encoder就是用一个神经网络来实现的。\n\n*   四个模态encode到一起之后，在这一步就用神经网络把四个模态下的脑部切割出来了，这是2D的情况下。\n\n*   然后再加上convolution LSTM把2D的切割、2D和2D之间的dependency描述出来之后就形成了3D的切割，然后再做一下decoder，展现成最后这种形式。在最中间有一个切割出来的东西，其他没被切割到的background。\n\n**方法**\n<center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/3D-Segmentation/7.png)</center>\n\n1. **MME(Multi-Modal Encoder)**\n\n  类似于SegNet里的编码器结构, 因为数据集比较小，因此网络也简化了，使用四个卷积核，通过batch-normalization，然后加一个非线性变换，在后面有四个最大池化层。\n  \n2. **MRF(Multi-Resolution Fusion)**\n    <center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/3D-Segmentation/8.png)</center>\n    结合多尺度多模态的信息，通过在不同的尺度的encoder和decoder中进行feature multiplication代替级联因此不会增加特征映射的大小。\n    \n3. **CMC(Cross-Modality COnvolution)**\n\n  CMC可以把空间信息以及不同模态的关系结合到一起。\n  \n  四个模态的数据进入到这个卷积网络之后，就会把相同channel下的每一个模态stack在一起形成一个block, 就是每个channel里面有 C 个slice，就是说它是一个立体结构了，一个的长宽是H、W，高是C的这种。四个模态弄到一起就是C×4×H×W。然后通过一个三维的卷积，卷积的大小里有个C×4，也就是用4×1×1的一个卷积核，做卷积之后得到每一层的切割出来的特征。\n\n4. **Slice Sequence Learning**\n\n  使用一个端到端的切片序列学习框架去建模切片之间的相关性。这个convolution LSTM跟普通的LSTM有一个区别，就是把原来的矩阵相乘替换为一个卷积操作，就是普通的乘法变成卷积层，这样它就能够在把之前状态的空间信息保留着。其实它的目的就是，卷积LSTM会描述一个2D切割边缘的趋势，比如说这一张中切片它的形态是这样的，然后到下一张它会有一个轻微的变化，要把这种变化描述出来。\n  \n  此外不同切片的convLSTM的权重是共享的因此需要更新的参数不会随着序列增加。\n  \n5. **Decoder**\n\n  包含上采样以及soft-max进行分类\n  \n**训练**\n1. **Single Slice Training**:\n\n  使用了median frequency balancing调节cross-entropy loss的权重来缓解数据不平衡的情况(98%的都是健康区域)\n  $$\\alpha_{c}=\\frac{median\\_freq}{freq(c)}$$\n  \n2. **Two-Phase Training**:\n\n  * 第一阶段：只采样包含肿瘤的切片并且使用median frequency balancing的方法进行训练。\n  * 第二阶段：去除median frequency策略并且调低学习率，在真实的肿瘤分布概率下进行训练\n\n**结果**\n<center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/3D-Segmentation/10.png)</center>","slug":"MalongTech-Learning-3D-Image-Segmentation","published":1,"updated":"2020-04-28T12:38:55.395Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck9jwdng60009jlrclgm4jkxk","content":"<h1 id=\"医学相关的图像分割技术\"><a href=\"#医学相关的图像分割技术\" class=\"headerlink\" title=\"医学相关的图像分割技术\"></a>医学相关的图像分割技术</h1><center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/3D-Segmentation/1.png\" alt=\"image\"></center>\n\n<table>\n<thead>\n<tr>\n<th>医学2D</th>\n<th>医学3D</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>A Novel Domain Adaptation Framework for Medical Image Segmentation</td>\n<td>3D U-Net: Learning Dense Volumetric Segmentation from Sparrse Annotation</td>\n</tr>\n<tr>\n<td>DeepMedic for Brain Tumor Segmentation</td>\n<td>Joint Sequence Learning and Cross-Modality Convolution for 3D Biomedical Segmentation</td>\n</tr>\n<tr>\n<td></td>\n<td>Simultaneous Super-Resolution and Cross-Modality Synthesis of 3D Medical Image using Weekly-Supervised Joint Convolutional Sparse Coding</td>\n</tr>\n</tbody>\n</table>\n<a id=\"more\"></a>\n<hr>\n<h2 id=\"A-Novel-Domain-Adaptation-Framework-for-Medical-Image-Segmentation\"><a href=\"#A-Novel-Domain-Adaptation-Framework-for-Medical-Image-Segmentation\" class=\"headerlink\" title=\"A Novel Domain Adaptation Framework for Medical Image Segmentation\"></a>A Novel Domain Adaptation Framework for Medical Image Segmentation</h2><p><a href=\"https://arxiv.org/pdf/1810.05732.pdf\" target=\"_blank\" rel=\"noopener\">论文地址</a></p>\n<p><strong>脑部肿瘤切割</strong></p>\n<ul>\n<li>难点在于难以精确定位肿瘤(肿瘤形状各异，分布广泛)</li>\n<li>核磁共振4种模态(Modality)<ol>\n<li>自旋晶格驰像(T1)</li>\n<li>T1对比(T2)</li>\n<li>自旋松弛(T2)</li>\n<li>流体衰减反转恢复(FLAIR)</li>\n</ol>\n</li>\n</ul>\n<p><strong>论文创新点</strong>:</p>\n<ul>\n<li>A biophysics based domain adaptation method(物理肿瘤生长模型加上对抗网络生成逼真的MR图像补充数据集)</li>\n<li>An automatic method to 分割健康的组织(白质灰质，脑脊液)，通过健康的组织轮廓辅助脑补图像切割</li>\n</ul>\n<p><strong>方法</strong></p>\n<ul>\n<li><strong>Data Augmentation:</strong> 使用基于生物学的肿瘤生长模型(PDE)模拟合成的肿瘤，在使用一个辅助的神经网络修正模拟肿瘤到correct intensities distribution对比真实的MR图像(会通过强加循环一致性限制分布情况)</li>\n<li></li>\n<li><p><strong>Extened segmentation:</strong> 扩展分割到健康的薄壁组织。先使用(in-house diffeomorphic registration code)微分同胚技术(是一种光滑可逆的变换，在MRI图像配准中可以保证形变后的拓扑结构保持不变，同时避免出现不合理的物理现象)处理过的数据进行训练。<center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/3D-Segmentation/2.png\" alt=\"image\"></center></p>\n<p>然后再使用DNN去分割健康的组织(神经胶质，脑脊液，灰质和白质)，结果如上图。 这样可以增加健康组织的轮廓作为重要的训练信息并且改进了原来的类不平衡的问题。</p>\n<p>具体的步骤分为:</p>\n<ul>\n<li>Affine registration of each atlas image to the brats image</li>\n<li>Diffeomorphic registration of each atlas image to the BraTS image(把健康组织的信息匹配BraTS的数据集结构)</li>\n<li>Majority voting to fuse labels of all deformed atlases to get the final healthy tissuse segmentation</li>\n</ul>\n</li>\n<li><p><strong>模型</strong></p>\n<ol>\n<li><p>3D U-Net(会在下面讨论)</p>\n<p>使用3D进行第一阶段的检测，获得肿瘤的初始位置。</p>\n</li>\n<li><p>U-Net(在前一章节讲过)</p>\n<p>使用2D的U-Net以及domain adaptation results获得最终的分割结果</p>\n<center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/3D-Segmentation/3.png\" alt=\"image\"></center>\n</li>\n</ol>\n</li>\n<li><p><strong>缺点</strong></p>\n<p>这个框架只支持2D的domain trasformations，所以对3D的数据只能进行切片并且最后是用的2D的神经网络，这样没有3D的网络来的efficient以及精确。</p>\n</li>\n</ul>\n<hr>\n<h2 id=\"3D-U-Net-Learning-Dense-Volumetric-Segmentation-from-Sparse-Annotation\"><a href=\"#3D-U-Net-Learning-Dense-Volumetric-Segmentation-from-Sparse-Annotation\" class=\"headerlink\" title=\"3D U-Net: Learning Dense Volumetric Segmentation from Sparse Annotation\"></a>3D U-Net: Learning Dense Volumetric Segmentation from Sparse Annotation</h2><p><a href=\"https://arxiv.org/pdf/1606.06650.pdf\" target=\"_blank\" rel=\"noopener\">论文地址</a></p>\n<p>在生物医学领域，3D数据是很多的，一层一层转化为2D数据去标注训是不现实的(及其耗时)，而且用整个3D体积的全部数据去训练既低效又极有可能过拟合(相邻切片的数据是非常相近的)。这篇文章提出的3D Unet只需要少部分2D的标注切片就可以生成密集的立体的分割。此网络主要有两个不同的作用方式:</p>\n<ul>\n<li>Semi-automated setup: 在一个少量稀疏标注的数据集上训练并在此数据集的图像上预测其他未标注的地方。</li>\n<li>Fully-automated setup: 在representative的稀疏标注的数据集训练，然后用来切割新的图像。<center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/3D-Segmentation/4.png\" alt=\"image\"></center>\n\n</li>\n</ul>\n<p><strong>改进</strong></p>\n<ul>\n<li>在2D U-Net的基础上，仍然使用encoder去分析整个图片, 但是扩展了decoder来产生full-resolution的切割。</li>\n<li>使用3D数据作为输入，因此网络改用3D convolutions, 3D max pooling 和 3D up-convolutional。</li>\n<li>避免使用bottlenecks结构因为输入的数据不会很多，避免丢失重要的信息。</li>\n</ul>\n<p><strong>网络</strong></p>\n<center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/3D-Segmentation/5.png\" alt=\"image\"></center>\n\n<ul>\n<li>Encoder部分: 每层包含两个$3 \\times 3 \\times 3$卷积和ReLU, 然后是一个strides为2的$2 \\times 2 \\times 2$的最大池化层。</li>\n<li>Decoder部分: 每层包含strides为2的$2 \\times 2 \\times 2$的upconvolution以及两个$3 \\times 3 \\times 3$卷积和ReLU</li>\n<li>这里作者还用了Batch Normalization 防止梯度爆炸，并且在BN后增加了缩放和平移$x_{new}=\\alpha \\cdot x + \\beta$, 其中两个超参是学习出来的。</li>\n<li>能在稀疏标注的训练集训练的原因是使用了weighted softmanx loss function, 把unlabeled pixels的权重置为0让网络只学习有标注的部分并且提高了泛化能力。</li>\n</ul>\n<hr>\n<h2 id=\"Joint-Sequence-Learning-and-Cross-Modality-Convolution-for-3D-Biomedical-Segmentation\"><a href=\"#Joint-Sequence-Learning-and-Cross-Modality-Convolution-for-3D-Biomedical-Segmentation\" class=\"headerlink\" title=\"Joint Sequence Learning and Cross-Modality Convolution for 3D Biomedical Segmentation\"></a>Joint Sequence Learning and Cross-Modality Convolution for 3D Biomedical Segmentation</h2><p><a href=\"https://arxiv.org/pdf/1704.07754.pdf\" target=\"_blank\" rel=\"noopener\">论文地址</a></p>\n<center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/3D-Segmentation/9.png\" alt=\"image\"></center>\n\n<p>目前对于定位肿瘤的难点在于：</p>\n<ol>\n<li>肿瘤形状各异，例如神经胶质瘤与胶质母细胞瘤形状不同</li>\n<li>肿瘤分布广泛，可能分布于大脑的任何区域</li>\n</ol>\n<p>这篇文章提出来一个思路就是交叉形态卷积的方法做一个 encoder-decoder 的网络结构，然后同时用LSTM对2D的切片序列建模。</p>\n<p>MRI也是跟CT一样断层扫描的过程且包含4种模态(Modality)，就是它一层一层，一层扫出来的就是一个2D的图片，然后多层累计起来就是3D的，但是其实切割是要切割出3D的脑部肿瘤位置，这样就需要把2D的变成3D的，把2D的切片之间的关系通过LSTM描述出来，最后把多模态卷积和LSTM网络结合在一起，达到3D切割。</p>\n<p><strong>模型</strong></p>\n<center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/3D-Segmentation/6.png\" alt=\"image\"></center>\n\n<p>这个方法的framework大概是这样的，从左到右看。</p>\n<ul>\n<li><p>首先每一个脑部的MRI数据，他都是通过四种模态切出来的，这里用四种不同的颜色来表示，相当于每一个slice就是我说的那个2D的图片。</p>\n</li>\n<li><p>切完之后他会把四个模态，就是到图b这个阶段了，四个模态交叉在一起做一个multi-modal的encoder，这个encoder就是用一个神经网络来实现的。</p>\n</li>\n<li><p>四个模态encode到一起之后，在这一步就用神经网络把四个模态下的脑部切割出来了，这是2D的情况下。</p>\n</li>\n<li><p>然后再加上convolution LSTM把2D的切割、2D和2D之间的dependency描述出来之后就形成了3D的切割，然后再做一下decoder，展现成最后这种形式。在最中间有一个切割出来的东西，其他没被切割到的background。</p>\n</li>\n</ul>\n<p><strong>方法</strong></p>\n<center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/3D-Segmentation/7.png\" alt=\"image\"></center>\n\n<ol>\n<li><p><strong>MME(Multi-Modal Encoder)</strong></p>\n<p>类似于SegNet里的编码器结构, 因为数据集比较小，因此网络也简化了，使用四个卷积核，通过batch-normalization，然后加一个非线性变换，在后面有四个最大池化层。</p>\n</li>\n<li><p><strong>MRF(Multi-Resolution Fusion)</strong><br> <center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/3D-Segmentation/8.png\" alt=\"image\"></center><br> 结合多尺度多模态的信息，通过在不同的尺度的encoder和decoder中进行feature multiplication代替级联因此不会增加特征映射的大小。</p>\n</li>\n<li><p><strong>CMC(Cross-Modality COnvolution)</strong></p>\n<p>CMC可以把空间信息以及不同模态的关系结合到一起。</p>\n<p>四个模态的数据进入到这个卷积网络之后，就会把相同channel下的每一个模态stack在一起形成一个block, 就是每个channel里面有 C 个slice，就是说它是一个立体结构了，一个的长宽是H、W，高是C的这种。四个模态弄到一起就是C×4×H×W。然后通过一个三维的卷积，卷积的大小里有个C×4，也就是用4×1×1的一个卷积核，做卷积之后得到每一层的切割出来的特征。</p>\n</li>\n<li><p><strong>Slice Sequence Learning</strong></p>\n<p>使用一个端到端的切片序列学习框架去建模切片之间的相关性。这个convolution LSTM跟普通的LSTM有一个区别，就是把原来的矩阵相乘替换为一个卷积操作，就是普通的乘法变成卷积层，这样它就能够在把之前状态的空间信息保留着。其实它的目的就是，卷积LSTM会描述一个2D切割边缘的趋势，比如说这一张中切片它的形态是这样的，然后到下一张它会有一个轻微的变化，要把这种变化描述出来。</p>\n<p>此外不同切片的convLSTM的权重是共享的因此需要更新的参数不会随着序列增加。</p>\n</li>\n<li><p><strong>Decoder</strong></p>\n<p>包含上采样以及soft-max进行分类</p>\n</li>\n</ol>\n<p><strong>训练</strong></p>\n<ol>\n<li><p><strong>Single Slice Training</strong>:</p>\n<p>使用了median frequency balancing调节cross-entropy loss的权重来缓解数据不平衡的情况(98%的都是健康区域)<br>$$\\alpha_{c}=\\frac{median_freq}{freq(c)}$$</p>\n</li>\n<li><p><strong>Two-Phase Training</strong>:</p>\n<ul>\n<li>第一阶段：只采样包含肿瘤的切片并且使用median frequency balancing的方法进行训练。</li>\n<li>第二阶段：去除median frequency策略并且调低学习率，在真实的肿瘤分布概率下进行训练</li>\n</ul>\n</li>\n</ol>\n<p><strong>结果</strong></p>\n<center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/3D-Segmentation/10.png\" alt=\"image\"></center>","site":{"data":{}},"excerpt":"<h1 id=\"医学相关的图像分割技术\"><a href=\"#医学相关的图像分割技术\" class=\"headerlink\" title=\"医学相关的图像分割技术\"></a>医学相关的图像分割技术</h1><center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/3D-Segmentation/1.png\" alt=\"image\"></center>\n\n<table>\n<thead>\n<tr>\n<th>医学2D</th>\n<th>医学3D</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>A Novel Domain Adaptation Framework for Medical Image Segmentation</td>\n<td>3D U-Net: Learning Dense Volumetric Segmentation from Sparrse Annotation</td>\n</tr>\n<tr>\n<td>DeepMedic for Brain Tumor Segmentation</td>\n<td>Joint Sequence Learning and Cross-Modality Convolution for 3D Biomedical Segmentation</td>\n</tr>\n<tr>\n<td></td>\n<td>Simultaneous Super-Resolution and Cross-Modality Synthesis of 3D Medical Image using Weekly-Supervised Joint Convolutional Sparse Coding</td>\n</tr>\n</tbody>\n</table>","more":"<hr>\n<h2 id=\"A-Novel-Domain-Adaptation-Framework-for-Medical-Image-Segmentation\"><a href=\"#A-Novel-Domain-Adaptation-Framework-for-Medical-Image-Segmentation\" class=\"headerlink\" title=\"A Novel Domain Adaptation Framework for Medical Image Segmentation\"></a>A Novel Domain Adaptation Framework for Medical Image Segmentation</h2><p><a href=\"https://arxiv.org/pdf/1810.05732.pdf\" target=\"_blank\" rel=\"noopener\">论文地址</a></p>\n<p><strong>脑部肿瘤切割</strong></p>\n<ul>\n<li>难点在于难以精确定位肿瘤(肿瘤形状各异，分布广泛)</li>\n<li>核磁共振4种模态(Modality)<ol>\n<li>自旋晶格驰像(T1)</li>\n<li>T1对比(T2)</li>\n<li>自旋松弛(T2)</li>\n<li>流体衰减反转恢复(FLAIR)</li>\n</ol>\n</li>\n</ul>\n<p><strong>论文创新点</strong>:</p>\n<ul>\n<li>A biophysics based domain adaptation method(物理肿瘤生长模型加上对抗网络生成逼真的MR图像补充数据集)</li>\n<li>An automatic method to 分割健康的组织(白质灰质，脑脊液)，通过健康的组织轮廓辅助脑补图像切割</li>\n</ul>\n<p><strong>方法</strong></p>\n<ul>\n<li><strong>Data Augmentation:</strong> 使用基于生物学的肿瘤生长模型(PDE)模拟合成的肿瘤，在使用一个辅助的神经网络修正模拟肿瘤到correct intensities distribution对比真实的MR图像(会通过强加循环一致性限制分布情况)</li>\n<li></li>\n<li><p><strong>Extened segmentation:</strong> 扩展分割到健康的薄壁组织。先使用(in-house diffeomorphic registration code)微分同胚技术(是一种光滑可逆的变换，在MRI图像配准中可以保证形变后的拓扑结构保持不变，同时避免出现不合理的物理现象)处理过的数据进行训练。<center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/3D-Segmentation/2.png\" alt=\"image\"></center></p>\n<p>然后再使用DNN去分割健康的组织(神经胶质，脑脊液，灰质和白质)，结果如上图。 这样可以增加健康组织的轮廓作为重要的训练信息并且改进了原来的类不平衡的问题。</p>\n<p>具体的步骤分为:</p>\n<ul>\n<li>Affine registration of each atlas image to the brats image</li>\n<li>Diffeomorphic registration of each atlas image to the BraTS image(把健康组织的信息匹配BraTS的数据集结构)</li>\n<li>Majority voting to fuse labels of all deformed atlases to get the final healthy tissuse segmentation</li>\n</ul>\n</li>\n<li><p><strong>模型</strong></p>\n<ol>\n<li><p>3D U-Net(会在下面讨论)</p>\n<p>使用3D进行第一阶段的检测，获得肿瘤的初始位置。</p>\n</li>\n<li><p>U-Net(在前一章节讲过)</p>\n<p>使用2D的U-Net以及domain adaptation results获得最终的分割结果</p>\n<center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/3D-Segmentation/3.png\" alt=\"image\"></center>\n</li>\n</ol>\n</li>\n<li><p><strong>缺点</strong></p>\n<p>这个框架只支持2D的domain trasformations，所以对3D的数据只能进行切片并且最后是用的2D的神经网络，这样没有3D的网络来的efficient以及精确。</p>\n</li>\n</ul>\n<hr>\n<h2 id=\"3D-U-Net-Learning-Dense-Volumetric-Segmentation-from-Sparse-Annotation\"><a href=\"#3D-U-Net-Learning-Dense-Volumetric-Segmentation-from-Sparse-Annotation\" class=\"headerlink\" title=\"3D U-Net: Learning Dense Volumetric Segmentation from Sparse Annotation\"></a>3D U-Net: Learning Dense Volumetric Segmentation from Sparse Annotation</h2><p><a href=\"https://arxiv.org/pdf/1606.06650.pdf\" target=\"_blank\" rel=\"noopener\">论文地址</a></p>\n<p>在生物医学领域，3D数据是很多的，一层一层转化为2D数据去标注训是不现实的(及其耗时)，而且用整个3D体积的全部数据去训练既低效又极有可能过拟合(相邻切片的数据是非常相近的)。这篇文章提出的3D Unet只需要少部分2D的标注切片就可以生成密集的立体的分割。此网络主要有两个不同的作用方式:</p>\n<ul>\n<li>Semi-automated setup: 在一个少量稀疏标注的数据集上训练并在此数据集的图像上预测其他未标注的地方。</li>\n<li>Fully-automated setup: 在representative的稀疏标注的数据集训练，然后用来切割新的图像。<center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/3D-Segmentation/4.png\" alt=\"image\"></center>\n\n</li>\n</ul>\n<p><strong>改进</strong></p>\n<ul>\n<li>在2D U-Net的基础上，仍然使用encoder去分析整个图片, 但是扩展了decoder来产生full-resolution的切割。</li>\n<li>使用3D数据作为输入，因此网络改用3D convolutions, 3D max pooling 和 3D up-convolutional。</li>\n<li>避免使用bottlenecks结构因为输入的数据不会很多，避免丢失重要的信息。</li>\n</ul>\n<p><strong>网络</strong></p>\n<center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/3D-Segmentation/5.png\" alt=\"image\"></center>\n\n<ul>\n<li>Encoder部分: 每层包含两个$3 \\times 3 \\times 3$卷积和ReLU, 然后是一个strides为2的$2 \\times 2 \\times 2$的最大池化层。</li>\n<li>Decoder部分: 每层包含strides为2的$2 \\times 2 \\times 2$的upconvolution以及两个$3 \\times 3 \\times 3$卷积和ReLU</li>\n<li>这里作者还用了Batch Normalization 防止梯度爆炸，并且在BN后增加了缩放和平移$x_{new}=\\alpha \\cdot x + \\beta$, 其中两个超参是学习出来的。</li>\n<li>能在稀疏标注的训练集训练的原因是使用了weighted softmanx loss function, 把unlabeled pixels的权重置为0让网络只学习有标注的部分并且提高了泛化能力。</li>\n</ul>\n<hr>\n<h2 id=\"Joint-Sequence-Learning-and-Cross-Modality-Convolution-for-3D-Biomedical-Segmentation\"><a href=\"#Joint-Sequence-Learning-and-Cross-Modality-Convolution-for-3D-Biomedical-Segmentation\" class=\"headerlink\" title=\"Joint Sequence Learning and Cross-Modality Convolution for 3D Biomedical Segmentation\"></a>Joint Sequence Learning and Cross-Modality Convolution for 3D Biomedical Segmentation</h2><p><a href=\"https://arxiv.org/pdf/1704.07754.pdf\" target=\"_blank\" rel=\"noopener\">论文地址</a></p>\n<center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/3D-Segmentation/9.png\" alt=\"image\"></center>\n\n<p>目前对于定位肿瘤的难点在于：</p>\n<ol>\n<li>肿瘤形状各异，例如神经胶质瘤与胶质母细胞瘤形状不同</li>\n<li>肿瘤分布广泛，可能分布于大脑的任何区域</li>\n</ol>\n<p>这篇文章提出来一个思路就是交叉形态卷积的方法做一个 encoder-decoder 的网络结构，然后同时用LSTM对2D的切片序列建模。</p>\n<p>MRI也是跟CT一样断层扫描的过程且包含4种模态(Modality)，就是它一层一层，一层扫出来的就是一个2D的图片，然后多层累计起来就是3D的，但是其实切割是要切割出3D的脑部肿瘤位置，这样就需要把2D的变成3D的，把2D的切片之间的关系通过LSTM描述出来，最后把多模态卷积和LSTM网络结合在一起，达到3D切割。</p>\n<p><strong>模型</strong></p>\n<center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/3D-Segmentation/6.png\" alt=\"image\"></center>\n\n<p>这个方法的framework大概是这样的，从左到右看。</p>\n<ul>\n<li><p>首先每一个脑部的MRI数据，他都是通过四种模态切出来的，这里用四种不同的颜色来表示，相当于每一个slice就是我说的那个2D的图片。</p>\n</li>\n<li><p>切完之后他会把四个模态，就是到图b这个阶段了，四个模态交叉在一起做一个multi-modal的encoder，这个encoder就是用一个神经网络来实现的。</p>\n</li>\n<li><p>四个模态encode到一起之后，在这一步就用神经网络把四个模态下的脑部切割出来了，这是2D的情况下。</p>\n</li>\n<li><p>然后再加上convolution LSTM把2D的切割、2D和2D之间的dependency描述出来之后就形成了3D的切割，然后再做一下decoder，展现成最后这种形式。在最中间有一个切割出来的东西，其他没被切割到的background。</p>\n</li>\n</ul>\n<p><strong>方法</strong></p>\n<center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/3D-Segmentation/7.png\" alt=\"image\"></center>\n\n<ol>\n<li><p><strong>MME(Multi-Modal Encoder)</strong></p>\n<p>类似于SegNet里的编码器结构, 因为数据集比较小，因此网络也简化了，使用四个卷积核，通过batch-normalization，然后加一个非线性变换，在后面有四个最大池化层。</p>\n</li>\n<li><p><strong>MRF(Multi-Resolution Fusion)</strong><br> <center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/3D-Segmentation/8.png\" alt=\"image\"></center><br> 结合多尺度多模态的信息，通过在不同的尺度的encoder和decoder中进行feature multiplication代替级联因此不会增加特征映射的大小。</p>\n</li>\n<li><p><strong>CMC(Cross-Modality COnvolution)</strong></p>\n<p>CMC可以把空间信息以及不同模态的关系结合到一起。</p>\n<p>四个模态的数据进入到这个卷积网络之后，就会把相同channel下的每一个模态stack在一起形成一个block, 就是每个channel里面有 C 个slice，就是说它是一个立体结构了，一个的长宽是H、W，高是C的这种。四个模态弄到一起就是C×4×H×W。然后通过一个三维的卷积，卷积的大小里有个C×4，也就是用4×1×1的一个卷积核，做卷积之后得到每一层的切割出来的特征。</p>\n</li>\n<li><p><strong>Slice Sequence Learning</strong></p>\n<p>使用一个端到端的切片序列学习框架去建模切片之间的相关性。这个convolution LSTM跟普通的LSTM有一个区别，就是把原来的矩阵相乘替换为一个卷积操作，就是普通的乘法变成卷积层，这样它就能够在把之前状态的空间信息保留着。其实它的目的就是，卷积LSTM会描述一个2D切割边缘的趋势，比如说这一张中切片它的形态是这样的，然后到下一张它会有一个轻微的变化，要把这种变化描述出来。</p>\n<p>此外不同切片的convLSTM的权重是共享的因此需要更新的参数不会随着序列增加。</p>\n</li>\n<li><p><strong>Decoder</strong></p>\n<p>包含上采样以及soft-max进行分类</p>\n</li>\n</ol>\n<p><strong>训练</strong></p>\n<ol>\n<li><p><strong>Single Slice Training</strong>:</p>\n<p>使用了median frequency balancing调节cross-entropy loss的权重来缓解数据不平衡的情况(98%的都是健康区域)<br>$$\\alpha_{c}=\\frac{median_freq}{freq(c)}$$</p>\n</li>\n<li><p><strong>Two-Phase Training</strong>:</p>\n<ul>\n<li>第一阶段：只采样包含肿瘤的切片并且使用median frequency balancing的方法进行训练。</li>\n<li>第二阶段：去除median frequency策略并且调低学习率，在真实的肿瘤分布概率下进行训练</li>\n</ul>\n</li>\n</ol>\n<p><strong>结果</strong></p>\n<center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/3D-Segmentation/10.png\" alt=\"image\"></center>"},{"title":"MalongTech 学习内容:图像分割-2D","date":"2018-11-26T03:00:00.000Z","_content":"\n# Image Segmentation\n\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Image-Segmentation-Learning/1.jpg)\n\n## Introduction\n在计算机视觉领域，图像分割（Segmentation）指的是将数字图像细分为多个图像子区域（像素的集合）（也被称作超像素）的过程。图像分割的目的是简化或改变图像的表示形式，使得图像更容易理解和分析。图像分割通常用于定位图像中的物体和边界（线，曲线等）。更精确的，图像分割是对图像中的每个像素加标签的一个过程，这一过程使得具有相同标签的像素具有某种共同视觉特性。<!-- more -->\n\n图像分割的结果是图像上子区域的集合（这些子区域的全体覆盖了整个图像），或是从图像中提取的轮廓线的集合（例如边缘检测）。一个子区域中的每个像素在某种特性的度量下或是由计算得出的特性都是相似的，例如颜色、亮度、纹理。邻接区域在某种特性的度量下有很大的不同。\n\n**应用**\n* 医学影像：1. 肿瘤和其他病理的定位；2. 组织体积的测量；3. 计算机引导的手术；4. 诊断；5. 治疗方案的定制；6. 解剖学结构的研究\n* 卫星图像中定位物体\n* 人脸识别\n* 指纹识别\n* 交通控制系统\n\n## Tranditional Methods\n[参考](https://zhuanlan.zhihu.com/p/30732385)\n* **基于阙值的分割方法**: 1. 固定阙值分割；2. 直方图双峰法；3. 迭代阙值图像分割；自适应阙值图像分割（最大分类方差法，均值法，最佳阙值）.\n* **基于边缘的分割方法**: 1. Canny边缘检测器；2. Harris角点检测器；3. SIFT检测器；3. SURF检测器.\n* **基于区域的分割方法**: 1. 种子区域生长法；2. 区域分裂合并法；3. 分水岭法.\n* **基于图论的分割方法**: 1. GraphCut; 2. GrabCut; 3. Random Walk.\n* **基于能量泛函的分割方法**: 参数活动轮廓模型（1. Snake模型；2. Active Shape Model; 3. Active Apperance Models; 4. Constrained local model）；几何活动轮廓模型.\n\n## Datasets\n1. Pascal VOC: 20个类别，6929张标注图片\n2. CityScapes：道路驾驶场景，30个类别，5000张精细标注，20000张粗糙标注\n3. MS COCO：80类，33万张图片，超过20万张有标注，150万个物体的个体\n4. 医学影像领域的ImageNet: [DeepLesion](https://www.52cv.net/?p=883), 10000多个病例研究的超过32000个病变标注\n\n## 图像分割的度量标准\n假设共有$k+1$个类（从$L_{o}$到$L_{k}$,其中包含一个空类或背景）, $P_{ij}$表示本属于类**i**但被预测为类**j**的像素数量。即，$p_{ii}$表示**真正**的数量，而$p_{ij}$和$p_{ji}$则分别被解释为假正和假负，尽管两者都是假正与假负之和。\n1. Pixel Accuracy(PA, 像素精度): 最简单的度量，为标记正确的像素占总像素的比例。\n$$PA=\\frac{\\sum_{i=0}^{k}p_{ii}}{\\sum_{i=0}^{k}\\sum_{j=0}^{k}p_{ij}}$$\n2. Mean Pixel Accuracy(MAP,均像素精度): PA的一种简单提升，计算每个类内被正确分类像素数的比例，之后求所有类的平均。\n$$MAP=\\frac{1}{k+1}\\sum_{i=0}^{k}\\frac{p_{ii}}{\\sum_{j=0}^{k}p_{ij}}$$\n3. Mean Intersection over Union(MIoU, 均交并比): 为语义分割的标准度量。其计算两个集合的交集和并集之比，这两个集合为真实值(ground truth)和预测值(predicted segmentation). 这个比例可以变形为正真数(intersection)比上真正、假负、假正(并集)之和，之后在每个类上计算IoU再平均。\n$$MIoU = \\frac{1}{k+1}\\sum_{i=0}^{k}\\frac{p_{ii}}{\\sum_{j=0}^{k}p_{ij}+\\sum_{j=0}^{k}(p_{ji}-p_{ii})}$$\n4. Frequency Weighted Intersection over Union(FWIoU, 频权交并比): 为MIoU的一种提升，根据每个类出现的频率为其设置权重。\n$$FWIoU = \\frac{1}{\\sum_{i=0}^{k}\\sum_{j=0}^{k}p_{ij}}\\sum_{i=0}^{k}\\frac{p_{ii}}{\\sum_{j=0}^{k}p_{ij}+\\sum_{j=0}^{k}(p_{ji}-p_{ii})}$$\n\n# 深度学习算法\n## Fully Convolutional Networks\n[论文地址](https://people.eecs.berkeley.edu/~jonlong/long_shelhamer_fcn.pdf)\n传统神经网络做分类的步骤是，首先是一个图像进来之后经过多层卷积得到降维之后的特征图，这个特征图经过全连接层变成一个分类器，最后输出一个类别的向量，这就是分类的结果。\n\n而 FCN 是把所有的全连接层换成卷基层，原来只能输出一个类别分类的网络可以在特征图的每一个像素输出一个分类结果。这样就把分类的向量，变成了一个分类的特征图。\n<center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Image-Segmentation-Learning/2.png)</center>\n\n上图中的猫, 输入AlexNet, 得到一个长为1000的输出向量, 表示输入图像属于每一类的概率, 其中在“tabby cat”这一类统计概率最高。而FCN对图像进行像素级的分类，从而解决了语义级别的图像分割（semantic segmentation）问题。FCN可以接受任意尺寸的输入图像，采用反卷积层对最后一个卷积层的feature map进行上采样, 使它恢复到输入图像相同的尺寸，从而可以对每个像素都产生了一个预测, 同时保留了原始输入图像中的空间信息, 最后在上采样的特征图上进行逐像素分类。最后逐个像素计算softmax分类的损失, 相当于每一个像素对应一个训练样本。\n\n**全连接->卷积层**：\n\n一个 K=4096 的全连接层，输入数据体的尺寸是 7∗7∗512，这个全连接层可以被等效地看做一个 F=7,P=0,S=1,K=4096 的卷积层.\n\n假设一个卷积神经网络的输入是 224x224x3 的图像，一系列的卷积层和下采样层将图像数据变为尺寸为 7x7x512 的激活数据体。AlexNet使用了两个尺寸为4096的全连接层，最后一个有1000个神经元的全连接层用于计算分类评分。我们可以将这3个全连接层中的任意一个转化为卷积层：\n* 针对第一个连接区域是[7x7x512]的全连接层，令其滤波器尺寸为F=7，这样输出数据体就为[1x1x4096]了。\n* 针对第二个全连接层，令其滤波器尺寸为F=1，这样输出数据体为[1x1x4096]。\n* 对最后一个全连接层也做类似的，令其F=1，最终输出为[1x1x1000]\n\n**end to end, pixels to pixels network**\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Image-Segmentation-Learning/3.png)\n经过多次卷积和pooling以后，得到的图像越来越小，分辨率越来越低。其中图像到$\\frac{H}{32} \\times \\frac{W}{32}$的时候图片是最小的一层时，所产生图叫做heatmap热图，热图就是最重要的高维特征图，得到高维特征的heatmap之后就是最重要的一步也是最后的一步对原图像进行upsampling，把图像进行放大、放大、放大，到原图像的大小。最后的输出是1000张heatmap经过upsampling变为原图大小的图片，为了对每个像素进行分类预测label成最后已经进行语义分割的图像，这里有一个小trick，就是最后通过逐个像素地求其在1000张图像该像素位置的最大数值描述（概率）作为该像素的分类。因此产生了一张已经分类好的图片，如上图右侧有狗狗和猫猫的图。\n\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Image-Segmentation-Learning/4.png)\n\n现在我们有1/32尺寸的heatMap，1/16尺寸的featureMap和1/8尺寸的featureMap，1/32尺寸的heatMap进行upsampling操作之后，因为这样的操作还原的图片仅仅是conv5中的卷积核中的特征，限于精度问题不能够很好地还原图像当中的特征，因此在这里向前迭代。把conv4中的卷积核对上一次upsampling之后的图进行反卷积补充细节（相当于一个插值过程），最后把conv3中的卷积核对刚才upsampling之后的图像进行再次反卷积补充细节，最后就完成了整个图像的还原。\n\n**缺点**：\n* 是得到的结果还是不够精细。进行8倍上采样虽然比32倍的效果好了很多，但是上采样的结果还是比较模糊和平滑，对图像中的细节不敏感。\n* 是对各个像素进行分类，没有充分考虑像素与像素之间的关系。忽略了在通常的基于像素分类的分割方法中使用的空间规整（spatial regularization）步骤，缺乏空间一致性。\n\n**补充：插值法**\n上采样upsampling的主要目的是放大图像，几乎都是采用内插值法，即在原有图像像素的基础上，在像素点值之间采用合适的**插值算法**插入新的元素。\n* 线性插值法：\n使用连接两个已知量的直线来确定在这个两个已知量之间的一个未知量的值的方法。\n<center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Image-Segmentation-Learning/5.png)</center>\n该直线方程可表示为：$\\frac{y-y_{0}}{y_{1}-y_{0}}=\\frac{x-x_{0}}{x_{1}-x_{0}}$ 假设方程两边的值是$\\alpha$，那么这个值就是插值系数，即$\\alpha =\\frac{y-y_{0}}{y_{1}-y_{0}}=\\frac{x-x_{0}}{x_{1}-x_{0}}$. 所以y可以表示为: $y=(1-\\alpha)y_{0}+\\alpha y_{1} = (1-\\frac{x-x_{0}}{x_{1}-x_{0}})y_{0}+\\frac{x-x_{0}}{x_{1}-x_{0}}y_{1}=\\frac{x_{1}-x}{x_{1}-x_{0}}y_{0}+\\frac{x-x_{0}}{x_{1}-x_{0}}y_{1}=\\frac{x_{1}-x}{x_{1}-x_{0}}f(x_{1})+\\frac{x-x_{0}}{x_{1}-x_{0}}f(x_{0})$\n\n* 双线性插值\n双线性插值是插值算法中的一种，是线性插值的扩展。利用原图像中目标点四周的四个真实存在的像素值来共同决定目标图中的一个像素值，其核心思想是在两个方向分别进行一次线性插值。\n<center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Image-Segmentation-Learning/6.png)</center>\n\nX方向的线性插值：在$Q_{12}$,$Q_{22}$中插入蓝色点$R_{2}$，$Q_{11}$，$Q_{21}$中插入蓝色点$R_{1}$\n\n$f(R_{1}=\\frac{x_{2}-x}{x_{2}-x_{1}}f(Q_{11})+\\frac{x-x_{1}}{x_{2}-x_{1}}f(Q_{21})$; $f(R_{2}=\\frac{x_{2}-x}{x_{2}-x_{1}}f(Q_{12})+\\frac{x-x_{1}}{x_{2}-x_{1}}f(Q_{22})$\n\nY方向的线性插值：通过第一步计算出的$R_{1$}与$R_{2}$在y方向上插值计算出P点\n\n$f(P)=\\frac{y_{2}-y}{y_{2}-y_{1}}f(R_{1})+\\frac{y-y_{1}}{y_{2}-y_{1}}f(R_{2})$\n\n---\n\n## U-Net: Convolutional Networks for Biomedical Image Segmentation\n[论文地址](https://arxiv.org/pdf/1505.04597.pdf)\n卷积网络被大规模应用在分类任务中，输出的结果是整个图像的类标签。然而，在许多视觉任务，尤其是生物医学图像处理领域，目标输出应该包括目标类别的位置，并且每个像素都应该有类标签。另外，在生物医学图像往往缺少训练图片。所以，Ciresan等人训练了一个卷积神经网络，用滑动窗口提供像素的周围区域（patch）作为输入来预测每个像素的类标签。这个网络有两个优点： 第一，输出结果可以定位出目标类别的位置； 第二，由于输入的训练数据是patches，这样就相当于进行了数据增广，解决了生物医学图像数量少的问题。\n\n但是，这个方法也有两个很明显缺点。\n\n第一，它很慢，因为这个网络必须训练每个patch，并且因为patch间的重叠有很多的冗余,造成资源的浪费，减慢训练时间和效率; 第二，定位准确性和获取上下文信息不可兼得。大的patches需要更多的max-pooling层这样减小了定位准确性,小的patches只能看到很小的局部信息，包含的背景信息不够。\n\n**U-Net Architecture**\n<center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Image-Segmentation-Learning/7.jpg)</center>\n\n1. 使用全卷积神经网络。(全卷积神经网络就是卷积取代了全连接层，全连接层必须固定图像大小而卷积不用，所以这个策略使得，你可以输入任意尺寸的图片，而且输出也是图片，所以这是一个端到端的网络。)\n2. 左边的网络是收缩路径：使用卷积和maxpooling。\n3. 右边的网络是扩张路径:使用上采样产生的特征图与左侧收缩路径对应层产生的特征图进行concatenate操作。（pooling层会丢失图像信息和降低图像分辨率且是不可逆的操作，对图像分割任务有一些影响，对图像分类任务的影响不大，为什么要做上采样？因为上采样可以补足一些图片的信息，但是信息补充的肯定不完全，所以还需要与左边的分辨率比较高的图片相连接起来（直接复制过来再裁剪到与上采样图片一样大小），这就相当于在高分辨率和更抽象特征当中做一个折衷，因为随着卷积次数增多，提取的特征也更加有效，更加抽象，上采样的图片是经历多次卷积后的图片，肯定是比较高效和抽象的图片，然后把它与左边不怎么抽象但更高分辨率的特征图片进行连接）。\n4. 最后再经过两次反卷积操作，生成特征图，再用两个1X1的卷积做分类得到最后的两张heatmap,例如第一张表示的是第一类的得分，第二张表示第二类的得分heatmap,然后作为softmax函数的输入，算出概率比较大的softmax类，选择它作为输入给交叉熵进行反向传播训练。\n\n**Overlap-tile strategy**\n<center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Image-Segmentation-Learning/8.png)</center>\n\n医学图像是一般相当大，但是分割时候不可能将原图太小输入网络，所以必须切成一张一张的小patch，在切成小patch的时候，Unet由于网络结构原因适合有overlap的切图，可以看图，红框是要分割区域，但是在切图时要包含周围区域，overlap另一个重要原因是周围overlap部分可以为分割区域边缘部分提供文理等信息。可以看黄框的边缘，分割结果并没有受到切成小patch而造成分割情况不好。 \n\n**训练**\n\n最后一层使用了交叉熵函数与softmax：\n$$E=\\sum_{x\\in \\Omega}w(x)log(p_{\\ell(x)}(x))$$\n\n并且为了补偿训练每个类像素的不同频率使得网络更注重学习相互接触的细胞之间的小的分割边界，引入了权重图计算$w(x)$:\n$$w(x)=w_{c}(x)+w_{0} \\times exp(-\\frac{(d_{1}(x)+d_{2}(x))}{2\\sigma^{2}})$$\n\n**Data Augmentation**\n在只有少量样本的情况况下，要想尽可能的让网络获得不变性和鲁棒性，数据增加是必不可少的。因为本论文需要处理显微镜图片，我们需要平移与旋转不变性，并且对形变和灰度变化鲁棒。将训练样本进行随机弹性形变是训练分割网络的关键。使用随机位移矢量在粗糙的3\\*3网格上产生平滑形变(smooth deformations)。 位移是从10像素标准偏差的高斯分布中采样的。然后使用双三次插值(Bicubic interpolation)计算每个像素的位移。在contracting path的末尾采用drop-out 层更进一步增加数据。\n\n**双三次插值**\n在这种方法中，函数f在点(x,y)的值可以通过矩形网络中最近的16个采样点加权平均得到。\n\n---\n\n## DeepLab V1\n[论文地址](https://arxiv.org/pdf/1412.7062v3.pdf)\n\nDeepLab是结合了深度卷积神经网络([DCNNs](https://www.cnblogs.com/wangxiaocvpr/p/8763510.html))和概率图模型([DenseCRFs](https://zhuanlan.zhihu.com/p/33397147))的方法.\n\nDCNN在图像标记任务中存在两个技术障碍：\n\n*   信号下采样\n*   空间不敏感(invariance)\n\n第一个问题涉及到：在DCNN中重复最大池化和下采样带来的分辨率下降问题，分辨率的下降会丢失细节。DeepLab是采用的`atrous`(带孔)算法扩展感受野，获取更多的上下文信息。\n\n第二个问题涉及到：分类器获取以对象中心的决策是需要空间变换的不变性，这天然的限制了DCNN的定位精度，DeepLab采用完全连接的条件随机场(DenseCRF)提高模型捕获细节的能力。\n\n除空洞卷积和 CRFs 之外，论文使用的 tricks 还有 Multi-Scale features。其实就是 U-Net 和 FPN 的思想，在输入图像和前四个最大池化层的输出上附加了两层的 MLP，第一层是 128 个 3×3 卷积，第二层是 128 个 1×1 卷积。最终输出的特征与主干网的最后一层特征图融合，特征图增加 5×128=640 个通道。实验表示多尺度有助于提升预测结果，但是效果不如 CRF 明显。\n\n**CRF->语义分割**\n\n对于每个像素位置$i$具有隐变量$x_{i}$(这里隐变量就是像素的真实类别标签，如果预测结果有21类，则$(i \\in 1,2,..,21)$ 还有对应的观测值 $y_{i}$(即像素点对应的颜色值)。以像素为节点，像素与像素间的关系作为边，构成了一个条件随机场(CRF)。通过观测变量$y_{i}$来推测像素位置$i$应的类别标签$x_{i}$.条件随机场示意图如下:\n<center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Image-Segmentation-Learning/9.png)</center>\n\n条件随机场符合吉布斯分布(x是上面的观测值，下面省略全局观测I):\n$$p(x|I)=\\frac{1}{Z}exp(-E(x|I))$$\n\n全连接的CRF模型使用的能量函数$E(x)$为:\n$$E(x)=\\sum_{i} \\theta_{i}(x_{i})+\\sum_{ij}\\theta_{ij}(x_{i},x_{j})$$\n\n这分为一元势函数$\\theta_{i}(x_{i})$和二元势函数$\\theta_{ij}(x_{i},x_{j})$两部分\n\n* 一元势函数是定义在观测序列位置i的状态特征函数，用于刻画观测序列对标记变量的影响（例如在城市道路任务中，观测到像素点为黑色，对应车子的可能比天空可能要大）。这里$P(x_{i})$是取DCNN计算关于像素i的输出的标签分配概率.\n\n$$\\theta_{i}(x_{i})=-logP(x_{i})$$\n\n* 二元势函数是定义在不同观测位置上的转移特征函数，用于刻画变量之间的相关关系以及观测序列对其影响。如果比较相似，那可能是一类，否则就裂开，这可以细化边缘。一般的二元势函数只取像素点与周围像素之间的边，这里使用的是全连接，即像素点与其他所有像素之间的关系。\n\n$$\\theta_{ij}(x_{i},x_{j})=\\mu(x_{i},x_{j})\\sum_{m=1}^{K}w_{m}k^{m}(f_{i},f_{j})$$\n\nDeepLab中高斯核采用双边位置和颜色的组合（第一核取决于像素位置(p)和像素颜色强度(I),第二核取决于像素位置(p)）:\n$$w_{1}exp(-\\frac{\\lVert p_{i}-p_{j} \\rVert^{2}}{2\\sigma_{\\alpha}^{2}}-\\frac{\\lVert I_{i}-I_{j} \\rVert^{2}}{2\\sigma_{\\beta}^{2}})+w_{2}exp(-\\frac{\\lVert p_{i}-p_{j} \\rVert^{2}}{2\\sigma_{\\gamma}^{2}})$$\n\n**实验**\n\n| 项目 | 设置 |\n| --- | --- |\n| 数据集 | PASCAL VOC 2012 segmentation benchmark |\n| DCNN模型 | 权重采用预训练的VGG16 |\n| DCNN损失函数 | 交叉熵 |\n| 训练器 | SGD，batch=20 |\n| 学习率 | 初始为0.001，最后的分类层是0.01。每2000次迭代乘0.1 |\n| 权重 | 0.9的动量， 0.0005的衰减 |\n\n---\n\n## DepLab V2\n[论文地址](https://arxiv.org/pdf/1606.00915.pdf)\n\nDeepLabv2 是相对于 DeepLabv1 基础上的优化。DeepLabv1 在三个方向努力解决，但是问题依然存在：\n1. 特征分辨率的降低\n2. 物体存在多尺度\n3. DCNN 的平移不变性\n\n针对这三个问题, DeepLabv2做出了3个主要贡献:\n1. 首先，强调使用空洞卷积，作为密集预测任务的强大工具。空洞卷积能够明确地控制DCNN内计算特征响应的分辨率，即可以有效的扩大感受野，在不增加参数量和计算量的同时获取更多的上下文。\n2. 其次，提出了空洞空间卷积池化金字塔(atrous spatial pyramid pooling (ASPP))，以多尺度的信息得到更强健的分割结果。ASPP并行的采用多个采样率的空洞卷积层来探测，以多个比例捕捉对象以及图像上下文。\n3. 最后，通过组合DCNN和概率图模型，改进分割边界结果。在DCNN中最大池化和下采样组合实现可平移不变性，但这对精度是有影响的。通过将最终的DCNN层响应与全连接的CRF结合来克服这个问题。\n\n**步骤**\n<center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Image-Segmentation-Learning/10.png)</center>\n\n*   输入经过改进的DCNN(带空洞卷积和ASPP模块)得到粗略预测结果，即`Aeroplane Coarse Score map`\n*   通过双线性插值扩大到原本大小，即`Bi-linear Interpolation`\n*   再通过全连接的CRF细化预测结果，得到最终输出`Final Output`\n\n**方法**\n1. **空洞卷积用于密集特征提取和扩大感受野**\n  \n  首先考虑一维信号，空洞卷积输出为$y[i]$, 输入为$x[i]$, 长度K的滤波器为$w[k]$, 则定义为：\n  $$y[k]=\\sum_{k=1}^{K}x[i+r\\cdot k]w[k]$$\n  输入采样的步幅为参数r, 标准采样率是$r=1$如图(a); 图(b)是采样率$r=2$的时候：\n  <center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Image-Segmentation-Learning/11.png)</center>\n  \n  二维信号(图片)上使用空洞卷积的表现,给定一个图像：\n  * 上分支：首先下采样将分辨率降低2倍，做卷积。再上采样得到结果。本质上这只是在原图片的1/4内容上做卷积响应。\n  * 下分支：如果将全分辨率图像做空洞卷积(采样率为2，核大小与上面卷积核相同)，直接得到结果。这样可以计算出整张图像的响应，如下图所示，这样做效果更佳。\n  <center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Image-Segmentation-Learning/12.png)</center>\n  \n  空洞卷积能够放大滤波器的感受野，速率r引入$r-1$个零，有效将感受野从$k\\times k$扩展到$k_{e}=k+(k-1)(r-1)$而不增加参数和计算量。在DCNN中，常见的做法是混合使用空洞卷积以高的分辨率(理解为采样密度)计算最终的DCNN网络响应。\n2. **使用ASPP模块表示多尺度图像**\n\n  DeepLabv2的做法与SPPNet类似，并行的采用多个采样率的空洞卷积提取特征，再将特征融合，类似于空间金字塔结构，形象的称为Atrous Spatial Pyramid Pooling (ASPP)。示意图如下：\n  <center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Image-Segmentation-Learning/13.png)</center>\n  \n  在同一Input Feature Map的基础上，并行的使用4个空洞卷积，空洞卷积配置为$r=6,12,18,24$, 核大小为$3 \\times 3$. 最终将不同卷积层得到的结果做像素加融合到一起.\n3. **使用全连接CRF做结构预测用于恢复边界精度**\n\n  <center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Image-Segmentation-Learning/14.png)</center>\n  \n**训练**\n\n| 项目 | 设置 |\n| --- | --- |\n| DCNN模型 | 权重采用预训练的VGG16，**ResNet101** |\n| DCNN损失函数 | 输出的结果与ground truth下采样8倍做像素交叉熵 |\n| 训练器 | SGD，batch=20 |\n| 学习率 | 初始为0.001，最后的分类层是0.01。每2000次迭代乘0.1 |\n| 权重 | 0.9的动量， 0.0005的衰减 |\n\n---\n\n## DepLab V3\n[论文地址](https://arxiv.org/pdf/1706.05587.pdf)\n\n语义分割任务，在应用深度卷积神经网络中的有两个挑战：\n\n*   第一个挑战：连续池化和下采样，让高层特征具有局部图像变换的内在不变性，这允许DCNN学习越来越抽象的特征表示。但同时引起的特征分辨率下降，会妨碍密集的定位预测任务，因为这需要详细的空间信息。\n*   第二个挑战：多尺度目标的存在\n\nDeepLabv3的主要贡献在于：\n\n*   重新讨论了空洞卷积的使用，在级联模块和空间金字塔池化的框架下，能够获取更大的感受野从而获取多尺度信息。\n*   改进了ASPP模块：由不同采样率的空洞卷积和BN层组成，尝试以级联或并行的方式布局模块。\n*   讨论了一个重要问题：使用大采样率的空洞卷积，因为图像边界响应无法捕捉远距离信息，会退化为1×1的卷积, 因此建议将图像级特征融合到ASPP模块中。\n*   阐述了训练细节并分享了训练经验，论文提出的”DeepLabv3”改进了以前的工作，获得了很好的结果\n\n**方法**\n1. **空洞卷积应用于密集的特征提取**\n2. **深层次的空洞卷积**\n\n  <center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Image-Segmentation-Learning/15.png)</center>\n  \n  <center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Image-Segmentation-Learning/18.png)</center>\n\n  将空洞卷积应用在级联模块, 取ResNet中最后一个block，在上图中为block4，并在其后面增加级联模块。图(a)所示，整体图片的信息总结到后面非常小的特征映射上，使用步幅越长的特征映射，得到的结果反倒会差，结果最好的out_stride = 8 需要占用较多的存储空间。因为连续的下采样会降低特征映射的分辨率，细节信息被抽取，这对语义分割是有害的。上图(b)所示，可使用不同采样率的空洞卷积保持输出步幅的为out_stride = 16.这样不增加参数量和计算量同时有效的缩小了步幅。\n  \n3. **Atrous Spatial Pyramid Pooling**\n\n  对于在DeepLabv2中提出的ASPP模块，其在特征顶部映射图并行使用了四种不同采样率的空洞卷积。这表明以不同尺度采样是有效的，在DeepLabv3中向ASPP中添加了BN层。不同采样率的空洞卷积可以有效的捕获多尺度信息，但是，随着采样率的增加，滤波器的有效权重(权重有效的应用在特征区域，而不是填充0)逐渐变小。如下图所示：\n  <center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Image-Segmentation-Learning/16.png)</center>\n  \n  当不同采样率的$3 \\times 3$卷积核应用在$65 \\times 65$的特征映射上，采样率接近特征映射大小时，$3 \\times 3$的滤波器不是捕捉全图像的上下文，而是退化为简单的$1 \\times 1$滤波器，只有滤波器中心点的权重起了作用。\n  \n  <center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Image-Segmentation-Learning/17.png)</center>\n  为了克服这个问题，改进了ASPP结构如上图：\n  1. 一个$1 \\times 1$卷积和三个$3 \\times 3$卷积的采样率为$rate = (6,12,18)$的空洞卷积，滤波器数量为256，包含BN层。针对output_stride=16的情况。（当output_stride=8的时候，采样率会加倍，所有的特征会通过$1 \\times 1$卷积级联到一起）\n  2. 使用了图片级特征。具体来说，在模型最后的特征映射上应用全局平均，将结果经过$1 \\times 1$的卷积，再双线性上采样得到所需的空间维度。\n\n**训练**\n\n| 部分 | 设置 |\n| --- | --- |\n| 数据集 | PASCAL VOC 2012 |\n| 工具 | TensorFlow |\n| 裁剪尺寸 | 采样513大小的裁剪尺寸 |\n| 学习率策略 | 采用poly策略， $learning rate = base\\_lr(1-\\frac{iter}{max\\_iters})^{power}$ |\n| **BN层策略** | 当output_stride=16时，我们采用batchsize=16，同时BN层的参数做参数衰减0.9997。在增强的数据集上，以初始学习率0.007训练30K后，冻结BN层参数。采用output_stride=8时，再使用初始学习率0.001训练30K。训练output_stride=16比output_stride=8要快很多，因为中间的特征映射在空间上小四倍。但因为output_stride=16在特征映射上粗糙的是牺牲了精度。 |\n| **上采样策略** | 在先前的工作上,将最终的输出与GroundTruth下采样8倍做比较之后发现保持GroundTruth更重要，故将最终的输出上采样8倍与完整的GroundTruth比较。 |\n\n---\n\n## DepLab-V$3^{+}$\n[论文地址](https://arxiv.org/pdf/1802.02611.pdf)\n\n因为深度网络存在pooling or convolutions with stride的层，会导致feature分辨率下降，从而导致预测精度降低，而造成的边界信息丢失问题. 这个问题可以通过使用空洞卷积替代更多的pooling层来获取分辨率更高的feature。但是feature分辨率更高会极大增加运算量。\n\n**方法**\n<center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Image-Segmentation-Learning/19.png)</center>\n\n所以DeepLabV$3^{+}$中通过采用了encoder-decoder结构，在DeepLab V3中加入了一个简单有效的decoder模块来改善物体边缘的分割结果(图c)：先上采样4倍，在与encoder中的特征图concatenate，最后在上采样4倍恢复到原始图像大小。除此之外还尝试使用Xception作为encoder，在Atrous Spatial Pyramid Pooling和decoder中应用depth-wise separable convolution得到了更快精度更高的网络。\n\n1. **Encoder**\n\n  * ResNet: encoder就是DeepLab V3，通过修改ResNet101最后两(一)个block的stride，使得output stride为8(16)。之后在block4后应用改进后的Atrous Spatial Pyramid Pooling，将所得的特征图concatenate用1×1的卷积得到256个通道的特征图。\n  * **Xecption**: 采用的Xception模型为MSRA team提出的改进的Xception，叫做Aligned Xception，并做了几点修改：\n\n    * 网络深度与Aligned Xception相同，不同的地方在于为了快速计算和有效的使用内存而不修改entry flow network的结构。\n    * 所有的max pooling操作替换成带stride的separable convolution，这能使得对任意分辨率的图像应用atrous separable convolution提取特征。\n    * 在每个3×3的depath-wise convolution后增加BN层和ReLU。\n<center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Image-Segmentation-Learning/21.png)</center>\n\n2. **Decoder**\n\n  * 在decoder中，特征图首先上采样4倍，然后与encoder中对应分辨率低级特征concatenate。在concatenate之前，由于低级特征图的通道数通常太多(256或512)，而从encoder中得到的富含语义信息的特征图通道数只有256，这样会淡化语义信息，因此在concatenate之前，需要将低级特征图通过1×1的卷积减少通道数。在concatenate之后用3×3的卷积改善特征，最后上采样4倍恢复到原始图像大小。\n  * 设计： \n    * $1 \\times 1$卷积的通道数采用48\n    * 用来获得更锋利的边界的3×3的卷积。最后采用了2个3×3的卷积\n    * 使用的encoder的低级特征（Conv2）\n\n<center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Image-Segmentation-Learning/20.png)</center>\n\n**结果**：\n\n* **Aligned Xception改**\n  <center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Image-Segmentation-Learning/22.png)</center>\n  \n  当train_stride=16和eval_stride=8的时候mIOU最好达到了84.56% 然而计算量比较高。使用train_stride和eval_stride都为16的时候，结果下降了1.53%但是计算量下降了60倍。","source":"_posts/MalongTech-Learning-Image-Segmentation.md","raw":"---\ntitle: MalongTech 学习内容:图像分割-2D\ndate: 2018-11-26 11:00:00\ntags: [Deep Learning]\ncategories: 实习\n---\n\n# Image Segmentation\n\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Image-Segmentation-Learning/1.jpg)\n\n## Introduction\n在计算机视觉领域，图像分割（Segmentation）指的是将数字图像细分为多个图像子区域（像素的集合）（也被称作超像素）的过程。图像分割的目的是简化或改变图像的表示形式，使得图像更容易理解和分析。图像分割通常用于定位图像中的物体和边界（线，曲线等）。更精确的，图像分割是对图像中的每个像素加标签的一个过程，这一过程使得具有相同标签的像素具有某种共同视觉特性。<!-- more -->\n\n图像分割的结果是图像上子区域的集合（这些子区域的全体覆盖了整个图像），或是从图像中提取的轮廓线的集合（例如边缘检测）。一个子区域中的每个像素在某种特性的度量下或是由计算得出的特性都是相似的，例如颜色、亮度、纹理。邻接区域在某种特性的度量下有很大的不同。\n\n**应用**\n* 医学影像：1. 肿瘤和其他病理的定位；2. 组织体积的测量；3. 计算机引导的手术；4. 诊断；5. 治疗方案的定制；6. 解剖学结构的研究\n* 卫星图像中定位物体\n* 人脸识别\n* 指纹识别\n* 交通控制系统\n\n## Tranditional Methods\n[参考](https://zhuanlan.zhihu.com/p/30732385)\n* **基于阙值的分割方法**: 1. 固定阙值分割；2. 直方图双峰法；3. 迭代阙值图像分割；自适应阙值图像分割（最大分类方差法，均值法，最佳阙值）.\n* **基于边缘的分割方法**: 1. Canny边缘检测器；2. Harris角点检测器；3. SIFT检测器；3. SURF检测器.\n* **基于区域的分割方法**: 1. 种子区域生长法；2. 区域分裂合并法；3. 分水岭法.\n* **基于图论的分割方法**: 1. GraphCut; 2. GrabCut; 3. Random Walk.\n* **基于能量泛函的分割方法**: 参数活动轮廓模型（1. Snake模型；2. Active Shape Model; 3. Active Apperance Models; 4. Constrained local model）；几何活动轮廓模型.\n\n## Datasets\n1. Pascal VOC: 20个类别，6929张标注图片\n2. CityScapes：道路驾驶场景，30个类别，5000张精细标注，20000张粗糙标注\n3. MS COCO：80类，33万张图片，超过20万张有标注，150万个物体的个体\n4. 医学影像领域的ImageNet: [DeepLesion](https://www.52cv.net/?p=883), 10000多个病例研究的超过32000个病变标注\n\n## 图像分割的度量标准\n假设共有$k+1$个类（从$L_{o}$到$L_{k}$,其中包含一个空类或背景）, $P_{ij}$表示本属于类**i**但被预测为类**j**的像素数量。即，$p_{ii}$表示**真正**的数量，而$p_{ij}$和$p_{ji}$则分别被解释为假正和假负，尽管两者都是假正与假负之和。\n1. Pixel Accuracy(PA, 像素精度): 最简单的度量，为标记正确的像素占总像素的比例。\n$$PA=\\frac{\\sum_{i=0}^{k}p_{ii}}{\\sum_{i=0}^{k}\\sum_{j=0}^{k}p_{ij}}$$\n2. Mean Pixel Accuracy(MAP,均像素精度): PA的一种简单提升，计算每个类内被正确分类像素数的比例，之后求所有类的平均。\n$$MAP=\\frac{1}{k+1}\\sum_{i=0}^{k}\\frac{p_{ii}}{\\sum_{j=0}^{k}p_{ij}}$$\n3. Mean Intersection over Union(MIoU, 均交并比): 为语义分割的标准度量。其计算两个集合的交集和并集之比，这两个集合为真实值(ground truth)和预测值(predicted segmentation). 这个比例可以变形为正真数(intersection)比上真正、假负、假正(并集)之和，之后在每个类上计算IoU再平均。\n$$MIoU = \\frac{1}{k+1}\\sum_{i=0}^{k}\\frac{p_{ii}}{\\sum_{j=0}^{k}p_{ij}+\\sum_{j=0}^{k}(p_{ji}-p_{ii})}$$\n4. Frequency Weighted Intersection over Union(FWIoU, 频权交并比): 为MIoU的一种提升，根据每个类出现的频率为其设置权重。\n$$FWIoU = \\frac{1}{\\sum_{i=0}^{k}\\sum_{j=0}^{k}p_{ij}}\\sum_{i=0}^{k}\\frac{p_{ii}}{\\sum_{j=0}^{k}p_{ij}+\\sum_{j=0}^{k}(p_{ji}-p_{ii})}$$\n\n# 深度学习算法\n## Fully Convolutional Networks\n[论文地址](https://people.eecs.berkeley.edu/~jonlong/long_shelhamer_fcn.pdf)\n传统神经网络做分类的步骤是，首先是一个图像进来之后经过多层卷积得到降维之后的特征图，这个特征图经过全连接层变成一个分类器，最后输出一个类别的向量，这就是分类的结果。\n\n而 FCN 是把所有的全连接层换成卷基层，原来只能输出一个类别分类的网络可以在特征图的每一个像素输出一个分类结果。这样就把分类的向量，变成了一个分类的特征图。\n<center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Image-Segmentation-Learning/2.png)</center>\n\n上图中的猫, 输入AlexNet, 得到一个长为1000的输出向量, 表示输入图像属于每一类的概率, 其中在“tabby cat”这一类统计概率最高。而FCN对图像进行像素级的分类，从而解决了语义级别的图像分割（semantic segmentation）问题。FCN可以接受任意尺寸的输入图像，采用反卷积层对最后一个卷积层的feature map进行上采样, 使它恢复到输入图像相同的尺寸，从而可以对每个像素都产生了一个预测, 同时保留了原始输入图像中的空间信息, 最后在上采样的特征图上进行逐像素分类。最后逐个像素计算softmax分类的损失, 相当于每一个像素对应一个训练样本。\n\n**全连接->卷积层**：\n\n一个 K=4096 的全连接层，输入数据体的尺寸是 7∗7∗512，这个全连接层可以被等效地看做一个 F=7,P=0,S=1,K=4096 的卷积层.\n\n假设一个卷积神经网络的输入是 224x224x3 的图像，一系列的卷积层和下采样层将图像数据变为尺寸为 7x7x512 的激活数据体。AlexNet使用了两个尺寸为4096的全连接层，最后一个有1000个神经元的全连接层用于计算分类评分。我们可以将这3个全连接层中的任意一个转化为卷积层：\n* 针对第一个连接区域是[7x7x512]的全连接层，令其滤波器尺寸为F=7，这样输出数据体就为[1x1x4096]了。\n* 针对第二个全连接层，令其滤波器尺寸为F=1，这样输出数据体为[1x1x4096]。\n* 对最后一个全连接层也做类似的，令其F=1，最终输出为[1x1x1000]\n\n**end to end, pixels to pixels network**\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Image-Segmentation-Learning/3.png)\n经过多次卷积和pooling以后，得到的图像越来越小，分辨率越来越低。其中图像到$\\frac{H}{32} \\times \\frac{W}{32}$的时候图片是最小的一层时，所产生图叫做heatmap热图，热图就是最重要的高维特征图，得到高维特征的heatmap之后就是最重要的一步也是最后的一步对原图像进行upsampling，把图像进行放大、放大、放大，到原图像的大小。最后的输出是1000张heatmap经过upsampling变为原图大小的图片，为了对每个像素进行分类预测label成最后已经进行语义分割的图像，这里有一个小trick，就是最后通过逐个像素地求其在1000张图像该像素位置的最大数值描述（概率）作为该像素的分类。因此产生了一张已经分类好的图片，如上图右侧有狗狗和猫猫的图。\n\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Image-Segmentation-Learning/4.png)\n\n现在我们有1/32尺寸的heatMap，1/16尺寸的featureMap和1/8尺寸的featureMap，1/32尺寸的heatMap进行upsampling操作之后，因为这样的操作还原的图片仅仅是conv5中的卷积核中的特征，限于精度问题不能够很好地还原图像当中的特征，因此在这里向前迭代。把conv4中的卷积核对上一次upsampling之后的图进行反卷积补充细节（相当于一个插值过程），最后把conv3中的卷积核对刚才upsampling之后的图像进行再次反卷积补充细节，最后就完成了整个图像的还原。\n\n**缺点**：\n* 是得到的结果还是不够精细。进行8倍上采样虽然比32倍的效果好了很多，但是上采样的结果还是比较模糊和平滑，对图像中的细节不敏感。\n* 是对各个像素进行分类，没有充分考虑像素与像素之间的关系。忽略了在通常的基于像素分类的分割方法中使用的空间规整（spatial regularization）步骤，缺乏空间一致性。\n\n**补充：插值法**\n上采样upsampling的主要目的是放大图像，几乎都是采用内插值法，即在原有图像像素的基础上，在像素点值之间采用合适的**插值算法**插入新的元素。\n* 线性插值法：\n使用连接两个已知量的直线来确定在这个两个已知量之间的一个未知量的值的方法。\n<center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Image-Segmentation-Learning/5.png)</center>\n该直线方程可表示为：$\\frac{y-y_{0}}{y_{1}-y_{0}}=\\frac{x-x_{0}}{x_{1}-x_{0}}$ 假设方程两边的值是$\\alpha$，那么这个值就是插值系数，即$\\alpha =\\frac{y-y_{0}}{y_{1}-y_{0}}=\\frac{x-x_{0}}{x_{1}-x_{0}}$. 所以y可以表示为: $y=(1-\\alpha)y_{0}+\\alpha y_{1} = (1-\\frac{x-x_{0}}{x_{1}-x_{0}})y_{0}+\\frac{x-x_{0}}{x_{1}-x_{0}}y_{1}=\\frac{x_{1}-x}{x_{1}-x_{0}}y_{0}+\\frac{x-x_{0}}{x_{1}-x_{0}}y_{1}=\\frac{x_{1}-x}{x_{1}-x_{0}}f(x_{1})+\\frac{x-x_{0}}{x_{1}-x_{0}}f(x_{0})$\n\n* 双线性插值\n双线性插值是插值算法中的一种，是线性插值的扩展。利用原图像中目标点四周的四个真实存在的像素值来共同决定目标图中的一个像素值，其核心思想是在两个方向分别进行一次线性插值。\n<center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Image-Segmentation-Learning/6.png)</center>\n\nX方向的线性插值：在$Q_{12}$,$Q_{22}$中插入蓝色点$R_{2}$，$Q_{11}$，$Q_{21}$中插入蓝色点$R_{1}$\n\n$f(R_{1}=\\frac{x_{2}-x}{x_{2}-x_{1}}f(Q_{11})+\\frac{x-x_{1}}{x_{2}-x_{1}}f(Q_{21})$; $f(R_{2}=\\frac{x_{2}-x}{x_{2}-x_{1}}f(Q_{12})+\\frac{x-x_{1}}{x_{2}-x_{1}}f(Q_{22})$\n\nY方向的线性插值：通过第一步计算出的$R_{1$}与$R_{2}$在y方向上插值计算出P点\n\n$f(P)=\\frac{y_{2}-y}{y_{2}-y_{1}}f(R_{1})+\\frac{y-y_{1}}{y_{2}-y_{1}}f(R_{2})$\n\n---\n\n## U-Net: Convolutional Networks for Biomedical Image Segmentation\n[论文地址](https://arxiv.org/pdf/1505.04597.pdf)\n卷积网络被大规模应用在分类任务中，输出的结果是整个图像的类标签。然而，在许多视觉任务，尤其是生物医学图像处理领域，目标输出应该包括目标类别的位置，并且每个像素都应该有类标签。另外，在生物医学图像往往缺少训练图片。所以，Ciresan等人训练了一个卷积神经网络，用滑动窗口提供像素的周围区域（patch）作为输入来预测每个像素的类标签。这个网络有两个优点： 第一，输出结果可以定位出目标类别的位置； 第二，由于输入的训练数据是patches，这样就相当于进行了数据增广，解决了生物医学图像数量少的问题。\n\n但是，这个方法也有两个很明显缺点。\n\n第一，它很慢，因为这个网络必须训练每个patch，并且因为patch间的重叠有很多的冗余,造成资源的浪费，减慢训练时间和效率; 第二，定位准确性和获取上下文信息不可兼得。大的patches需要更多的max-pooling层这样减小了定位准确性,小的patches只能看到很小的局部信息，包含的背景信息不够。\n\n**U-Net Architecture**\n<center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Image-Segmentation-Learning/7.jpg)</center>\n\n1. 使用全卷积神经网络。(全卷积神经网络就是卷积取代了全连接层，全连接层必须固定图像大小而卷积不用，所以这个策略使得，你可以输入任意尺寸的图片，而且输出也是图片，所以这是一个端到端的网络。)\n2. 左边的网络是收缩路径：使用卷积和maxpooling。\n3. 右边的网络是扩张路径:使用上采样产生的特征图与左侧收缩路径对应层产生的特征图进行concatenate操作。（pooling层会丢失图像信息和降低图像分辨率且是不可逆的操作，对图像分割任务有一些影响，对图像分类任务的影响不大，为什么要做上采样？因为上采样可以补足一些图片的信息，但是信息补充的肯定不完全，所以还需要与左边的分辨率比较高的图片相连接起来（直接复制过来再裁剪到与上采样图片一样大小），这就相当于在高分辨率和更抽象特征当中做一个折衷，因为随着卷积次数增多，提取的特征也更加有效，更加抽象，上采样的图片是经历多次卷积后的图片，肯定是比较高效和抽象的图片，然后把它与左边不怎么抽象但更高分辨率的特征图片进行连接）。\n4. 最后再经过两次反卷积操作，生成特征图，再用两个1X1的卷积做分类得到最后的两张heatmap,例如第一张表示的是第一类的得分，第二张表示第二类的得分heatmap,然后作为softmax函数的输入，算出概率比较大的softmax类，选择它作为输入给交叉熵进行反向传播训练。\n\n**Overlap-tile strategy**\n<center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Image-Segmentation-Learning/8.png)</center>\n\n医学图像是一般相当大，但是分割时候不可能将原图太小输入网络，所以必须切成一张一张的小patch，在切成小patch的时候，Unet由于网络结构原因适合有overlap的切图，可以看图，红框是要分割区域，但是在切图时要包含周围区域，overlap另一个重要原因是周围overlap部分可以为分割区域边缘部分提供文理等信息。可以看黄框的边缘，分割结果并没有受到切成小patch而造成分割情况不好。 \n\n**训练**\n\n最后一层使用了交叉熵函数与softmax：\n$$E=\\sum_{x\\in \\Omega}w(x)log(p_{\\ell(x)}(x))$$\n\n并且为了补偿训练每个类像素的不同频率使得网络更注重学习相互接触的细胞之间的小的分割边界，引入了权重图计算$w(x)$:\n$$w(x)=w_{c}(x)+w_{0} \\times exp(-\\frac{(d_{1}(x)+d_{2}(x))}{2\\sigma^{2}})$$\n\n**Data Augmentation**\n在只有少量样本的情况况下，要想尽可能的让网络获得不变性和鲁棒性，数据增加是必不可少的。因为本论文需要处理显微镜图片，我们需要平移与旋转不变性，并且对形变和灰度变化鲁棒。将训练样本进行随机弹性形变是训练分割网络的关键。使用随机位移矢量在粗糙的3\\*3网格上产生平滑形变(smooth deformations)。 位移是从10像素标准偏差的高斯分布中采样的。然后使用双三次插值(Bicubic interpolation)计算每个像素的位移。在contracting path的末尾采用drop-out 层更进一步增加数据。\n\n**双三次插值**\n在这种方法中，函数f在点(x,y)的值可以通过矩形网络中最近的16个采样点加权平均得到。\n\n---\n\n## DeepLab V1\n[论文地址](https://arxiv.org/pdf/1412.7062v3.pdf)\n\nDeepLab是结合了深度卷积神经网络([DCNNs](https://www.cnblogs.com/wangxiaocvpr/p/8763510.html))和概率图模型([DenseCRFs](https://zhuanlan.zhihu.com/p/33397147))的方法.\n\nDCNN在图像标记任务中存在两个技术障碍：\n\n*   信号下采样\n*   空间不敏感(invariance)\n\n第一个问题涉及到：在DCNN中重复最大池化和下采样带来的分辨率下降问题，分辨率的下降会丢失细节。DeepLab是采用的`atrous`(带孔)算法扩展感受野，获取更多的上下文信息。\n\n第二个问题涉及到：分类器获取以对象中心的决策是需要空间变换的不变性，这天然的限制了DCNN的定位精度，DeepLab采用完全连接的条件随机场(DenseCRF)提高模型捕获细节的能力。\n\n除空洞卷积和 CRFs 之外，论文使用的 tricks 还有 Multi-Scale features。其实就是 U-Net 和 FPN 的思想，在输入图像和前四个最大池化层的输出上附加了两层的 MLP，第一层是 128 个 3×3 卷积，第二层是 128 个 1×1 卷积。最终输出的特征与主干网的最后一层特征图融合，特征图增加 5×128=640 个通道。实验表示多尺度有助于提升预测结果，但是效果不如 CRF 明显。\n\n**CRF->语义分割**\n\n对于每个像素位置$i$具有隐变量$x_{i}$(这里隐变量就是像素的真实类别标签，如果预测结果有21类，则$(i \\in 1,2,..,21)$ 还有对应的观测值 $y_{i}$(即像素点对应的颜色值)。以像素为节点，像素与像素间的关系作为边，构成了一个条件随机场(CRF)。通过观测变量$y_{i}$来推测像素位置$i$应的类别标签$x_{i}$.条件随机场示意图如下:\n<center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Image-Segmentation-Learning/9.png)</center>\n\n条件随机场符合吉布斯分布(x是上面的观测值，下面省略全局观测I):\n$$p(x|I)=\\frac{1}{Z}exp(-E(x|I))$$\n\n全连接的CRF模型使用的能量函数$E(x)$为:\n$$E(x)=\\sum_{i} \\theta_{i}(x_{i})+\\sum_{ij}\\theta_{ij}(x_{i},x_{j})$$\n\n这分为一元势函数$\\theta_{i}(x_{i})$和二元势函数$\\theta_{ij}(x_{i},x_{j})$两部分\n\n* 一元势函数是定义在观测序列位置i的状态特征函数，用于刻画观测序列对标记变量的影响（例如在城市道路任务中，观测到像素点为黑色，对应车子的可能比天空可能要大）。这里$P(x_{i})$是取DCNN计算关于像素i的输出的标签分配概率.\n\n$$\\theta_{i}(x_{i})=-logP(x_{i})$$\n\n* 二元势函数是定义在不同观测位置上的转移特征函数，用于刻画变量之间的相关关系以及观测序列对其影响。如果比较相似，那可能是一类，否则就裂开，这可以细化边缘。一般的二元势函数只取像素点与周围像素之间的边，这里使用的是全连接，即像素点与其他所有像素之间的关系。\n\n$$\\theta_{ij}(x_{i},x_{j})=\\mu(x_{i},x_{j})\\sum_{m=1}^{K}w_{m}k^{m}(f_{i},f_{j})$$\n\nDeepLab中高斯核采用双边位置和颜色的组合（第一核取决于像素位置(p)和像素颜色强度(I),第二核取决于像素位置(p)）:\n$$w_{1}exp(-\\frac{\\lVert p_{i}-p_{j} \\rVert^{2}}{2\\sigma_{\\alpha}^{2}}-\\frac{\\lVert I_{i}-I_{j} \\rVert^{2}}{2\\sigma_{\\beta}^{2}})+w_{2}exp(-\\frac{\\lVert p_{i}-p_{j} \\rVert^{2}}{2\\sigma_{\\gamma}^{2}})$$\n\n**实验**\n\n| 项目 | 设置 |\n| --- | --- |\n| 数据集 | PASCAL VOC 2012 segmentation benchmark |\n| DCNN模型 | 权重采用预训练的VGG16 |\n| DCNN损失函数 | 交叉熵 |\n| 训练器 | SGD，batch=20 |\n| 学习率 | 初始为0.001，最后的分类层是0.01。每2000次迭代乘0.1 |\n| 权重 | 0.9的动量， 0.0005的衰减 |\n\n---\n\n## DepLab V2\n[论文地址](https://arxiv.org/pdf/1606.00915.pdf)\n\nDeepLabv2 是相对于 DeepLabv1 基础上的优化。DeepLabv1 在三个方向努力解决，但是问题依然存在：\n1. 特征分辨率的降低\n2. 物体存在多尺度\n3. DCNN 的平移不变性\n\n针对这三个问题, DeepLabv2做出了3个主要贡献:\n1. 首先，强调使用空洞卷积，作为密集预测任务的强大工具。空洞卷积能够明确地控制DCNN内计算特征响应的分辨率，即可以有效的扩大感受野，在不增加参数量和计算量的同时获取更多的上下文。\n2. 其次，提出了空洞空间卷积池化金字塔(atrous spatial pyramid pooling (ASPP))，以多尺度的信息得到更强健的分割结果。ASPP并行的采用多个采样率的空洞卷积层来探测，以多个比例捕捉对象以及图像上下文。\n3. 最后，通过组合DCNN和概率图模型，改进分割边界结果。在DCNN中最大池化和下采样组合实现可平移不变性，但这对精度是有影响的。通过将最终的DCNN层响应与全连接的CRF结合来克服这个问题。\n\n**步骤**\n<center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Image-Segmentation-Learning/10.png)</center>\n\n*   输入经过改进的DCNN(带空洞卷积和ASPP模块)得到粗略预测结果，即`Aeroplane Coarse Score map`\n*   通过双线性插值扩大到原本大小，即`Bi-linear Interpolation`\n*   再通过全连接的CRF细化预测结果，得到最终输出`Final Output`\n\n**方法**\n1. **空洞卷积用于密集特征提取和扩大感受野**\n  \n  首先考虑一维信号，空洞卷积输出为$y[i]$, 输入为$x[i]$, 长度K的滤波器为$w[k]$, 则定义为：\n  $$y[k]=\\sum_{k=1}^{K}x[i+r\\cdot k]w[k]$$\n  输入采样的步幅为参数r, 标准采样率是$r=1$如图(a); 图(b)是采样率$r=2$的时候：\n  <center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Image-Segmentation-Learning/11.png)</center>\n  \n  二维信号(图片)上使用空洞卷积的表现,给定一个图像：\n  * 上分支：首先下采样将分辨率降低2倍，做卷积。再上采样得到结果。本质上这只是在原图片的1/4内容上做卷积响应。\n  * 下分支：如果将全分辨率图像做空洞卷积(采样率为2，核大小与上面卷积核相同)，直接得到结果。这样可以计算出整张图像的响应，如下图所示，这样做效果更佳。\n  <center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Image-Segmentation-Learning/12.png)</center>\n  \n  空洞卷积能够放大滤波器的感受野，速率r引入$r-1$个零，有效将感受野从$k\\times k$扩展到$k_{e}=k+(k-1)(r-1)$而不增加参数和计算量。在DCNN中，常见的做法是混合使用空洞卷积以高的分辨率(理解为采样密度)计算最终的DCNN网络响应。\n2. **使用ASPP模块表示多尺度图像**\n\n  DeepLabv2的做法与SPPNet类似，并行的采用多个采样率的空洞卷积提取特征，再将特征融合，类似于空间金字塔结构，形象的称为Atrous Spatial Pyramid Pooling (ASPP)。示意图如下：\n  <center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Image-Segmentation-Learning/13.png)</center>\n  \n  在同一Input Feature Map的基础上，并行的使用4个空洞卷积，空洞卷积配置为$r=6,12,18,24$, 核大小为$3 \\times 3$. 最终将不同卷积层得到的结果做像素加融合到一起.\n3. **使用全连接CRF做结构预测用于恢复边界精度**\n\n  <center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Image-Segmentation-Learning/14.png)</center>\n  \n**训练**\n\n| 项目 | 设置 |\n| --- | --- |\n| DCNN模型 | 权重采用预训练的VGG16，**ResNet101** |\n| DCNN损失函数 | 输出的结果与ground truth下采样8倍做像素交叉熵 |\n| 训练器 | SGD，batch=20 |\n| 学习率 | 初始为0.001，最后的分类层是0.01。每2000次迭代乘0.1 |\n| 权重 | 0.9的动量， 0.0005的衰减 |\n\n---\n\n## DepLab V3\n[论文地址](https://arxiv.org/pdf/1706.05587.pdf)\n\n语义分割任务，在应用深度卷积神经网络中的有两个挑战：\n\n*   第一个挑战：连续池化和下采样，让高层特征具有局部图像变换的内在不变性，这允许DCNN学习越来越抽象的特征表示。但同时引起的特征分辨率下降，会妨碍密集的定位预测任务，因为这需要详细的空间信息。\n*   第二个挑战：多尺度目标的存在\n\nDeepLabv3的主要贡献在于：\n\n*   重新讨论了空洞卷积的使用，在级联模块和空间金字塔池化的框架下，能够获取更大的感受野从而获取多尺度信息。\n*   改进了ASPP模块：由不同采样率的空洞卷积和BN层组成，尝试以级联或并行的方式布局模块。\n*   讨论了一个重要问题：使用大采样率的空洞卷积，因为图像边界响应无法捕捉远距离信息，会退化为1×1的卷积, 因此建议将图像级特征融合到ASPP模块中。\n*   阐述了训练细节并分享了训练经验，论文提出的”DeepLabv3”改进了以前的工作，获得了很好的结果\n\n**方法**\n1. **空洞卷积应用于密集的特征提取**\n2. **深层次的空洞卷积**\n\n  <center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Image-Segmentation-Learning/15.png)</center>\n  \n  <center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Image-Segmentation-Learning/18.png)</center>\n\n  将空洞卷积应用在级联模块, 取ResNet中最后一个block，在上图中为block4，并在其后面增加级联模块。图(a)所示，整体图片的信息总结到后面非常小的特征映射上，使用步幅越长的特征映射，得到的结果反倒会差，结果最好的out_stride = 8 需要占用较多的存储空间。因为连续的下采样会降低特征映射的分辨率，细节信息被抽取，这对语义分割是有害的。上图(b)所示，可使用不同采样率的空洞卷积保持输出步幅的为out_stride = 16.这样不增加参数量和计算量同时有效的缩小了步幅。\n  \n3. **Atrous Spatial Pyramid Pooling**\n\n  对于在DeepLabv2中提出的ASPP模块，其在特征顶部映射图并行使用了四种不同采样率的空洞卷积。这表明以不同尺度采样是有效的，在DeepLabv3中向ASPP中添加了BN层。不同采样率的空洞卷积可以有效的捕获多尺度信息，但是，随着采样率的增加，滤波器的有效权重(权重有效的应用在特征区域，而不是填充0)逐渐变小。如下图所示：\n  <center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Image-Segmentation-Learning/16.png)</center>\n  \n  当不同采样率的$3 \\times 3$卷积核应用在$65 \\times 65$的特征映射上，采样率接近特征映射大小时，$3 \\times 3$的滤波器不是捕捉全图像的上下文，而是退化为简单的$1 \\times 1$滤波器，只有滤波器中心点的权重起了作用。\n  \n  <center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Image-Segmentation-Learning/17.png)</center>\n  为了克服这个问题，改进了ASPP结构如上图：\n  1. 一个$1 \\times 1$卷积和三个$3 \\times 3$卷积的采样率为$rate = (6,12,18)$的空洞卷积，滤波器数量为256，包含BN层。针对output_stride=16的情况。（当output_stride=8的时候，采样率会加倍，所有的特征会通过$1 \\times 1$卷积级联到一起）\n  2. 使用了图片级特征。具体来说，在模型最后的特征映射上应用全局平均，将结果经过$1 \\times 1$的卷积，再双线性上采样得到所需的空间维度。\n\n**训练**\n\n| 部分 | 设置 |\n| --- | --- |\n| 数据集 | PASCAL VOC 2012 |\n| 工具 | TensorFlow |\n| 裁剪尺寸 | 采样513大小的裁剪尺寸 |\n| 学习率策略 | 采用poly策略， $learning rate = base\\_lr(1-\\frac{iter}{max\\_iters})^{power}$ |\n| **BN层策略** | 当output_stride=16时，我们采用batchsize=16，同时BN层的参数做参数衰减0.9997。在增强的数据集上，以初始学习率0.007训练30K后，冻结BN层参数。采用output_stride=8时，再使用初始学习率0.001训练30K。训练output_stride=16比output_stride=8要快很多，因为中间的特征映射在空间上小四倍。但因为output_stride=16在特征映射上粗糙的是牺牲了精度。 |\n| **上采样策略** | 在先前的工作上,将最终的输出与GroundTruth下采样8倍做比较之后发现保持GroundTruth更重要，故将最终的输出上采样8倍与完整的GroundTruth比较。 |\n\n---\n\n## DepLab-V$3^{+}$\n[论文地址](https://arxiv.org/pdf/1802.02611.pdf)\n\n因为深度网络存在pooling or convolutions with stride的层，会导致feature分辨率下降，从而导致预测精度降低，而造成的边界信息丢失问题. 这个问题可以通过使用空洞卷积替代更多的pooling层来获取分辨率更高的feature。但是feature分辨率更高会极大增加运算量。\n\n**方法**\n<center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Image-Segmentation-Learning/19.png)</center>\n\n所以DeepLabV$3^{+}$中通过采用了encoder-decoder结构，在DeepLab V3中加入了一个简单有效的decoder模块来改善物体边缘的分割结果(图c)：先上采样4倍，在与encoder中的特征图concatenate，最后在上采样4倍恢复到原始图像大小。除此之外还尝试使用Xception作为encoder，在Atrous Spatial Pyramid Pooling和decoder中应用depth-wise separable convolution得到了更快精度更高的网络。\n\n1. **Encoder**\n\n  * ResNet: encoder就是DeepLab V3，通过修改ResNet101最后两(一)个block的stride，使得output stride为8(16)。之后在block4后应用改进后的Atrous Spatial Pyramid Pooling，将所得的特征图concatenate用1×1的卷积得到256个通道的特征图。\n  * **Xecption**: 采用的Xception模型为MSRA team提出的改进的Xception，叫做Aligned Xception，并做了几点修改：\n\n    * 网络深度与Aligned Xception相同，不同的地方在于为了快速计算和有效的使用内存而不修改entry flow network的结构。\n    * 所有的max pooling操作替换成带stride的separable convolution，这能使得对任意分辨率的图像应用atrous separable convolution提取特征。\n    * 在每个3×3的depath-wise convolution后增加BN层和ReLU。\n<center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Image-Segmentation-Learning/21.png)</center>\n\n2. **Decoder**\n\n  * 在decoder中，特征图首先上采样4倍，然后与encoder中对应分辨率低级特征concatenate。在concatenate之前，由于低级特征图的通道数通常太多(256或512)，而从encoder中得到的富含语义信息的特征图通道数只有256，这样会淡化语义信息，因此在concatenate之前，需要将低级特征图通过1×1的卷积减少通道数。在concatenate之后用3×3的卷积改善特征，最后上采样4倍恢复到原始图像大小。\n  * 设计： \n    * $1 \\times 1$卷积的通道数采用48\n    * 用来获得更锋利的边界的3×3的卷积。最后采用了2个3×3的卷积\n    * 使用的encoder的低级特征（Conv2）\n\n<center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Image-Segmentation-Learning/20.png)</center>\n\n**结果**：\n\n* **Aligned Xception改**\n  <center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Image-Segmentation-Learning/22.png)</center>\n  \n  当train_stride=16和eval_stride=8的时候mIOU最好达到了84.56% 然而计算量比较高。使用train_stride和eval_stride都为16的时候，结果下降了1.53%但是计算量下降了60倍。","slug":"MalongTech-Learning-Image-Segmentation","published":1,"updated":"2020-04-28T12:38:55.395Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck9jwdng7000ajlrckzg7vut2","content":"<h1 id=\"Image-Segmentation\"><a href=\"#Image-Segmentation\" class=\"headerlink\" title=\"Image Segmentation\"></a>Image Segmentation</h1><p><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Image-Segmentation-Learning/1.jpg\" alt=\"image\"></p>\n<h2 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h2><p>在计算机视觉领域，图像分割（Segmentation）指的是将数字图像细分为多个图像子区域（像素的集合）（也被称作超像素）的过程。图像分割的目的是简化或改变图像的表示形式，使得图像更容易理解和分析。图像分割通常用于定位图像中的物体和边界（线，曲线等）。更精确的，图像分割是对图像中的每个像素加标签的一个过程，这一过程使得具有相同标签的像素具有某种共同视觉特性。<a id=\"more\"></a></p>\n<p>图像分割的结果是图像上子区域的集合（这些子区域的全体覆盖了整个图像），或是从图像中提取的轮廓线的集合（例如边缘检测）。一个子区域中的每个像素在某种特性的度量下或是由计算得出的特性都是相似的，例如颜色、亮度、纹理。邻接区域在某种特性的度量下有很大的不同。</p>\n<p><strong>应用</strong></p>\n<ul>\n<li>医学影像：1. 肿瘤和其他病理的定位；2. 组织体积的测量；3. 计算机引导的手术；4. 诊断；5. 治疗方案的定制；6. 解剖学结构的研究</li>\n<li>卫星图像中定位物体</li>\n<li>人脸识别</li>\n<li>指纹识别</li>\n<li>交通控制系统</li>\n</ul>\n<h2 id=\"Tranditional-Methods\"><a href=\"#Tranditional-Methods\" class=\"headerlink\" title=\"Tranditional Methods\"></a>Tranditional Methods</h2><p><a href=\"https://zhuanlan.zhihu.com/p/30732385\" target=\"_blank\" rel=\"noopener\">参考</a></p>\n<ul>\n<li><strong>基于阙值的分割方法</strong>: 1. 固定阙值分割；2. 直方图双峰法；3. 迭代阙值图像分割；自适应阙值图像分割（最大分类方差法，均值法，最佳阙值）.</li>\n<li><strong>基于边缘的分割方法</strong>: 1. Canny边缘检测器；2. Harris角点检测器；3. SIFT检测器；3. SURF检测器.</li>\n<li><strong>基于区域的分割方法</strong>: 1. 种子区域生长法；2. 区域分裂合并法；3. 分水岭法.</li>\n<li><strong>基于图论的分割方法</strong>: 1. GraphCut; 2. GrabCut; 3. Random Walk.</li>\n<li><strong>基于能量泛函的分割方法</strong>: 参数活动轮廓模型（1. Snake模型；2. Active Shape Model; 3. Active Apperance Models; 4. Constrained local model）；几何活动轮廓模型.</li>\n</ul>\n<h2 id=\"Datasets\"><a href=\"#Datasets\" class=\"headerlink\" title=\"Datasets\"></a>Datasets</h2><ol>\n<li>Pascal VOC: 20个类别，6929张标注图片</li>\n<li>CityScapes：道路驾驶场景，30个类别，5000张精细标注，20000张粗糙标注</li>\n<li>MS COCO：80类，33万张图片，超过20万张有标注，150万个物体的个体</li>\n<li>医学影像领域的ImageNet: <a href=\"https://www.52cv.net/?p=883\" target=\"_blank\" rel=\"noopener\">DeepLesion</a>, 10000多个病例研究的超过32000个病变标注</li>\n</ol>\n<h2 id=\"图像分割的度量标准\"><a href=\"#图像分割的度量标准\" class=\"headerlink\" title=\"图像分割的度量标准\"></a>图像分割的度量标准</h2><p>假设共有$k+1$个类（从$L_{o}$到$L_{k}$,其中包含一个空类或背景）, $P_{ij}$表示本属于类<strong>i</strong>但被预测为类<strong>j</strong>的像素数量。即，$p_{ii}$表示<strong>真正</strong>的数量，而$p_{ij}$和$p_{ji}$则分别被解释为假正和假负，尽管两者都是假正与假负之和。</p>\n<ol>\n<li>Pixel Accuracy(PA, 像素精度): 最简单的度量，为标记正确的像素占总像素的比例。<br>$$PA=\\frac{\\sum_{i=0}^{k}p_{ii}}{\\sum_{i=0}^{k}\\sum_{j=0}^{k}p_{ij}}$$</li>\n<li>Mean Pixel Accuracy(MAP,均像素精度): PA的一种简单提升，计算每个类内被正确分类像素数的比例，之后求所有类的平均。<br>$$MAP=\\frac{1}{k+1}\\sum_{i=0}^{k}\\frac{p_{ii}}{\\sum_{j=0}^{k}p_{ij}}$$</li>\n<li>Mean Intersection over Union(MIoU, 均交并比): 为语义分割的标准度量。其计算两个集合的交集和并集之比，这两个集合为真实值(ground truth)和预测值(predicted segmentation). 这个比例可以变形为正真数(intersection)比上真正、假负、假正(并集)之和，之后在每个类上计算IoU再平均。<br>$$MIoU = \\frac{1}{k+1}\\sum_{i=0}^{k}\\frac{p_{ii}}{\\sum_{j=0}^{k}p_{ij}+\\sum_{j=0}^{k}(p_{ji}-p_{ii})}$$</li>\n<li>Frequency Weighted Intersection over Union(FWIoU, 频权交并比): 为MIoU的一种提升，根据每个类出现的频率为其设置权重。<br>$$FWIoU = \\frac{1}{\\sum_{i=0}^{k}\\sum_{j=0}^{k}p_{ij}}\\sum_{i=0}^{k}\\frac{p_{ii}}{\\sum_{j=0}^{k}p_{ij}+\\sum_{j=0}^{k}(p_{ji}-p_{ii})}$$</li>\n</ol>\n<h1 id=\"深度学习算法\"><a href=\"#深度学习算法\" class=\"headerlink\" title=\"深度学习算法\"></a>深度学习算法</h1><h2 id=\"Fully-Convolutional-Networks\"><a href=\"#Fully-Convolutional-Networks\" class=\"headerlink\" title=\"Fully Convolutional Networks\"></a>Fully Convolutional Networks</h2><p><a href=\"https://people.eecs.berkeley.edu/~jonlong/long_shelhamer_fcn.pdf\" target=\"_blank\" rel=\"noopener\">论文地址</a><br>传统神经网络做分类的步骤是，首先是一个图像进来之后经过多层卷积得到降维之后的特征图，这个特征图经过全连接层变成一个分类器，最后输出一个类别的向量，这就是分类的结果。</p>\n<p>而 FCN 是把所有的全连接层换成卷基层，原来只能输出一个类别分类的网络可以在特征图的每一个像素输出一个分类结果。这样就把分类的向量，变成了一个分类的特征图。</p>\n<center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Image-Segmentation-Learning/2.png\" alt=\"image\"></center>\n\n<p>上图中的猫, 输入AlexNet, 得到一个长为1000的输出向量, 表示输入图像属于每一类的概率, 其中在“tabby cat”这一类统计概率最高。而FCN对图像进行像素级的分类，从而解决了语义级别的图像分割（semantic segmentation）问题。FCN可以接受任意尺寸的输入图像，采用反卷积层对最后一个卷积层的feature map进行上采样, 使它恢复到输入图像相同的尺寸，从而可以对每个像素都产生了一个预测, 同时保留了原始输入图像中的空间信息, 最后在上采样的特征图上进行逐像素分类。最后逐个像素计算softmax分类的损失, 相当于每一个像素对应一个训练样本。</p>\n<p><strong>全连接-&gt;卷积层</strong>：</p>\n<p>一个 K=4096 的全连接层，输入数据体的尺寸是 7∗7∗512，这个全连接层可以被等效地看做一个 F=7,P=0,S=1,K=4096 的卷积层.</p>\n<p>假设一个卷积神经网络的输入是 224x224x3 的图像，一系列的卷积层和下采样层将图像数据变为尺寸为 7x7x512 的激活数据体。AlexNet使用了两个尺寸为4096的全连接层，最后一个有1000个神经元的全连接层用于计算分类评分。我们可以将这3个全连接层中的任意一个转化为卷积层：</p>\n<ul>\n<li>针对第一个连接区域是[7x7x512]的全连接层，令其滤波器尺寸为F=7，这样输出数据体就为[1x1x4096]了。</li>\n<li>针对第二个全连接层，令其滤波器尺寸为F=1，这样输出数据体为[1x1x4096]。</li>\n<li>对最后一个全连接层也做类似的，令其F=1，最终输出为[1x1x1000]</li>\n</ul>\n<p><strong>end to end, pixels to pixels network</strong><br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Image-Segmentation-Learning/3.png\" alt=\"image\"><br>经过多次卷积和pooling以后，得到的图像越来越小，分辨率越来越低。其中图像到$\\frac{H}{32} \\times \\frac{W}{32}$的时候图片是最小的一层时，所产生图叫做heatmap热图，热图就是最重要的高维特征图，得到高维特征的heatmap之后就是最重要的一步也是最后的一步对原图像进行upsampling，把图像进行放大、放大、放大，到原图像的大小。最后的输出是1000张heatmap经过upsampling变为原图大小的图片，为了对每个像素进行分类预测label成最后已经进行语义分割的图像，这里有一个小trick，就是最后通过逐个像素地求其在1000张图像该像素位置的最大数值描述（概率）作为该像素的分类。因此产生了一张已经分类好的图片，如上图右侧有狗狗和猫猫的图。</p>\n<p><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Image-Segmentation-Learning/4.png\" alt=\"image\"></p>\n<p>现在我们有1/32尺寸的heatMap，1/16尺寸的featureMap和1/8尺寸的featureMap，1/32尺寸的heatMap进行upsampling操作之后，因为这样的操作还原的图片仅仅是conv5中的卷积核中的特征，限于精度问题不能够很好地还原图像当中的特征，因此在这里向前迭代。把conv4中的卷积核对上一次upsampling之后的图进行反卷积补充细节（相当于一个插值过程），最后把conv3中的卷积核对刚才upsampling之后的图像进行再次反卷积补充细节，最后就完成了整个图像的还原。</p>\n<p><strong>缺点</strong>：</p>\n<ul>\n<li>是得到的结果还是不够精细。进行8倍上采样虽然比32倍的效果好了很多，但是上采样的结果还是比较模糊和平滑，对图像中的细节不敏感。</li>\n<li>是对各个像素进行分类，没有充分考虑像素与像素之间的关系。忽略了在通常的基于像素分类的分割方法中使用的空间规整（spatial regularization）步骤，缺乏空间一致性。</li>\n</ul>\n<p><strong>补充：插值法</strong><br>上采样upsampling的主要目的是放大图像，几乎都是采用内插值法，即在原有图像像素的基础上，在像素点值之间采用合适的<strong>插值算法</strong>插入新的元素。</p>\n<ul>\n<li><p>线性插值法：<br>使用连接两个已知量的直线来确定在这个两个已知量之间的一个未知量的值的方法。<br><center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Image-Segmentation-Learning/5.png\" alt=\"image\"></center><br>该直线方程可表示为：$\\frac{y-y_{0}}{y_{1}-y_{0}}=\\frac{x-x_{0}}{x_{1}-x_{0}}$ 假设方程两边的值是$\\alpha$，那么这个值就是插值系数，即$\\alpha =\\frac{y-y_{0}}{y_{1}-y_{0}}=\\frac{x-x_{0}}{x_{1}-x_{0}}$. 所以y可以表示为: $y=(1-\\alpha)y_{0}+\\alpha y_{1} = (1-\\frac{x-x_{0}}{x_{1}-x_{0}})y_{0}+\\frac{x-x_{0}}{x_{1}-x_{0}}y_{1}=\\frac{x_{1}-x}{x_{1}-x_{0}}y_{0}+\\frac{x-x_{0}}{x_{1}-x_{0}}y_{1}=\\frac{x_{1}-x}{x_{1}-x_{0}}f(x_{1})+\\frac{x-x_{0}}{x_{1}-x_{0}}f(x_{0})$</p>\n</li>\n<li><p>双线性插值<br>双线性插值是插值算法中的一种，是线性插值的扩展。利用原图像中目标点四周的四个真实存在的像素值来共同决定目标图中的一个像素值，其核心思想是在两个方向分别进行一次线性插值。</p>\n<center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Image-Segmentation-Learning/6.png\" alt=\"image\"></center>\n\n</li>\n</ul>\n<p>X方向的线性插值：在$Q_{12}$,$Q_{22}$中插入蓝色点$R_{2}$，$Q_{11}$，$Q_{21}$中插入蓝色点$R_{1}$</p>\n<p>$f(R_{1}=\\frac{x_{2}-x}{x_{2}-x_{1}}f(Q_{11})+\\frac{x-x_{1}}{x_{2}-x_{1}}f(Q_{21})$; $f(R_{2}=\\frac{x_{2}-x}{x_{2}-x_{1}}f(Q_{12})+\\frac{x-x_{1}}{x_{2}-x_{1}}f(Q_{22})$</p>\n<p>Y方向的线性插值：通过第一步计算出的$R_{1$}与$R_{2}$在y方向上插值计算出P点</p>\n<p>$f(P)=\\frac{y_{2}-y}{y_{2}-y_{1}}f(R_{1})+\\frac{y-y_{1}}{y_{2}-y_{1}}f(R_{2})$</p>\n<hr>\n<h2 id=\"U-Net-Convolutional-Networks-for-Biomedical-Image-Segmentation\"><a href=\"#U-Net-Convolutional-Networks-for-Biomedical-Image-Segmentation\" class=\"headerlink\" title=\"U-Net: Convolutional Networks for Biomedical Image Segmentation\"></a>U-Net: Convolutional Networks for Biomedical Image Segmentation</h2><p><a href=\"https://arxiv.org/pdf/1505.04597.pdf\" target=\"_blank\" rel=\"noopener\">论文地址</a><br>卷积网络被大规模应用在分类任务中，输出的结果是整个图像的类标签。然而，在许多视觉任务，尤其是生物医学图像处理领域，目标输出应该包括目标类别的位置，并且每个像素都应该有类标签。另外，在生物医学图像往往缺少训练图片。所以，Ciresan等人训练了一个卷积神经网络，用滑动窗口提供像素的周围区域（patch）作为输入来预测每个像素的类标签。这个网络有两个优点： 第一，输出结果可以定位出目标类别的位置； 第二，由于输入的训练数据是patches，这样就相当于进行了数据增广，解决了生物医学图像数量少的问题。</p>\n<p>但是，这个方法也有两个很明显缺点。</p>\n<p>第一，它很慢，因为这个网络必须训练每个patch，并且因为patch间的重叠有很多的冗余,造成资源的浪费，减慢训练时间和效率; 第二，定位准确性和获取上下文信息不可兼得。大的patches需要更多的max-pooling层这样减小了定位准确性,小的patches只能看到很小的局部信息，包含的背景信息不够。</p>\n<p><strong>U-Net Architecture</strong></p>\n<center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Image-Segmentation-Learning/7.jpg\" alt=\"image\"></center>\n\n<ol>\n<li>使用全卷积神经网络。(全卷积神经网络就是卷积取代了全连接层，全连接层必须固定图像大小而卷积不用，所以这个策略使得，你可以输入任意尺寸的图片，而且输出也是图片，所以这是一个端到端的网络。)</li>\n<li>左边的网络是收缩路径：使用卷积和maxpooling。</li>\n<li>右边的网络是扩张路径:使用上采样产生的特征图与左侧收缩路径对应层产生的特征图进行concatenate操作。（pooling层会丢失图像信息和降低图像分辨率且是不可逆的操作，对图像分割任务有一些影响，对图像分类任务的影响不大，为什么要做上采样？因为上采样可以补足一些图片的信息，但是信息补充的肯定不完全，所以还需要与左边的分辨率比较高的图片相连接起来（直接复制过来再裁剪到与上采样图片一样大小），这就相当于在高分辨率和更抽象特征当中做一个折衷，因为随着卷积次数增多，提取的特征也更加有效，更加抽象，上采样的图片是经历多次卷积后的图片，肯定是比较高效和抽象的图片，然后把它与左边不怎么抽象但更高分辨率的特征图片进行连接）。</li>\n<li>最后再经过两次反卷积操作，生成特征图，再用两个1X1的卷积做分类得到最后的两张heatmap,例如第一张表示的是第一类的得分，第二张表示第二类的得分heatmap,然后作为softmax函数的输入，算出概率比较大的softmax类，选择它作为输入给交叉熵进行反向传播训练。</li>\n</ol>\n<p><strong>Overlap-tile strategy</strong></p>\n<center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Image-Segmentation-Learning/8.png\" alt=\"image\"></center>\n\n<p>医学图像是一般相当大，但是分割时候不可能将原图太小输入网络，所以必须切成一张一张的小patch，在切成小patch的时候，Unet由于网络结构原因适合有overlap的切图，可以看图，红框是要分割区域，但是在切图时要包含周围区域，overlap另一个重要原因是周围overlap部分可以为分割区域边缘部分提供文理等信息。可以看黄框的边缘，分割结果并没有受到切成小patch而造成分割情况不好。 </p>\n<p><strong>训练</strong></p>\n<p>最后一层使用了交叉熵函数与softmax：<br>$$E=\\sum_{x\\in \\Omega}w(x)log(p_{\\ell(x)}(x))$$</p>\n<p>并且为了补偿训练每个类像素的不同频率使得网络更注重学习相互接触的细胞之间的小的分割边界，引入了权重图计算$w(x)$:<br>$$w(x)=w_{c}(x)+w_{0} \\times exp(-\\frac{(d_{1}(x)+d_{2}(x))}{2\\sigma^{2}})$$</p>\n<p><strong>Data Augmentation</strong><br>在只有少量样本的情况况下，要想尽可能的让网络获得不变性和鲁棒性，数据增加是必不可少的。因为本论文需要处理显微镜图片，我们需要平移与旋转不变性，并且对形变和灰度变化鲁棒。将训练样本进行随机弹性形变是训练分割网络的关键。使用随机位移矢量在粗糙的3*3网格上产生平滑形变(smooth deformations)。 位移是从10像素标准偏差的高斯分布中采样的。然后使用双三次插值(Bicubic interpolation)计算每个像素的位移。在contracting path的末尾采用drop-out 层更进一步增加数据。</p>\n<p><strong>双三次插值</strong><br>在这种方法中，函数f在点(x,y)的值可以通过矩形网络中最近的16个采样点加权平均得到。</p>\n<hr>\n<h2 id=\"DeepLab-V1\"><a href=\"#DeepLab-V1\" class=\"headerlink\" title=\"DeepLab V1\"></a>DeepLab V1</h2><p><a href=\"https://arxiv.org/pdf/1412.7062v3.pdf\" target=\"_blank\" rel=\"noopener\">论文地址</a></p>\n<p>DeepLab是结合了深度卷积神经网络(<a href=\"https://www.cnblogs.com/wangxiaocvpr/p/8763510.html\" target=\"_blank\" rel=\"noopener\">DCNNs</a>)和概率图模型(<a href=\"https://zhuanlan.zhihu.com/p/33397147\" target=\"_blank\" rel=\"noopener\">DenseCRFs</a>)的方法.</p>\n<p>DCNN在图像标记任务中存在两个技术障碍：</p>\n<ul>\n<li>信号下采样</li>\n<li>空间不敏感(invariance)</li>\n</ul>\n<p>第一个问题涉及到：在DCNN中重复最大池化和下采样带来的分辨率下降问题，分辨率的下降会丢失细节。DeepLab是采用的<code>atrous</code>(带孔)算法扩展感受野，获取更多的上下文信息。</p>\n<p>第二个问题涉及到：分类器获取以对象中心的决策是需要空间变换的不变性，这天然的限制了DCNN的定位精度，DeepLab采用完全连接的条件随机场(DenseCRF)提高模型捕获细节的能力。</p>\n<p>除空洞卷积和 CRFs 之外，论文使用的 tricks 还有 Multi-Scale features。其实就是 U-Net 和 FPN 的思想，在输入图像和前四个最大池化层的输出上附加了两层的 MLP，第一层是 128 个 3×3 卷积，第二层是 128 个 1×1 卷积。最终输出的特征与主干网的最后一层特征图融合，特征图增加 5×128=640 个通道。实验表示多尺度有助于提升预测结果，但是效果不如 CRF 明显。</p>\n<p><strong>CRF-&gt;语义分割</strong></p>\n<p>对于每个像素位置$i$具有隐变量$x_{i}$(这里隐变量就是像素的真实类别标签，如果预测结果有21类，则$(i \\in 1,2,..,21)$ 还有对应的观测值 $y_{i}$(即像素点对应的颜色值)。以像素为节点，像素与像素间的关系作为边，构成了一个条件随机场(CRF)。通过观测变量$y_{i}$来推测像素位置$i$应的类别标签$x_{i}$.条件随机场示意图如下:</p>\n<center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Image-Segmentation-Learning/9.png\" alt=\"image\"></center>\n\n<p>条件随机场符合吉布斯分布(x是上面的观测值，下面省略全局观测I):<br>$$p(x|I)=\\frac{1}{Z}exp(-E(x|I))$$</p>\n<p>全连接的CRF模型使用的能量函数$E(x)$为:<br>$$E(x)=\\sum_{i} \\theta_{i}(x_{i})+\\sum_{ij}\\theta_{ij}(x_{i},x_{j})$$</p>\n<p>这分为一元势函数$\\theta_{i}(x_{i})$和二元势函数$\\theta_{ij}(x_{i},x_{j})$两部分</p>\n<ul>\n<li>一元势函数是定义在观测序列位置i的状态特征函数，用于刻画观测序列对标记变量的影响（例如在城市道路任务中，观测到像素点为黑色，对应车子的可能比天空可能要大）。这里$P(x_{i})$是取DCNN计算关于像素i的输出的标签分配概率.</li>\n</ul>\n<p>$$\\theta_{i}(x_{i})=-logP(x_{i})$$</p>\n<ul>\n<li>二元势函数是定义在不同观测位置上的转移特征函数，用于刻画变量之间的相关关系以及观测序列对其影响。如果比较相似，那可能是一类，否则就裂开，这可以细化边缘。一般的二元势函数只取像素点与周围像素之间的边，这里使用的是全连接，即像素点与其他所有像素之间的关系。</li>\n</ul>\n<p>$$\\theta_{ij}(x_{i},x_{j})=\\mu(x_{i},x_{j})\\sum_{m=1}^{K}w_{m}k^{m}(f_{i},f_{j})$$</p>\n<p>DeepLab中高斯核采用双边位置和颜色的组合（第一核取决于像素位置(p)和像素颜色强度(I),第二核取决于像素位置(p)）:<br>$$w_{1}exp(-\\frac{\\lVert p_{i}-p_{j} \\rVert^{2}}{2\\sigma_{\\alpha}^{2}}-\\frac{\\lVert I_{i}-I_{j} \\rVert^{2}}{2\\sigma_{\\beta}^{2}})+w_{2}exp(-\\frac{\\lVert p_{i}-p_{j} \\rVert^{2}}{2\\sigma_{\\gamma}^{2}})$$</p>\n<p><strong>实验</strong></p>\n<table>\n<thead>\n<tr>\n<th>项目</th>\n<th>设置</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>数据集</td>\n<td>PASCAL VOC 2012 segmentation benchmark</td>\n</tr>\n<tr>\n<td>DCNN模型</td>\n<td>权重采用预训练的VGG16</td>\n</tr>\n<tr>\n<td>DCNN损失函数</td>\n<td>交叉熵</td>\n</tr>\n<tr>\n<td>训练器</td>\n<td>SGD，batch=20</td>\n</tr>\n<tr>\n<td>学习率</td>\n<td>初始为0.001，最后的分类层是0.01。每2000次迭代乘0.1</td>\n</tr>\n<tr>\n<td>权重</td>\n<td>0.9的动量， 0.0005的衰减</td>\n</tr>\n</tbody>\n</table>\n<hr>\n<h2 id=\"DepLab-V2\"><a href=\"#DepLab-V2\" class=\"headerlink\" title=\"DepLab V2\"></a>DepLab V2</h2><p><a href=\"https://arxiv.org/pdf/1606.00915.pdf\" target=\"_blank\" rel=\"noopener\">论文地址</a></p>\n<p>DeepLabv2 是相对于 DeepLabv1 基础上的优化。DeepLabv1 在三个方向努力解决，但是问题依然存在：</p>\n<ol>\n<li>特征分辨率的降低</li>\n<li>物体存在多尺度</li>\n<li>DCNN 的平移不变性</li>\n</ol>\n<p>针对这三个问题, DeepLabv2做出了3个主要贡献:</p>\n<ol>\n<li>首先，强调使用空洞卷积，作为密集预测任务的强大工具。空洞卷积能够明确地控制DCNN内计算特征响应的分辨率，即可以有效的扩大感受野，在不增加参数量和计算量的同时获取更多的上下文。</li>\n<li>其次，提出了空洞空间卷积池化金字塔(atrous spatial pyramid pooling (ASPP))，以多尺度的信息得到更强健的分割结果。ASPP并行的采用多个采样率的空洞卷积层来探测，以多个比例捕捉对象以及图像上下文。</li>\n<li>最后，通过组合DCNN和概率图模型，改进分割边界结果。在DCNN中最大池化和下采样组合实现可平移不变性，但这对精度是有影响的。通过将最终的DCNN层响应与全连接的CRF结合来克服这个问题。</li>\n</ol>\n<p><strong>步骤</strong></p>\n<center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Image-Segmentation-Learning/10.png\" alt=\"image\"></center>\n\n<ul>\n<li>输入经过改进的DCNN(带空洞卷积和ASPP模块)得到粗略预测结果，即<code>Aeroplane Coarse Score map</code></li>\n<li>通过双线性插值扩大到原本大小，即<code>Bi-linear Interpolation</code></li>\n<li>再通过全连接的CRF细化预测结果，得到最终输出<code>Final Output</code></li>\n</ul>\n<p><strong>方法</strong></p>\n<ol>\n<li><p><strong>空洞卷积用于密集特征提取和扩大感受野</strong></p>\n<p>首先考虑一维信号，空洞卷积输出为$y[i]$, 输入为$x[i]$, 长度K的滤波器为$w[k]$, 则定义为：<br>$$y[k]=\\sum_{k=1}^{K}x[i+r\\cdot k]w[k]$$<br>输入采样的步幅为参数r, 标准采样率是$r=1$如图(a); 图(b)是采样率$r=2$的时候：</p>\n<center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Image-Segmentation-Learning/11.png\" alt=\"image\"></center>\n\n<p>二维信号(图片)上使用空洞卷积的表现,给定一个图像：</p>\n<ul>\n<li>上分支：首先下采样将分辨率降低2倍，做卷积。再上采样得到结果。本质上这只是在原图片的1/4内容上做卷积响应。</li>\n<li>下分支：如果将全分辨率图像做空洞卷积(采样率为2，核大小与上面卷积核相同)，直接得到结果。这样可以计算出整张图像的响应，如下图所示，这样做效果更佳。<center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Image-Segmentation-Learning/12.png\" alt=\"image\"></center>\n\n</li>\n</ul>\n<p>空洞卷积能够放大滤波器的感受野，速率r引入$r-1$个零，有效将感受野从$k\\times k$扩展到$k_{e}=k+(k-1)(r-1)$而不增加参数和计算量。在DCNN中，常见的做法是混合使用空洞卷积以高的分辨率(理解为采样密度)计算最终的DCNN网络响应。</p>\n</li>\n<li><p><strong>使用ASPP模块表示多尺度图像</strong></p>\n<p>DeepLabv2的做法与SPPNet类似，并行的采用多个采样率的空洞卷积提取特征，再将特征融合，类似于空间金字塔结构，形象的称为Atrous Spatial Pyramid Pooling (ASPP)。示意图如下：</p>\n<center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Image-Segmentation-Learning/13.png\" alt=\"image\"></center>\n\n<p>在同一Input Feature Map的基础上，并行的使用4个空洞卷积，空洞卷积配置为$r=6,12,18,24$, 核大小为$3 \\times 3$. 最终将不同卷积层得到的结果做像素加融合到一起.</p>\n</li>\n<li><p><strong>使用全连接CRF做结构预测用于恢复边界精度</strong></p>\n<center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Image-Segmentation-Learning/14.png\" alt=\"image\"></center>\n\n</li>\n</ol>\n<p><strong>训练</strong></p>\n<table>\n<thead>\n<tr>\n<th>项目</th>\n<th>设置</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>DCNN模型</td>\n<td>权重采用预训练的VGG16，<strong>ResNet101</strong></td>\n</tr>\n<tr>\n<td>DCNN损失函数</td>\n<td>输出的结果与ground truth下采样8倍做像素交叉熵</td>\n</tr>\n<tr>\n<td>训练器</td>\n<td>SGD，batch=20</td>\n</tr>\n<tr>\n<td>学习率</td>\n<td>初始为0.001，最后的分类层是0.01。每2000次迭代乘0.1</td>\n</tr>\n<tr>\n<td>权重</td>\n<td>0.9的动量， 0.0005的衰减</td>\n</tr>\n</tbody>\n</table>\n<hr>\n<h2 id=\"DepLab-V3\"><a href=\"#DepLab-V3\" class=\"headerlink\" title=\"DepLab V3\"></a>DepLab V3</h2><p><a href=\"https://arxiv.org/pdf/1706.05587.pdf\" target=\"_blank\" rel=\"noopener\">论文地址</a></p>\n<p>语义分割任务，在应用深度卷积神经网络中的有两个挑战：</p>\n<ul>\n<li>第一个挑战：连续池化和下采样，让高层特征具有局部图像变换的内在不变性，这允许DCNN学习越来越抽象的特征表示。但同时引起的特征分辨率下降，会妨碍密集的定位预测任务，因为这需要详细的空间信息。</li>\n<li>第二个挑战：多尺度目标的存在</li>\n</ul>\n<p>DeepLabv3的主要贡献在于：</p>\n<ul>\n<li>重新讨论了空洞卷积的使用，在级联模块和空间金字塔池化的框架下，能够获取更大的感受野从而获取多尺度信息。</li>\n<li>改进了ASPP模块：由不同采样率的空洞卷积和BN层组成，尝试以级联或并行的方式布局模块。</li>\n<li>讨论了一个重要问题：使用大采样率的空洞卷积，因为图像边界响应无法捕捉远距离信息，会退化为1×1的卷积, 因此建议将图像级特征融合到ASPP模块中。</li>\n<li>阐述了训练细节并分享了训练经验，论文提出的”DeepLabv3”改进了以前的工作，获得了很好的结果</li>\n</ul>\n<p><strong>方法</strong></p>\n<ol>\n<li><strong>空洞卷积应用于密集的特征提取</strong></li>\n<li><p><strong>深层次的空洞卷积</strong></p>\n<center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Image-Segmentation-Learning/15.png\" alt=\"image\"></center>\n\n<center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Image-Segmentation-Learning/18.png\" alt=\"image\"></center>\n\n<p>将空洞卷积应用在级联模块, 取ResNet中最后一个block，在上图中为block4，并在其后面增加级联模块。图(a)所示，整体图片的信息总结到后面非常小的特征映射上，使用步幅越长的特征映射，得到的结果反倒会差，结果最好的out_stride = 8 需要占用较多的存储空间。因为连续的下采样会降低特征映射的分辨率，细节信息被抽取，这对语义分割是有害的。上图(b)所示，可使用不同采样率的空洞卷积保持输出步幅的为out_stride = 16.这样不增加参数量和计算量同时有效的缩小了步幅。</p>\n</li>\n<li><p><strong>Atrous Spatial Pyramid Pooling</strong></p>\n<p>对于在DeepLabv2中提出的ASPP模块，其在特征顶部映射图并行使用了四种不同采样率的空洞卷积。这表明以不同尺度采样是有效的，在DeepLabv3中向ASPP中添加了BN层。不同采样率的空洞卷积可以有效的捕获多尺度信息，但是，随着采样率的增加，滤波器的有效权重(权重有效的应用在特征区域，而不是填充0)逐渐变小。如下图所示：</p>\n<center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Image-Segmentation-Learning/16.png\" alt=\"image\"></center>\n\n<p>当不同采样率的$3 \\times 3$卷积核应用在$65 \\times 65$的特征映射上，采样率接近特征映射大小时，$3 \\times 3$的滤波器不是捕捉全图像的上下文，而是退化为简单的$1 \\times 1$滤波器，只有滤波器中心点的权重起了作用。</p>\n<p><center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Image-Segmentation-Learning/17.png\" alt=\"image\"></center><br>为了克服这个问题，改进了ASPP结构如上图：</p>\n<ol>\n<li>一个$1 \\times 1$卷积和三个$3 \\times 3$卷积的采样率为$rate = (6,12,18)$的空洞卷积，滤波器数量为256，包含BN层。针对output_stride=16的情况。（当output_stride=8的时候，采样率会加倍，所有的特征会通过$1 \\times 1$卷积级联到一起）</li>\n<li>使用了图片级特征。具体来说，在模型最后的特征映射上应用全局平均，将结果经过$1 \\times 1$的卷积，再双线性上采样得到所需的空间维度。</li>\n</ol>\n</li>\n</ol>\n<p><strong>训练</strong></p>\n<table>\n<thead>\n<tr>\n<th>部分</th>\n<th>设置</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>数据集</td>\n<td>PASCAL VOC 2012</td>\n</tr>\n<tr>\n<td>工具</td>\n<td>TensorFlow</td>\n</tr>\n<tr>\n<td>裁剪尺寸</td>\n<td>采样513大小的裁剪尺寸</td>\n</tr>\n<tr>\n<td>学习率策略</td>\n<td>采用poly策略， $learning rate = base_lr(1-\\frac{iter}{max_iters})^{power}$</td>\n</tr>\n<tr>\n<td><strong>BN层策略</strong></td>\n<td>当output_stride=16时，我们采用batchsize=16，同时BN层的参数做参数衰减0.9997。在增强的数据集上，以初始学习率0.007训练30K后，冻结BN层参数。采用output_stride=8时，再使用初始学习率0.001训练30K。训练output_stride=16比output_stride=8要快很多，因为中间的特征映射在空间上小四倍。但因为output_stride=16在特征映射上粗糙的是牺牲了精度。</td>\n</tr>\n<tr>\n<td><strong>上采样策略</strong></td>\n<td>在先前的工作上,将最终的输出与GroundTruth下采样8倍做比较之后发现保持GroundTruth更重要，故将最终的输出上采样8倍与完整的GroundTruth比较。</td>\n</tr>\n</tbody>\n</table>\n<hr>\n<h2 id=\"DepLab-V-3\"><a href=\"#DepLab-V-3\" class=\"headerlink\" title=\"DepLab-V$3^{+}$\"></a>DepLab-V$3^{+}$</h2><p><a href=\"https://arxiv.org/pdf/1802.02611.pdf\" target=\"_blank\" rel=\"noopener\">论文地址</a></p>\n<p>因为深度网络存在pooling or convolutions with stride的层，会导致feature分辨率下降，从而导致预测精度降低，而造成的边界信息丢失问题. 这个问题可以通过使用空洞卷积替代更多的pooling层来获取分辨率更高的feature。但是feature分辨率更高会极大增加运算量。</p>\n<p><strong>方法</strong></p>\n<center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Image-Segmentation-Learning/19.png\" alt=\"image\"></center>\n\n<p>所以DeepLabV$3^{+}$中通过采用了encoder-decoder结构，在DeepLab V3中加入了一个简单有效的decoder模块来改善物体边缘的分割结果(图c)：先上采样4倍，在与encoder中的特征图concatenate，最后在上采样4倍恢复到原始图像大小。除此之外还尝试使用Xception作为encoder，在Atrous Spatial Pyramid Pooling和decoder中应用depth-wise separable convolution得到了更快精度更高的网络。</p>\n<ol>\n<li><p><strong>Encoder</strong></p>\n<ul>\n<li>ResNet: encoder就是DeepLab V3，通过修改ResNet101最后两(一)个block的stride，使得output stride为8(16)。之后在block4后应用改进后的Atrous Spatial Pyramid Pooling，将所得的特征图concatenate用1×1的卷积得到256个通道的特征图。</li>\n<li><p><strong>Xecption</strong>: 采用的Xception模型为MSRA team提出的改进的Xception，叫做Aligned Xception，并做了几点修改：</p>\n<ul>\n<li>网络深度与Aligned Xception相同，不同的地方在于为了快速计算和有效的使用内存而不修改entry flow network的结构。</li>\n<li>所有的max pooling操作替换成带stride的separable convolution，这能使得对任意分辨率的图像应用atrous separable convolution提取特征。</li>\n<li>在每个3×3的depath-wise convolution后增加BN层和ReLU。<center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Image-Segmentation-Learning/21.png\" alt=\"image\"></center>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n<li><p><strong>Decoder</strong></p>\n<ul>\n<li>在decoder中，特征图首先上采样4倍，然后与encoder中对应分辨率低级特征concatenate。在concatenate之前，由于低级特征图的通道数通常太多(256或512)，而从encoder中得到的富含语义信息的特征图通道数只有256，这样会淡化语义信息，因此在concatenate之前，需要将低级特征图通过1×1的卷积减少通道数。在concatenate之后用3×3的卷积改善特征，最后上采样4倍恢复到原始图像大小。</li>\n<li>设计： <ul>\n<li>$1 \\times 1$卷积的通道数采用48</li>\n<li>用来获得更锋利的边界的3×3的卷积。最后采用了2个3×3的卷积</li>\n<li>使用的encoder的低级特征（Conv2）</li>\n</ul>\n</li>\n</ul>\n</li>\n</ol>\n<center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Image-Segmentation-Learning/20.png\" alt=\"image\"></center>\n\n<p><strong>结果</strong>：</p>\n<ul>\n<li><p><strong>Aligned Xception改</strong></p>\n<center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Image-Segmentation-Learning/22.png\" alt=\"image\"></center>\n\n<p>当train_stride=16和eval_stride=8的时候mIOU最好达到了84.56% 然而计算量比较高。使用train_stride和eval_stride都为16的时候，结果下降了1.53%但是计算量下降了60倍。</p>\n</li>\n</ul>\n","site":{"data":{}},"excerpt":"<h1 id=\"Image-Segmentation\"><a href=\"#Image-Segmentation\" class=\"headerlink\" title=\"Image Segmentation\"></a>Image Segmentation</h1><p><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Image-Segmentation-Learning/1.jpg\" alt=\"image\"></p>\n<h2 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h2><p>在计算机视觉领域，图像分割（Segmentation）指的是将数字图像细分为多个图像子区域（像素的集合）（也被称作超像素）的过程。图像分割的目的是简化或改变图像的表示形式，使得图像更容易理解和分析。图像分割通常用于定位图像中的物体和边界（线，曲线等）。更精确的，图像分割是对图像中的每个像素加标签的一个过程，这一过程使得具有相同标签的像素具有某种共同视觉特性。","more":"</p>\n<p>图像分割的结果是图像上子区域的集合（这些子区域的全体覆盖了整个图像），或是从图像中提取的轮廓线的集合（例如边缘检测）。一个子区域中的每个像素在某种特性的度量下或是由计算得出的特性都是相似的，例如颜色、亮度、纹理。邻接区域在某种特性的度量下有很大的不同。</p>\n<p><strong>应用</strong></p>\n<ul>\n<li>医学影像：1. 肿瘤和其他病理的定位；2. 组织体积的测量；3. 计算机引导的手术；4. 诊断；5. 治疗方案的定制；6. 解剖学结构的研究</li>\n<li>卫星图像中定位物体</li>\n<li>人脸识别</li>\n<li>指纹识别</li>\n<li>交通控制系统</li>\n</ul>\n<h2 id=\"Tranditional-Methods\"><a href=\"#Tranditional-Methods\" class=\"headerlink\" title=\"Tranditional Methods\"></a>Tranditional Methods</h2><p><a href=\"https://zhuanlan.zhihu.com/p/30732385\" target=\"_blank\" rel=\"noopener\">参考</a></p>\n<ul>\n<li><strong>基于阙值的分割方法</strong>: 1. 固定阙值分割；2. 直方图双峰法；3. 迭代阙值图像分割；自适应阙值图像分割（最大分类方差法，均值法，最佳阙值）.</li>\n<li><strong>基于边缘的分割方法</strong>: 1. Canny边缘检测器；2. Harris角点检测器；3. SIFT检测器；3. SURF检测器.</li>\n<li><strong>基于区域的分割方法</strong>: 1. 种子区域生长法；2. 区域分裂合并法；3. 分水岭法.</li>\n<li><strong>基于图论的分割方法</strong>: 1. GraphCut; 2. GrabCut; 3. Random Walk.</li>\n<li><strong>基于能量泛函的分割方法</strong>: 参数活动轮廓模型（1. Snake模型；2. Active Shape Model; 3. Active Apperance Models; 4. Constrained local model）；几何活动轮廓模型.</li>\n</ul>\n<h2 id=\"Datasets\"><a href=\"#Datasets\" class=\"headerlink\" title=\"Datasets\"></a>Datasets</h2><ol>\n<li>Pascal VOC: 20个类别，6929张标注图片</li>\n<li>CityScapes：道路驾驶场景，30个类别，5000张精细标注，20000张粗糙标注</li>\n<li>MS COCO：80类，33万张图片，超过20万张有标注，150万个物体的个体</li>\n<li>医学影像领域的ImageNet: <a href=\"https://www.52cv.net/?p=883\" target=\"_blank\" rel=\"noopener\">DeepLesion</a>, 10000多个病例研究的超过32000个病变标注</li>\n</ol>\n<h2 id=\"图像分割的度量标准\"><a href=\"#图像分割的度量标准\" class=\"headerlink\" title=\"图像分割的度量标准\"></a>图像分割的度量标准</h2><p>假设共有$k+1$个类（从$L_{o}$到$L_{k}$,其中包含一个空类或背景）, $P_{ij}$表示本属于类<strong>i</strong>但被预测为类<strong>j</strong>的像素数量。即，$p_{ii}$表示<strong>真正</strong>的数量，而$p_{ij}$和$p_{ji}$则分别被解释为假正和假负，尽管两者都是假正与假负之和。</p>\n<ol>\n<li>Pixel Accuracy(PA, 像素精度): 最简单的度量，为标记正确的像素占总像素的比例。<br>$$PA=\\frac{\\sum_{i=0}^{k}p_{ii}}{\\sum_{i=0}^{k}\\sum_{j=0}^{k}p_{ij}}$$</li>\n<li>Mean Pixel Accuracy(MAP,均像素精度): PA的一种简单提升，计算每个类内被正确分类像素数的比例，之后求所有类的平均。<br>$$MAP=\\frac{1}{k+1}\\sum_{i=0}^{k}\\frac{p_{ii}}{\\sum_{j=0}^{k}p_{ij}}$$</li>\n<li>Mean Intersection over Union(MIoU, 均交并比): 为语义分割的标准度量。其计算两个集合的交集和并集之比，这两个集合为真实值(ground truth)和预测值(predicted segmentation). 这个比例可以变形为正真数(intersection)比上真正、假负、假正(并集)之和，之后在每个类上计算IoU再平均。<br>$$MIoU = \\frac{1}{k+1}\\sum_{i=0}^{k}\\frac{p_{ii}}{\\sum_{j=0}^{k}p_{ij}+\\sum_{j=0}^{k}(p_{ji}-p_{ii})}$$</li>\n<li>Frequency Weighted Intersection over Union(FWIoU, 频权交并比): 为MIoU的一种提升，根据每个类出现的频率为其设置权重。<br>$$FWIoU = \\frac{1}{\\sum_{i=0}^{k}\\sum_{j=0}^{k}p_{ij}}\\sum_{i=0}^{k}\\frac{p_{ii}}{\\sum_{j=0}^{k}p_{ij}+\\sum_{j=0}^{k}(p_{ji}-p_{ii})}$$</li>\n</ol>\n<h1 id=\"深度学习算法\"><a href=\"#深度学习算法\" class=\"headerlink\" title=\"深度学习算法\"></a>深度学习算法</h1><h2 id=\"Fully-Convolutional-Networks\"><a href=\"#Fully-Convolutional-Networks\" class=\"headerlink\" title=\"Fully Convolutional Networks\"></a>Fully Convolutional Networks</h2><p><a href=\"https://people.eecs.berkeley.edu/~jonlong/long_shelhamer_fcn.pdf\" target=\"_blank\" rel=\"noopener\">论文地址</a><br>传统神经网络做分类的步骤是，首先是一个图像进来之后经过多层卷积得到降维之后的特征图，这个特征图经过全连接层变成一个分类器，最后输出一个类别的向量，这就是分类的结果。</p>\n<p>而 FCN 是把所有的全连接层换成卷基层，原来只能输出一个类别分类的网络可以在特征图的每一个像素输出一个分类结果。这样就把分类的向量，变成了一个分类的特征图。</p>\n<center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Image-Segmentation-Learning/2.png\" alt=\"image\"></center>\n\n<p>上图中的猫, 输入AlexNet, 得到一个长为1000的输出向量, 表示输入图像属于每一类的概率, 其中在“tabby cat”这一类统计概率最高。而FCN对图像进行像素级的分类，从而解决了语义级别的图像分割（semantic segmentation）问题。FCN可以接受任意尺寸的输入图像，采用反卷积层对最后一个卷积层的feature map进行上采样, 使它恢复到输入图像相同的尺寸，从而可以对每个像素都产生了一个预测, 同时保留了原始输入图像中的空间信息, 最后在上采样的特征图上进行逐像素分类。最后逐个像素计算softmax分类的损失, 相当于每一个像素对应一个训练样本。</p>\n<p><strong>全连接-&gt;卷积层</strong>：</p>\n<p>一个 K=4096 的全连接层，输入数据体的尺寸是 7∗7∗512，这个全连接层可以被等效地看做一个 F=7,P=0,S=1,K=4096 的卷积层.</p>\n<p>假设一个卷积神经网络的输入是 224x224x3 的图像，一系列的卷积层和下采样层将图像数据变为尺寸为 7x7x512 的激活数据体。AlexNet使用了两个尺寸为4096的全连接层，最后一个有1000个神经元的全连接层用于计算分类评分。我们可以将这3个全连接层中的任意一个转化为卷积层：</p>\n<ul>\n<li>针对第一个连接区域是[7x7x512]的全连接层，令其滤波器尺寸为F=7，这样输出数据体就为[1x1x4096]了。</li>\n<li>针对第二个全连接层，令其滤波器尺寸为F=1，这样输出数据体为[1x1x4096]。</li>\n<li>对最后一个全连接层也做类似的，令其F=1，最终输出为[1x1x1000]</li>\n</ul>\n<p><strong>end to end, pixels to pixels network</strong><br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Image-Segmentation-Learning/3.png\" alt=\"image\"><br>经过多次卷积和pooling以后，得到的图像越来越小，分辨率越来越低。其中图像到$\\frac{H}{32} \\times \\frac{W}{32}$的时候图片是最小的一层时，所产生图叫做heatmap热图，热图就是最重要的高维特征图，得到高维特征的heatmap之后就是最重要的一步也是最后的一步对原图像进行upsampling，把图像进行放大、放大、放大，到原图像的大小。最后的输出是1000张heatmap经过upsampling变为原图大小的图片，为了对每个像素进行分类预测label成最后已经进行语义分割的图像，这里有一个小trick，就是最后通过逐个像素地求其在1000张图像该像素位置的最大数值描述（概率）作为该像素的分类。因此产生了一张已经分类好的图片，如上图右侧有狗狗和猫猫的图。</p>\n<p><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Image-Segmentation-Learning/4.png\" alt=\"image\"></p>\n<p>现在我们有1/32尺寸的heatMap，1/16尺寸的featureMap和1/8尺寸的featureMap，1/32尺寸的heatMap进行upsampling操作之后，因为这样的操作还原的图片仅仅是conv5中的卷积核中的特征，限于精度问题不能够很好地还原图像当中的特征，因此在这里向前迭代。把conv4中的卷积核对上一次upsampling之后的图进行反卷积补充细节（相当于一个插值过程），最后把conv3中的卷积核对刚才upsampling之后的图像进行再次反卷积补充细节，最后就完成了整个图像的还原。</p>\n<p><strong>缺点</strong>：</p>\n<ul>\n<li>是得到的结果还是不够精细。进行8倍上采样虽然比32倍的效果好了很多，但是上采样的结果还是比较模糊和平滑，对图像中的细节不敏感。</li>\n<li>是对各个像素进行分类，没有充分考虑像素与像素之间的关系。忽略了在通常的基于像素分类的分割方法中使用的空间规整（spatial regularization）步骤，缺乏空间一致性。</li>\n</ul>\n<p><strong>补充：插值法</strong><br>上采样upsampling的主要目的是放大图像，几乎都是采用内插值法，即在原有图像像素的基础上，在像素点值之间采用合适的<strong>插值算法</strong>插入新的元素。</p>\n<ul>\n<li><p>线性插值法：<br>使用连接两个已知量的直线来确定在这个两个已知量之间的一个未知量的值的方法。<br><center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Image-Segmentation-Learning/5.png\" alt=\"image\"></center><br>该直线方程可表示为：$\\frac{y-y_{0}}{y_{1}-y_{0}}=\\frac{x-x_{0}}{x_{1}-x_{0}}$ 假设方程两边的值是$\\alpha$，那么这个值就是插值系数，即$\\alpha =\\frac{y-y_{0}}{y_{1}-y_{0}}=\\frac{x-x_{0}}{x_{1}-x_{0}}$. 所以y可以表示为: $y=(1-\\alpha)y_{0}+\\alpha y_{1} = (1-\\frac{x-x_{0}}{x_{1}-x_{0}})y_{0}+\\frac{x-x_{0}}{x_{1}-x_{0}}y_{1}=\\frac{x_{1}-x}{x_{1}-x_{0}}y_{0}+\\frac{x-x_{0}}{x_{1}-x_{0}}y_{1}=\\frac{x_{1}-x}{x_{1}-x_{0}}f(x_{1})+\\frac{x-x_{0}}{x_{1}-x_{0}}f(x_{0})$</p>\n</li>\n<li><p>双线性插值<br>双线性插值是插值算法中的一种，是线性插值的扩展。利用原图像中目标点四周的四个真实存在的像素值来共同决定目标图中的一个像素值，其核心思想是在两个方向分别进行一次线性插值。</p>\n<center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Image-Segmentation-Learning/6.png\" alt=\"image\"></center>\n\n</li>\n</ul>\n<p>X方向的线性插值：在$Q_{12}$,$Q_{22}$中插入蓝色点$R_{2}$，$Q_{11}$，$Q_{21}$中插入蓝色点$R_{1}$</p>\n<p>$f(R_{1}=\\frac{x_{2}-x}{x_{2}-x_{1}}f(Q_{11})+\\frac{x-x_{1}}{x_{2}-x_{1}}f(Q_{21})$; $f(R_{2}=\\frac{x_{2}-x}{x_{2}-x_{1}}f(Q_{12})+\\frac{x-x_{1}}{x_{2}-x_{1}}f(Q_{22})$</p>\n<p>Y方向的线性插值：通过第一步计算出的$R_{1$}与$R_{2}$在y方向上插值计算出P点</p>\n<p>$f(P)=\\frac{y_{2}-y}{y_{2}-y_{1}}f(R_{1})+\\frac{y-y_{1}}{y_{2}-y_{1}}f(R_{2})$</p>\n<hr>\n<h2 id=\"U-Net-Convolutional-Networks-for-Biomedical-Image-Segmentation\"><a href=\"#U-Net-Convolutional-Networks-for-Biomedical-Image-Segmentation\" class=\"headerlink\" title=\"U-Net: Convolutional Networks for Biomedical Image Segmentation\"></a>U-Net: Convolutional Networks for Biomedical Image Segmentation</h2><p><a href=\"https://arxiv.org/pdf/1505.04597.pdf\" target=\"_blank\" rel=\"noopener\">论文地址</a><br>卷积网络被大规模应用在分类任务中，输出的结果是整个图像的类标签。然而，在许多视觉任务，尤其是生物医学图像处理领域，目标输出应该包括目标类别的位置，并且每个像素都应该有类标签。另外，在生物医学图像往往缺少训练图片。所以，Ciresan等人训练了一个卷积神经网络，用滑动窗口提供像素的周围区域（patch）作为输入来预测每个像素的类标签。这个网络有两个优点： 第一，输出结果可以定位出目标类别的位置； 第二，由于输入的训练数据是patches，这样就相当于进行了数据增广，解决了生物医学图像数量少的问题。</p>\n<p>但是，这个方法也有两个很明显缺点。</p>\n<p>第一，它很慢，因为这个网络必须训练每个patch，并且因为patch间的重叠有很多的冗余,造成资源的浪费，减慢训练时间和效率; 第二，定位准确性和获取上下文信息不可兼得。大的patches需要更多的max-pooling层这样减小了定位准确性,小的patches只能看到很小的局部信息，包含的背景信息不够。</p>\n<p><strong>U-Net Architecture</strong></p>\n<center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Image-Segmentation-Learning/7.jpg\" alt=\"image\"></center>\n\n<ol>\n<li>使用全卷积神经网络。(全卷积神经网络就是卷积取代了全连接层，全连接层必须固定图像大小而卷积不用，所以这个策略使得，你可以输入任意尺寸的图片，而且输出也是图片，所以这是一个端到端的网络。)</li>\n<li>左边的网络是收缩路径：使用卷积和maxpooling。</li>\n<li>右边的网络是扩张路径:使用上采样产生的特征图与左侧收缩路径对应层产生的特征图进行concatenate操作。（pooling层会丢失图像信息和降低图像分辨率且是不可逆的操作，对图像分割任务有一些影响，对图像分类任务的影响不大，为什么要做上采样？因为上采样可以补足一些图片的信息，但是信息补充的肯定不完全，所以还需要与左边的分辨率比较高的图片相连接起来（直接复制过来再裁剪到与上采样图片一样大小），这就相当于在高分辨率和更抽象特征当中做一个折衷，因为随着卷积次数增多，提取的特征也更加有效，更加抽象，上采样的图片是经历多次卷积后的图片，肯定是比较高效和抽象的图片，然后把它与左边不怎么抽象但更高分辨率的特征图片进行连接）。</li>\n<li>最后再经过两次反卷积操作，生成特征图，再用两个1X1的卷积做分类得到最后的两张heatmap,例如第一张表示的是第一类的得分，第二张表示第二类的得分heatmap,然后作为softmax函数的输入，算出概率比较大的softmax类，选择它作为输入给交叉熵进行反向传播训练。</li>\n</ol>\n<p><strong>Overlap-tile strategy</strong></p>\n<center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Image-Segmentation-Learning/8.png\" alt=\"image\"></center>\n\n<p>医学图像是一般相当大，但是分割时候不可能将原图太小输入网络，所以必须切成一张一张的小patch，在切成小patch的时候，Unet由于网络结构原因适合有overlap的切图，可以看图，红框是要分割区域，但是在切图时要包含周围区域，overlap另一个重要原因是周围overlap部分可以为分割区域边缘部分提供文理等信息。可以看黄框的边缘，分割结果并没有受到切成小patch而造成分割情况不好。 </p>\n<p><strong>训练</strong></p>\n<p>最后一层使用了交叉熵函数与softmax：<br>$$E=\\sum_{x\\in \\Omega}w(x)log(p_{\\ell(x)}(x))$$</p>\n<p>并且为了补偿训练每个类像素的不同频率使得网络更注重学习相互接触的细胞之间的小的分割边界，引入了权重图计算$w(x)$:<br>$$w(x)=w_{c}(x)+w_{0} \\times exp(-\\frac{(d_{1}(x)+d_{2}(x))}{2\\sigma^{2}})$$</p>\n<p><strong>Data Augmentation</strong><br>在只有少量样本的情况况下，要想尽可能的让网络获得不变性和鲁棒性，数据增加是必不可少的。因为本论文需要处理显微镜图片，我们需要平移与旋转不变性，并且对形变和灰度变化鲁棒。将训练样本进行随机弹性形变是训练分割网络的关键。使用随机位移矢量在粗糙的3*3网格上产生平滑形变(smooth deformations)。 位移是从10像素标准偏差的高斯分布中采样的。然后使用双三次插值(Bicubic interpolation)计算每个像素的位移。在contracting path的末尾采用drop-out 层更进一步增加数据。</p>\n<p><strong>双三次插值</strong><br>在这种方法中，函数f在点(x,y)的值可以通过矩形网络中最近的16个采样点加权平均得到。</p>\n<hr>\n<h2 id=\"DeepLab-V1\"><a href=\"#DeepLab-V1\" class=\"headerlink\" title=\"DeepLab V1\"></a>DeepLab V1</h2><p><a href=\"https://arxiv.org/pdf/1412.7062v3.pdf\" target=\"_blank\" rel=\"noopener\">论文地址</a></p>\n<p>DeepLab是结合了深度卷积神经网络(<a href=\"https://www.cnblogs.com/wangxiaocvpr/p/8763510.html\" target=\"_blank\" rel=\"noopener\">DCNNs</a>)和概率图模型(<a href=\"https://zhuanlan.zhihu.com/p/33397147\" target=\"_blank\" rel=\"noopener\">DenseCRFs</a>)的方法.</p>\n<p>DCNN在图像标记任务中存在两个技术障碍：</p>\n<ul>\n<li>信号下采样</li>\n<li>空间不敏感(invariance)</li>\n</ul>\n<p>第一个问题涉及到：在DCNN中重复最大池化和下采样带来的分辨率下降问题，分辨率的下降会丢失细节。DeepLab是采用的<code>atrous</code>(带孔)算法扩展感受野，获取更多的上下文信息。</p>\n<p>第二个问题涉及到：分类器获取以对象中心的决策是需要空间变换的不变性，这天然的限制了DCNN的定位精度，DeepLab采用完全连接的条件随机场(DenseCRF)提高模型捕获细节的能力。</p>\n<p>除空洞卷积和 CRFs 之外，论文使用的 tricks 还有 Multi-Scale features。其实就是 U-Net 和 FPN 的思想，在输入图像和前四个最大池化层的输出上附加了两层的 MLP，第一层是 128 个 3×3 卷积，第二层是 128 个 1×1 卷积。最终输出的特征与主干网的最后一层特征图融合，特征图增加 5×128=640 个通道。实验表示多尺度有助于提升预测结果，但是效果不如 CRF 明显。</p>\n<p><strong>CRF-&gt;语义分割</strong></p>\n<p>对于每个像素位置$i$具有隐变量$x_{i}$(这里隐变量就是像素的真实类别标签，如果预测结果有21类，则$(i \\in 1,2,..,21)$ 还有对应的观测值 $y_{i}$(即像素点对应的颜色值)。以像素为节点，像素与像素间的关系作为边，构成了一个条件随机场(CRF)。通过观测变量$y_{i}$来推测像素位置$i$应的类别标签$x_{i}$.条件随机场示意图如下:</p>\n<center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Image-Segmentation-Learning/9.png\" alt=\"image\"></center>\n\n<p>条件随机场符合吉布斯分布(x是上面的观测值，下面省略全局观测I):<br>$$p(x|I)=\\frac{1}{Z}exp(-E(x|I))$$</p>\n<p>全连接的CRF模型使用的能量函数$E(x)$为:<br>$$E(x)=\\sum_{i} \\theta_{i}(x_{i})+\\sum_{ij}\\theta_{ij}(x_{i},x_{j})$$</p>\n<p>这分为一元势函数$\\theta_{i}(x_{i})$和二元势函数$\\theta_{ij}(x_{i},x_{j})$两部分</p>\n<ul>\n<li>一元势函数是定义在观测序列位置i的状态特征函数，用于刻画观测序列对标记变量的影响（例如在城市道路任务中，观测到像素点为黑色，对应车子的可能比天空可能要大）。这里$P(x_{i})$是取DCNN计算关于像素i的输出的标签分配概率.</li>\n</ul>\n<p>$$\\theta_{i}(x_{i})=-logP(x_{i})$$</p>\n<ul>\n<li>二元势函数是定义在不同观测位置上的转移特征函数，用于刻画变量之间的相关关系以及观测序列对其影响。如果比较相似，那可能是一类，否则就裂开，这可以细化边缘。一般的二元势函数只取像素点与周围像素之间的边，这里使用的是全连接，即像素点与其他所有像素之间的关系。</li>\n</ul>\n<p>$$\\theta_{ij}(x_{i},x_{j})=\\mu(x_{i},x_{j})\\sum_{m=1}^{K}w_{m}k^{m}(f_{i},f_{j})$$</p>\n<p>DeepLab中高斯核采用双边位置和颜色的组合（第一核取决于像素位置(p)和像素颜色强度(I),第二核取决于像素位置(p)）:<br>$$w_{1}exp(-\\frac{\\lVert p_{i}-p_{j} \\rVert^{2}}{2\\sigma_{\\alpha}^{2}}-\\frac{\\lVert I_{i}-I_{j} \\rVert^{2}}{2\\sigma_{\\beta}^{2}})+w_{2}exp(-\\frac{\\lVert p_{i}-p_{j} \\rVert^{2}}{2\\sigma_{\\gamma}^{2}})$$</p>\n<p><strong>实验</strong></p>\n<table>\n<thead>\n<tr>\n<th>项目</th>\n<th>设置</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>数据集</td>\n<td>PASCAL VOC 2012 segmentation benchmark</td>\n</tr>\n<tr>\n<td>DCNN模型</td>\n<td>权重采用预训练的VGG16</td>\n</tr>\n<tr>\n<td>DCNN损失函数</td>\n<td>交叉熵</td>\n</tr>\n<tr>\n<td>训练器</td>\n<td>SGD，batch=20</td>\n</tr>\n<tr>\n<td>学习率</td>\n<td>初始为0.001，最后的分类层是0.01。每2000次迭代乘0.1</td>\n</tr>\n<tr>\n<td>权重</td>\n<td>0.9的动量， 0.0005的衰减</td>\n</tr>\n</tbody>\n</table>\n<hr>\n<h2 id=\"DepLab-V2\"><a href=\"#DepLab-V2\" class=\"headerlink\" title=\"DepLab V2\"></a>DepLab V2</h2><p><a href=\"https://arxiv.org/pdf/1606.00915.pdf\" target=\"_blank\" rel=\"noopener\">论文地址</a></p>\n<p>DeepLabv2 是相对于 DeepLabv1 基础上的优化。DeepLabv1 在三个方向努力解决，但是问题依然存在：</p>\n<ol>\n<li>特征分辨率的降低</li>\n<li>物体存在多尺度</li>\n<li>DCNN 的平移不变性</li>\n</ol>\n<p>针对这三个问题, DeepLabv2做出了3个主要贡献:</p>\n<ol>\n<li>首先，强调使用空洞卷积，作为密集预测任务的强大工具。空洞卷积能够明确地控制DCNN内计算特征响应的分辨率，即可以有效的扩大感受野，在不增加参数量和计算量的同时获取更多的上下文。</li>\n<li>其次，提出了空洞空间卷积池化金字塔(atrous spatial pyramid pooling (ASPP))，以多尺度的信息得到更强健的分割结果。ASPP并行的采用多个采样率的空洞卷积层来探测，以多个比例捕捉对象以及图像上下文。</li>\n<li>最后，通过组合DCNN和概率图模型，改进分割边界结果。在DCNN中最大池化和下采样组合实现可平移不变性，但这对精度是有影响的。通过将最终的DCNN层响应与全连接的CRF结合来克服这个问题。</li>\n</ol>\n<p><strong>步骤</strong></p>\n<center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Image-Segmentation-Learning/10.png\" alt=\"image\"></center>\n\n<ul>\n<li>输入经过改进的DCNN(带空洞卷积和ASPP模块)得到粗略预测结果，即<code>Aeroplane Coarse Score map</code></li>\n<li>通过双线性插值扩大到原本大小，即<code>Bi-linear Interpolation</code></li>\n<li>再通过全连接的CRF细化预测结果，得到最终输出<code>Final Output</code></li>\n</ul>\n<p><strong>方法</strong></p>\n<ol>\n<li><p><strong>空洞卷积用于密集特征提取和扩大感受野</strong></p>\n<p>首先考虑一维信号，空洞卷积输出为$y[i]$, 输入为$x[i]$, 长度K的滤波器为$w[k]$, 则定义为：<br>$$y[k]=\\sum_{k=1}^{K}x[i+r\\cdot k]w[k]$$<br>输入采样的步幅为参数r, 标准采样率是$r=1$如图(a); 图(b)是采样率$r=2$的时候：</p>\n<center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Image-Segmentation-Learning/11.png\" alt=\"image\"></center>\n\n<p>二维信号(图片)上使用空洞卷积的表现,给定一个图像：</p>\n<ul>\n<li>上分支：首先下采样将分辨率降低2倍，做卷积。再上采样得到结果。本质上这只是在原图片的1/4内容上做卷积响应。</li>\n<li>下分支：如果将全分辨率图像做空洞卷积(采样率为2，核大小与上面卷积核相同)，直接得到结果。这样可以计算出整张图像的响应，如下图所示，这样做效果更佳。<center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Image-Segmentation-Learning/12.png\" alt=\"image\"></center>\n\n</li>\n</ul>\n<p>空洞卷积能够放大滤波器的感受野，速率r引入$r-1$个零，有效将感受野从$k\\times k$扩展到$k_{e}=k+(k-1)(r-1)$而不增加参数和计算量。在DCNN中，常见的做法是混合使用空洞卷积以高的分辨率(理解为采样密度)计算最终的DCNN网络响应。</p>\n</li>\n<li><p><strong>使用ASPP模块表示多尺度图像</strong></p>\n<p>DeepLabv2的做法与SPPNet类似，并行的采用多个采样率的空洞卷积提取特征，再将特征融合，类似于空间金字塔结构，形象的称为Atrous Spatial Pyramid Pooling (ASPP)。示意图如下：</p>\n<center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Image-Segmentation-Learning/13.png\" alt=\"image\"></center>\n\n<p>在同一Input Feature Map的基础上，并行的使用4个空洞卷积，空洞卷积配置为$r=6,12,18,24$, 核大小为$3 \\times 3$. 最终将不同卷积层得到的结果做像素加融合到一起.</p>\n</li>\n<li><p><strong>使用全连接CRF做结构预测用于恢复边界精度</strong></p>\n<center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Image-Segmentation-Learning/14.png\" alt=\"image\"></center>\n\n</li>\n</ol>\n<p><strong>训练</strong></p>\n<table>\n<thead>\n<tr>\n<th>项目</th>\n<th>设置</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>DCNN模型</td>\n<td>权重采用预训练的VGG16，<strong>ResNet101</strong></td>\n</tr>\n<tr>\n<td>DCNN损失函数</td>\n<td>输出的结果与ground truth下采样8倍做像素交叉熵</td>\n</tr>\n<tr>\n<td>训练器</td>\n<td>SGD，batch=20</td>\n</tr>\n<tr>\n<td>学习率</td>\n<td>初始为0.001，最后的分类层是0.01。每2000次迭代乘0.1</td>\n</tr>\n<tr>\n<td>权重</td>\n<td>0.9的动量， 0.0005的衰减</td>\n</tr>\n</tbody>\n</table>\n<hr>\n<h2 id=\"DepLab-V3\"><a href=\"#DepLab-V3\" class=\"headerlink\" title=\"DepLab V3\"></a>DepLab V3</h2><p><a href=\"https://arxiv.org/pdf/1706.05587.pdf\" target=\"_blank\" rel=\"noopener\">论文地址</a></p>\n<p>语义分割任务，在应用深度卷积神经网络中的有两个挑战：</p>\n<ul>\n<li>第一个挑战：连续池化和下采样，让高层特征具有局部图像变换的内在不变性，这允许DCNN学习越来越抽象的特征表示。但同时引起的特征分辨率下降，会妨碍密集的定位预测任务，因为这需要详细的空间信息。</li>\n<li>第二个挑战：多尺度目标的存在</li>\n</ul>\n<p>DeepLabv3的主要贡献在于：</p>\n<ul>\n<li>重新讨论了空洞卷积的使用，在级联模块和空间金字塔池化的框架下，能够获取更大的感受野从而获取多尺度信息。</li>\n<li>改进了ASPP模块：由不同采样率的空洞卷积和BN层组成，尝试以级联或并行的方式布局模块。</li>\n<li>讨论了一个重要问题：使用大采样率的空洞卷积，因为图像边界响应无法捕捉远距离信息，会退化为1×1的卷积, 因此建议将图像级特征融合到ASPP模块中。</li>\n<li>阐述了训练细节并分享了训练经验，论文提出的”DeepLabv3”改进了以前的工作，获得了很好的结果</li>\n</ul>\n<p><strong>方法</strong></p>\n<ol>\n<li><strong>空洞卷积应用于密集的特征提取</strong></li>\n<li><p><strong>深层次的空洞卷积</strong></p>\n<center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Image-Segmentation-Learning/15.png\" alt=\"image\"></center>\n\n<center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Image-Segmentation-Learning/18.png\" alt=\"image\"></center>\n\n<p>将空洞卷积应用在级联模块, 取ResNet中最后一个block，在上图中为block4，并在其后面增加级联模块。图(a)所示，整体图片的信息总结到后面非常小的特征映射上，使用步幅越长的特征映射，得到的结果反倒会差，结果最好的out_stride = 8 需要占用较多的存储空间。因为连续的下采样会降低特征映射的分辨率，细节信息被抽取，这对语义分割是有害的。上图(b)所示，可使用不同采样率的空洞卷积保持输出步幅的为out_stride = 16.这样不增加参数量和计算量同时有效的缩小了步幅。</p>\n</li>\n<li><p><strong>Atrous Spatial Pyramid Pooling</strong></p>\n<p>对于在DeepLabv2中提出的ASPP模块，其在特征顶部映射图并行使用了四种不同采样率的空洞卷积。这表明以不同尺度采样是有效的，在DeepLabv3中向ASPP中添加了BN层。不同采样率的空洞卷积可以有效的捕获多尺度信息，但是，随着采样率的增加，滤波器的有效权重(权重有效的应用在特征区域，而不是填充0)逐渐变小。如下图所示：</p>\n<center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Image-Segmentation-Learning/16.png\" alt=\"image\"></center>\n\n<p>当不同采样率的$3 \\times 3$卷积核应用在$65 \\times 65$的特征映射上，采样率接近特征映射大小时，$3 \\times 3$的滤波器不是捕捉全图像的上下文，而是退化为简单的$1 \\times 1$滤波器，只有滤波器中心点的权重起了作用。</p>\n<p><center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Image-Segmentation-Learning/17.png\" alt=\"image\"></center><br>为了克服这个问题，改进了ASPP结构如上图：</p>\n<ol>\n<li>一个$1 \\times 1$卷积和三个$3 \\times 3$卷积的采样率为$rate = (6,12,18)$的空洞卷积，滤波器数量为256，包含BN层。针对output_stride=16的情况。（当output_stride=8的时候，采样率会加倍，所有的特征会通过$1 \\times 1$卷积级联到一起）</li>\n<li>使用了图片级特征。具体来说，在模型最后的特征映射上应用全局平均，将结果经过$1 \\times 1$的卷积，再双线性上采样得到所需的空间维度。</li>\n</ol>\n</li>\n</ol>\n<p><strong>训练</strong></p>\n<table>\n<thead>\n<tr>\n<th>部分</th>\n<th>设置</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>数据集</td>\n<td>PASCAL VOC 2012</td>\n</tr>\n<tr>\n<td>工具</td>\n<td>TensorFlow</td>\n</tr>\n<tr>\n<td>裁剪尺寸</td>\n<td>采样513大小的裁剪尺寸</td>\n</tr>\n<tr>\n<td>学习率策略</td>\n<td>采用poly策略， $learning rate = base_lr(1-\\frac{iter}{max_iters})^{power}$</td>\n</tr>\n<tr>\n<td><strong>BN层策略</strong></td>\n<td>当output_stride=16时，我们采用batchsize=16，同时BN层的参数做参数衰减0.9997。在增强的数据集上，以初始学习率0.007训练30K后，冻结BN层参数。采用output_stride=8时，再使用初始学习率0.001训练30K。训练output_stride=16比output_stride=8要快很多，因为中间的特征映射在空间上小四倍。但因为output_stride=16在特征映射上粗糙的是牺牲了精度。</td>\n</tr>\n<tr>\n<td><strong>上采样策略</strong></td>\n<td>在先前的工作上,将最终的输出与GroundTruth下采样8倍做比较之后发现保持GroundTruth更重要，故将最终的输出上采样8倍与完整的GroundTruth比较。</td>\n</tr>\n</tbody>\n</table>\n<hr>\n<h2 id=\"DepLab-V-3\"><a href=\"#DepLab-V-3\" class=\"headerlink\" title=\"DepLab-V$3^{+}$\"></a>DepLab-V$3^{+}$</h2><p><a href=\"https://arxiv.org/pdf/1802.02611.pdf\" target=\"_blank\" rel=\"noopener\">论文地址</a></p>\n<p>因为深度网络存在pooling or convolutions with stride的层，会导致feature分辨率下降，从而导致预测精度降低，而造成的边界信息丢失问题. 这个问题可以通过使用空洞卷积替代更多的pooling层来获取分辨率更高的feature。但是feature分辨率更高会极大增加运算量。</p>\n<p><strong>方法</strong></p>\n<center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Image-Segmentation-Learning/19.png\" alt=\"image\"></center>\n\n<p>所以DeepLabV$3^{+}$中通过采用了encoder-decoder结构，在DeepLab V3中加入了一个简单有效的decoder模块来改善物体边缘的分割结果(图c)：先上采样4倍，在与encoder中的特征图concatenate，最后在上采样4倍恢复到原始图像大小。除此之外还尝试使用Xception作为encoder，在Atrous Spatial Pyramid Pooling和decoder中应用depth-wise separable convolution得到了更快精度更高的网络。</p>\n<ol>\n<li><p><strong>Encoder</strong></p>\n<ul>\n<li>ResNet: encoder就是DeepLab V3，通过修改ResNet101最后两(一)个block的stride，使得output stride为8(16)。之后在block4后应用改进后的Atrous Spatial Pyramid Pooling，将所得的特征图concatenate用1×1的卷积得到256个通道的特征图。</li>\n<li><p><strong>Xecption</strong>: 采用的Xception模型为MSRA team提出的改进的Xception，叫做Aligned Xception，并做了几点修改：</p>\n<ul>\n<li>网络深度与Aligned Xception相同，不同的地方在于为了快速计算和有效的使用内存而不修改entry flow network的结构。</li>\n<li>所有的max pooling操作替换成带stride的separable convolution，这能使得对任意分辨率的图像应用atrous separable convolution提取特征。</li>\n<li>在每个3×3的depath-wise convolution后增加BN层和ReLU。<center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Image-Segmentation-Learning/21.png\" alt=\"image\"></center>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n<li><p><strong>Decoder</strong></p>\n<ul>\n<li>在decoder中，特征图首先上采样4倍，然后与encoder中对应分辨率低级特征concatenate。在concatenate之前，由于低级特征图的通道数通常太多(256或512)，而从encoder中得到的富含语义信息的特征图通道数只有256，这样会淡化语义信息，因此在concatenate之前，需要将低级特征图通过1×1的卷积减少通道数。在concatenate之后用3×3的卷积改善特征，最后上采样4倍恢复到原始图像大小。</li>\n<li>设计： <ul>\n<li>$1 \\times 1$卷积的通道数采用48</li>\n<li>用来获得更锋利的边界的3×3的卷积。最后采用了2个3×3的卷积</li>\n<li>使用的encoder的低级特征（Conv2）</li>\n</ul>\n</li>\n</ul>\n</li>\n</ol>\n<center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Image-Segmentation-Learning/20.png\" alt=\"image\"></center>\n\n<p><strong>结果</strong>：</p>\n<ul>\n<li><p><strong>Aligned Xception改</strong></p>\n<center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Image-Segmentation-Learning/22.png\" alt=\"image\"></center>\n\n<p>当train_stride=16和eval_stride=8的时候mIOU最好达到了84.56% 然而计算量比较高。使用train_stride和eval_stride都为16的时候，结果下降了1.53%但是计算量下降了60倍。</p>\n</li>\n</ul>"},{"title":"MalongTech 学习内容:目标检测","date":"2018-11-12T03:00:00.000Z","_content":"\n# Object detection\n\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/14.png)\n\n| one-stage系  | two-stage系|\n| ---------- | -----------|\n| YOLO V1,V2,V3   | FPN   |\n| SSD   | RFCN   |\n| RetinalNet | LIghthead |\n<!-- more -->\n## One Stage\n\n### YOLO V1\n** You only look once unified real-time object detection**\n\n[PAPER ADDRESS](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Redmon_You_Only_Look_CVPR_2016_paper.pdf)\n\n作者在YOLO算法中把物体检测（object detection）问题处理成回归问题，用一个卷积神经网络结构就可以从输入图像直接预测bounding box和类别概率。\n\n**优点**: \n1. YOLO的速度非常快。在Titan X GPU上的速度是45 fps, 加速版155 fps。\n2. YOLO是基于图像的全局信息进行预测的。这一点和基于sliding window以及region proposal等检测算法不一样。与Fast R-CNN相比，YOLO在误检测（将背景检测为物体）方面的错误率能降低一半多。\n3. 泛化能力强。\n\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/1.png)\n\n**缺点**:\n1. accuracy 还落后于同期 state-of-the-art 目标检测方法。\n2. 难于检测小目标。\n3. 定位不够精准。\n4. 虽然降低了背景检测为物体的概率但同事导致了召回率较低。\n\n**流程**\n1. 调整图像大小至$448\\times448$.\n2. 运行卷积网络同时预测多目标的边界框和所属类的概率\n3. NMX(非极大值抑制)\n\n**Unified Detection**\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/2.png)\n1. 将图片分为$S\\times S$格子。\n2. 对每个格子都预测B个边界框并且每个边界框包含5个预测值：x,t,w,h以及confidence。x,y就是bounding box的中心坐标，与grid cell对齐（即相对于当前grid cell的偏移值），使得范围变成0到1；w和h进行归一化（分别除以图像的w和h，这样最后的w和h就在0到1范围。\n3. 每个格子都预测C个假定类别的概率。\n4. 在Pascal VOC中， S=7,B=2,C=20. 所以有 $S\\times S\\times (B \\times 5 + C)$ 即 $7\\times 7\\times 30$ 维张量。\n\nConfidence计算：$Pr(Object) * IOU_{pred}^{turth} $\n\n每个bounding box都对应一个confidence score，如果grid cell里面没有object，confidence就是0，如果有，则confidence score等于预测的box和ground truth的IOU值，见上面公式。并且如果一个object的ground truth的中心点坐标在一个grid cell中，那么这个grid cell就是包含这个object，也就是说这个object的预测就由该grid cell负责。 \n每个grid cell都预测C个类别概率，表示一个grid cell在包含object的条件下属于某个类别的概率：$Pr(Class_{i}|Object)$\n\n每个bounding box的confidence和每个类别的score相乘，得到每个bounding box属于哪一类的confidence score。\n\n即得到每个bounding box属于哪一类的confidence score。也就是说最后会得到20\\*(7\\*7\\*2)的score矩阵，括号里面是bounding box的数量，20代表类别。接下来的操作都是20个类别轮流进行：在某个类别中（即矩阵的某一行），将得分少于阈值（0.2）的设置为0，然后再按得分从高到低排序。最后再用NMS算法去掉重复率较大的bounding box（NMS:针对某一类别，选择得分最大的bounding box，然后计算它和其它bounding box的IOU值，如果IOU大于0.5，说明重复率较大，该得分设为0，如果不大于0.5，则不改；这样一轮后，再选择剩下的score里面最大的那个bounding box，然后计算该bounding box和其它bounding box的IOU，重复以上过程直到最后）。最后每个bounding box的20个score取最大的score，如果这个score大于0，那么这个bounding box就是这个socre对应的类别（矩阵的行），如果小于0，说明这个bounding box里面没有物体，跳过即可。\n\n\n**网络设计**\n灵感来源于GoogLeNet,如下图：\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/3.png)\n训练过程中：\n1. 作者先在ImageNet数据集上预训练网络，而且网络只采用图中的前面20个卷积层，输入是224\\*224大小的图像。然后在检测的时候再加上随机初始化的4个卷积层和2个全连接层，同时输入改为更高分辨率的448\\*448。\n2. Relu层改为leaky Relu，即当x<0时，激活值是0.1\\*x，而不是传统的0。\n3. 作者采用sum-squared error的方式把localization error（bounding box的坐标误差）和classificaton error整合在一起。但是如果二者的权值一致，容易导致模型不稳定，训练发散。因为很多grid cell是不包含物体的，这样的话很多grid cell的confidence score为0。所以采用设置不同权重方式来解决，一方面提高localization error的权重，另一方面降低没有object的box的confidence loss权值，loss权重分别是5和0.5。而对于包含object的box的confidence loss权值还是原来的1。\n4. 用宽和高的开根号代替原来的宽和高，这样做主要是因为相同的宽和高误差对于小的目标精度影响比大的目标要大.\n5. Loss Function如下：![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/4.png)\n\n训练的时候：输入N个图像，每个图像包含M个objec，每个object包含4个坐标（x，y，w，h）和1个label。然后通过网络得到7\\*7\\*30大小的三维矩阵。每个1\\*30的向量前5个元素表示第一个bounding box的4个坐标和1个confidence，第6到10元素表示第二个bounding box的4个坐标和1个confidence。最后20个表示这个grid cell所属类别。注意这30个都是预测的结果。然后就可以计算损失函数的第一、二 、五行。至于第二三行，confidence可以根据ground truth和预测的bounding box计算出的IOU和是否有object的0,1值相乘得到。真实的confidence是0或1值，即有object则为1，没有object则为0。 这样就能计算出loss function的值了。\n\n测试的时候：输入一张图像，跑到网络的末端得到7\\*7\\*30的三维矩阵，这里虽然没有计算IOU，但是由训练好的权重已经直接计算出了bounding box的confidence。然后再跟预测的类别概率相乘就得到每个bounding box属于哪一类的概率。\n\n**YOLO效果**\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/5.png)\n由于yolo更少的识别背景为物体对比Faster RCNN,因此结合YOLO作为背景检测器与Faster RCNN可以带来更大的提升，不过速度方面就没有优势了。\n\n### YOLO V2 and YOLO 9000\n[论文地址](https://arxiv.org/pdf/1612.08242.pdf)\n\n与分类和标记等其他任务的数据集相比，目前目标检测数据集是有限的。最常见的检测数据集包含成千上万到数十万张具有成百上千个标签的图像。分类数据集有数以百万计的图像，数十或数十万个类别。为了扩大当前检测系统的范围。我们的方法使用目标分类的分层视图，允许我们将不同的数据集组合在一起。此外联合训练算法，使我们能够在检测和分类数据上训练目标检测器。我们的方法利用标记的检测图像来学习精确定位物体，同时使用分类图像来增加词表和鲁棒性。\n\n**对比YOLO V1的改进**\n1. YOLO有许多缺点。YOLO与Fast R-CNN相比的误差分析表明，YOLO造成了大量的定位误差。此外，与基于区域提出的方法相比，YOLO召回率相对较低。因此，我们主要侧重于提高召回率和改进定位，同时保持分类准确性。\n2. 在YOLOv2中，一个更精确的检测器被设计出来，它仍然很快。但是不是通过扩大网络，而是简化网络，然后让其更容易学习。结合了以往的一些新方法，以提高YOLO的性能：\n    * Batch Normalization: map提升2%\n    * High Resolution Classifier: 先在ImageNet上以448×448的分辨率对分类网络进行10个迭代周期的微调。这给了网络时间来调整其滤波器以便更好地处理更高分辨率的输入。map提升4%,\n    * Convolutional With Anchor Boxes: 从YOLO中移除全连接层，并使用锚盒来预测边界框。首先，我们消除了一个池化层，使网络卷积层输出具有更高的分辨率。我们还缩小了网络，操作416×416的输入图像而不是448×448。我们这样做是因为我们要在我们的特征映射中有奇数个位置，所以只有一个中心单元。目标，特别是大目标，往往占据图像的中心，所以在中心有一个单独的位置来预测这些目标，而不是四个都在附近的位置是很好的。YOLO的卷积层将图像下采样32倍，所以通过使用416的输入图像，我们得到了13×13的输出特征映射。map有所下降但是召回率达到了88%.\n    * Dimension Clusters: Anchors 尺寸的选择用k-means聚类来挑选合适的锚盒尺寸。如果我们使用具有欧几里得距离的标准k-means，那么较大的边界框比较小的边界框产生更多的误差。然而，我们真正想要的是导致好的IOU分数的先验，这是独立于边界框大小的。因此，对于我们的距离度量，我们使用：$d(box,centroid)=1-IOU(box,centroid)$. 在voc和coco的测试中，更薄更高的边界框会带来更好的结果在k=5的时候，如图所示：![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/6.png)\n    * Direct location prediction: 传统的RPN中，特别是在早期的迭代过程中。大部分的不稳定来自预测边界框的(x,y)位置，他的位置修正方法是不受限制的，所以任何锚盒都可以在图像任一点结束，而不管在哪个位置预测该边界框。随机初始化模型需要很长时间才能稳定以预测合理的偏移量。所以这一步就优化为直接预测相对于网格单元位置的位置坐标。逻辑激活备用来限制网络的预测落在这个范围内。Sigmoid使输出在0~1之间这样映射到原图中时候不会位于其他的网格（在中心目标处）。![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/7.png)网络预测输出特征映射中每个单元的5个边界框。网络预测每个边界框的5个坐标，$t_{x}$,$t_{y}$,$t_{w}$,$t_{h}$,$t_{o}$,如果单元从图像的左上角偏移了$(c_{x},c_{y})$,并且边界框先验的宽度和高度为$p_{w}$,$p_{h}$. 预测就可以对应如图公式计算。$Pr(object)\\times IOU(b,object)=\\sigma(t_{o}) $ 该方法结合维度聚类，map 提升了5%对比与其他的锚盒方法。\n    * Fine-Grained Features(细粒度特征): 对于小目标物体，更细的力度特种可以带来更好的效果，因此直通层通过将相邻特征堆叠到不同的通道而不是空间位置来连接较高分辨率特征和较低分辨率特征，类似于ResNet中的恒等映射。这将26×26×512特征映射变成13×13×2048特征映射，其可以与原始特征连接。我们的检测器运行在这个扩展的特征映射的顶部，以便它可以访问细粒度的特征。这会使性能提高1%。\n    * Multi-Scale Training:  由于模型只使用卷积层和池化层，因此它可以实时调整大小。所以每隔10个批次会随机选择一个新的图像尺寸大小（330到608，从32的倍数中选择因为模型缩减了32倍），强迫网络学习在不同维度上预测，并且小尺度的网络运行更快。![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/8.png)\n\n3. Darknet-19， 它有19个卷积层和5个最大池化层并且只需要55.8亿次运算处理图像获得了比复杂运算VGG和前一代YOLO更高的top-5精度在ImageNet上，结构如下：\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/9.png)\n4. 分类训练：使用Darknet神经网络结构，使用随机梯度下降，初始学习率为0.1，学习率多项式衰减系数为4，权重衰减为0.0005，动量为0.9，在标准ImageNet 1000类分类数据集上训练网络160个迭代周期。在训练过程中，标准的数据增强技巧，包括随机裁剪，旋转，色调，饱和度和曝光偏移被使用来防止over-fitting. 在在对224×224的图像进行初始训练之后，对网络在更大的尺寸448上进行了微调。\n5. 检测训练：删除了最后一个卷积层，加上了三个具有1024个滤波器的3×3卷积层，其后是最后的1×1卷积层与我们检测需要的输出数量。对于VOC，我们预测5个边界框，每个边界框有5个坐标和20个类别，所以有125个滤波器。还添加了从最后的3×3×512层到倒数第二层卷积层的直通层，以便模型可以使用细粒度特征。\n6. 联合训练分类和检测数据：\n    * 网络看到标记为检测的图像时，可以基于完整的YOLOv2损失函数进行反向传播。当它看到一个分类图像时，只能从该架构的分类特定部分反向传播损失。\n    * Hierarchical classification(分层分类)：ImageNet标签是从WordNet中提取的，这是一个构建概念及其相互关系的语言数据库，在这里通过构建简单的分层树简化问题。最终的结果是WordTree，一个视觉概念的分层模型。为了使用WordTree进行分类，我们预测每个节点的条件概率，以得到同义词集合中每个同义词下义词的概率。如果想要计算一个特定节点的绝对概率，只需沿着通过树到达根节点的路径，再乘以条件概率。可以使用WordTree以合理的方式将多个数据集组合在一起。只需将数据集中的类别映射到树中的synsets即可。![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/10.png)\n    * YOLO 9000（anchors尺寸3个限制输出大小）: 使用COCO检测数据集和完整的ImageNet版本中的前9000个类来创建的组合数据集。该数据集的相应WordTree有9418个类别。ImageNet是一个更大的数据集，所以通过对COCO进行过采样来平衡数据集，使得ImageNet仅仅大于4:1的比例。当分析YOLO9000在ImageNet上的表现时，发现它很好地学习了新的动物种类，但是却在像服装和设备这样的学习类别中挣扎。新动物更容易学习，因为目标预测可以从COCO中的动物泛化的很好。相反，COCO没有任何类型的衣服的边界框标签，只针对人，因此效果不好3\n\n### YOLO V3\n[论文地址](https://pjreddie.com/media/files/papers/YOLOv3.pdf)\n\n改进：\n1. 将YOLO V3替换了V2中的Softmax loss变成Logistic loss(每个类一个logistic)，而且每个GT只匹配一个先验框.\n2. Anchor bbox prior不同：V2用了5个anchor，V3用了9个anchor，提高了IOU.\n3. Detection的策略不同：V2只有一个detection，V3设置有3个，分别是一个下采样的，Feature map为13\\*13，还有2个上采样的eltwise sum(feature pyramid networks)，Feature map分别为26*\\26和52\\*52，也就是说，V3的416版本已经用到了52的Feature map，而V2把多尺度考虑到训练的data采样上，最后也只是用到了13的Feature map，这应该是对小目标影响最大的地方。\n总结：![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/11.png)\n4. 网络改进 DarkNet-53: 融合了YOLOv2、Darknet-19以及其他新型残差网络，由连续的3×3和1×1卷积层组合而成，当然，其中也添加了一些shortcut connection，整体体量也更大。因为一共有53个卷积层。![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/12.png)\n5. YOLO V3在Pascal Titan X上处理608x608图像速度达到20FPS，在 COCO test-dev 上 mAP@0.5 达到 57.9%，与RetinaNet的结果相近，并且速度快了4倍。  YOLO V3的模型比之前的模型复杂了不少，可以通过改变模型结构的大小来权衡速度与精度。  速度对比如下：![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/13.png)\n6. 失败的尝试：\n    * Anchor box坐标的偏移预测\n    * 用线性方法预测x,y，而不是使用逻辑方法\n    * focal loss\n    * 双IOU阈值和真值分配\n\n\n### SSD: Single Shot MultiBox Detector\n[论文地址](https://arxiv.org/pdf/1512.02325.pdf)\n\nSSD将边界框的输出空间离散化为不同长宽比的一组默认框和并缩放每个特征映射的位置。在预测时，网络会在每个默认框中为每个目标类别的出现生成分数，并对框进行调整以更好地匹配目标形状。此外，网络还结合了不同分辨率的多个特征映射的预测，自然地处理各种尺寸的目标。\n\n**改进**:\n1. 针对多个类别的单次检测器\n2. 预测固定的一系列默认边界框的类别分数和边界框偏移，使用更小的卷积滤波器应用到特征映射上\n3. 根据不同尺度的特征映射生成不同尺度的预测，并通过纵横比明确分开预测\n4. 在低分辨率输入图像上也能实现简单的端到端训练和高精度，从而进一步提高速度与精度之间的权衡。\n\n**模型**:\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/15.png)\nSSD方法基于前馈卷积网络，该网络产生固定大小的边界框集合，并对这些边界框中存在的目标类别实例进行评分，然后进行非极大值抑制步骤来产生最终的检测结果。早期的网络层基于用于高质量图像分类的标准架构将其称为基础网络。然后，将辅助结构添加到网络中以产生具有以下关键特征的检测：\n* **用于检测的多尺度特征映射**。我们将卷积特征层添加到截取的基础网络的末端。这些层在尺寸上逐渐减小，并允许在多个尺度上对检测结果进行预测。用于预测检测的卷积模型对于每个特征层都是不同的\n* **用于检测的卷积预测器**。每个添加的特征层（或者任选的来自基础网络的现有特征层）可以使用一组卷积滤波器产生固定的检测预测集合。\n* **默认边界框和长宽比**。默认边界框与Faster R-CNN[2]中使用的锚边界框相似，但是我们将它们应用到不同分辨率的几个特征映射上。在几个特征映射中允许不同的默认边界框形状可以有效地离散可能的输出框形状的空间。![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/16.png)\n* 总结： 末尾添加的特征层预测不同尺度的长宽比的默认边界框的偏移量以及相关的置信度。PS: 空洞版本VGG更快。\n\n**训练**：\n1. **匹配策略**：默认边界框匹配到IOU重叠高于阈值（0.5）的任何实际边界框。这简化了学习问题，允许网络为多个重叠的默认边界框预测高分，而不是要求它只挑选具有最大重叠的一个边界框。\n2. **训练目标函数**：定位损失加上置信度损失![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/17.png)\n3. **为默认边界框选择尺度和长宽比**: ![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/18.png)\n4. **难例挖掘**: 在匹配步骤之后，大多数默认边界框为负例，尤其是当可能的默认边界框数量较多时。这在正的训练实例和负的训练实例之间引入了显著的不平衡。不使用所有负例，而是使用每个默认边界框的最高置信度损失来排序它们，并挑选最高的置信度，以便负例和正例之间的比例至多为3:1。\n5. **数据增强**![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/20.png)\n6. **小目标数据增强**： 将图像随机放置在填充了平均值的原始图像大小为16x的画布上，然后再进行任意的随机裁剪操作。因为通过引入这个新的“扩展”数据增强技巧，有更多的训练图像，所以必须将训练迭代次数加倍。\n\n**优劣**:\nSSD对类似的目标类别（特别是对于动物）有更多的混淆，部分原因是共享多个类别的位置。SSD对边界框大小非常敏感。换句话说，它在较小目标上比在较大目标上的性能要差得多。这并不奇怪，因为这些小目标甚至可能在顶层没有任何信息。增加输入尺寸（例如从300×300到512×512）可以帮助改进检测小目标，但仍然有很大的改进空间。积极的一面，SSD在大型目标上的表现非常好。而且对于不同长宽比的目标，它是非常鲁棒的，因为使用每个特征映射位置的各种长宽比的默认框。\n\n### RetinaNet Focal Loss for Dense Object Detection\n[论文地址](https://arxiv.org/abs/1708.02002)\n\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/32.png)\n\n稠密分类的后者精度不够高，核心问题（central issus）是稠密proposal中前景和背景的极度不平衡。以我更熟悉的YOLO举例子，比如在PASCAL VOC数据集中，每张图片上标注的目标可能也就几个。但是YOLO V2最后一层的输出是13×13×5，也就是845个候选目标！大量（简单易区分）的负样本在loss中占据了很大比重，使得有用的loss不能回传回来。基于此，作者将经典的交叉熵损失做了变形（见下），给那些易于被分类的简单例子小的权重，给不易区分的难例更大的权重。同时，作者提出了一个新的one-stage的检测器RetinaNet，达到了速度和精度很好地trade-off。\n\n$FL(p_{t}=-(1-p_{t})^{\\gamma}log(p_{t})$\n\n**Focal Loss**\nFocal Loss从交叉熵损失而来。二分类的交叉熵损失如下：\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/33.png)\n\n对应的，多分类的交叉熵损失是这样的：\n\n$CE(p,y)=-log(p_{y})$\n\n因此可以使用添加权重的交叉熵损失：\n$CE(p)=-\\alpha_{t}log(p_{t})$\n\n而作者提出的是一个自适应调节的权重：(可加入权重$\\alpha$平衡)\n\n**$FL(p_{t}=-(1-p_{t})^{\\gamma}log(p_{t})$**\n\nPytorch实现：\n\n$L=-\\sum_{i}^{C}onehot\\odot(1-p_{t})^{\\gamma}log(p_{t})$\n\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\n\ndef one_hot(index, classes):\n    size = index.size() + (classes,)\n    view = index.size() + (1,)\n    mask = torch.Tensor(*size).fill_(0)\n    index = index.view(*view)\n    ones = 1.\n    if isinstance(index, Variable):\n        ones = Variable(torch.Tensor(index.size()).fill_(1))\n        mask = Variable(mask, volatile=index.volatile)\n    return mask.scatter_(1, index, ones)\n    \nclass FocalLoss(nn.Module):\n    def __init__(self, gamma=0, eps=1e-7):\n        super(FocalLoss, self).__init__()\n        self.gamma = gamma\n        self.eps = eps\n        \n    def forward(self, input, target):\n        y = one_hot(target, input.size(-1))\n        logit = F.softmax(input)\n        logit = logit.clamp(self.eps, 1. - self.eps)\n        loss = -1 * y * torch.log(logit) # cross entropy\n        loss = loss * (1 - logit) ** self.gamma # focal loss\n        return loss.sum()\n```\n\n**模型**:\n\n1. 模型初始化： 对于一般的分类网络，初始化之后，往往其输出的预测结果是均等的（随机猜测）。然而作者认为，这种初始化方式在类别极度不均衡的时候是有害的。作者提出，应该初始化模型参数，使得初始化之后，模型输出稀有类别的概率变小（如0.01），作者发现这种初始化方法对于交叉熵损失和Focal Loss的性能提升都有帮助。首先，从imagenet预训练得到的base net不做调整，新加入的卷积层权重均初始化为$\\sigma$=0.01的高斯分布，偏置项为0.对于分类网络的最后一个卷积层，偏置项为$b=-log(\\frac{(1-\\pi)}{\\pi})$, $\\pi$是一个超参数，其意义是在训练的初始阶段，每个anchor被分类为前景的概率。\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/34.png)\n\n作者利用前面介绍的发现和结论，基于ResNet和Feature Pyramid Net（FPN）设计了一种新的one-stage检测框架.RetinaNet 是由一个骨干网络和两个特定任务子网组成的单一网络。骨感网络负责在整个输入图像上计算卷积特征图，并且是一个现成的卷积网络。\n* 第一个子网在骨干网络的输出上执行卷积对象分类(small FCN attached to each FPN level)子网的参数在所有金字塔级别共享\n* 第二个子网执行卷积边界框回归(attach another samll FCN to each pyramid level)\n* 对象分类子网和框回归子网，尽管共享一个共同的结构，使用单独的参数。\n\nAnchors: \n在金字塔等级P3到P7上，锚点的面积分别为$32^{2}$到$512^{2}$, 使用的长宽比为[1:2,1:1,2:1]. 对于更密集的比例覆盖，每个级别添加锚点的尺寸$[2^{0},2^{\\frac{1}{3}},2^{\\frac{1}{2}}]$\n\nIOU:\n\\[0,0.4)的为背景，\\[0.4,0.5)的忽略，大于0.5的为前景\n\n## Two Stage\n\n### R-FCN Object Detection via Region-based Fully Convolutional Networks\n\n[论文地址](https://link.jianshu.com/?t=https%3A%2F%2Farxiv.org%2Fpdf%2F1605.06409.pdf)\n\nR-FCN 通过添加 Position-sensitive score map 解决了把 ROI pooling 放到网络最后一层降低平移可变性的问题，以此改进了 Faster R-CNN 中检测速度慢的问题。\n\nPS: \n* 分类需要特征具有平移不变性，检测则要求对目标的平移做出准确响应。论文中作者给了测试的数据：ROI放在ResNet-101的conv5后，mAP是68.9%；ROI放到conv5前（就是标准的Faster R-CNN结构）的mAP是76.4%，差距是巨大的，这能证明平移可变性对目标检测的重要性。\n\n* Faster R-CNN检测速度慢的问题，速度慢是因为ROI层后的结构对不同的proposal是不共享的，试想下如果有300个proposal，ROI后的全连接网络就要计算300次, 非常耗时。\n\n**模型**：\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/19.png)\n\n* Backbone architecture: ResNet-101有100个卷积层，后面是全局平均池化和1000类的全连接层。删除了平均池化层和全连接层，只使用卷积层来计算特征映射。最后一个卷积块是2048维，附加一个随机初始化的1024维的1×1卷积层来降维\n* $k^{2}(C+1)$Conv: ResNet101的输出是W\\*H\\*1024，用$k^{2}(C+1)$个1024\\*1\\*1的卷积核去卷积即可得到$k^{2}(C+1)$个大小为W\\*H的position sensitive的score map。这步的卷积操作就是在做prediction。k = 3，表示把一个ROI划分成3\\*3，对应的9个位置\n* ROI pooling: 一层的SPP结构。主要用来将不同大小的ROI对应的feature map映射成同样维度的特征![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/21.png)\n* Vote:k\\*k个bin直接进行求和（每个类单独做）得到每一类的score，并进行softmax得到每类的最终得分，并用于计算损失![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/22.png)\n\n**训练**:\n* 损失函数： $L(s,t_{x,y,w,h})=L_{cls}(s_{c^{\\star}}+ \\lambda [c^{\\star}>0]L_{reg}(t,t^{\\star})$ \n将正样本定义为与真实边界框IOU至少为0.5的ROI，否则为负样本\n* 在线难例挖掘（**OHEM**): 其主要考虑训练样本集总是包含较多easy examples而相对较少hard examples，而自动选择困难样本能够使得训练更为有效，此外还有：S-OHEM: Stratified Online Hard Example Mining for Object Detection. **S-OHEM** 利用OHEM和stratified sampling技术。其主要考虑OHEM训练过程忽略了不同损失分布的影响，因此S-OHEM根据分布抽样训练样本。A-Fast-RCNN: Hard positive generation via adversary for object detection从更好的利用数据的角度出发，OHEM和S-OHEM都是发现困难样本，而A-Fast-RCNN的方法则是通过**GAN**的方式在特征空间产生具有部分遮挡和形变的困难样本。\n* **空洞和步长**:我们的全卷积架构享有FCN广泛使用的语义分割的网络修改的好处。特别的是，将ResNet-101的有效步长从32像素降低到了16像素，增加了分数图的分辨率。第一个conv5块中的stride=2操作被修改为stride=1，并且conv5阶段的所有卷积滤波器都被“hole algorithm” 修改来弥补减少的步幅.\n\n**位置敏感分数图**：\n我们可以想想一下这种情况，M 是一个 5\\*5 大小，有一个蓝色的正方形物体在其中的特征图，我们将方形物体平均分割成 3\\*3 的区域。现在我们从 M 中创建一个新的特征图并只用其来检测方形区域的左上角。这个新的特征图如下右图，只有黄色网格单元被激活![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/23.png)\n\n因为我们将方形分为了 9 个部分，我们可以创建 9 张特征图分别来检测对应的物体区域。因为每张图检测的是目标物体的子区域，所以这些特征图被称为位置敏感分数图（position-sensitive score maps）。![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/24.png)\n比如，我们可以说，下图由虚线所画的红色矩形是被提议的 ROIs 。我们将其分为 3\\*3 区域并得出每个区域可能包含其对应的物体部分的可能性。我们将此结果储存在 3\\*3 的投票阵列（如下右图）中。比如，投票阵列 [0][0] 中数值的意义是在此找到方形目标左上区域的可能性。![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/25.png)\n将分数图和 ROIs 映射到投票阵列的过程叫做位置敏感 ROI 池化（position-sensitive ROI-pool）。\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/26.png)\n在计算完位置敏感 ROI 池化所有的值之后，分类的得分就是所有它元素的平均值\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/27.png)\n如果说我们有 C 类物体需要检测。我们将使用 C+1个类，因为其中多包括了一个背景（无目标物体）类。每类都分别有一个 3×3 分数图，因此一共有 (C+1)×3×3 张分数图。通过使用自己类别的那组分数图，我们可以预测出每一类的分数。然后我们使用 softmax 来操作这些分数从而计算出每一类的概率。\n\n\n### FPN Feature Pyramid Networks for Object Detection\n[论文地址](http://openaccess.thecvf.com/content_cvpr_2017/papers/Lin_Feature_Pyramid_Networks_CVPR_2017_paper.pdf)\n\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/28.png)\n\na. 通过缩放图片获取不同尺度的特征图\n\nb. ConvNET\n\nc. 通过不同特征分层\n\nd. FPN\n\n特征金字塔网络FPN，网络直接在原来的单网络上做修改，每个分辨率的 feature map 引入后一分辨率缩放两倍的 feature map 做 element-wise 相加的操作。通过这样的连接，每一层预测所用的 feature map 都融合了不同分辨率、不同语义强度的特征，融合的不同分辨率的 feature map 分别做对应分辨率大小的物体检测。这样保证了每一层都有合适的分辨率以及强语义特征。同时，由于此方法只是在原网络基础上加上了额外的跨层连接，在实际应用中几乎不增加额外的时间和计算量。\n\n**FPN**:\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/29.png)\n\n1. 图中feature map用蓝色轮廓表示，较粗的表示语义上较强特征\n2. 采用的bottom-up 和 top-down 的方法， bottom-up这条含有较低级别的语义但其激活可以更精确的定位因为下采样的次数更少。 Top-down的这条路更粗糙但是语义更强。\n3. top-down的特征随后通过bottom-up的特征经由横向连接进行增强如图，使用较粗糙分辨率的特征映射时候将空间分辨率上采样x2倍。bottom-up的特征要经过1x1卷积层来生成最粗糙分辨率映射。\n4. 每个横向连接合并来自自下而上路径和自顶向下路径的具有相同空间大小的特征映射。\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/30.png)\n\n\n**RPN结合FPN**\n1. 通过用FPN替换单尺度特征映射来适应RPN。在特征金字塔的每个层级上附加一个相同设计的头部（3x3 conv（目标/非目标二分类和边界框回归）和 两个1x1convs（分类和回归））由于头部在所有金字塔等级上的所有位置密集滑动，所以不需要在特定层级上具有多尺度锚点。相反，为每个层级分配单尺度的锚点。定义锚点$[P_{2},P_{3},P_{4},P_{5},P_{6}]$分别具有$[32^{2},64^{2},128^{2},256^{2},512^{2}]$个像素 面积，以及多个长宽比{1:2,1:1,2:1}所以总共15个锚点在金字塔上。\n2. 其余同RPN网络\n2. 不同的尺度ROI用不同层的特征，每个box根据公式计算后提取其中某一层特征图对应的特征ROI，大尺度就用后面一些的金字塔层（P5），小尺度就用前面一点的层（P4）\n可根据公式：$k=[k_{0}+log_{2}(\\frac{\\sqrt{wh}}{224}]$ 计算需要哪一层的特征\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/31.png)\n\n### Light-Head R-CNN: In Defense of Two-Stage Object Detector\n[论文地址](https://arxiv.org/abs/1711.07264)\n\ntwo-stage 的方法在本身的一个基础网络上都会附加计算量很大的一个用于 classification+regression 的网络，导致速度变慢\n* Faster R-CNN: two fully connected layers for RoI recognition\n* R-FCN: produces a large score maps.\n\n因此，作者为了解决 detection 的速度问题，提出了一种新的 two-stage detector，就是Light-Head R-CNN。速度和准确率都有提升。Light-Head R-CNN 重点是 head 的结构设计。包括两部分： R-CNN subnet（ROI pooling 之后的network） 和ROI warping。\n\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/35.png)\n\n**方法**\n* **Thin feature map**: ![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/36.png)降低进入head部分feature map的channel数也就是将r-fcn的score map从$P\\times P(C+1)$减小到$P\\times P\\times \\alpha$，$\\alpha$是一个与类别数无关且较小的值，比如10。这样，score map的channel数与类别数无关，使得后面的分类不能像r-fcn那样vote，于是在roi pooling之后添加了一个fc层进行预测类别和位置。\n* **Large separable convolution**: \n\n\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/38.png)![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/39.png)  \n* **Cheap R-CNN**: ![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/37.png) \n\n**模型**：\n* **Large**: (1) ResNEt-101; (2) atrous algorithm; (3) RPN chanels 512; (4) Large separable convolution with $C_{mid}=256$\n* **Small**: (1) Xception like; (2) abbandon atrous algorithm; (3) RPN convolution to 256; (4) Large separable convolution with $C_{mid}=64$; (5) Apply PSPooling with alignment techniques as RoI warping(better results involve RoI-align).","source":"_posts/MalongTech-Learning-Object-Detection.md","raw":"---\ntitle: MalongTech 学习内容:目标检测\ndate: 2018-11-12 11:00:00\ntags: [Deep Learning]\ncategories: 实习\n---\n\n# Object detection\n\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/14.png)\n\n| one-stage系  | two-stage系|\n| ---------- | -----------|\n| YOLO V1,V2,V3   | FPN   |\n| SSD   | RFCN   |\n| RetinalNet | LIghthead |\n<!-- more -->\n## One Stage\n\n### YOLO V1\n** You only look once unified real-time object detection**\n\n[PAPER ADDRESS](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Redmon_You_Only_Look_CVPR_2016_paper.pdf)\n\n作者在YOLO算法中把物体检测（object detection）问题处理成回归问题，用一个卷积神经网络结构就可以从输入图像直接预测bounding box和类别概率。\n\n**优点**: \n1. YOLO的速度非常快。在Titan X GPU上的速度是45 fps, 加速版155 fps。\n2. YOLO是基于图像的全局信息进行预测的。这一点和基于sliding window以及region proposal等检测算法不一样。与Fast R-CNN相比，YOLO在误检测（将背景检测为物体）方面的错误率能降低一半多。\n3. 泛化能力强。\n\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/1.png)\n\n**缺点**:\n1. accuracy 还落后于同期 state-of-the-art 目标检测方法。\n2. 难于检测小目标。\n3. 定位不够精准。\n4. 虽然降低了背景检测为物体的概率但同事导致了召回率较低。\n\n**流程**\n1. 调整图像大小至$448\\times448$.\n2. 运行卷积网络同时预测多目标的边界框和所属类的概率\n3. NMX(非极大值抑制)\n\n**Unified Detection**\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/2.png)\n1. 将图片分为$S\\times S$格子。\n2. 对每个格子都预测B个边界框并且每个边界框包含5个预测值：x,t,w,h以及confidence。x,y就是bounding box的中心坐标，与grid cell对齐（即相对于当前grid cell的偏移值），使得范围变成0到1；w和h进行归一化（分别除以图像的w和h，这样最后的w和h就在0到1范围。\n3. 每个格子都预测C个假定类别的概率。\n4. 在Pascal VOC中， S=7,B=2,C=20. 所以有 $S\\times S\\times (B \\times 5 + C)$ 即 $7\\times 7\\times 30$ 维张量。\n\nConfidence计算：$Pr(Object) * IOU_{pred}^{turth} $\n\n每个bounding box都对应一个confidence score，如果grid cell里面没有object，confidence就是0，如果有，则confidence score等于预测的box和ground truth的IOU值，见上面公式。并且如果一个object的ground truth的中心点坐标在一个grid cell中，那么这个grid cell就是包含这个object，也就是说这个object的预测就由该grid cell负责。 \n每个grid cell都预测C个类别概率，表示一个grid cell在包含object的条件下属于某个类别的概率：$Pr(Class_{i}|Object)$\n\n每个bounding box的confidence和每个类别的score相乘，得到每个bounding box属于哪一类的confidence score。\n\n即得到每个bounding box属于哪一类的confidence score。也就是说最后会得到20\\*(7\\*7\\*2)的score矩阵，括号里面是bounding box的数量，20代表类别。接下来的操作都是20个类别轮流进行：在某个类别中（即矩阵的某一行），将得分少于阈值（0.2）的设置为0，然后再按得分从高到低排序。最后再用NMS算法去掉重复率较大的bounding box（NMS:针对某一类别，选择得分最大的bounding box，然后计算它和其它bounding box的IOU值，如果IOU大于0.5，说明重复率较大，该得分设为0，如果不大于0.5，则不改；这样一轮后，再选择剩下的score里面最大的那个bounding box，然后计算该bounding box和其它bounding box的IOU，重复以上过程直到最后）。最后每个bounding box的20个score取最大的score，如果这个score大于0，那么这个bounding box就是这个socre对应的类别（矩阵的行），如果小于0，说明这个bounding box里面没有物体，跳过即可。\n\n\n**网络设计**\n灵感来源于GoogLeNet,如下图：\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/3.png)\n训练过程中：\n1. 作者先在ImageNet数据集上预训练网络，而且网络只采用图中的前面20个卷积层，输入是224\\*224大小的图像。然后在检测的时候再加上随机初始化的4个卷积层和2个全连接层，同时输入改为更高分辨率的448\\*448。\n2. Relu层改为leaky Relu，即当x<0时，激活值是0.1\\*x，而不是传统的0。\n3. 作者采用sum-squared error的方式把localization error（bounding box的坐标误差）和classificaton error整合在一起。但是如果二者的权值一致，容易导致模型不稳定，训练发散。因为很多grid cell是不包含物体的，这样的话很多grid cell的confidence score为0。所以采用设置不同权重方式来解决，一方面提高localization error的权重，另一方面降低没有object的box的confidence loss权值，loss权重分别是5和0.5。而对于包含object的box的confidence loss权值还是原来的1。\n4. 用宽和高的开根号代替原来的宽和高，这样做主要是因为相同的宽和高误差对于小的目标精度影响比大的目标要大.\n5. Loss Function如下：![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/4.png)\n\n训练的时候：输入N个图像，每个图像包含M个objec，每个object包含4个坐标（x，y，w，h）和1个label。然后通过网络得到7\\*7\\*30大小的三维矩阵。每个1\\*30的向量前5个元素表示第一个bounding box的4个坐标和1个confidence，第6到10元素表示第二个bounding box的4个坐标和1个confidence。最后20个表示这个grid cell所属类别。注意这30个都是预测的结果。然后就可以计算损失函数的第一、二 、五行。至于第二三行，confidence可以根据ground truth和预测的bounding box计算出的IOU和是否有object的0,1值相乘得到。真实的confidence是0或1值，即有object则为1，没有object则为0。 这样就能计算出loss function的值了。\n\n测试的时候：输入一张图像，跑到网络的末端得到7\\*7\\*30的三维矩阵，这里虽然没有计算IOU，但是由训练好的权重已经直接计算出了bounding box的confidence。然后再跟预测的类别概率相乘就得到每个bounding box属于哪一类的概率。\n\n**YOLO效果**\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/5.png)\n由于yolo更少的识别背景为物体对比Faster RCNN,因此结合YOLO作为背景检测器与Faster RCNN可以带来更大的提升，不过速度方面就没有优势了。\n\n### YOLO V2 and YOLO 9000\n[论文地址](https://arxiv.org/pdf/1612.08242.pdf)\n\n与分类和标记等其他任务的数据集相比，目前目标检测数据集是有限的。最常见的检测数据集包含成千上万到数十万张具有成百上千个标签的图像。分类数据集有数以百万计的图像，数十或数十万个类别。为了扩大当前检测系统的范围。我们的方法使用目标分类的分层视图，允许我们将不同的数据集组合在一起。此外联合训练算法，使我们能够在检测和分类数据上训练目标检测器。我们的方法利用标记的检测图像来学习精确定位物体，同时使用分类图像来增加词表和鲁棒性。\n\n**对比YOLO V1的改进**\n1. YOLO有许多缺点。YOLO与Fast R-CNN相比的误差分析表明，YOLO造成了大量的定位误差。此外，与基于区域提出的方法相比，YOLO召回率相对较低。因此，我们主要侧重于提高召回率和改进定位，同时保持分类准确性。\n2. 在YOLOv2中，一个更精确的检测器被设计出来，它仍然很快。但是不是通过扩大网络，而是简化网络，然后让其更容易学习。结合了以往的一些新方法，以提高YOLO的性能：\n    * Batch Normalization: map提升2%\n    * High Resolution Classifier: 先在ImageNet上以448×448的分辨率对分类网络进行10个迭代周期的微调。这给了网络时间来调整其滤波器以便更好地处理更高分辨率的输入。map提升4%,\n    * Convolutional With Anchor Boxes: 从YOLO中移除全连接层，并使用锚盒来预测边界框。首先，我们消除了一个池化层，使网络卷积层输出具有更高的分辨率。我们还缩小了网络，操作416×416的输入图像而不是448×448。我们这样做是因为我们要在我们的特征映射中有奇数个位置，所以只有一个中心单元。目标，特别是大目标，往往占据图像的中心，所以在中心有一个单独的位置来预测这些目标，而不是四个都在附近的位置是很好的。YOLO的卷积层将图像下采样32倍，所以通过使用416的输入图像，我们得到了13×13的输出特征映射。map有所下降但是召回率达到了88%.\n    * Dimension Clusters: Anchors 尺寸的选择用k-means聚类来挑选合适的锚盒尺寸。如果我们使用具有欧几里得距离的标准k-means，那么较大的边界框比较小的边界框产生更多的误差。然而，我们真正想要的是导致好的IOU分数的先验，这是独立于边界框大小的。因此，对于我们的距离度量，我们使用：$d(box,centroid)=1-IOU(box,centroid)$. 在voc和coco的测试中，更薄更高的边界框会带来更好的结果在k=5的时候，如图所示：![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/6.png)\n    * Direct location prediction: 传统的RPN中，特别是在早期的迭代过程中。大部分的不稳定来自预测边界框的(x,y)位置，他的位置修正方法是不受限制的，所以任何锚盒都可以在图像任一点结束，而不管在哪个位置预测该边界框。随机初始化模型需要很长时间才能稳定以预测合理的偏移量。所以这一步就优化为直接预测相对于网格单元位置的位置坐标。逻辑激活备用来限制网络的预测落在这个范围内。Sigmoid使输出在0~1之间这样映射到原图中时候不会位于其他的网格（在中心目标处）。![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/7.png)网络预测输出特征映射中每个单元的5个边界框。网络预测每个边界框的5个坐标，$t_{x}$,$t_{y}$,$t_{w}$,$t_{h}$,$t_{o}$,如果单元从图像的左上角偏移了$(c_{x},c_{y})$,并且边界框先验的宽度和高度为$p_{w}$,$p_{h}$. 预测就可以对应如图公式计算。$Pr(object)\\times IOU(b,object)=\\sigma(t_{o}) $ 该方法结合维度聚类，map 提升了5%对比与其他的锚盒方法。\n    * Fine-Grained Features(细粒度特征): 对于小目标物体，更细的力度特种可以带来更好的效果，因此直通层通过将相邻特征堆叠到不同的通道而不是空间位置来连接较高分辨率特征和较低分辨率特征，类似于ResNet中的恒等映射。这将26×26×512特征映射变成13×13×2048特征映射，其可以与原始特征连接。我们的检测器运行在这个扩展的特征映射的顶部，以便它可以访问细粒度的特征。这会使性能提高1%。\n    * Multi-Scale Training:  由于模型只使用卷积层和池化层，因此它可以实时调整大小。所以每隔10个批次会随机选择一个新的图像尺寸大小（330到608，从32的倍数中选择因为模型缩减了32倍），强迫网络学习在不同维度上预测，并且小尺度的网络运行更快。![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/8.png)\n\n3. Darknet-19， 它有19个卷积层和5个最大池化层并且只需要55.8亿次运算处理图像获得了比复杂运算VGG和前一代YOLO更高的top-5精度在ImageNet上，结构如下：\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/9.png)\n4. 分类训练：使用Darknet神经网络结构，使用随机梯度下降，初始学习率为0.1，学习率多项式衰减系数为4，权重衰减为0.0005，动量为0.9，在标准ImageNet 1000类分类数据集上训练网络160个迭代周期。在训练过程中，标准的数据增强技巧，包括随机裁剪，旋转，色调，饱和度和曝光偏移被使用来防止over-fitting. 在在对224×224的图像进行初始训练之后，对网络在更大的尺寸448上进行了微调。\n5. 检测训练：删除了最后一个卷积层，加上了三个具有1024个滤波器的3×3卷积层，其后是最后的1×1卷积层与我们检测需要的输出数量。对于VOC，我们预测5个边界框，每个边界框有5个坐标和20个类别，所以有125个滤波器。还添加了从最后的3×3×512层到倒数第二层卷积层的直通层，以便模型可以使用细粒度特征。\n6. 联合训练分类和检测数据：\n    * 网络看到标记为检测的图像时，可以基于完整的YOLOv2损失函数进行反向传播。当它看到一个分类图像时，只能从该架构的分类特定部分反向传播损失。\n    * Hierarchical classification(分层分类)：ImageNet标签是从WordNet中提取的，这是一个构建概念及其相互关系的语言数据库，在这里通过构建简单的分层树简化问题。最终的结果是WordTree，一个视觉概念的分层模型。为了使用WordTree进行分类，我们预测每个节点的条件概率，以得到同义词集合中每个同义词下义词的概率。如果想要计算一个特定节点的绝对概率，只需沿着通过树到达根节点的路径，再乘以条件概率。可以使用WordTree以合理的方式将多个数据集组合在一起。只需将数据集中的类别映射到树中的synsets即可。![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/10.png)\n    * YOLO 9000（anchors尺寸3个限制输出大小）: 使用COCO检测数据集和完整的ImageNet版本中的前9000个类来创建的组合数据集。该数据集的相应WordTree有9418个类别。ImageNet是一个更大的数据集，所以通过对COCO进行过采样来平衡数据集，使得ImageNet仅仅大于4:1的比例。当分析YOLO9000在ImageNet上的表现时，发现它很好地学习了新的动物种类，但是却在像服装和设备这样的学习类别中挣扎。新动物更容易学习，因为目标预测可以从COCO中的动物泛化的很好。相反，COCO没有任何类型的衣服的边界框标签，只针对人，因此效果不好3\n\n### YOLO V3\n[论文地址](https://pjreddie.com/media/files/papers/YOLOv3.pdf)\n\n改进：\n1. 将YOLO V3替换了V2中的Softmax loss变成Logistic loss(每个类一个logistic)，而且每个GT只匹配一个先验框.\n2. Anchor bbox prior不同：V2用了5个anchor，V3用了9个anchor，提高了IOU.\n3. Detection的策略不同：V2只有一个detection，V3设置有3个，分别是一个下采样的，Feature map为13\\*13，还有2个上采样的eltwise sum(feature pyramid networks)，Feature map分别为26*\\26和52\\*52，也就是说，V3的416版本已经用到了52的Feature map，而V2把多尺度考虑到训练的data采样上，最后也只是用到了13的Feature map，这应该是对小目标影响最大的地方。\n总结：![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/11.png)\n4. 网络改进 DarkNet-53: 融合了YOLOv2、Darknet-19以及其他新型残差网络，由连续的3×3和1×1卷积层组合而成，当然，其中也添加了一些shortcut connection，整体体量也更大。因为一共有53个卷积层。![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/12.png)\n5. YOLO V3在Pascal Titan X上处理608x608图像速度达到20FPS，在 COCO test-dev 上 mAP@0.5 达到 57.9%，与RetinaNet的结果相近，并且速度快了4倍。  YOLO V3的模型比之前的模型复杂了不少，可以通过改变模型结构的大小来权衡速度与精度。  速度对比如下：![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/13.png)\n6. 失败的尝试：\n    * Anchor box坐标的偏移预测\n    * 用线性方法预测x,y，而不是使用逻辑方法\n    * focal loss\n    * 双IOU阈值和真值分配\n\n\n### SSD: Single Shot MultiBox Detector\n[论文地址](https://arxiv.org/pdf/1512.02325.pdf)\n\nSSD将边界框的输出空间离散化为不同长宽比的一组默认框和并缩放每个特征映射的位置。在预测时，网络会在每个默认框中为每个目标类别的出现生成分数，并对框进行调整以更好地匹配目标形状。此外，网络还结合了不同分辨率的多个特征映射的预测，自然地处理各种尺寸的目标。\n\n**改进**:\n1. 针对多个类别的单次检测器\n2. 预测固定的一系列默认边界框的类别分数和边界框偏移，使用更小的卷积滤波器应用到特征映射上\n3. 根据不同尺度的特征映射生成不同尺度的预测，并通过纵横比明确分开预测\n4. 在低分辨率输入图像上也能实现简单的端到端训练和高精度，从而进一步提高速度与精度之间的权衡。\n\n**模型**:\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/15.png)\nSSD方法基于前馈卷积网络，该网络产生固定大小的边界框集合，并对这些边界框中存在的目标类别实例进行评分，然后进行非极大值抑制步骤来产生最终的检测结果。早期的网络层基于用于高质量图像分类的标准架构将其称为基础网络。然后，将辅助结构添加到网络中以产生具有以下关键特征的检测：\n* **用于检测的多尺度特征映射**。我们将卷积特征层添加到截取的基础网络的末端。这些层在尺寸上逐渐减小，并允许在多个尺度上对检测结果进行预测。用于预测检测的卷积模型对于每个特征层都是不同的\n* **用于检测的卷积预测器**。每个添加的特征层（或者任选的来自基础网络的现有特征层）可以使用一组卷积滤波器产生固定的检测预测集合。\n* **默认边界框和长宽比**。默认边界框与Faster R-CNN[2]中使用的锚边界框相似，但是我们将它们应用到不同分辨率的几个特征映射上。在几个特征映射中允许不同的默认边界框形状可以有效地离散可能的输出框形状的空间。![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/16.png)\n* 总结： 末尾添加的特征层预测不同尺度的长宽比的默认边界框的偏移量以及相关的置信度。PS: 空洞版本VGG更快。\n\n**训练**：\n1. **匹配策略**：默认边界框匹配到IOU重叠高于阈值（0.5）的任何实际边界框。这简化了学习问题，允许网络为多个重叠的默认边界框预测高分，而不是要求它只挑选具有最大重叠的一个边界框。\n2. **训练目标函数**：定位损失加上置信度损失![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/17.png)\n3. **为默认边界框选择尺度和长宽比**: ![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/18.png)\n4. **难例挖掘**: 在匹配步骤之后，大多数默认边界框为负例，尤其是当可能的默认边界框数量较多时。这在正的训练实例和负的训练实例之间引入了显著的不平衡。不使用所有负例，而是使用每个默认边界框的最高置信度损失来排序它们，并挑选最高的置信度，以便负例和正例之间的比例至多为3:1。\n5. **数据增强**![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/20.png)\n6. **小目标数据增强**： 将图像随机放置在填充了平均值的原始图像大小为16x的画布上，然后再进行任意的随机裁剪操作。因为通过引入这个新的“扩展”数据增强技巧，有更多的训练图像，所以必须将训练迭代次数加倍。\n\n**优劣**:\nSSD对类似的目标类别（特别是对于动物）有更多的混淆，部分原因是共享多个类别的位置。SSD对边界框大小非常敏感。换句话说，它在较小目标上比在较大目标上的性能要差得多。这并不奇怪，因为这些小目标甚至可能在顶层没有任何信息。增加输入尺寸（例如从300×300到512×512）可以帮助改进检测小目标，但仍然有很大的改进空间。积极的一面，SSD在大型目标上的表现非常好。而且对于不同长宽比的目标，它是非常鲁棒的，因为使用每个特征映射位置的各种长宽比的默认框。\n\n### RetinaNet Focal Loss for Dense Object Detection\n[论文地址](https://arxiv.org/abs/1708.02002)\n\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/32.png)\n\n稠密分类的后者精度不够高，核心问题（central issus）是稠密proposal中前景和背景的极度不平衡。以我更熟悉的YOLO举例子，比如在PASCAL VOC数据集中，每张图片上标注的目标可能也就几个。但是YOLO V2最后一层的输出是13×13×5，也就是845个候选目标！大量（简单易区分）的负样本在loss中占据了很大比重，使得有用的loss不能回传回来。基于此，作者将经典的交叉熵损失做了变形（见下），给那些易于被分类的简单例子小的权重，给不易区分的难例更大的权重。同时，作者提出了一个新的one-stage的检测器RetinaNet，达到了速度和精度很好地trade-off。\n\n$FL(p_{t}=-(1-p_{t})^{\\gamma}log(p_{t})$\n\n**Focal Loss**\nFocal Loss从交叉熵损失而来。二分类的交叉熵损失如下：\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/33.png)\n\n对应的，多分类的交叉熵损失是这样的：\n\n$CE(p,y)=-log(p_{y})$\n\n因此可以使用添加权重的交叉熵损失：\n$CE(p)=-\\alpha_{t}log(p_{t})$\n\n而作者提出的是一个自适应调节的权重：(可加入权重$\\alpha$平衡)\n\n**$FL(p_{t}=-(1-p_{t})^{\\gamma}log(p_{t})$**\n\nPytorch实现：\n\n$L=-\\sum_{i}^{C}onehot\\odot(1-p_{t})^{\\gamma}log(p_{t})$\n\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\n\ndef one_hot(index, classes):\n    size = index.size() + (classes,)\n    view = index.size() + (1,)\n    mask = torch.Tensor(*size).fill_(0)\n    index = index.view(*view)\n    ones = 1.\n    if isinstance(index, Variable):\n        ones = Variable(torch.Tensor(index.size()).fill_(1))\n        mask = Variable(mask, volatile=index.volatile)\n    return mask.scatter_(1, index, ones)\n    \nclass FocalLoss(nn.Module):\n    def __init__(self, gamma=0, eps=1e-7):\n        super(FocalLoss, self).__init__()\n        self.gamma = gamma\n        self.eps = eps\n        \n    def forward(self, input, target):\n        y = one_hot(target, input.size(-1))\n        logit = F.softmax(input)\n        logit = logit.clamp(self.eps, 1. - self.eps)\n        loss = -1 * y * torch.log(logit) # cross entropy\n        loss = loss * (1 - logit) ** self.gamma # focal loss\n        return loss.sum()\n```\n\n**模型**:\n\n1. 模型初始化： 对于一般的分类网络，初始化之后，往往其输出的预测结果是均等的（随机猜测）。然而作者认为，这种初始化方式在类别极度不均衡的时候是有害的。作者提出，应该初始化模型参数，使得初始化之后，模型输出稀有类别的概率变小（如0.01），作者发现这种初始化方法对于交叉熵损失和Focal Loss的性能提升都有帮助。首先，从imagenet预训练得到的base net不做调整，新加入的卷积层权重均初始化为$\\sigma$=0.01的高斯分布，偏置项为0.对于分类网络的最后一个卷积层，偏置项为$b=-log(\\frac{(1-\\pi)}{\\pi})$, $\\pi$是一个超参数，其意义是在训练的初始阶段，每个anchor被分类为前景的概率。\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/34.png)\n\n作者利用前面介绍的发现和结论，基于ResNet和Feature Pyramid Net（FPN）设计了一种新的one-stage检测框架.RetinaNet 是由一个骨干网络和两个特定任务子网组成的单一网络。骨感网络负责在整个输入图像上计算卷积特征图，并且是一个现成的卷积网络。\n* 第一个子网在骨干网络的输出上执行卷积对象分类(small FCN attached to each FPN level)子网的参数在所有金字塔级别共享\n* 第二个子网执行卷积边界框回归(attach another samll FCN to each pyramid level)\n* 对象分类子网和框回归子网，尽管共享一个共同的结构，使用单独的参数。\n\nAnchors: \n在金字塔等级P3到P7上，锚点的面积分别为$32^{2}$到$512^{2}$, 使用的长宽比为[1:2,1:1,2:1]. 对于更密集的比例覆盖，每个级别添加锚点的尺寸$[2^{0},2^{\\frac{1}{3}},2^{\\frac{1}{2}}]$\n\nIOU:\n\\[0,0.4)的为背景，\\[0.4,0.5)的忽略，大于0.5的为前景\n\n## Two Stage\n\n### R-FCN Object Detection via Region-based Fully Convolutional Networks\n\n[论文地址](https://link.jianshu.com/?t=https%3A%2F%2Farxiv.org%2Fpdf%2F1605.06409.pdf)\n\nR-FCN 通过添加 Position-sensitive score map 解决了把 ROI pooling 放到网络最后一层降低平移可变性的问题，以此改进了 Faster R-CNN 中检测速度慢的问题。\n\nPS: \n* 分类需要特征具有平移不变性，检测则要求对目标的平移做出准确响应。论文中作者给了测试的数据：ROI放在ResNet-101的conv5后，mAP是68.9%；ROI放到conv5前（就是标准的Faster R-CNN结构）的mAP是76.4%，差距是巨大的，这能证明平移可变性对目标检测的重要性。\n\n* Faster R-CNN检测速度慢的问题，速度慢是因为ROI层后的结构对不同的proposal是不共享的，试想下如果有300个proposal，ROI后的全连接网络就要计算300次, 非常耗时。\n\n**模型**：\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/19.png)\n\n* Backbone architecture: ResNet-101有100个卷积层，后面是全局平均池化和1000类的全连接层。删除了平均池化层和全连接层，只使用卷积层来计算特征映射。最后一个卷积块是2048维，附加一个随机初始化的1024维的1×1卷积层来降维\n* $k^{2}(C+1)$Conv: ResNet101的输出是W\\*H\\*1024，用$k^{2}(C+1)$个1024\\*1\\*1的卷积核去卷积即可得到$k^{2}(C+1)$个大小为W\\*H的position sensitive的score map。这步的卷积操作就是在做prediction。k = 3，表示把一个ROI划分成3\\*3，对应的9个位置\n* ROI pooling: 一层的SPP结构。主要用来将不同大小的ROI对应的feature map映射成同样维度的特征![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/21.png)\n* Vote:k\\*k个bin直接进行求和（每个类单独做）得到每一类的score，并进行softmax得到每类的最终得分，并用于计算损失![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/22.png)\n\n**训练**:\n* 损失函数： $L(s,t_{x,y,w,h})=L_{cls}(s_{c^{\\star}}+ \\lambda [c^{\\star}>0]L_{reg}(t,t^{\\star})$ \n将正样本定义为与真实边界框IOU至少为0.5的ROI，否则为负样本\n* 在线难例挖掘（**OHEM**): 其主要考虑训练样本集总是包含较多easy examples而相对较少hard examples，而自动选择困难样本能够使得训练更为有效，此外还有：S-OHEM: Stratified Online Hard Example Mining for Object Detection. **S-OHEM** 利用OHEM和stratified sampling技术。其主要考虑OHEM训练过程忽略了不同损失分布的影响，因此S-OHEM根据分布抽样训练样本。A-Fast-RCNN: Hard positive generation via adversary for object detection从更好的利用数据的角度出发，OHEM和S-OHEM都是发现困难样本，而A-Fast-RCNN的方法则是通过**GAN**的方式在特征空间产生具有部分遮挡和形变的困难样本。\n* **空洞和步长**:我们的全卷积架构享有FCN广泛使用的语义分割的网络修改的好处。特别的是，将ResNet-101的有效步长从32像素降低到了16像素，增加了分数图的分辨率。第一个conv5块中的stride=2操作被修改为stride=1，并且conv5阶段的所有卷积滤波器都被“hole algorithm” 修改来弥补减少的步幅.\n\n**位置敏感分数图**：\n我们可以想想一下这种情况，M 是一个 5\\*5 大小，有一个蓝色的正方形物体在其中的特征图，我们将方形物体平均分割成 3\\*3 的区域。现在我们从 M 中创建一个新的特征图并只用其来检测方形区域的左上角。这个新的特征图如下右图，只有黄色网格单元被激活![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/23.png)\n\n因为我们将方形分为了 9 个部分，我们可以创建 9 张特征图分别来检测对应的物体区域。因为每张图检测的是目标物体的子区域，所以这些特征图被称为位置敏感分数图（position-sensitive score maps）。![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/24.png)\n比如，我们可以说，下图由虚线所画的红色矩形是被提议的 ROIs 。我们将其分为 3\\*3 区域并得出每个区域可能包含其对应的物体部分的可能性。我们将此结果储存在 3\\*3 的投票阵列（如下右图）中。比如，投票阵列 [0][0] 中数值的意义是在此找到方形目标左上区域的可能性。![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/25.png)\n将分数图和 ROIs 映射到投票阵列的过程叫做位置敏感 ROI 池化（position-sensitive ROI-pool）。\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/26.png)\n在计算完位置敏感 ROI 池化所有的值之后，分类的得分就是所有它元素的平均值\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/27.png)\n如果说我们有 C 类物体需要检测。我们将使用 C+1个类，因为其中多包括了一个背景（无目标物体）类。每类都分别有一个 3×3 分数图，因此一共有 (C+1)×3×3 张分数图。通过使用自己类别的那组分数图，我们可以预测出每一类的分数。然后我们使用 softmax 来操作这些分数从而计算出每一类的概率。\n\n\n### FPN Feature Pyramid Networks for Object Detection\n[论文地址](http://openaccess.thecvf.com/content_cvpr_2017/papers/Lin_Feature_Pyramid_Networks_CVPR_2017_paper.pdf)\n\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/28.png)\n\na. 通过缩放图片获取不同尺度的特征图\n\nb. ConvNET\n\nc. 通过不同特征分层\n\nd. FPN\n\n特征金字塔网络FPN，网络直接在原来的单网络上做修改，每个分辨率的 feature map 引入后一分辨率缩放两倍的 feature map 做 element-wise 相加的操作。通过这样的连接，每一层预测所用的 feature map 都融合了不同分辨率、不同语义强度的特征，融合的不同分辨率的 feature map 分别做对应分辨率大小的物体检测。这样保证了每一层都有合适的分辨率以及强语义特征。同时，由于此方法只是在原网络基础上加上了额外的跨层连接，在实际应用中几乎不增加额外的时间和计算量。\n\n**FPN**:\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/29.png)\n\n1. 图中feature map用蓝色轮廓表示，较粗的表示语义上较强特征\n2. 采用的bottom-up 和 top-down 的方法， bottom-up这条含有较低级别的语义但其激活可以更精确的定位因为下采样的次数更少。 Top-down的这条路更粗糙但是语义更强。\n3. top-down的特征随后通过bottom-up的特征经由横向连接进行增强如图，使用较粗糙分辨率的特征映射时候将空间分辨率上采样x2倍。bottom-up的特征要经过1x1卷积层来生成最粗糙分辨率映射。\n4. 每个横向连接合并来自自下而上路径和自顶向下路径的具有相同空间大小的特征映射。\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/30.png)\n\n\n**RPN结合FPN**\n1. 通过用FPN替换单尺度特征映射来适应RPN。在特征金字塔的每个层级上附加一个相同设计的头部（3x3 conv（目标/非目标二分类和边界框回归）和 两个1x1convs（分类和回归））由于头部在所有金字塔等级上的所有位置密集滑动，所以不需要在特定层级上具有多尺度锚点。相反，为每个层级分配单尺度的锚点。定义锚点$[P_{2},P_{3},P_{4},P_{5},P_{6}]$分别具有$[32^{2},64^{2},128^{2},256^{2},512^{2}]$个像素 面积，以及多个长宽比{1:2,1:1,2:1}所以总共15个锚点在金字塔上。\n2. 其余同RPN网络\n2. 不同的尺度ROI用不同层的特征，每个box根据公式计算后提取其中某一层特征图对应的特征ROI，大尺度就用后面一些的金字塔层（P5），小尺度就用前面一点的层（P4）\n可根据公式：$k=[k_{0}+log_{2}(\\frac{\\sqrt{wh}}{224}]$ 计算需要哪一层的特征\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/31.png)\n\n### Light-Head R-CNN: In Defense of Two-Stage Object Detector\n[论文地址](https://arxiv.org/abs/1711.07264)\n\ntwo-stage 的方法在本身的一个基础网络上都会附加计算量很大的一个用于 classification+regression 的网络，导致速度变慢\n* Faster R-CNN: two fully connected layers for RoI recognition\n* R-FCN: produces a large score maps.\n\n因此，作者为了解决 detection 的速度问题，提出了一种新的 two-stage detector，就是Light-Head R-CNN。速度和准确率都有提升。Light-Head R-CNN 重点是 head 的结构设计。包括两部分： R-CNN subnet（ROI pooling 之后的network） 和ROI warping。\n\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/35.png)\n\n**方法**\n* **Thin feature map**: ![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/36.png)降低进入head部分feature map的channel数也就是将r-fcn的score map从$P\\times P(C+1)$减小到$P\\times P\\times \\alpha$，$\\alpha$是一个与类别数无关且较小的值，比如10。这样，score map的channel数与类别数无关，使得后面的分类不能像r-fcn那样vote，于是在roi pooling之后添加了一个fc层进行预测类别和位置。\n* **Large separable convolution**: \n\n\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/38.png)![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/39.png)  \n* **Cheap R-CNN**: ![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/37.png) \n\n**模型**：\n* **Large**: (1) ResNEt-101; (2) atrous algorithm; (3) RPN chanels 512; (4) Large separable convolution with $C_{mid}=256$\n* **Small**: (1) Xception like; (2) abbandon atrous algorithm; (3) RPN convolution to 256; (4) Large separable convolution with $C_{mid}=64$; (5) Apply PSPooling with alignment techniques as RoI warping(better results involve RoI-align).","slug":"MalongTech-Learning-Object-Detection","published":1,"updated":"2020-04-28T12:38:55.395Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck9jwdng9000bjlrcae01whra","content":"<h1 id=\"Object-detection\"><a href=\"#Object-detection\" class=\"headerlink\" title=\"Object detection\"></a>Object detection</h1><p><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/14.png\" alt=\"image\"></p>\n<table>\n<thead>\n<tr>\n<th>one-stage系</th>\n<th>two-stage系</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>YOLO V1,V2,V3</td>\n<td>FPN</td>\n</tr>\n<tr>\n<td>SSD</td>\n<td>RFCN</td>\n</tr>\n<tr>\n<td>RetinalNet</td>\n<td>LIghthead</td>\n</tr>\n</tbody>\n</table>\n<a id=\"more\"></a>\n<h2 id=\"One-Stage\"><a href=\"#One-Stage\" class=\"headerlink\" title=\"One Stage\"></a>One Stage</h2><h3 id=\"YOLO-V1\"><a href=\"#YOLO-V1\" class=\"headerlink\" title=\"YOLO V1\"></a>YOLO V1</h3><p><strong> You only look once unified real-time object detection</strong></p>\n<p><a href=\"https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Redmon_You_Only_Look_CVPR_2016_paper.pdf\" target=\"_blank\" rel=\"noopener\">PAPER ADDRESS</a></p>\n<p>作者在YOLO算法中把物体检测（object detection）问题处理成回归问题，用一个卷积神经网络结构就可以从输入图像直接预测bounding box和类别概率。</p>\n<p><strong>优点</strong>: </p>\n<ol>\n<li>YOLO的速度非常快。在Titan X GPU上的速度是45 fps, 加速版155 fps。</li>\n<li>YOLO是基于图像的全局信息进行预测的。这一点和基于sliding window以及region proposal等检测算法不一样。与Fast R-CNN相比，YOLO在误检测（将背景检测为物体）方面的错误率能降低一半多。</li>\n<li>泛化能力强。</li>\n</ol>\n<p><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/1.png\" alt=\"image\"></p>\n<p><strong>缺点</strong>:</p>\n<ol>\n<li>accuracy 还落后于同期 state-of-the-art 目标检测方法。</li>\n<li>难于检测小目标。</li>\n<li>定位不够精准。</li>\n<li>虽然降低了背景检测为物体的概率但同事导致了召回率较低。</li>\n</ol>\n<p><strong>流程</strong></p>\n<ol>\n<li>调整图像大小至$448\\times448$.</li>\n<li>运行卷积网络同时预测多目标的边界框和所属类的概率</li>\n<li>NMX(非极大值抑制)</li>\n</ol>\n<p><strong>Unified Detection</strong><br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/2.png\" alt=\"image\"></p>\n<ol>\n<li>将图片分为$S\\times S$格子。</li>\n<li>对每个格子都预测B个边界框并且每个边界框包含5个预测值：x,t,w,h以及confidence。x,y就是bounding box的中心坐标，与grid cell对齐（即相对于当前grid cell的偏移值），使得范围变成0到1；w和h进行归一化（分别除以图像的w和h，这样最后的w和h就在0到1范围。</li>\n<li>每个格子都预测C个假定类别的概率。</li>\n<li>在Pascal VOC中， S=7,B=2,C=20. 所以有 $S\\times S\\times (B \\times 5 + C)$ 即 $7\\times 7\\times 30$ 维张量。</li>\n</ol>\n<p>Confidence计算：$Pr(Object) * IOU_{pred}^{turth} $</p>\n<p>每个bounding box都对应一个confidence score，如果grid cell里面没有object，confidence就是0，如果有，则confidence score等于预测的box和ground truth的IOU值，见上面公式。并且如果一个object的ground truth的中心点坐标在一个grid cell中，那么这个grid cell就是包含这个object，也就是说这个object的预测就由该grid cell负责。<br>每个grid cell都预测C个类别概率，表示一个grid cell在包含object的条件下属于某个类别的概率：$Pr(Class_{i}|Object)$</p>\n<p>每个bounding box的confidence和每个类别的score相乘，得到每个bounding box属于哪一类的confidence score。</p>\n<p>即得到每个bounding box属于哪一类的confidence score。也就是说最后会得到20*(7*7*2)的score矩阵，括号里面是bounding box的数量，20代表类别。接下来的操作都是20个类别轮流进行：在某个类别中（即矩阵的某一行），将得分少于阈值（0.2）的设置为0，然后再按得分从高到低排序。最后再用NMS算法去掉重复率较大的bounding box（NMS:针对某一类别，选择得分最大的bounding box，然后计算它和其它bounding box的IOU值，如果IOU大于0.5，说明重复率较大，该得分设为0，如果不大于0.5，则不改；这样一轮后，再选择剩下的score里面最大的那个bounding box，然后计算该bounding box和其它bounding box的IOU，重复以上过程直到最后）。最后每个bounding box的20个score取最大的score，如果这个score大于0，那么这个bounding box就是这个socre对应的类别（矩阵的行），如果小于0，说明这个bounding box里面没有物体，跳过即可。</p>\n<p><strong>网络设计</strong><br>灵感来源于GoogLeNet,如下图：<br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/3.png\" alt=\"image\"><br>训练过程中：</p>\n<ol>\n<li>作者先在ImageNet数据集上预训练网络，而且网络只采用图中的前面20个卷积层，输入是224*224大小的图像。然后在检测的时候再加上随机初始化的4个卷积层和2个全连接层，同时输入改为更高分辨率的448*448。</li>\n<li>Relu层改为leaky Relu，即当x&lt;0时，激活值是0.1*x，而不是传统的0。</li>\n<li>作者采用sum-squared error的方式把localization error（bounding box的坐标误差）和classificaton error整合在一起。但是如果二者的权值一致，容易导致模型不稳定，训练发散。因为很多grid cell是不包含物体的，这样的话很多grid cell的confidence score为0。所以采用设置不同权重方式来解决，一方面提高localization error的权重，另一方面降低没有object的box的confidence loss权值，loss权重分别是5和0.5。而对于包含object的box的confidence loss权值还是原来的1。</li>\n<li>用宽和高的开根号代替原来的宽和高，这样做主要是因为相同的宽和高误差对于小的目标精度影响比大的目标要大.</li>\n<li>Loss Function如下：<img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/4.png\" alt=\"image\"></li>\n</ol>\n<p>训练的时候：输入N个图像，每个图像包含M个objec，每个object包含4个坐标（x，y，w，h）和1个label。然后通过网络得到7*7*30大小的三维矩阵。每个1*30的向量前5个元素表示第一个bounding box的4个坐标和1个confidence，第6到10元素表示第二个bounding box的4个坐标和1个confidence。最后20个表示这个grid cell所属类别。注意这30个都是预测的结果。然后就可以计算损失函数的第一、二 、五行。至于第二三行，confidence可以根据ground truth和预测的bounding box计算出的IOU和是否有object的0,1值相乘得到。真实的confidence是0或1值，即有object则为1，没有object则为0。 这样就能计算出loss function的值了。</p>\n<p>测试的时候：输入一张图像，跑到网络的末端得到7*7*30的三维矩阵，这里虽然没有计算IOU，但是由训练好的权重已经直接计算出了bounding box的confidence。然后再跟预测的类别概率相乘就得到每个bounding box属于哪一类的概率。</p>\n<p><strong>YOLO效果</strong><br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/5.png\" alt=\"image\"><br>由于yolo更少的识别背景为物体对比Faster RCNN,因此结合YOLO作为背景检测器与Faster RCNN可以带来更大的提升，不过速度方面就没有优势了。</p>\n<h3 id=\"YOLO-V2-and-YOLO-9000\"><a href=\"#YOLO-V2-and-YOLO-9000\" class=\"headerlink\" title=\"YOLO V2 and YOLO 9000\"></a>YOLO V2 and YOLO 9000</h3><p><a href=\"https://arxiv.org/pdf/1612.08242.pdf\" target=\"_blank\" rel=\"noopener\">论文地址</a></p>\n<p>与分类和标记等其他任务的数据集相比，目前目标检测数据集是有限的。最常见的检测数据集包含成千上万到数十万张具有成百上千个标签的图像。分类数据集有数以百万计的图像，数十或数十万个类别。为了扩大当前检测系统的范围。我们的方法使用目标分类的分层视图，允许我们将不同的数据集组合在一起。此外联合训练算法，使我们能够在检测和分类数据上训练目标检测器。我们的方法利用标记的检测图像来学习精确定位物体，同时使用分类图像来增加词表和鲁棒性。</p>\n<p><strong>对比YOLO V1的改进</strong></p>\n<ol>\n<li>YOLO有许多缺点。YOLO与Fast R-CNN相比的误差分析表明，YOLO造成了大量的定位误差。此外，与基于区域提出的方法相比，YOLO召回率相对较低。因此，我们主要侧重于提高召回率和改进定位，同时保持分类准确性。</li>\n<li><p>在YOLOv2中，一个更精确的检测器被设计出来，它仍然很快。但是不是通过扩大网络，而是简化网络，然后让其更容易学习。结合了以往的一些新方法，以提高YOLO的性能：</p>\n<ul>\n<li>Batch Normalization: map提升2%</li>\n<li>High Resolution Classifier: 先在ImageNet上以448×448的分辨率对分类网络进行10个迭代周期的微调。这给了网络时间来调整其滤波器以便更好地处理更高分辨率的输入。map提升4%,</li>\n<li>Convolutional With Anchor Boxes: 从YOLO中移除全连接层，并使用锚盒来预测边界框。首先，我们消除了一个池化层，使网络卷积层输出具有更高的分辨率。我们还缩小了网络，操作416×416的输入图像而不是448×448。我们这样做是因为我们要在我们的特征映射中有奇数个位置，所以只有一个中心单元。目标，特别是大目标，往往占据图像的中心，所以在中心有一个单独的位置来预测这些目标，而不是四个都在附近的位置是很好的。YOLO的卷积层将图像下采样32倍，所以通过使用416的输入图像，我们得到了13×13的输出特征映射。map有所下降但是召回率达到了88%.</li>\n<li>Dimension Clusters: Anchors 尺寸的选择用k-means聚类来挑选合适的锚盒尺寸。如果我们使用具有欧几里得距离的标准k-means，那么较大的边界框比较小的边界框产生更多的误差。然而，我们真正想要的是导致好的IOU分数的先验，这是独立于边界框大小的。因此，对于我们的距离度量，我们使用：$d(box,centroid)=1-IOU(box,centroid)$. 在voc和coco的测试中，更薄更高的边界框会带来更好的结果在k=5的时候，如图所示：<img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/6.png\" alt=\"image\"></li>\n<li>Direct location prediction: 传统的RPN中，特别是在早期的迭代过程中。大部分的不稳定来自预测边界框的(x,y)位置，他的位置修正方法是不受限制的，所以任何锚盒都可以在图像任一点结束，而不管在哪个位置预测该边界框。随机初始化模型需要很长时间才能稳定以预测合理的偏移量。所以这一步就优化为直接预测相对于网格单元位置的位置坐标。逻辑激活备用来限制网络的预测落在这个范围内。Sigmoid使输出在0~1之间这样映射到原图中时候不会位于其他的网格（在中心目标处）。<img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/7.png\" alt=\"image\">网络预测输出特征映射中每个单元的5个边界框。网络预测每个边界框的5个坐标，$t_{x}$,$t_{y}$,$t_{w}$,$t_{h}$,$t_{o}$,如果单元从图像的左上角偏移了$(c_{x},c_{y})$,并且边界框先验的宽度和高度为$p_{w}$,$p_{h}$. 预测就可以对应如图公式计算。$Pr(object)\\times IOU(b,object)=\\sigma(t_{o}) $ 该方法结合维度聚类，map 提升了5%对比与其他的锚盒方法。</li>\n<li>Fine-Grained Features(细粒度特征): 对于小目标物体，更细的力度特种可以带来更好的效果，因此直通层通过将相邻特征堆叠到不同的通道而不是空间位置来连接较高分辨率特征和较低分辨率特征，类似于ResNet中的恒等映射。这将26×26×512特征映射变成13×13×2048特征映射，其可以与原始特征连接。我们的检测器运行在这个扩展的特征映射的顶部，以便它可以访问细粒度的特征。这会使性能提高1%。</li>\n<li>Multi-Scale Training:  由于模型只使用卷积层和池化层，因此它可以实时调整大小。所以每隔10个批次会随机选择一个新的图像尺寸大小（330到608，从32的倍数中选择因为模型缩减了32倍），强迫网络学习在不同维度上预测，并且小尺度的网络运行更快。<img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/8.png\" alt=\"image\"></li>\n</ul>\n</li>\n<li><p>Darknet-19， 它有19个卷积层和5个最大池化层并且只需要55.8亿次运算处理图像获得了比复杂运算VGG和前一代YOLO更高的top-5精度在ImageNet上，结构如下：<br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/9.png\" alt=\"image\"></p>\n</li>\n<li>分类训练：使用Darknet神经网络结构，使用随机梯度下降，初始学习率为0.1，学习率多项式衰减系数为4，权重衰减为0.0005，动量为0.9，在标准ImageNet 1000类分类数据集上训练网络160个迭代周期。在训练过程中，标准的数据增强技巧，包括随机裁剪，旋转，色调，饱和度和曝光偏移被使用来防止over-fitting. 在在对224×224的图像进行初始训练之后，对网络在更大的尺寸448上进行了微调。</li>\n<li>检测训练：删除了最后一个卷积层，加上了三个具有1024个滤波器的3×3卷积层，其后是最后的1×1卷积层与我们检测需要的输出数量。对于VOC，我们预测5个边界框，每个边界框有5个坐标和20个类别，所以有125个滤波器。还添加了从最后的3×3×512层到倒数第二层卷积层的直通层，以便模型可以使用细粒度特征。</li>\n<li>联合训练分类和检测数据：<ul>\n<li>网络看到标记为检测的图像时，可以基于完整的YOLOv2损失函数进行反向传播。当它看到一个分类图像时，只能从该架构的分类特定部分反向传播损失。</li>\n<li>Hierarchical classification(分层分类)：ImageNet标签是从WordNet中提取的，这是一个构建概念及其相互关系的语言数据库，在这里通过构建简单的分层树简化问题。最终的结果是WordTree，一个视觉概念的分层模型。为了使用WordTree进行分类，我们预测每个节点的条件概率，以得到同义词集合中每个同义词下义词的概率。如果想要计算一个特定节点的绝对概率，只需沿着通过树到达根节点的路径，再乘以条件概率。可以使用WordTree以合理的方式将多个数据集组合在一起。只需将数据集中的类别映射到树中的synsets即可。<img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/10.png\" alt=\"image\"></li>\n<li>YOLO 9000（anchors尺寸3个限制输出大小）: 使用COCO检测数据集和完整的ImageNet版本中的前9000个类来创建的组合数据集。该数据集的相应WordTree有9418个类别。ImageNet是一个更大的数据集，所以通过对COCO进行过采样来平衡数据集，使得ImageNet仅仅大于4:1的比例。当分析YOLO9000在ImageNet上的表现时，发现它很好地学习了新的动物种类，但是却在像服装和设备这样的学习类别中挣扎。新动物更容易学习，因为目标预测可以从COCO中的动物泛化的很好。相反，COCO没有任何类型的衣服的边界框标签，只针对人，因此效果不好3</li>\n</ul>\n</li>\n</ol>\n<h3 id=\"YOLO-V3\"><a href=\"#YOLO-V3\" class=\"headerlink\" title=\"YOLO V3\"></a>YOLO V3</h3><p><a href=\"https://pjreddie.com/media/files/papers/YOLOv3.pdf\" target=\"_blank\" rel=\"noopener\">论文地址</a></p>\n<p>改进：</p>\n<ol>\n<li>将YOLO V3替换了V2中的Softmax loss变成Logistic loss(每个类一个logistic)，而且每个GT只匹配一个先验框.</li>\n<li>Anchor bbox prior不同：V2用了5个anchor，V3用了9个anchor，提高了IOU.</li>\n<li>Detection的策略不同：V2只有一个detection，V3设置有3个，分别是一个下采样的，Feature map为13*13，还有2个上采样的eltwise sum(feature pyramid networks)，Feature map分别为26<em>\\26和52\\</em>52，也就是说，V3的416版本已经用到了52的Feature map，而V2把多尺度考虑到训练的data采样上，最后也只是用到了13的Feature map，这应该是对小目标影响最大的地方。<br>总结：<img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/11.png\" alt=\"image\"></li>\n<li>网络改进 DarkNet-53: 融合了YOLOv2、Darknet-19以及其他新型残差网络，由连续的3×3和1×1卷积层组合而成，当然，其中也添加了一些shortcut connection，整体体量也更大。因为一共有53个卷积层。<img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/12.png\" alt=\"image\"></li>\n<li>YOLO V3在Pascal Titan X上处理608x608图像速度达到20FPS，在 COCO test-dev 上 <a href=\"mailto:mAP@0.5\" target=\"_blank\" rel=\"noopener\">mAP@0.5</a> 达到 57.9%，与RetinaNet的结果相近，并且速度快了4倍。  YOLO V3的模型比之前的模型复杂了不少，可以通过改变模型结构的大小来权衡速度与精度。  速度对比如下：<img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/13.png\" alt=\"image\"></li>\n<li>失败的尝试：<ul>\n<li>Anchor box坐标的偏移预测</li>\n<li>用线性方法预测x,y，而不是使用逻辑方法</li>\n<li>focal loss</li>\n<li>双IOU阈值和真值分配</li>\n</ul>\n</li>\n</ol>\n<h3 id=\"SSD-Single-Shot-MultiBox-Detector\"><a href=\"#SSD-Single-Shot-MultiBox-Detector\" class=\"headerlink\" title=\"SSD: Single Shot MultiBox Detector\"></a>SSD: Single Shot MultiBox Detector</h3><p><a href=\"https://arxiv.org/pdf/1512.02325.pdf\" target=\"_blank\" rel=\"noopener\">论文地址</a></p>\n<p>SSD将边界框的输出空间离散化为不同长宽比的一组默认框和并缩放每个特征映射的位置。在预测时，网络会在每个默认框中为每个目标类别的出现生成分数，并对框进行调整以更好地匹配目标形状。此外，网络还结合了不同分辨率的多个特征映射的预测，自然地处理各种尺寸的目标。</p>\n<p><strong>改进</strong>:</p>\n<ol>\n<li>针对多个类别的单次检测器</li>\n<li>预测固定的一系列默认边界框的类别分数和边界框偏移，使用更小的卷积滤波器应用到特征映射上</li>\n<li>根据不同尺度的特征映射生成不同尺度的预测，并通过纵横比明确分开预测</li>\n<li>在低分辨率输入图像上也能实现简单的端到端训练和高精度，从而进一步提高速度与精度之间的权衡。</li>\n</ol>\n<p><strong>模型</strong>:<br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/15.png\" alt=\"image\"><br>SSD方法基于前馈卷积网络，该网络产生固定大小的边界框集合，并对这些边界框中存在的目标类别实例进行评分，然后进行非极大值抑制步骤来产生最终的检测结果。早期的网络层基于用于高质量图像分类的标准架构将其称为基础网络。然后，将辅助结构添加到网络中以产生具有以下关键特征的检测：</p>\n<ul>\n<li><strong>用于检测的多尺度特征映射</strong>。我们将卷积特征层添加到截取的基础网络的末端。这些层在尺寸上逐渐减小，并允许在多个尺度上对检测结果进行预测。用于预测检测的卷积模型对于每个特征层都是不同的</li>\n<li><strong>用于检测的卷积预测器</strong>。每个添加的特征层（或者任选的来自基础网络的现有特征层）可以使用一组卷积滤波器产生固定的检测预测集合。</li>\n<li><strong>默认边界框和长宽比</strong>。默认边界框与Faster R-CNN[2]中使用的锚边界框相似，但是我们将它们应用到不同分辨率的几个特征映射上。在几个特征映射中允许不同的默认边界框形状可以有效地离散可能的输出框形状的空间。<img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/16.png\" alt=\"image\"></li>\n<li>总结： 末尾添加的特征层预测不同尺度的长宽比的默认边界框的偏移量以及相关的置信度。PS: 空洞版本VGG更快。</li>\n</ul>\n<p><strong>训练</strong>：</p>\n<ol>\n<li><strong>匹配策略</strong>：默认边界框匹配到IOU重叠高于阈值（0.5）的任何实际边界框。这简化了学习问题，允许网络为多个重叠的默认边界框预测高分，而不是要求它只挑选具有最大重叠的一个边界框。</li>\n<li><strong>训练目标函数</strong>：定位损失加上置信度损失<img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/17.png\" alt=\"image\"></li>\n<li><strong>为默认边界框选择尺度和长宽比</strong>: <img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/18.png\" alt=\"image\"></li>\n<li><strong>难例挖掘</strong>: 在匹配步骤之后，大多数默认边界框为负例，尤其是当可能的默认边界框数量较多时。这在正的训练实例和负的训练实例之间引入了显著的不平衡。不使用所有负例，而是使用每个默认边界框的最高置信度损失来排序它们，并挑选最高的置信度，以便负例和正例之间的比例至多为3:1。</li>\n<li><strong>数据增强</strong><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/20.png\" alt=\"image\"></li>\n<li><strong>小目标数据增强</strong>： 将图像随机放置在填充了平均值的原始图像大小为16x的画布上，然后再进行任意的随机裁剪操作。因为通过引入这个新的“扩展”数据增强技巧，有更多的训练图像，所以必须将训练迭代次数加倍。</li>\n</ol>\n<p><strong>优劣</strong>:<br>SSD对类似的目标类别（特别是对于动物）有更多的混淆，部分原因是共享多个类别的位置。SSD对边界框大小非常敏感。换句话说，它在较小目标上比在较大目标上的性能要差得多。这并不奇怪，因为这些小目标甚至可能在顶层没有任何信息。增加输入尺寸（例如从300×300到512×512）可以帮助改进检测小目标，但仍然有很大的改进空间。积极的一面，SSD在大型目标上的表现非常好。而且对于不同长宽比的目标，它是非常鲁棒的，因为使用每个特征映射位置的各种长宽比的默认框。</p>\n<h3 id=\"RetinaNet-Focal-Loss-for-Dense-Object-Detection\"><a href=\"#RetinaNet-Focal-Loss-for-Dense-Object-Detection\" class=\"headerlink\" title=\"RetinaNet Focal Loss for Dense Object Detection\"></a>RetinaNet Focal Loss for Dense Object Detection</h3><p><a href=\"https://arxiv.org/abs/1708.02002\" target=\"_blank\" rel=\"noopener\">论文地址</a></p>\n<p><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/32.png\" alt=\"image\"></p>\n<p>稠密分类的后者精度不够高，核心问题（central issus）是稠密proposal中前景和背景的极度不平衡。以我更熟悉的YOLO举例子，比如在PASCAL VOC数据集中，每张图片上标注的目标可能也就几个。但是YOLO V2最后一层的输出是13×13×5，也就是845个候选目标！大量（简单易区分）的负样本在loss中占据了很大比重，使得有用的loss不能回传回来。基于此，作者将经典的交叉熵损失做了变形（见下），给那些易于被分类的简单例子小的权重，给不易区分的难例更大的权重。同时，作者提出了一个新的one-stage的检测器RetinaNet，达到了速度和精度很好地trade-off。</p>\n<p>$FL(p_{t}=-(1-p_{t})^{\\gamma}log(p_{t})$</p>\n<p><strong>Focal Loss</strong><br>Focal Loss从交叉熵损失而来。二分类的交叉熵损失如下：<br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/33.png\" alt=\"image\"></p>\n<p>对应的，多分类的交叉熵损失是这样的：</p>\n<p>$CE(p,y)=-log(p_{y})$</p>\n<p>因此可以使用添加权重的交叉熵损失：<br>$CE(p)=-\\alpha_{t}log(p_{t})$</p>\n<p>而作者提出的是一个自适应调节的权重：(可加入权重$\\alpha$平衡)</p>\n<p><strong>$FL(p_{t}=-(1-p_{t})^{\\gamma}log(p_{t})$</strong></p>\n<p>Pytorch实现：</p>\n<p>$L=-\\sum_{i}^{C}onehot\\odot(1-p_{t})^{\\gamma}log(p_{t})$</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">import</span> torch.nn <span class=\"keyword\">as</span> nn</span><br><span class=\"line\"><span class=\"keyword\">import</span> torch.nn.functional <span class=\"keyword\">as</span> F</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch.autograd <span class=\"keyword\">import</span> Variable</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">one_hot</span><span class=\"params\">(index, classes)</span>:</span></span><br><span class=\"line\">    size = index.size() + (classes,)</span><br><span class=\"line\">    view = index.size() + (<span class=\"number\">1</span>,)</span><br><span class=\"line\">    mask = torch.Tensor(*size).fill_(<span class=\"number\">0</span>)</span><br><span class=\"line\">    index = index.view(*view)</span><br><span class=\"line\">    ones = <span class=\"number\">1.</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> isinstance(index, Variable):</span><br><span class=\"line\">        ones = Variable(torch.Tensor(index.size()).fill_(<span class=\"number\">1</span>))</span><br><span class=\"line\">        mask = Variable(mask, volatile=index.volatile)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> mask.scatter_(<span class=\"number\">1</span>, index, ones)</span><br><span class=\"line\">    </span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">FocalLoss</span><span class=\"params\">(nn.Module)</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self, gamma=<span class=\"number\">0</span>, eps=<span class=\"number\">1e-7</span>)</span>:</span></span><br><span class=\"line\">        super(FocalLoss, self).__init__()</span><br><span class=\"line\">        self.gamma = gamma</span><br><span class=\"line\">        self.eps = eps</span><br><span class=\"line\">        </span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">forward</span><span class=\"params\">(self, input, target)</span>:</span></span><br><span class=\"line\">        y = one_hot(target, input.size(<span class=\"number\">-1</span>))</span><br><span class=\"line\">        logit = F.softmax(input)</span><br><span class=\"line\">        logit = logit.clamp(self.eps, <span class=\"number\">1.</span> - self.eps)</span><br><span class=\"line\">        loss = <span class=\"number\">-1</span> * y * torch.log(logit) <span class=\"comment\"># cross entropy</span></span><br><span class=\"line\">        loss = loss * (<span class=\"number\">1</span> - logit) ** self.gamma <span class=\"comment\"># focal loss</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> loss.sum()</span><br></pre></td></tr></table></figure>\n<p><strong>模型</strong>:</p>\n<ol>\n<li>模型初始化： 对于一般的分类网络，初始化之后，往往其输出的预测结果是均等的（随机猜测）。然而作者认为，这种初始化方式在类别极度不均衡的时候是有害的。作者提出，应该初始化模型参数，使得初始化之后，模型输出稀有类别的概率变小（如0.01），作者发现这种初始化方法对于交叉熵损失和Focal Loss的性能提升都有帮助。首先，从imagenet预训练得到的base net不做调整，新加入的卷积层权重均初始化为$\\sigma$=0.01的高斯分布，偏置项为0.对于分类网络的最后一个卷积层，偏置项为$b=-log(\\frac{(1-\\pi)}{\\pi})$, $\\pi$是一个超参数，其意义是在训练的初始阶段，每个anchor被分类为前景的概率。<br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/34.png\" alt=\"image\"></li>\n</ol>\n<p>作者利用前面介绍的发现和结论，基于ResNet和Feature Pyramid Net（FPN）设计了一种新的one-stage检测框架.RetinaNet 是由一个骨干网络和两个特定任务子网组成的单一网络。骨感网络负责在整个输入图像上计算卷积特征图，并且是一个现成的卷积网络。</p>\n<ul>\n<li>第一个子网在骨干网络的输出上执行卷积对象分类(small FCN attached to each FPN level)子网的参数在所有金字塔级别共享</li>\n<li>第二个子网执行卷积边界框回归(attach another samll FCN to each pyramid level)</li>\n<li>对象分类子网和框回归子网，尽管共享一个共同的结构，使用单独的参数。</li>\n</ul>\n<p>Anchors:<br>在金字塔等级P3到P7上，锚点的面积分别为$32^{2}$到$512^{2}$, 使用的长宽比为[1:2,1:1,2:1]. 对于更密集的比例覆盖，每个级别添加锚点的尺寸$[2^{0},2^{\\frac{1}{3}},2^{\\frac{1}{2}}]$</p>\n<p>IOU:<br>[0,0.4)的为背景，[0.4,0.5)的忽略，大于0.5的为前景</p>\n<h2 id=\"Two-Stage\"><a href=\"#Two-Stage\" class=\"headerlink\" title=\"Two Stage\"></a>Two Stage</h2><h3 id=\"R-FCN-Object-Detection-via-Region-based-Fully-Convolutional-Networks\"><a href=\"#R-FCN-Object-Detection-via-Region-based-Fully-Convolutional-Networks\" class=\"headerlink\" title=\"R-FCN Object Detection via Region-based Fully Convolutional Networks\"></a>R-FCN Object Detection via Region-based Fully Convolutional Networks</h3><p><a href=\"https://link.jianshu.com/?t=https%3A%2F%2Farxiv.org%2Fpdf%2F1605.06409.pdf\" target=\"_blank\" rel=\"noopener\">论文地址</a></p>\n<p>R-FCN 通过添加 Position-sensitive score map 解决了把 ROI pooling 放到网络最后一层降低平移可变性的问题，以此改进了 Faster R-CNN 中检测速度慢的问题。</p>\n<p>PS: </p>\n<ul>\n<li><p>分类需要特征具有平移不变性，检测则要求对目标的平移做出准确响应。论文中作者给了测试的数据：ROI放在ResNet-101的conv5后，mAP是68.9%；ROI放到conv5前（就是标准的Faster R-CNN结构）的mAP是76.4%，差距是巨大的，这能证明平移可变性对目标检测的重要性。</p>\n</li>\n<li><p>Faster R-CNN检测速度慢的问题，速度慢是因为ROI层后的结构对不同的proposal是不共享的，试想下如果有300个proposal，ROI后的全连接网络就要计算300次, 非常耗时。</p>\n</li>\n</ul>\n<p><strong>模型</strong>：<br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/19.png\" alt=\"image\"></p>\n<ul>\n<li>Backbone architecture: ResNet-101有100个卷积层，后面是全局平均池化和1000类的全连接层。删除了平均池化层和全连接层，只使用卷积层来计算特征映射。最后一个卷积块是2048维，附加一个随机初始化的1024维的1×1卷积层来降维</li>\n<li>$k^{2}(C+1)$Conv: ResNet101的输出是W*H*1024，用$k^{2}(C+1)$个1024*1*1的卷积核去卷积即可得到$k^{2}(C+1)$个大小为W*H的position sensitive的score map。这步的卷积操作就是在做prediction。k = 3，表示把一个ROI划分成3*3，对应的9个位置</li>\n<li>ROI pooling: 一层的SPP结构。主要用来将不同大小的ROI对应的feature map映射成同样维度的特征<img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/21.png\" alt=\"image\"></li>\n<li>Vote:k*k个bin直接进行求和（每个类单独做）得到每一类的score，并进行softmax得到每类的最终得分，并用于计算损失<img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/22.png\" alt=\"image\"></li>\n</ul>\n<p><strong>训练</strong>:</p>\n<ul>\n<li>损失函数： $L(s,t_{x,y,w,h})=L_{cls}(s_{c^{\\star}}+ \\lambda [c^{\\star}&gt;0]L_{reg}(t,t^{\\star})$<br>将正样本定义为与真实边界框IOU至少为0.5的ROI，否则为负样本</li>\n<li>在线难例挖掘（<strong>OHEM</strong>): 其主要考虑训练样本集总是包含较多easy examples而相对较少hard examples，而自动选择困难样本能够使得训练更为有效，此外还有：S-OHEM: Stratified Online Hard Example Mining for Object Detection. <strong>S-OHEM</strong> 利用OHEM和stratified sampling技术。其主要考虑OHEM训练过程忽略了不同损失分布的影响，因此S-OHEM根据分布抽样训练样本。A-Fast-RCNN: Hard positive generation via adversary for object detection从更好的利用数据的角度出发，OHEM和S-OHEM都是发现困难样本，而A-Fast-RCNN的方法则是通过<strong>GAN</strong>的方式在特征空间产生具有部分遮挡和形变的困难样本。</li>\n<li><strong>空洞和步长</strong>:我们的全卷积架构享有FCN广泛使用的语义分割的网络修改的好处。特别的是，将ResNet-101的有效步长从32像素降低到了16像素，增加了分数图的分辨率。第一个conv5块中的stride=2操作被修改为stride=1，并且conv5阶段的所有卷积滤波器都被“hole algorithm” 修改来弥补减少的步幅.</li>\n</ul>\n<p><strong>位置敏感分数图</strong>：<br>我们可以想想一下这种情况，M 是一个 5*5 大小，有一个蓝色的正方形物体在其中的特征图，我们将方形物体平均分割成 3*3 的区域。现在我们从 M 中创建一个新的特征图并只用其来检测方形区域的左上角。这个新的特征图如下右图，只有黄色网格单元被激活<img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/23.png\" alt=\"image\"></p>\n<p>因为我们将方形分为了 9 个部分，我们可以创建 9 张特征图分别来检测对应的物体区域。因为每张图检测的是目标物体的子区域，所以这些特征图被称为位置敏感分数图（position-sensitive score maps）。<img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/24.png\" alt=\"image\"><br>比如，我们可以说，下图由虚线所画的红色矩形是被提议的 ROIs 。我们将其分为 3*3 区域并得出每个区域可能包含其对应的物体部分的可能性。我们将此结果储存在 3*3 的投票阵列（如下右图）中。比如，投票阵列 [0][0] 中数值的意义是在此找到方形目标左上区域的可能性。<img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/25.png\" alt=\"image\"><br>将分数图和 ROIs 映射到投票阵列的过程叫做位置敏感 ROI 池化（position-sensitive ROI-pool）。<br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/26.png\" alt=\"image\"><br>在计算完位置敏感 ROI 池化所有的值之后，分类的得分就是所有它元素的平均值<br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/27.png\" alt=\"image\"><br>如果说我们有 C 类物体需要检测。我们将使用 C+1个类，因为其中多包括了一个背景（无目标物体）类。每类都分别有一个 3×3 分数图，因此一共有 (C+1)×3×3 张分数图。通过使用自己类别的那组分数图，我们可以预测出每一类的分数。然后我们使用 softmax 来操作这些分数从而计算出每一类的概率。</p>\n<h3 id=\"FPN-Feature-Pyramid-Networks-for-Object-Detection\"><a href=\"#FPN-Feature-Pyramid-Networks-for-Object-Detection\" class=\"headerlink\" title=\"FPN Feature Pyramid Networks for Object Detection\"></a>FPN Feature Pyramid Networks for Object Detection</h3><p><a href=\"http://openaccess.thecvf.com/content_cvpr_2017/papers/Lin_Feature_Pyramid_Networks_CVPR_2017_paper.pdf\" target=\"_blank\" rel=\"noopener\">论文地址</a></p>\n<p><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/28.png\" alt=\"image\"></p>\n<p>a. 通过缩放图片获取不同尺度的特征图</p>\n<p>b. ConvNET</p>\n<p>c. 通过不同特征分层</p>\n<p>d. FPN</p>\n<p>特征金字塔网络FPN，网络直接在原来的单网络上做修改，每个分辨率的 feature map 引入后一分辨率缩放两倍的 feature map 做 element-wise 相加的操作。通过这样的连接，每一层预测所用的 feature map 都融合了不同分辨率、不同语义强度的特征，融合的不同分辨率的 feature map 分别做对应分辨率大小的物体检测。这样保证了每一层都有合适的分辨率以及强语义特征。同时，由于此方法只是在原网络基础上加上了额外的跨层连接，在实际应用中几乎不增加额外的时间和计算量。</p>\n<p><strong>FPN</strong>:<br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/29.png\" alt=\"image\"></p>\n<ol>\n<li>图中feature map用蓝色轮廓表示，较粗的表示语义上较强特征</li>\n<li>采用的bottom-up 和 top-down 的方法， bottom-up这条含有较低级别的语义但其激活可以更精确的定位因为下采样的次数更少。 Top-down的这条路更粗糙但是语义更强。</li>\n<li>top-down的特征随后通过bottom-up的特征经由横向连接进行增强如图，使用较粗糙分辨率的特征映射时候将空间分辨率上采样x2倍。bottom-up的特征要经过1x1卷积层来生成最粗糙分辨率映射。</li>\n<li>每个横向连接合并来自自下而上路径和自顶向下路径的具有相同空间大小的特征映射。<br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/30.png\" alt=\"image\"></li>\n</ol>\n<p><strong>RPN结合FPN</strong></p>\n<ol>\n<li>通过用FPN替换单尺度特征映射来适应RPN。在特征金字塔的每个层级上附加一个相同设计的头部（3x3 conv（目标/非目标二分类和边界框回归）和 两个1x1convs（分类和回归））由于头部在所有金字塔等级上的所有位置密集滑动，所以不需要在特定层级上具有多尺度锚点。相反，为每个层级分配单尺度的锚点。定义锚点$[P_{2},P_{3},P_{4},P_{5},P_{6}]$分别具有$[32^{2},64^{2},128^{2},256^{2},512^{2}]$个像素 面积，以及多个长宽比{1:2,1:1,2:1}所以总共15个锚点在金字塔上。</li>\n<li>其余同RPN网络</li>\n<li>不同的尺度ROI用不同层的特征，每个box根据公式计算后提取其中某一层特征图对应的特征ROI，大尺度就用后面一些的金字塔层（P5），小尺度就用前面一点的层（P4）<br>可根据公式：$k=[k_{0}+log_{2}(\\frac{\\sqrt{wh}}{224}]$ 计算需要哪一层的特征<br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/31.png\" alt=\"image\"></li>\n</ol>\n<h3 id=\"Light-Head-R-CNN-In-Defense-of-Two-Stage-Object-Detector\"><a href=\"#Light-Head-R-CNN-In-Defense-of-Two-Stage-Object-Detector\" class=\"headerlink\" title=\"Light-Head R-CNN: In Defense of Two-Stage Object Detector\"></a>Light-Head R-CNN: In Defense of Two-Stage Object Detector</h3><p><a href=\"https://arxiv.org/abs/1711.07264\" target=\"_blank\" rel=\"noopener\">论文地址</a></p>\n<p>two-stage 的方法在本身的一个基础网络上都会附加计算量很大的一个用于 classification+regression 的网络，导致速度变慢</p>\n<ul>\n<li>Faster R-CNN: two fully connected layers for RoI recognition</li>\n<li>R-FCN: produces a large score maps.</li>\n</ul>\n<p>因此，作者为了解决 detection 的速度问题，提出了一种新的 two-stage detector，就是Light-Head R-CNN。速度和准确率都有提升。Light-Head R-CNN 重点是 head 的结构设计。包括两部分： R-CNN subnet（ROI pooling 之后的network） 和ROI warping。</p>\n<p><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/35.png\" alt=\"image\"></p>\n<p><strong>方法</strong></p>\n<ul>\n<li><strong>Thin feature map</strong>: <img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/36.png\" alt=\"image\">降低进入head部分feature map的channel数也就是将r-fcn的score map从$P\\times P(C+1)$减小到$P\\times P\\times \\alpha$，$\\alpha$是一个与类别数无关且较小的值，比如10。这样，score map的channel数与类别数无关，使得后面的分类不能像r-fcn那样vote，于是在roi pooling之后添加了一个fc层进行预测类别和位置。</li>\n<li><strong>Large separable convolution</strong>: </li>\n</ul>\n<p><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/38.png\" alt=\"image\"><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/39.png\" alt=\"image\">  </p>\n<ul>\n<li><strong>Cheap R-CNN</strong>: <img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/37.png\" alt=\"image\"> </li>\n</ul>\n<p><strong>模型</strong>：</p>\n<ul>\n<li><strong>Large</strong>: (1) ResNEt-101; (2) atrous algorithm; (3) RPN chanels 512; (4) Large separable convolution with $C_{mid}=256$</li>\n<li><strong>Small</strong>: (1) Xception like; (2) abbandon atrous algorithm; (3) RPN convolution to 256; (4) Large separable convolution with $C_{mid}=64$; (5) Apply PSPooling with alignment techniques as RoI warping(better results involve RoI-align).</li>\n</ul>\n","site":{"data":{}},"excerpt":"<h1 id=\"Object-detection\"><a href=\"#Object-detection\" class=\"headerlink\" title=\"Object detection\"></a>Object detection</h1><p><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/14.png\" alt=\"image\"></p>\n<table>\n<thead>\n<tr>\n<th>one-stage系</th>\n<th>two-stage系</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>YOLO V1,V2,V3</td>\n<td>FPN</td>\n</tr>\n<tr>\n<td>SSD</td>\n<td>RFCN</td>\n</tr>\n<tr>\n<td>RetinalNet</td>\n<td>LIghthead</td>\n</tr>\n</tbody>\n</table>","more":"<h2 id=\"One-Stage\"><a href=\"#One-Stage\" class=\"headerlink\" title=\"One Stage\"></a>One Stage</h2><h3 id=\"YOLO-V1\"><a href=\"#YOLO-V1\" class=\"headerlink\" title=\"YOLO V1\"></a>YOLO V1</h3><p><strong> You only look once unified real-time object detection</strong></p>\n<p><a href=\"https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Redmon_You_Only_Look_CVPR_2016_paper.pdf\" target=\"_blank\" rel=\"noopener\">PAPER ADDRESS</a></p>\n<p>作者在YOLO算法中把物体检测（object detection）问题处理成回归问题，用一个卷积神经网络结构就可以从输入图像直接预测bounding box和类别概率。</p>\n<p><strong>优点</strong>: </p>\n<ol>\n<li>YOLO的速度非常快。在Titan X GPU上的速度是45 fps, 加速版155 fps。</li>\n<li>YOLO是基于图像的全局信息进行预测的。这一点和基于sliding window以及region proposal等检测算法不一样。与Fast R-CNN相比，YOLO在误检测（将背景检测为物体）方面的错误率能降低一半多。</li>\n<li>泛化能力强。</li>\n</ol>\n<p><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/1.png\" alt=\"image\"></p>\n<p><strong>缺点</strong>:</p>\n<ol>\n<li>accuracy 还落后于同期 state-of-the-art 目标检测方法。</li>\n<li>难于检测小目标。</li>\n<li>定位不够精准。</li>\n<li>虽然降低了背景检测为物体的概率但同事导致了召回率较低。</li>\n</ol>\n<p><strong>流程</strong></p>\n<ol>\n<li>调整图像大小至$448\\times448$.</li>\n<li>运行卷积网络同时预测多目标的边界框和所属类的概率</li>\n<li>NMX(非极大值抑制)</li>\n</ol>\n<p><strong>Unified Detection</strong><br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/2.png\" alt=\"image\"></p>\n<ol>\n<li>将图片分为$S\\times S$格子。</li>\n<li>对每个格子都预测B个边界框并且每个边界框包含5个预测值：x,t,w,h以及confidence。x,y就是bounding box的中心坐标，与grid cell对齐（即相对于当前grid cell的偏移值），使得范围变成0到1；w和h进行归一化（分别除以图像的w和h，这样最后的w和h就在0到1范围。</li>\n<li>每个格子都预测C个假定类别的概率。</li>\n<li>在Pascal VOC中， S=7,B=2,C=20. 所以有 $S\\times S\\times (B \\times 5 + C)$ 即 $7\\times 7\\times 30$ 维张量。</li>\n</ol>\n<p>Confidence计算：$Pr(Object) * IOU_{pred}^{turth} $</p>\n<p>每个bounding box都对应一个confidence score，如果grid cell里面没有object，confidence就是0，如果有，则confidence score等于预测的box和ground truth的IOU值，见上面公式。并且如果一个object的ground truth的中心点坐标在一个grid cell中，那么这个grid cell就是包含这个object，也就是说这个object的预测就由该grid cell负责。<br>每个grid cell都预测C个类别概率，表示一个grid cell在包含object的条件下属于某个类别的概率：$Pr(Class_{i}|Object)$</p>\n<p>每个bounding box的confidence和每个类别的score相乘，得到每个bounding box属于哪一类的confidence score。</p>\n<p>即得到每个bounding box属于哪一类的confidence score。也就是说最后会得到20*(7*7*2)的score矩阵，括号里面是bounding box的数量，20代表类别。接下来的操作都是20个类别轮流进行：在某个类别中（即矩阵的某一行），将得分少于阈值（0.2）的设置为0，然后再按得分从高到低排序。最后再用NMS算法去掉重复率较大的bounding box（NMS:针对某一类别，选择得分最大的bounding box，然后计算它和其它bounding box的IOU值，如果IOU大于0.5，说明重复率较大，该得分设为0，如果不大于0.5，则不改；这样一轮后，再选择剩下的score里面最大的那个bounding box，然后计算该bounding box和其它bounding box的IOU，重复以上过程直到最后）。最后每个bounding box的20个score取最大的score，如果这个score大于0，那么这个bounding box就是这个socre对应的类别（矩阵的行），如果小于0，说明这个bounding box里面没有物体，跳过即可。</p>\n<p><strong>网络设计</strong><br>灵感来源于GoogLeNet,如下图：<br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/3.png\" alt=\"image\"><br>训练过程中：</p>\n<ol>\n<li>作者先在ImageNet数据集上预训练网络，而且网络只采用图中的前面20个卷积层，输入是224*224大小的图像。然后在检测的时候再加上随机初始化的4个卷积层和2个全连接层，同时输入改为更高分辨率的448*448。</li>\n<li>Relu层改为leaky Relu，即当x&lt;0时，激活值是0.1*x，而不是传统的0。</li>\n<li>作者采用sum-squared error的方式把localization error（bounding box的坐标误差）和classificaton error整合在一起。但是如果二者的权值一致，容易导致模型不稳定，训练发散。因为很多grid cell是不包含物体的，这样的话很多grid cell的confidence score为0。所以采用设置不同权重方式来解决，一方面提高localization error的权重，另一方面降低没有object的box的confidence loss权值，loss权重分别是5和0.5。而对于包含object的box的confidence loss权值还是原来的1。</li>\n<li>用宽和高的开根号代替原来的宽和高，这样做主要是因为相同的宽和高误差对于小的目标精度影响比大的目标要大.</li>\n<li>Loss Function如下：<img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/4.png\" alt=\"image\"></li>\n</ol>\n<p>训练的时候：输入N个图像，每个图像包含M个objec，每个object包含4个坐标（x，y，w，h）和1个label。然后通过网络得到7*7*30大小的三维矩阵。每个1*30的向量前5个元素表示第一个bounding box的4个坐标和1个confidence，第6到10元素表示第二个bounding box的4个坐标和1个confidence。最后20个表示这个grid cell所属类别。注意这30个都是预测的结果。然后就可以计算损失函数的第一、二 、五行。至于第二三行，confidence可以根据ground truth和预测的bounding box计算出的IOU和是否有object的0,1值相乘得到。真实的confidence是0或1值，即有object则为1，没有object则为0。 这样就能计算出loss function的值了。</p>\n<p>测试的时候：输入一张图像，跑到网络的末端得到7*7*30的三维矩阵，这里虽然没有计算IOU，但是由训练好的权重已经直接计算出了bounding box的confidence。然后再跟预测的类别概率相乘就得到每个bounding box属于哪一类的概率。</p>\n<p><strong>YOLO效果</strong><br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/5.png\" alt=\"image\"><br>由于yolo更少的识别背景为物体对比Faster RCNN,因此结合YOLO作为背景检测器与Faster RCNN可以带来更大的提升，不过速度方面就没有优势了。</p>\n<h3 id=\"YOLO-V2-and-YOLO-9000\"><a href=\"#YOLO-V2-and-YOLO-9000\" class=\"headerlink\" title=\"YOLO V2 and YOLO 9000\"></a>YOLO V2 and YOLO 9000</h3><p><a href=\"https://arxiv.org/pdf/1612.08242.pdf\" target=\"_blank\" rel=\"noopener\">论文地址</a></p>\n<p>与分类和标记等其他任务的数据集相比，目前目标检测数据集是有限的。最常见的检测数据集包含成千上万到数十万张具有成百上千个标签的图像。分类数据集有数以百万计的图像，数十或数十万个类别。为了扩大当前检测系统的范围。我们的方法使用目标分类的分层视图，允许我们将不同的数据集组合在一起。此外联合训练算法，使我们能够在检测和分类数据上训练目标检测器。我们的方法利用标记的检测图像来学习精确定位物体，同时使用分类图像来增加词表和鲁棒性。</p>\n<p><strong>对比YOLO V1的改进</strong></p>\n<ol>\n<li>YOLO有许多缺点。YOLO与Fast R-CNN相比的误差分析表明，YOLO造成了大量的定位误差。此外，与基于区域提出的方法相比，YOLO召回率相对较低。因此，我们主要侧重于提高召回率和改进定位，同时保持分类准确性。</li>\n<li><p>在YOLOv2中，一个更精确的检测器被设计出来，它仍然很快。但是不是通过扩大网络，而是简化网络，然后让其更容易学习。结合了以往的一些新方法，以提高YOLO的性能：</p>\n<ul>\n<li>Batch Normalization: map提升2%</li>\n<li>High Resolution Classifier: 先在ImageNet上以448×448的分辨率对分类网络进行10个迭代周期的微调。这给了网络时间来调整其滤波器以便更好地处理更高分辨率的输入。map提升4%,</li>\n<li>Convolutional With Anchor Boxes: 从YOLO中移除全连接层，并使用锚盒来预测边界框。首先，我们消除了一个池化层，使网络卷积层输出具有更高的分辨率。我们还缩小了网络，操作416×416的输入图像而不是448×448。我们这样做是因为我们要在我们的特征映射中有奇数个位置，所以只有一个中心单元。目标，特别是大目标，往往占据图像的中心，所以在中心有一个单独的位置来预测这些目标，而不是四个都在附近的位置是很好的。YOLO的卷积层将图像下采样32倍，所以通过使用416的输入图像，我们得到了13×13的输出特征映射。map有所下降但是召回率达到了88%.</li>\n<li>Dimension Clusters: Anchors 尺寸的选择用k-means聚类来挑选合适的锚盒尺寸。如果我们使用具有欧几里得距离的标准k-means，那么较大的边界框比较小的边界框产生更多的误差。然而，我们真正想要的是导致好的IOU分数的先验，这是独立于边界框大小的。因此，对于我们的距离度量，我们使用：$d(box,centroid)=1-IOU(box,centroid)$. 在voc和coco的测试中，更薄更高的边界框会带来更好的结果在k=5的时候，如图所示：<img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/6.png\" alt=\"image\"></li>\n<li>Direct location prediction: 传统的RPN中，特别是在早期的迭代过程中。大部分的不稳定来自预测边界框的(x,y)位置，他的位置修正方法是不受限制的，所以任何锚盒都可以在图像任一点结束，而不管在哪个位置预测该边界框。随机初始化模型需要很长时间才能稳定以预测合理的偏移量。所以这一步就优化为直接预测相对于网格单元位置的位置坐标。逻辑激活备用来限制网络的预测落在这个范围内。Sigmoid使输出在0~1之间这样映射到原图中时候不会位于其他的网格（在中心目标处）。<img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/7.png\" alt=\"image\">网络预测输出特征映射中每个单元的5个边界框。网络预测每个边界框的5个坐标，$t_{x}$,$t_{y}$,$t_{w}$,$t_{h}$,$t_{o}$,如果单元从图像的左上角偏移了$(c_{x},c_{y})$,并且边界框先验的宽度和高度为$p_{w}$,$p_{h}$. 预测就可以对应如图公式计算。$Pr(object)\\times IOU(b,object)=\\sigma(t_{o}) $ 该方法结合维度聚类，map 提升了5%对比与其他的锚盒方法。</li>\n<li>Fine-Grained Features(细粒度特征): 对于小目标物体，更细的力度特种可以带来更好的效果，因此直通层通过将相邻特征堆叠到不同的通道而不是空间位置来连接较高分辨率特征和较低分辨率特征，类似于ResNet中的恒等映射。这将26×26×512特征映射变成13×13×2048特征映射，其可以与原始特征连接。我们的检测器运行在这个扩展的特征映射的顶部，以便它可以访问细粒度的特征。这会使性能提高1%。</li>\n<li>Multi-Scale Training:  由于模型只使用卷积层和池化层，因此它可以实时调整大小。所以每隔10个批次会随机选择一个新的图像尺寸大小（330到608，从32的倍数中选择因为模型缩减了32倍），强迫网络学习在不同维度上预测，并且小尺度的网络运行更快。<img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/8.png\" alt=\"image\"></li>\n</ul>\n</li>\n<li><p>Darknet-19， 它有19个卷积层和5个最大池化层并且只需要55.8亿次运算处理图像获得了比复杂运算VGG和前一代YOLO更高的top-5精度在ImageNet上，结构如下：<br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/9.png\" alt=\"image\"></p>\n</li>\n<li>分类训练：使用Darknet神经网络结构，使用随机梯度下降，初始学习率为0.1，学习率多项式衰减系数为4，权重衰减为0.0005，动量为0.9，在标准ImageNet 1000类分类数据集上训练网络160个迭代周期。在训练过程中，标准的数据增强技巧，包括随机裁剪，旋转，色调，饱和度和曝光偏移被使用来防止over-fitting. 在在对224×224的图像进行初始训练之后，对网络在更大的尺寸448上进行了微调。</li>\n<li>检测训练：删除了最后一个卷积层，加上了三个具有1024个滤波器的3×3卷积层，其后是最后的1×1卷积层与我们检测需要的输出数量。对于VOC，我们预测5个边界框，每个边界框有5个坐标和20个类别，所以有125个滤波器。还添加了从最后的3×3×512层到倒数第二层卷积层的直通层，以便模型可以使用细粒度特征。</li>\n<li>联合训练分类和检测数据：<ul>\n<li>网络看到标记为检测的图像时，可以基于完整的YOLOv2损失函数进行反向传播。当它看到一个分类图像时，只能从该架构的分类特定部分反向传播损失。</li>\n<li>Hierarchical classification(分层分类)：ImageNet标签是从WordNet中提取的，这是一个构建概念及其相互关系的语言数据库，在这里通过构建简单的分层树简化问题。最终的结果是WordTree，一个视觉概念的分层模型。为了使用WordTree进行分类，我们预测每个节点的条件概率，以得到同义词集合中每个同义词下义词的概率。如果想要计算一个特定节点的绝对概率，只需沿着通过树到达根节点的路径，再乘以条件概率。可以使用WordTree以合理的方式将多个数据集组合在一起。只需将数据集中的类别映射到树中的synsets即可。<img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/10.png\" alt=\"image\"></li>\n<li>YOLO 9000（anchors尺寸3个限制输出大小）: 使用COCO检测数据集和完整的ImageNet版本中的前9000个类来创建的组合数据集。该数据集的相应WordTree有9418个类别。ImageNet是一个更大的数据集，所以通过对COCO进行过采样来平衡数据集，使得ImageNet仅仅大于4:1的比例。当分析YOLO9000在ImageNet上的表现时，发现它很好地学习了新的动物种类，但是却在像服装和设备这样的学习类别中挣扎。新动物更容易学习，因为目标预测可以从COCO中的动物泛化的很好。相反，COCO没有任何类型的衣服的边界框标签，只针对人，因此效果不好3</li>\n</ul>\n</li>\n</ol>\n<h3 id=\"YOLO-V3\"><a href=\"#YOLO-V3\" class=\"headerlink\" title=\"YOLO V3\"></a>YOLO V3</h3><p><a href=\"https://pjreddie.com/media/files/papers/YOLOv3.pdf\" target=\"_blank\" rel=\"noopener\">论文地址</a></p>\n<p>改进：</p>\n<ol>\n<li>将YOLO V3替换了V2中的Softmax loss变成Logistic loss(每个类一个logistic)，而且每个GT只匹配一个先验框.</li>\n<li>Anchor bbox prior不同：V2用了5个anchor，V3用了9个anchor，提高了IOU.</li>\n<li>Detection的策略不同：V2只有一个detection，V3设置有3个，分别是一个下采样的，Feature map为13*13，还有2个上采样的eltwise sum(feature pyramid networks)，Feature map分别为26<em>\\26和52\\</em>52，也就是说，V3的416版本已经用到了52的Feature map，而V2把多尺度考虑到训练的data采样上，最后也只是用到了13的Feature map，这应该是对小目标影响最大的地方。<br>总结：<img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/11.png\" alt=\"image\"></li>\n<li>网络改进 DarkNet-53: 融合了YOLOv2、Darknet-19以及其他新型残差网络，由连续的3×3和1×1卷积层组合而成，当然，其中也添加了一些shortcut connection，整体体量也更大。因为一共有53个卷积层。<img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/12.png\" alt=\"image\"></li>\n<li>YOLO V3在Pascal Titan X上处理608x608图像速度达到20FPS，在 COCO test-dev 上 <a href=\"mailto:mAP@0.5\" target=\"_blank\" rel=\"noopener\">mAP@0.5</a> 达到 57.9%，与RetinaNet的结果相近，并且速度快了4倍。  YOLO V3的模型比之前的模型复杂了不少，可以通过改变模型结构的大小来权衡速度与精度。  速度对比如下：<img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/13.png\" alt=\"image\"></li>\n<li>失败的尝试：<ul>\n<li>Anchor box坐标的偏移预测</li>\n<li>用线性方法预测x,y，而不是使用逻辑方法</li>\n<li>focal loss</li>\n<li>双IOU阈值和真值分配</li>\n</ul>\n</li>\n</ol>\n<h3 id=\"SSD-Single-Shot-MultiBox-Detector\"><a href=\"#SSD-Single-Shot-MultiBox-Detector\" class=\"headerlink\" title=\"SSD: Single Shot MultiBox Detector\"></a>SSD: Single Shot MultiBox Detector</h3><p><a href=\"https://arxiv.org/pdf/1512.02325.pdf\" target=\"_blank\" rel=\"noopener\">论文地址</a></p>\n<p>SSD将边界框的输出空间离散化为不同长宽比的一组默认框和并缩放每个特征映射的位置。在预测时，网络会在每个默认框中为每个目标类别的出现生成分数，并对框进行调整以更好地匹配目标形状。此外，网络还结合了不同分辨率的多个特征映射的预测，自然地处理各种尺寸的目标。</p>\n<p><strong>改进</strong>:</p>\n<ol>\n<li>针对多个类别的单次检测器</li>\n<li>预测固定的一系列默认边界框的类别分数和边界框偏移，使用更小的卷积滤波器应用到特征映射上</li>\n<li>根据不同尺度的特征映射生成不同尺度的预测，并通过纵横比明确分开预测</li>\n<li>在低分辨率输入图像上也能实现简单的端到端训练和高精度，从而进一步提高速度与精度之间的权衡。</li>\n</ol>\n<p><strong>模型</strong>:<br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/15.png\" alt=\"image\"><br>SSD方法基于前馈卷积网络，该网络产生固定大小的边界框集合，并对这些边界框中存在的目标类别实例进行评分，然后进行非极大值抑制步骤来产生最终的检测结果。早期的网络层基于用于高质量图像分类的标准架构将其称为基础网络。然后，将辅助结构添加到网络中以产生具有以下关键特征的检测：</p>\n<ul>\n<li><strong>用于检测的多尺度特征映射</strong>。我们将卷积特征层添加到截取的基础网络的末端。这些层在尺寸上逐渐减小，并允许在多个尺度上对检测结果进行预测。用于预测检测的卷积模型对于每个特征层都是不同的</li>\n<li><strong>用于检测的卷积预测器</strong>。每个添加的特征层（或者任选的来自基础网络的现有特征层）可以使用一组卷积滤波器产生固定的检测预测集合。</li>\n<li><strong>默认边界框和长宽比</strong>。默认边界框与Faster R-CNN[2]中使用的锚边界框相似，但是我们将它们应用到不同分辨率的几个特征映射上。在几个特征映射中允许不同的默认边界框形状可以有效地离散可能的输出框形状的空间。<img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/16.png\" alt=\"image\"></li>\n<li>总结： 末尾添加的特征层预测不同尺度的长宽比的默认边界框的偏移量以及相关的置信度。PS: 空洞版本VGG更快。</li>\n</ul>\n<p><strong>训练</strong>：</p>\n<ol>\n<li><strong>匹配策略</strong>：默认边界框匹配到IOU重叠高于阈值（0.5）的任何实际边界框。这简化了学习问题，允许网络为多个重叠的默认边界框预测高分，而不是要求它只挑选具有最大重叠的一个边界框。</li>\n<li><strong>训练目标函数</strong>：定位损失加上置信度损失<img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/17.png\" alt=\"image\"></li>\n<li><strong>为默认边界框选择尺度和长宽比</strong>: <img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/18.png\" alt=\"image\"></li>\n<li><strong>难例挖掘</strong>: 在匹配步骤之后，大多数默认边界框为负例，尤其是当可能的默认边界框数量较多时。这在正的训练实例和负的训练实例之间引入了显著的不平衡。不使用所有负例，而是使用每个默认边界框的最高置信度损失来排序它们，并挑选最高的置信度，以便负例和正例之间的比例至多为3:1。</li>\n<li><strong>数据增强</strong><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/20.png\" alt=\"image\"></li>\n<li><strong>小目标数据增强</strong>： 将图像随机放置在填充了平均值的原始图像大小为16x的画布上，然后再进行任意的随机裁剪操作。因为通过引入这个新的“扩展”数据增强技巧，有更多的训练图像，所以必须将训练迭代次数加倍。</li>\n</ol>\n<p><strong>优劣</strong>:<br>SSD对类似的目标类别（特别是对于动物）有更多的混淆，部分原因是共享多个类别的位置。SSD对边界框大小非常敏感。换句话说，它在较小目标上比在较大目标上的性能要差得多。这并不奇怪，因为这些小目标甚至可能在顶层没有任何信息。增加输入尺寸（例如从300×300到512×512）可以帮助改进检测小目标，但仍然有很大的改进空间。积极的一面，SSD在大型目标上的表现非常好。而且对于不同长宽比的目标，它是非常鲁棒的，因为使用每个特征映射位置的各种长宽比的默认框。</p>\n<h3 id=\"RetinaNet-Focal-Loss-for-Dense-Object-Detection\"><a href=\"#RetinaNet-Focal-Loss-for-Dense-Object-Detection\" class=\"headerlink\" title=\"RetinaNet Focal Loss for Dense Object Detection\"></a>RetinaNet Focal Loss for Dense Object Detection</h3><p><a href=\"https://arxiv.org/abs/1708.02002\" target=\"_blank\" rel=\"noopener\">论文地址</a></p>\n<p><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/32.png\" alt=\"image\"></p>\n<p>稠密分类的后者精度不够高，核心问题（central issus）是稠密proposal中前景和背景的极度不平衡。以我更熟悉的YOLO举例子，比如在PASCAL VOC数据集中，每张图片上标注的目标可能也就几个。但是YOLO V2最后一层的输出是13×13×5，也就是845个候选目标！大量（简单易区分）的负样本在loss中占据了很大比重，使得有用的loss不能回传回来。基于此，作者将经典的交叉熵损失做了变形（见下），给那些易于被分类的简单例子小的权重，给不易区分的难例更大的权重。同时，作者提出了一个新的one-stage的检测器RetinaNet，达到了速度和精度很好地trade-off。</p>\n<p>$FL(p_{t}=-(1-p_{t})^{\\gamma}log(p_{t})$</p>\n<p><strong>Focal Loss</strong><br>Focal Loss从交叉熵损失而来。二分类的交叉熵损失如下：<br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/33.png\" alt=\"image\"></p>\n<p>对应的，多分类的交叉熵损失是这样的：</p>\n<p>$CE(p,y)=-log(p_{y})$</p>\n<p>因此可以使用添加权重的交叉熵损失：<br>$CE(p)=-\\alpha_{t}log(p_{t})$</p>\n<p>而作者提出的是一个自适应调节的权重：(可加入权重$\\alpha$平衡)</p>\n<p><strong>$FL(p_{t}=-(1-p_{t})^{\\gamma}log(p_{t})$</strong></p>\n<p>Pytorch实现：</p>\n<p>$L=-\\sum_{i}^{C}onehot\\odot(1-p_{t})^{\\gamma}log(p_{t})$</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">import</span> torch.nn <span class=\"keyword\">as</span> nn</span><br><span class=\"line\"><span class=\"keyword\">import</span> torch.nn.functional <span class=\"keyword\">as</span> F</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch.autograd <span class=\"keyword\">import</span> Variable</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">one_hot</span><span class=\"params\">(index, classes)</span>:</span></span><br><span class=\"line\">    size = index.size() + (classes,)</span><br><span class=\"line\">    view = index.size() + (<span class=\"number\">1</span>,)</span><br><span class=\"line\">    mask = torch.Tensor(*size).fill_(<span class=\"number\">0</span>)</span><br><span class=\"line\">    index = index.view(*view)</span><br><span class=\"line\">    ones = <span class=\"number\">1.</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> isinstance(index, Variable):</span><br><span class=\"line\">        ones = Variable(torch.Tensor(index.size()).fill_(<span class=\"number\">1</span>))</span><br><span class=\"line\">        mask = Variable(mask, volatile=index.volatile)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> mask.scatter_(<span class=\"number\">1</span>, index, ones)</span><br><span class=\"line\">    </span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">FocalLoss</span><span class=\"params\">(nn.Module)</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self, gamma=<span class=\"number\">0</span>, eps=<span class=\"number\">1e-7</span>)</span>:</span></span><br><span class=\"line\">        super(FocalLoss, self).__init__()</span><br><span class=\"line\">        self.gamma = gamma</span><br><span class=\"line\">        self.eps = eps</span><br><span class=\"line\">        </span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">forward</span><span class=\"params\">(self, input, target)</span>:</span></span><br><span class=\"line\">        y = one_hot(target, input.size(<span class=\"number\">-1</span>))</span><br><span class=\"line\">        logit = F.softmax(input)</span><br><span class=\"line\">        logit = logit.clamp(self.eps, <span class=\"number\">1.</span> - self.eps)</span><br><span class=\"line\">        loss = <span class=\"number\">-1</span> * y * torch.log(logit) <span class=\"comment\"># cross entropy</span></span><br><span class=\"line\">        loss = loss * (<span class=\"number\">1</span> - logit) ** self.gamma <span class=\"comment\"># focal loss</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> loss.sum()</span><br></pre></td></tr></table></figure>\n<p><strong>模型</strong>:</p>\n<ol>\n<li>模型初始化： 对于一般的分类网络，初始化之后，往往其输出的预测结果是均等的（随机猜测）。然而作者认为，这种初始化方式在类别极度不均衡的时候是有害的。作者提出，应该初始化模型参数，使得初始化之后，模型输出稀有类别的概率变小（如0.01），作者发现这种初始化方法对于交叉熵损失和Focal Loss的性能提升都有帮助。首先，从imagenet预训练得到的base net不做调整，新加入的卷积层权重均初始化为$\\sigma$=0.01的高斯分布，偏置项为0.对于分类网络的最后一个卷积层，偏置项为$b=-log(\\frac{(1-\\pi)}{\\pi})$, $\\pi$是一个超参数，其意义是在训练的初始阶段，每个anchor被分类为前景的概率。<br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/34.png\" alt=\"image\"></li>\n</ol>\n<p>作者利用前面介绍的发现和结论，基于ResNet和Feature Pyramid Net（FPN）设计了一种新的one-stage检测框架.RetinaNet 是由一个骨干网络和两个特定任务子网组成的单一网络。骨感网络负责在整个输入图像上计算卷积特征图，并且是一个现成的卷积网络。</p>\n<ul>\n<li>第一个子网在骨干网络的输出上执行卷积对象分类(small FCN attached to each FPN level)子网的参数在所有金字塔级别共享</li>\n<li>第二个子网执行卷积边界框回归(attach another samll FCN to each pyramid level)</li>\n<li>对象分类子网和框回归子网，尽管共享一个共同的结构，使用单独的参数。</li>\n</ul>\n<p>Anchors:<br>在金字塔等级P3到P7上，锚点的面积分别为$32^{2}$到$512^{2}$, 使用的长宽比为[1:2,1:1,2:1]. 对于更密集的比例覆盖，每个级别添加锚点的尺寸$[2^{0},2^{\\frac{1}{3}},2^{\\frac{1}{2}}]$</p>\n<p>IOU:<br>[0,0.4)的为背景，[0.4,0.5)的忽略，大于0.5的为前景</p>\n<h2 id=\"Two-Stage\"><a href=\"#Two-Stage\" class=\"headerlink\" title=\"Two Stage\"></a>Two Stage</h2><h3 id=\"R-FCN-Object-Detection-via-Region-based-Fully-Convolutional-Networks\"><a href=\"#R-FCN-Object-Detection-via-Region-based-Fully-Convolutional-Networks\" class=\"headerlink\" title=\"R-FCN Object Detection via Region-based Fully Convolutional Networks\"></a>R-FCN Object Detection via Region-based Fully Convolutional Networks</h3><p><a href=\"https://link.jianshu.com/?t=https%3A%2F%2Farxiv.org%2Fpdf%2F1605.06409.pdf\" target=\"_blank\" rel=\"noopener\">论文地址</a></p>\n<p>R-FCN 通过添加 Position-sensitive score map 解决了把 ROI pooling 放到网络最后一层降低平移可变性的问题，以此改进了 Faster R-CNN 中检测速度慢的问题。</p>\n<p>PS: </p>\n<ul>\n<li><p>分类需要特征具有平移不变性，检测则要求对目标的平移做出准确响应。论文中作者给了测试的数据：ROI放在ResNet-101的conv5后，mAP是68.9%；ROI放到conv5前（就是标准的Faster R-CNN结构）的mAP是76.4%，差距是巨大的，这能证明平移可变性对目标检测的重要性。</p>\n</li>\n<li><p>Faster R-CNN检测速度慢的问题，速度慢是因为ROI层后的结构对不同的proposal是不共享的，试想下如果有300个proposal，ROI后的全连接网络就要计算300次, 非常耗时。</p>\n</li>\n</ul>\n<p><strong>模型</strong>：<br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/19.png\" alt=\"image\"></p>\n<ul>\n<li>Backbone architecture: ResNet-101有100个卷积层，后面是全局平均池化和1000类的全连接层。删除了平均池化层和全连接层，只使用卷积层来计算特征映射。最后一个卷积块是2048维，附加一个随机初始化的1024维的1×1卷积层来降维</li>\n<li>$k^{2}(C+1)$Conv: ResNet101的输出是W*H*1024，用$k^{2}(C+1)$个1024*1*1的卷积核去卷积即可得到$k^{2}(C+1)$个大小为W*H的position sensitive的score map。这步的卷积操作就是在做prediction。k = 3，表示把一个ROI划分成3*3，对应的9个位置</li>\n<li>ROI pooling: 一层的SPP结构。主要用来将不同大小的ROI对应的feature map映射成同样维度的特征<img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/21.png\" alt=\"image\"></li>\n<li>Vote:k*k个bin直接进行求和（每个类单独做）得到每一类的score，并进行softmax得到每类的最终得分，并用于计算损失<img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/22.png\" alt=\"image\"></li>\n</ul>\n<p><strong>训练</strong>:</p>\n<ul>\n<li>损失函数： $L(s,t_{x,y,w,h})=L_{cls}(s_{c^{\\star}}+ \\lambda [c^{\\star}&gt;0]L_{reg}(t,t^{\\star})$<br>将正样本定义为与真实边界框IOU至少为0.5的ROI，否则为负样本</li>\n<li>在线难例挖掘（<strong>OHEM</strong>): 其主要考虑训练样本集总是包含较多easy examples而相对较少hard examples，而自动选择困难样本能够使得训练更为有效，此外还有：S-OHEM: Stratified Online Hard Example Mining for Object Detection. <strong>S-OHEM</strong> 利用OHEM和stratified sampling技术。其主要考虑OHEM训练过程忽略了不同损失分布的影响，因此S-OHEM根据分布抽样训练样本。A-Fast-RCNN: Hard positive generation via adversary for object detection从更好的利用数据的角度出发，OHEM和S-OHEM都是发现困难样本，而A-Fast-RCNN的方法则是通过<strong>GAN</strong>的方式在特征空间产生具有部分遮挡和形变的困难样本。</li>\n<li><strong>空洞和步长</strong>:我们的全卷积架构享有FCN广泛使用的语义分割的网络修改的好处。特别的是，将ResNet-101的有效步长从32像素降低到了16像素，增加了分数图的分辨率。第一个conv5块中的stride=2操作被修改为stride=1，并且conv5阶段的所有卷积滤波器都被“hole algorithm” 修改来弥补减少的步幅.</li>\n</ul>\n<p><strong>位置敏感分数图</strong>：<br>我们可以想想一下这种情况，M 是一个 5*5 大小，有一个蓝色的正方形物体在其中的特征图，我们将方形物体平均分割成 3*3 的区域。现在我们从 M 中创建一个新的特征图并只用其来检测方形区域的左上角。这个新的特征图如下右图，只有黄色网格单元被激活<img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/23.png\" alt=\"image\"></p>\n<p>因为我们将方形分为了 9 个部分，我们可以创建 9 张特征图分别来检测对应的物体区域。因为每张图检测的是目标物体的子区域，所以这些特征图被称为位置敏感分数图（position-sensitive score maps）。<img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/24.png\" alt=\"image\"><br>比如，我们可以说，下图由虚线所画的红色矩形是被提议的 ROIs 。我们将其分为 3*3 区域并得出每个区域可能包含其对应的物体部分的可能性。我们将此结果储存在 3*3 的投票阵列（如下右图）中。比如，投票阵列 [0][0] 中数值的意义是在此找到方形目标左上区域的可能性。<img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/25.png\" alt=\"image\"><br>将分数图和 ROIs 映射到投票阵列的过程叫做位置敏感 ROI 池化（position-sensitive ROI-pool）。<br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/26.png\" alt=\"image\"><br>在计算完位置敏感 ROI 池化所有的值之后，分类的得分就是所有它元素的平均值<br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/27.png\" alt=\"image\"><br>如果说我们有 C 类物体需要检测。我们将使用 C+1个类，因为其中多包括了一个背景（无目标物体）类。每类都分别有一个 3×3 分数图，因此一共有 (C+1)×3×3 张分数图。通过使用自己类别的那组分数图，我们可以预测出每一类的分数。然后我们使用 softmax 来操作这些分数从而计算出每一类的概率。</p>\n<h3 id=\"FPN-Feature-Pyramid-Networks-for-Object-Detection\"><a href=\"#FPN-Feature-Pyramid-Networks-for-Object-Detection\" class=\"headerlink\" title=\"FPN Feature Pyramid Networks for Object Detection\"></a>FPN Feature Pyramid Networks for Object Detection</h3><p><a href=\"http://openaccess.thecvf.com/content_cvpr_2017/papers/Lin_Feature_Pyramid_Networks_CVPR_2017_paper.pdf\" target=\"_blank\" rel=\"noopener\">论文地址</a></p>\n<p><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/28.png\" alt=\"image\"></p>\n<p>a. 通过缩放图片获取不同尺度的特征图</p>\n<p>b. ConvNET</p>\n<p>c. 通过不同特征分层</p>\n<p>d. FPN</p>\n<p>特征金字塔网络FPN，网络直接在原来的单网络上做修改，每个分辨率的 feature map 引入后一分辨率缩放两倍的 feature map 做 element-wise 相加的操作。通过这样的连接，每一层预测所用的 feature map 都融合了不同分辨率、不同语义强度的特征，融合的不同分辨率的 feature map 分别做对应分辨率大小的物体检测。这样保证了每一层都有合适的分辨率以及强语义特征。同时，由于此方法只是在原网络基础上加上了额外的跨层连接，在实际应用中几乎不增加额外的时间和计算量。</p>\n<p><strong>FPN</strong>:<br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/29.png\" alt=\"image\"></p>\n<ol>\n<li>图中feature map用蓝色轮廓表示，较粗的表示语义上较强特征</li>\n<li>采用的bottom-up 和 top-down 的方法， bottom-up这条含有较低级别的语义但其激活可以更精确的定位因为下采样的次数更少。 Top-down的这条路更粗糙但是语义更强。</li>\n<li>top-down的特征随后通过bottom-up的特征经由横向连接进行增强如图，使用较粗糙分辨率的特征映射时候将空间分辨率上采样x2倍。bottom-up的特征要经过1x1卷积层来生成最粗糙分辨率映射。</li>\n<li>每个横向连接合并来自自下而上路径和自顶向下路径的具有相同空间大小的特征映射。<br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/30.png\" alt=\"image\"></li>\n</ol>\n<p><strong>RPN结合FPN</strong></p>\n<ol>\n<li>通过用FPN替换单尺度特征映射来适应RPN。在特征金字塔的每个层级上附加一个相同设计的头部（3x3 conv（目标/非目标二分类和边界框回归）和 两个1x1convs（分类和回归））由于头部在所有金字塔等级上的所有位置密集滑动，所以不需要在特定层级上具有多尺度锚点。相反，为每个层级分配单尺度的锚点。定义锚点$[P_{2},P_{3},P_{4},P_{5},P_{6}]$分别具有$[32^{2},64^{2},128^{2},256^{2},512^{2}]$个像素 面积，以及多个长宽比{1:2,1:1,2:1}所以总共15个锚点在金字塔上。</li>\n<li>其余同RPN网络</li>\n<li>不同的尺度ROI用不同层的特征，每个box根据公式计算后提取其中某一层特征图对应的特征ROI，大尺度就用后面一些的金字塔层（P5），小尺度就用前面一点的层（P4）<br>可根据公式：$k=[k_{0}+log_{2}(\\frac{\\sqrt{wh}}{224}]$ 计算需要哪一层的特征<br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/31.png\" alt=\"image\"></li>\n</ol>\n<h3 id=\"Light-Head-R-CNN-In-Defense-of-Two-Stage-Object-Detector\"><a href=\"#Light-Head-R-CNN-In-Defense-of-Two-Stage-Object-Detector\" class=\"headerlink\" title=\"Light-Head R-CNN: In Defense of Two-Stage Object Detector\"></a>Light-Head R-CNN: In Defense of Two-Stage Object Detector</h3><p><a href=\"https://arxiv.org/abs/1711.07264\" target=\"_blank\" rel=\"noopener\">论文地址</a></p>\n<p>two-stage 的方法在本身的一个基础网络上都会附加计算量很大的一个用于 classification+regression 的网络，导致速度变慢</p>\n<ul>\n<li>Faster R-CNN: two fully connected layers for RoI recognition</li>\n<li>R-FCN: produces a large score maps.</li>\n</ul>\n<p>因此，作者为了解决 detection 的速度问题，提出了一种新的 two-stage detector，就是Light-Head R-CNN。速度和准确率都有提升。Light-Head R-CNN 重点是 head 的结构设计。包括两部分： R-CNN subnet（ROI pooling 之后的network） 和ROI warping。</p>\n<p><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/35.png\" alt=\"image\"></p>\n<p><strong>方法</strong></p>\n<ul>\n<li><strong>Thin feature map</strong>: <img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/36.png\" alt=\"image\">降低进入head部分feature map的channel数也就是将r-fcn的score map从$P\\times P(C+1)$减小到$P\\times P\\times \\alpha$，$\\alpha$是一个与类别数无关且较小的值，比如10。这样，score map的channel数与类别数无关，使得后面的分类不能像r-fcn那样vote，于是在roi pooling之后添加了一个fc层进行预测类别和位置。</li>\n<li><strong>Large separable convolution</strong>: </li>\n</ul>\n<p><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/38.png\" alt=\"image\"><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/39.png\" alt=\"image\">  </p>\n<ul>\n<li><strong>Cheap R-CNN</strong>: <img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/Object-detection-learning/37.png\" alt=\"image\"> </li>\n</ul>\n<p><strong>模型</strong>：</p>\n<ul>\n<li><strong>Large</strong>: (1) ResNEt-101; (2) atrous algorithm; (3) RPN chanels 512; (4) Large separable convolution with $C_{mid}=256$</li>\n<li><strong>Small</strong>: (1) Xception like; (2) abbandon atrous algorithm; (3) RPN convolution to 256; (4) Large separable convolution with $C_{mid}=64$; (5) Apply PSPooling with alignment techniques as RoI warping(better results involve RoI-align).</li>\n</ul>"},{"title":"(Logbook) -- Object Detection System Based on CNN and Capsule Network","date":"2018-05-24T16:00:00.000Z","_content":"\n## Gantt chart\n![image](https://raw.githubusercontent.com/Trouble404/Object-Detection-System-Based-on-CNN-and-Capsule-Network/master/Project%20Plan/gantt%20chart%20of%20project.PNG)\n<!-- more -->\n---\n\n## Check list\n- [x] **1) preparation**\n- [x] ***1.1) Familiarization with develop tools***\n- [x] 1.1.1) Keras\n- [x] 1.1.2) Pythrch\n- [x] ***1.2) Presentation***\n- [x] 1.2.1) Poster conference\n- [x] **2) Create image database**\n- [x] 2.1) Confirmation of detected objects\n- [x] 2.2) Collect and generate the dataset\n- [x] **3) Familiarization with CNN based object detection methods**\n- [x] 3.1) R-CNN\n- [x] 3.2) SPP-net\n- [x] 3.3) Fast R-CNN\n- [x] 3.4) Faster R-CNN\n- [x] **4) Implement object detection system based on one chosen CNN method**\n- [x] 4.1) Pre-processing of images\n- [x] 4.2) Extracting features\n- [x] 4.3) Mode architecture\n- [x] 4.4) Train model and optimization\n- [x] 4.5) Models ensemble\n- [x] **5) Analysis work**\n- [x] 5.1) Evaluation of detection result.\n- [x] **6) Paperwork and bench inspection**\n- [x] 6.1) Logbook\n- [x] 6.2) Write the thesis\n- [x] 6.3) Project video\n- [x] 6.4) Speech and ppt of bench inspection\n- [x] **7) Documents**\n- [x] 7.1) Project Brief\n\n---\n\n## May\n### 【28/05/2018】\nKeras is a high-level neural networks API, written in Python and capable of running on top of [TensorFlow](https://github.com/tensorflow/tensorflow), CNTK, or Theano.\n\n* **[Keras document](https://keras.io/)**\n\n* **[Keras 文档](https://keras-cn.readthedocs.io/en/latest/#keraspython)**\n\n---\n#### Installation\n\n* **TensorFlow**\n  [Microsoft Visual Studio 2015](https://www.visualstudio.com/zh-hans/vs/older-downloads/)\n  [CUDA 9.0](http://docs.nvidia.com/cuda/cuda-installation-guide-microsoft-windows/)\n  [cuDNN7](https://developer.nvidia.com/cudnn)\n  [Anaconda](https://www.anaconda.com/download/)\n  \n  * Step 1: Install VS2015\n  * Step 2: Install CUDA 9.0 并添加环境变量\n  * Step 3: Install cuDNN7 解压后把cudnn目录下的bin目录加到PATH环境变量里\n  * Step 4: Install Anaconda 把安装路径添加到PATH里去, 在这里我用了 **Python 3.5**\n  * Step 5: 使用Anaconda的命令行 新建一个虚拟环境,激活并且关联到jupyterbook\n  {% codeblock %}\n  conda create  --name tensorflow python=3.5\n  activate tensorflow\n  conda install nb_conda\n  {% endcodeblock %}\n  * Step 6: Install GPU version TensorFlow.\n  {% codeblock %}\n  pip install -i https://pypi.tuna.tsinghua.edu.cn/simple --ignore-installed --upgrade tensorflow-gpu \n  {% endcodeblock %}\n  \n* **Keras**\n  * Step 1: 启动之前的 虚拟环境， 并且安装Keras GPU 版本\n    {% codeblock %}\n    activate tensorflow\n    pip install keras -U --pre\n    {% endcodeblock %}\n  \n#### 在硕士学习过程中，使用Keras的项目**\n* **[NBA with Machine Learning](https://github.com/Trouble404/NBA-with-Machine-Learning)**\n* **[Kaggle- Job salary prediction](https://github.com/Trouble404/kaggle-Job-Salary-Prediction)**\n \n#### TensorFlow CPU 切换\n```python\nimport tensorflow as tf  \nimport os\nimport keras.backend.tensorflow_backend as KTF  \n\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  #设置需要使用的GPU的编号\nconfig = tf.ConfigProto()\nconfig.gpu_options.per_process_gpu_memory_fraction = 0.4 #设置使用GPU容量占GPU总容量的比例\nsess = tf.Session(config=config)\nKTF.set_session(sess)\n\nwith tf.device('/cpu:0'):\n```\n这样可以在GPU版本的虚拟环境里面使用CPU计算\n\n#### Jupyter Notebook 工作目录设置\n启动命令行，切换至预设的工作目录， 运行：\n```\njupyter notebook --generate-config\n```\n![image](https://raw.githubusercontent.com/Trouble404/Object-Detection-System-Based-on-CNN-and-Capsule-Network/master/Learned%20skills/jupyter%20_setting.PNG)\n\n## June\n### 【01/06/2018】\n**[PyTorch](https://pytorch.org/about/)** is a python package that provides two high-level features:\n* Tensor computation (like numpy) with strong GPU acceleration\n* Deep Neural Networks built on a tape-based autodiff system\n\n| Package | Description |\n|:----|:----|\n|torch|a Tensor library like NumPy, with strong GPU support|\n|torch.autograd|a tape based automatic differentiation library that supports all differentiable Tensor operations in torch|\n|torch.nn|a neural networks library deeply integrated with autograd designed for maximum flexibility|\n|torch.optim|an optimization package to be used with torch.nn with standard optimization methods such as SGD, RMSProp, LBFGS, Adam etc.|\n|torch.multiprocessing|python multiprocessing, but with magical memory sharing of torch Tensors across processes. Useful for data loading and hogwild training.|\n|torch.utils|DataLoader, Trainer and other utility functions for convenience|\n|torch.legacy(.nn/.optim)|legacy code that has been ported over from torch for backward compatibility reasons|\n\n---\n\n#### Installation\n\n* Step 1: 使用Anaconda的命令行 新建一个虚拟环境,激活并且关联到jupyterbook\n  {% codeblock %}\n  conda create  --name pytorch python=3.5\n  activate pytorch\n  conda install nb_conda\n  {% endcodeblock %}\n* Step 2: Install GPU version PyTorch.\n  {% codeblock %}\n  conda install pytorch cuda90 -c pytorch \n  pip install torchvision\n  {% endcodeblock %}\n\n#### Understanding of PyTorch\n\n* **Tensors**\n  Tensors和numpy中的ndarrays较为相似, 与此同时Tensor也能够使用GPU来加速运算\n  ```python\n  from __future__ import print_function\n  import torch\n  x = torch.Tensor(5, 3)  # 构造一个未初始化的5*3的矩阵\n  x = torch.rand(5, 3)  # 构造一个随机初始化的矩阵\n  x # 此处在notebook中输出x的值来查看具体的x内容\n  x.size()\n\n  #NOTE: torch.Size 事实上是一个tuple, 所以其支持相关的操作*\n  y = torch.rand(5, 3)\n\n  #此处 将两个同形矩阵相加有两种语法结构\n  x + y # 语法一\n  torch.add(x, y) # 语法二\n\n  # 另外输出tensor也有两种写法\n  result = torch.Tensor(5, 3) # 语法一\n  torch.add(x, y, out=result) # 语法二\n  y.add_(x) # 将y与x相加\n\n  # 特别注明：任何可以改变tensor内容的操作都会在方法名后加一个下划线'_'\n  # 例如：x.copy_(y), x.t_(), 这俩都会改变x的值。\n\n  #另外python中的切片操作也是资次的。\n  x[:,1] #这一操作会输出x矩阵的第二列的所有值\n  ```\n\n* **Numpy桥**\n  将Torch的Tensor和numpy的array相互转换简，注意Torch的Tensor和numpy的array会共享他们的存储空间，修改一个会导致另外的一个也被修改。\n  \n  ```python\n# 此处演示tensor和numpy数据结构的相互转换\na = torch.ones(5)\nb = a.numpy()\n\n# 此处演示当修改numpy数组之后,与之相关联的tensor也会相应的被修改\na.add_(1)\nprint(a)\nprint(b)\n\n# 将numpy的Array转换为torch的Tensor\nimport numpy as np\na = np.ones(5)\nb = torch.from_numpy(a)\nnp.add(a, 1, out=a)\nprint(a)\nprint(b)\n\n# 另外除了CharTensor之外，所有的tensor都可以在CPU运算和GPU预算之间相互转换\n# 使用CUDA函数来将Tensor移动到GPU上\n# 当CUDA可用时会进行GPU的运算\nif torch.cuda.is_available():\n    x = x.cuda()\n    y = y.cuda() \n  ```\n* **使用PyTorch设计一个CIFAR10数据集的分类模型**\n**[code](https://github.com/Trouble404/Object-Detection-System-Based-on-CNN-and-Capsule-Network/blob/master/Learned%20skills/pytorch.ipynb)**\n\n* **MMdnn**\n  MMdnn is a set of tools to help users inter-operate among different deep learning frameworks. E.g. model conversion and visualization. Convert models between Caffe, Keras, MXNet, Tensorflow, CNTK, PyTorch Onnx and CoreML.\n  \n  ![iamge](https://raw.githubusercontent.com/Microsoft/MMdnn/master/docs/supported.jpg)\n  \n  MMdnn主要有以下特征：\n\n  * 模型文件转换器，不同的框架间转换DNN模型\n  * 模型代码片段生成器，生成适合不同框架的代码\n  * 模型可视化，DNN网络结构和框架参数可视化\n  * 模型兼容性测试（正在进行中）\n \n **[Github](https://github.com/Microsoft/MMdnn)**\n \n### 【04/06/2018】\n#### **Dataset:**\n **[VOC 2012 Dataset](http://host.robots.ox.ac.uk/pascal/VOC/voc2012/index.html)**\n \n#### **Introduce:**\n **Visual Object Classes Challenge 2012 (VOC2012)**\n[PASCAL](http://host.robots.ox.ac.uk/pascal/VOC/)'s full name is Pattern Analysis, Statistical Modelling and Computational Learning.\nVOC's full name is **Visual OBject Classes**.\nThe first competition was held in 2005 and terminated in 2012. I will use the last updated dataset which is [VOC2012](http://host.robots.ox.ac.uk/pascal/VOC/voc2012/index.html) dataset.\n\nThe main aim of this competition is object detection, there are 20 classes objects in the dataset:\n* person\n* bird, cat, cow, dog, horse, sheep\n* aeroplane, bicycle, boat, bus, car, motorbike, train\n* bottle, chair, dining table, potted plant, sofa, tv/monitor\n\n#### **Detection Task**\nReferenced: \n**The PASCAL Visual Object Classes Challenge 2012 (VOC2012) Development Kit**\n**Mark Everingham - John Winn**\nhttp://host.robots.ox.ac.uk/pascal/VOC/voc2012/htmldoc/index.html\n\n**Task:**\nFor each of the twenty classes predict the bounding boxes of each object of that class in a test image (if any). Each bounding box should be output with an associated real-valued confidence of the detection so that a precision/recall curve can be drawn. Participants may choose to tackle all, or any subset of object classes, for example 'cars only' or 'motorbikes and cars'.\n\n**Competitions**:\nTwo competitions are defined according to the choice of training data:\n*  taken from the $VOC_{trainval}$ data provided.\n*  from any source excluding the $VOC_{test}$ data provided.\n\n**Submission of Results**:\nA separate text file of results should be generated for each competition and each class e.g. \\`car'. Each line should be a detection output by the detector in the following format:\n    ```\n    <image identifier> <confidence> <left> <top> <right> <bottom>\n    ```\nwhere (left,top)-(right,bottom) defines the bounding box of the detected object. The top-left pixel in the image has coordinates $(1,1)$. Greater confidence values signify greater confidence that the detection is correct. An example file excerpt is shown below. Note that for the image 2009_000032, multiple objects are detected:\n```\ncomp3_det_test_car.txt:\n    ...\n    2009_000026 0.949297 172.000000 233.000000 191.000000 248.000000\n    2009_000032 0.013737 1.000000 147.000000 114.000000 242.000000\n    2009_000032 0.013737 1.000000 134.000000 94.000000 168.000000\n    2009_000035 0.063948 455.000000 229.000000 491.000000 243.000000\n    ...\n```\n\n**Evaluation**:\nThe detection task will be judged by the precision/recall curve. The principal quantitative measure used will be the average precision (AP). Detections are considered true or false positives based on the area of overlap with ground truth bounding boxes. To be considered a correct detection, the area of overlap $a_o$ between the predicted bounding box $B_p$ and ground truth bounding box $B_{gt}$ must exceed $50\\%$ by the formula: \n<center> $a_o = \\frac{area(B_p \\cap B_{gt})}{area(B_p \\cup B_{gt})}$ </center>\n\n #### **XML标注格式**\n 对于目标检测来说，每一张图片对应一个xml格式的标注文件。所以你会猜到，就像gemfield准备的训练集有8万张照片一样，在存放xml文件的目录里，这里也将会有8万个xml文件。下面是其中一个xml文件的示例：\n ```html\n <?xml version=\"1.0\" encoding=\"utf-8\"?>\n<annotation>\n    <folder>VOC2007</folder>\n    <filename>test100.mp4_3380.jpeg</filename>\n    <size>\n        <width>1280</width>\n        <height>720</height>\n        <depth>3</depth>\n    </size>\n    <object>\n        <name>gemfield</name>\n        <bndbox>\n            <xmin>549</xmin>\n            <xmax>715</xmax>\n            <ymin>257</ymin>\n            <ymax>289</ymax>\n        </bndbox>\n        <truncated>0</truncated>\n        <difficult>0</difficult>\n    </object>\n    <object>\n        <name>civilnet</name>\n        <bndbox>\n            <xmin>842</xmin>\n            <xmax>1009</xmax>\n            <ymin>138</ymin>\n            <ymax>171</ymax>\n        </bndbox>\n        <truncated>0</truncated>\n        <difficult>0</difficult>\n    </object>\n    <segmented>0</segmented>\n</annotation>\n```\n\n在这个测试图片上，我们标注了2个object，一个是gemfield，另一个是civilnet。\n\n在这个xml例子中：\n* bndbox是一个轴对齐的矩形，它框住的是目标在照片中的可见部分；\n* truncated表明这个目标因为各种原因没有被框完整（被截断了），比如说一辆车有一部分在画面外；\n* occluded是说一个目标的重要部分被遮挡了（不管是被背景的什么东西，还是被另一个待检测目标遮挡）；\n* difficult表明这个待检测目标很难识别，有可能是虽然视觉上很清楚，但是没有上下文的话还是很难确认它属于哪个分类；标为difficult的目标在测试成绩的评估中一般会被忽略。\n\n**注意：在一个object中，name 标签要放在前面，否则的话，目标检测的一个重要工程实现SSD会出现解析数据集错误（另一个重要工程实现py-faster-rcnn则不会）。**\n\n### 【07/06/2018】\n#### **Poster conference**\n![iamge](https://raw.githubusercontent.com/Trouble404/Object-Detection-System-Based-on-CNN-and-Capsule-Network/master/Poster/poster.png)\n\n5 People in one group to present their object.\nI present this object to my supervisor in this conference.\n\n### 【11/06/2018】\n#### **R-CNN**\nPaper: [Rich feature hierarchies for accurate object detection and semantic segmentation](https://arxiv.org/abs/1311.2524)\n\n【**论文主要特点**】（相对传统方法的改进）\n\n* 速度： 经典的目标检测算法使用滑动窗法依次判断所有可能的区域。本文则(采用Selective Search方法)预先提取一系列较可能是物体的候选区域，之后仅在这些候选区域上(采用CNN)提取特征，进行判断。\n* 训练集： 经典的目标检测算法在区域中提取人工设定的特征。本文则采用深度网络进行特征提取。使用两个数据库： 一个较大的识别库   （ImageNet ILSVC 2012）：标定每张图片中物体的类别。一千万图像，1000类。 一个较小的检测库（PASCAL VOC 2007）：标定每张   图片中，物体的类别和位置，一万图像，20类。 本文使用识别库进行预训练得到CNN（有监督预训练），而后用检测库调优参数，最后在   检测库上评测。\n\n【**流程**】\n\n1. 候选区域生成： 一张图像生成1K~2K个候选区域 （采用Selective Search 方法）\n2. 特征提取： 对每个候选区域，使用深度卷积网络提取特征 （CNN） \n3. 类别判断： 特征送入每一类的SVM 分类器，判别是否属于该类\n4. 位置精修： 使用回归器精细修正候选框位置\n<center>![image](https://raw.githubusercontent.com/Trouble404/Object-Detection-System-Based-on-CNN-and-Capsule-Network/master/Research%20Review/LaTex/1.PNG)</center>\n\n【**[Selective Search](https://www.koen.me/research/pub/uijlings-ijcv2013-draft.pdf)**】\n1. 使用一种过分割手段，将图像分割成小区域 (1k~2k 个)\n2. 查看现有小区域，按照合并规则合并可能性最高的相邻两个区域。重复直到整张图像合并成一个区域位置\n3. 输出所有曾经存在过的区域，所谓候选区域\n   其中合并规则如下： 优先合并以下四种区域：\n   * 颜色（颜色直方图）相近的\n   * 纹理（梯度直方图）相近的\n   * 合并后总面积小的： 保证合并操作的尺度较为均匀，避免一个大区域陆续“吃掉”其他小区域 （例：设有区域a-b-c-d-e-f-g-h。较好的合并方式是：ab-cd-ef-gh -> abcd-efgh -> abcdefgh。 不好的合并方法是：ab-c-d-e-f-g-h ->abcd-e-f-g-h ->abcdef-gh -> abcdefgh）\n   * 合并后，总面积在其BBOX中所占比例大的： 保证合并后形状规则。\n\n### 【12/06/2018】\n#### **SPP-CNN**\nPaper: [Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition](https://arxiv.org/abs/1406.4729)\n\n【**论文主要特点**】（相对传统方法的改进）\n\nRCNN使用CNN作为特征提取器，首次使得目标检测跨入深度学习的阶段。但是RCNN对于每一个区域候选都需要首先将图片放缩到固定的尺寸（224\\*224），然后为每个区域候选提取CNN特征。容易看出这里面存在的一些性能瓶颈：\n* 速度瓶颈：重复为每个region proposal提取特征是极其费时的，Selective Search对于每幅图片产生2K左右个region proposal，也就是意味着一幅图片需要经过2K次的完整的CNN计算得到最终的结果。\n* 性能瓶颈：对于所有的region proposal防缩到固定的尺寸会导致我们不期望看到的几何形变，而且由于速度瓶颈的存在，不可能采用多尺度或者是大量的数据增强去训练模型。\n\n\n【**流程**】\n\n1. 首先通过selective search产生一系列的region proposal\n2. 然后训练多尺寸识别网络用以提取区域特征，其中处理方法是每个尺寸的最短边大小在尺寸集合中：\n   $s \\in S = \\{480,576,688,864,1200\\}$\n   训练的时候通过上面提到的多尺寸训练方法，也就是在每个epoch中首先训练一个尺寸产生一个model，然后加载这个model并训练第二个尺寸，直到训练完所有的尺寸。空间金字塔池化使用的尺度为：1\\*1，2\\*2，3\\*3，6\\*6，一共是50个bins。\n3. 在测试时，每个region proposal选择能使其包含的像素个数最接近224\\*224的尺寸，提取相 应特征。\n4. 训练SVM，BoundingBox回归.\n<center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/SPP-NET.jpg)</center>\n\n\n### 【13/06/2018】\n#### **FAST R-CNN**\nPaper: [Fast R-CNN](https://arxiv.org/abs/1504.08083)\n\n【**论文主要特点**】（相对传统方法的改进）\n\n* 测试时速度慢：RCNN一张图像内候选框之间大量重叠，提取特征操作冗余。本文将整张图像归一化后直接送入深度网络。在邻接时，才加入候选框信息，在末尾的少数几层处理每个候选框。\n* 训练时速度慢 ：原因同上。在训练时，本文先一张图像送入网络，紧接着送入从这幅图像上提取出的候选区域。这些候选区域的前几层特征不需要再重复计算。\n* 训练所需空间大: RCNN中独立的分类器和回归器需要大量特征作为训练样本。本文把类别判断和位置精调统一用深度网络实现，不再需要额外存储。\n\n\n【**流程**】\n\n1. 网络首先用几个卷积层（conv）和最大池化层处理整个图像以产生conv特征图。\n2. 然后，对于每个对象建议框（object proposals ），感兴趣区域（region of interest——RoI）池层从特征图提取固定长度的特征向量。\n3. 每个特征向量被输送到分支成两个同级输出层的全连接（fc）层序列中：\n   其中一层进行分类，对 目标关于K个对象类（包括全部“背景background”类）产生softmax概率估计，即输出每一个RoI的概率分布；\n另一层进行bbox regression，输出K个对象类中每一个类的四个实数值。每4个值编码K个类中的每个类的精确边界盒（bounding-box）位置，即输出每一个种类的的边界盒回归偏差。整个结构是使用多任务损失的端到端训练（trained end-to-end with a multi-task loss）。\n<center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/fast-rcnn.png)</center>\n\n### 【14~18/06/2018】\n#### **FASTER R-CNN**\nI want to use **Faster R-cnn** as the first method to implement object detection system.\n\nPaper: [Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks](https://arxiv.org/abs/1506.01497)\n\n在结构上，Faster RCNN已经将特征抽取(feature extraction)，proposal提取，bounding box regression(rect refine)，classification都整合在了一个网络中，使得综合性能有较大提高，在检测速度方面尤为明显。\n<center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster-rcnn.jpg)</center>\n\n #### 流程\n\n1. Conv layers：作为一种CNN网络目标检测方法，Faster R-CNN首先使用一组基础的conv+relu+pooling层提取image的feature maps。该feature maps被共享用于后续RPN层和全连接层。\n2. Region Proposal Networks：RPN网络用于生成region proposals。该层通过softmax判断anchors属于foreground或者background，再利用bounding box regression修正anchors获得精确的proposals。\n3. Roi Pooling：该层收集输入的feature maps和proposals，综合这些信息后提取proposal feature maps，送入后续全连接层判定目标类别。\n4. Classification：利用proposal feature maps计算proposal的类别，同时再次bounding box regression获得检测框最终的精确位置。\n\n#### 解释\n\n**\\[1. Conv layers\\]**\n   <center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_1.jpg)</center>\n   Conv layers包含了conv，pooling，relu三种层。以python版本中的VGG16模型中的faster_rcnn_test.pt的网络结构为例，如图,    Conv layers部分共有13个conv层，13个relu层，4个pooling层。这里有一个非常容易被忽略但是又无比重要的信息，在Conv          layers中：\n   \n  * 所有的conv层都是： $kernel\\_size=3$ ， $pad=1$ ， $stride=1$ <br>\n  * 所有的pooling层都是： $kernel\\_size=2$ ， $pad=0$ ， $stride=2$\n  \n   为何重要？在Faster RCNN Conv layers中对所有的卷积都做了扩边处理（ $pad=1$ ，即填充一圈0），导致原图变为                $(M+2)\\times (N+2)$ 大小，再做3x3卷积后输出 $M\\times N$ 。正是这种设置，导致Conv layers中的conv层不改变输入和输出    矩阵大小。如下图：\n<center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_2.jpg)</center>\n   类似的是，Conv layers中的pooling层 $kernel\\_size=2$ ， $stride=2$ 。这样每个经过pooling层的 $M\\times N$ 矩阵，都会变为 $(M/2) \\times(N/2)$ 大小。综上所述，在整个Conv layers中，conv和relu层不改变输入输出大小，只有pooling层使输出长宽都变为输入的1/2。\n那么，一个 $M\\times N$ 大小的矩阵经过Conv layers固定变为 $(M/16)\\times (N/16)$ ！这样Conv layers生成的featuure map中都可以和原图对应起来。\n\n**\\[2. Region Proposal Networks(RPN)\\]**\n   经典的检测方法生成检测框都非常耗时，如OpenCV adaboost使用滑动窗口+图像金字塔生成检测框；或如R-CNN使用SS(Selective      Search)方法生成检测框。而Faster RCNN则抛弃了传统的滑动窗口和SS方法，直接使用RPN生成检测框，这也是Faster R-CNN的巨大    优势，能极大提升检测框的生成速度。\n   <center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_3.jpg)</center>\n   上图展示了RPN网络的具体结构。可以看到RPN网络实际分为2条线，上面一条通过softmax分类anchors获得foreground和              background（检测目标是foreground），下面一条用于计算对于anchors的bounding box regression偏移量，以获得精确的          proposal。而最后的Proposal层则负责综合foreground anchors和bounding box regression偏移量获取proposals，同时剔除太    小和超出边界的proposals。其实整个网络到了Proposal Layer这里，就完成了相当于目标定位的功能。\n   \n   **2.1 多通道图像卷积基础知识介绍**\n   * 对于单通道图像+单卷积核做卷积，之前展示了；\n   * 对于多通道图像+多卷积核做卷积，计算方式如下：\n     <center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_4.jpg)</center>\n     输入有3个通道，同时有2个卷积核。对于每个卷积核，先在输入3个通道分别作卷积，再将3个通道结果加起来得到卷积输出。所以对     于某个卷积层，无论输入图像有多少个通道，输出图像通道数总是等于卷积核数量！\n    对多通道图像做 $1\\times1$ 卷积，其实就是将输入图像于每个通道乘以卷积系数后加在一起，即相当于把原图像中本来各个独立的     通道“联通”在了一起。\n    \n   **2.2 Anchors**\n   提到RPN网络，就不能不说anchors。所谓anchors，实际上就是一组由rpn/generate_anchors.py生成的矩形。直接运行作者demo中    的generate_anchors.py可以得到以下输出：\n   [[ -84.  -40.   99.   55.]\n   [-176.  -88.  191.  103.]\n   [-360. -184.  375.  199.]\n   [ -56.  -56.   71.   71.]\n   [-120. -120.  135.  135.]\n   [-248. -248.  263.  263.]\n   [ -36.  -80.   51.   95.]\n   [ -80. -168.   95.  183.]\n   [-168. -344.  183.  359.]]\n\n   其中每行的4个值 $(x1,y1,x2,y2)$ 代表矩形左上和右下角点坐标。9个矩形共有3种形状，长宽比为大约为 $width:height = [1:1, 1:2, 2:1]$ 三种，如下图。实际上通过anchors就引入了检测中常用到的多尺度方法。\n   <center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_5.jpg)</center>\n   注：关于上面的anchors size，其实是根据检测图像设置的。在python demo中，会把任意大小的输入图像reshape成 $800\\times600$。再回头来看anchors的大小，anchors中长宽 1:2 中最大为 $352\\times704$ ，长宽 2:1 中最大 $736\\times384$ ，基本是cover了 $800\\times600$ 的各个尺度和形状。\n那么这9个anchors是做什么的呢？借用Faster RCNN论文中的原图，如下图，遍历Conv layers计算获得的feature maps，为每一个点都配备这9种anchors作为初始的检测框。这样做获得检测框很不准确，不用担心，后面还有2次bounding box regression可以修正检测框位置。\n  <center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_6.jpg)</center>\n  \n  解释一下上面这张图的数字。\n\n* 在原文中使用的是ZF model中，其Conv Layers中最后的conv5层num_output=256，对应生成256张特征图，所以相当于feature map每个点都是256-dimensions\n* 在conv5之后，做了rpn_conv/3x3卷积且num_output=256，相当于每个点又融合了周围3x3的空间信息（猜测这样做也许更鲁棒？反正我没测试），同时256-d不变（如图4和图7中的红框）\n* 假设在conv5 feature map中每个点上有k个anchor（默认k=9），而每个anhcor要分foreground和background，所以每个点由256d feature转化为cls=2k scores；而每个anchor都有\\[x, y, w, h\\]对应4个偏移量，所以reg=4k coordinates\n* 补充一点，全部anchors拿去训练太多了，训练程序会在合适的anchors中随机选取128个postive anchors+128个negative anchors进行训练（什么是合适的anchors下文5.1有解释）\n\n   **2.3 softmax判定foreground与background**\n   一副MxN大小的矩阵送入Faster RCNN网络后，到RPN网络变为(M/16)x(N/16)，不妨设 W=M/16 ， H=N/16 。在进入reshape与softmax之前，先做了1x1卷积，如下图：\n   <center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_7.jpg)</center>\n   该1x1卷积的caffe prototxt定义如下：\n   <center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_19.PNG)</center>\n可以看到其num_output=18，也就是经过该卷积的输出图像为 $W\\times H \\times 18$ 大小（注意第二章开头提到的卷积计算方式）。这也就刚好对应了feature maps每一个点都有9个anchors，同时每个anchors又有可能是foreground和background，所有这些信息都保存 $W\\times H\\times (9\\cdot2)$ 大小的矩阵。为何这样做？后面接softmax分类获得foreground anchors，也就相当于初步提取了检测目标候选区域box（一般认为目标在foreground anchors中）。\n综上所述，RPN网络中利用anchors和softmax初步提取出foreground anchors作为候选区域。\n\n   **2.4 bounding box regression原理**\n 如图所示绿色框为飞机的Ground Truth(GT)，红色为提取的foreground anchors，即便红色的框被分类器识别为飞机，但是由于红色的框定位不准，这张图相当于没有正确的检测出飞机。所以我们希望采用一种方法对红色的框进行微调，使得foreground anchors和GT更加接近。\n<center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_8.jpg)</center>\n对于窗口一般使用四维向量 (x, y, w, h) 表示，分别表示窗口的中心点坐标和宽高。对于下图，红色的框A代表原始的Foreground Anchors，绿色的框G代表目标的GT，我们的目标是寻找一种关系，使得输入原始的anchor A经过映射得到一个跟真实窗口G更接近的回归窗口G'，即：\n* 给定：$anchor A=(A_{x}, A_{y}, A_{w}, A_{h})$ 和 $GT=[G_{x}, G_{y}, G_{w}, G_{h}]$\n* 寻找一种变换F，使得：$F(A_{x}, A_{y}, A_{w}, A_{h})=(G_{x}^{'}, G_{y}^{'}, G_{w}^{'}, G_{h}^{'})$，其中 $(G_{x}^{'}, G_{y}^{'}, G_{w}^{'}, G_{h}^{'}) \\approx (G_{x}, G_{y}, G_{w}, G_{h})$\n<center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_9.jpg)</center>\n那么经过何种变换F才能从图10中的anchor A变为G'呢？ 比较简单的思路就是:\n\n* 先做平移\n<center>\n$G^{'}_{x} = A_{w} \\cdot d_{x}(A) + A_{x} $\n$G^{'}_{y} = A_{y} \\cdot d_{y}(A) + A_{y} $\n</center>\n* 再做缩放\n<center>\n$G^{'}_{w} = A_{w} \\cdot exp(d_{w}(A)) $\n$G^{'}_{h} = A_{h} \\cdot exp(d_{h}(A)) $\n</center>\n\n观察上面4个公式发现，需要学习的是 $d_{x}(A),d_{y}(A),d_{w}(A),d_{h}(A)$ 这四个变换。当输入的anchor A与GT相差较小时，可以认为这种变换是一种线性变换， 那么就可以用线性回归来建模对窗口进行微调（注意，只有当anchors A和GT比较接近时，才能使用线性回归模型，否则就是复杂的非线性问题了）。\n\n接下来的问题就是如何通过线性回归获得 $d_{x}(A),d_{y}(A),d_{w}(A),d_{h}(A)$ 了。线性回归就是给定输入的特征向量X, 学习一组参数W, 使得经过线性回归后的值跟真实值Y非常接近，即$Y=WX$。对于该问题，输入X是cnn feature map，定义为Φ；同时还有训练传入A与GT之间的变换量，即$(t_{x}, t_{y}, t_{w}, t_{h})$。输出是$d_{x}(A),d_{y}(A),d_{w}(A),d_{h}(A)$四个变换。那么目标函数可以表示为：\n<center>\n$d_{*}(A) = w^{T}_{*} \\cdot \\phi(A)$\n</center>\n\n其中Φ(A)是对应anchor的feature map组成的特征向量，w是需要学习的参数，d(A)是得到的预测值（\\*表示 x，y，w，h，也就是每一个变换对应一个上述目标函数）。为了让预测值$(t_{x}, t_{y}, t_{w}, t_{h})$与真实值差距最小，设计损失函数：\n<center>\n$Loss = \\sum^{N}_{i}(t^{i}_{*} - \\hat{w}^{T}_{*} \\cdot \\phi(A^{i}))^{2}$\n</center>\n函数优化目标为：\n<center>\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_20.jpg)\n</center>\n需要说明，只有在GT与需要回归框位置比较接近时，才可近似认为上述线性变换成立。\n说完原理，对应于Faster RCNN原文，foreground anchor与ground truth之间的平移量 $(t_x, t_y)$ 与尺度因子 $(t_w, t_h)$ 如下：\n<center>\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_21.jpg)\n</center>\n对于训练bouding box regression网络回归分支，输入是cnn feature Φ，监督信号是Anchor与GT的差距 $(t_x, t_y, t_w, t_h)$，即训练目标是：输入Φ的情况下使网络输出与监督信号尽可能接近。\n那么当bouding box regression工作时，再输入Φ时，回归网络分支的输出就是每个Anchor的平移量和变换尺度 $(t_x, t_y, t_w, t_h)$，显然即可用来修正Anchor位置了。\n\n   **2.5 对proposals进行bounding box regression**\n在了解bounding box regression后，再回头来看RPN网络第二条线路，如下图。\n<center>\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_10.jpg)\n</center>\n其 $num\\_output=36$ ，即经过该卷积输出图像为 $W\\times H\\times 36$ ，在caffe blob存储为 \\[1, 36, H, W\\] ，这里相当于feature maps每个点都有9个anchors，每个anchors又都有4个用于回归的$d_{x}(A),d_{y}(A),d_{w}(A),d_{h}(A)$变换量。\n\n   **2.6 Proposal Layer**\nProposal Layer负责综合所有 $d_{x}(A),d_{y}(A),d_{w}(A),d_{h}(A)$ 变换量和foreground anchors，计算出精准的proposal，送入后续RoI Pooling Layer。\n首先解释im_info。对于一副任意大小PxQ图像，传入Faster RCNN前首先reshape到固定 $M\\times N$ ，im_info=\\[M, N, scale_factor\\]则保存了此次缩放的所有信息。然后经过Conv Layers，经过4次pooling变为 $W\\times H=(M/16)\\times(N/16)$ 大小，其中feature_stride=16则保存了该信息，用于计算anchor偏移量。\n<center>\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_11.jpg)\n</center>\n\nProposal Layer forward（caffe layer的前传函数）按照以下顺序依次处理：\n1. 生成anchors，利用$[d_{x}(A),d_{y}(A),d_{w}(A),d_{h}(A)]$对所有的anchors做bbox regression回归（这里的anchors生成和训练时完全一致）\n2. 按照输入的foreground softmax scores由大到小排序anchors，提取前pre_nms_topN(e.g. 6000)个anchors，即提取修正位置后的foreground anchors。\n3. 限定超出图像边界的foreground anchors为图像边界（防止后续roi pooling时proposal超出图像边界）\n4. 剔除非常小（width<threshold or height<threshold）的foreground anchors\n5. 进行nonmaximum suppression\n6. 再次按照nms后的foreground softmax scores由大到小排序fg anchors，提取前post_nms_topN(e.g. 300)结果作为proposal输出。\n   \n之后输出 proposal=\\[x1, y1, x2, y2\\] ，注意，由于在第三步中将anchors映射回原图判断是否超出边界，所以这里输出的proposal是对应 $M\\times N$ 输入图像尺度的，这点在后续网络中有用。另外我认为，严格意义上的检测应该到此就结束了，后续部分应该属于识别了~   \n   \n**RPN**网络结构就介绍到这里，总结起来就是：\n**生成anchors -> softmax分类器提取fg anchors -> bbox reg回归fg anchors -> Proposal Layer生成proposals**\n\n### 【19/06/2018】\n#### 处理 XML 文档\n使用 xml.etree.ElementTree 这个包去解析XML文件， 并且整理成为list形式\n【流程】\n* 读取XML文件\n* 区分训练集测试集根据竞赛要求\n* 解析XML文档收录到PYTHON词典中\n<center>\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_23.PNG)\n</center> \nGithub 的 jupyter notebook [地址](https://github.com/Trouble404/Object-Detection-System-Based-on-CNN-and-Capsule-Network/blob/master/Faster-RCNN/parser_voc2012_xml_and_plotting.ipynb) \n\n训练集根据竞赛的 trainval.txt 文件给的图片作为训练集\n其余的作为训练集\n\n解析后， 总共有 17125 张图片，\n其中 11540 张作为训练集\n\n图片中的20个类的统计情况：\n<center>\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_24.PNG)\n</center> \n\n\n### 【20/06/2018】\n#### 根据信息画出BBOXES\n安装 cv2 这个包\n  {% codeblock %}\n  pip install opencv-python\n  {% endcodeblock %}\n注意： OpenCV-python 中颜色格式 是BGR 而不是 RGB\n\n在VOC2012数据集里面，总共有20类， 根据不同的种类用不同的颜色和唯一的编码画BBOXES。\n\n| class | class_mapping | BGR of bbox |\n| :--- | :---- | :---- |\n| Person | 0 | (0, 0, 255) | \n| Aeroplane | 1 | (0, 0, 255) | \n| Tvmonitor | 2 | (0, 128, 0) | \n| Train | 3 | (128, 128, 128) | \n| Boat | 4 | (0, 165, 255) | \n| Dog | 5 | (0, 255, 255) | \n| Chair | 6 | (80, 127, 255) | \n| Bird | 7 | (208, 224, 64) | \n| Bicycle | 8 | (235, 206, 135) | \n| Bottle | 9 | (128, 0, 0) | \n| Sheep | 10 | (140, 180, 210) | \n| Diningtable | 11 | (0, 255, 0) | \n| Horse | 12 | (133, 21, 199) | \n| Motorbike | 13 | (47, 107, 85) | \n| Sofa | 14 | (19, 69, 139) | \n| Cow | 15 | (222, 196, 176) | \n| Car | 16 | (0, 0, 0) | \n| Cat | 17 |  (225, 105, 65) | \n| Bus | 18 | (255, 255, 255) | \n| Pottedplant | 19 | (205, 250, 255) | \n\n我写了一个show_image_with_bbox函数去画出带BBOXES的图根据处理XML文件得到的list:\n<center>\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_22.PNG)\n</center>  \nGithub 的 jupyter notebook [地址](https://github.com/Trouble404/Object-Detection-System-Based-on-CNN-and-Capsule-Network/blob/master/Faster-RCNN/parser_voc2012_xml_and_plotting.ipynb) \n\nEXAMPLE:\n<center>\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/img_with_bboex_1.PNG)\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/img_with_bboex_2.PNG)\n</center>  \n\n### 【21/06/2018】\n#### config setting\nset config class:\n                 for image enhancement:\n<center>\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/21_1.PNG)\n</center>  \n\n#### image enhancement\n<center>\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/21_2.PNG)\n</center>  \nAccording to the config of three peremeters, users could augment image with 3 different ways or using them all.\nFor horizontal and vertical flips, 1/3 probability to triggle\nWith 0,90,180,270 rotation, \nThis function could increase the number of datasets.\n\nimage flips and rotation are realized by opencv and replace of height and width\nNew cordinates of bboxes are calculated acccording to different change of image\n\ndetailed in Github, jupyter notebook: [address](https://github.com/Trouble404/Object-Detection-System-Based-on-CNN-and-Capsule-Network/blob/master/Faster-RCNN/config_set_and_image_enhance.ipynb)\n\nOrignal image:\n<center>\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/21_3.PNG)\n</center>  \nhorizontal flip:\n<center>\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/21_4.PNG)\n</center>  \nVertical filp:\n<center>\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/21_5.PNG)\n</center>  \nRandom rotation:\n<center>\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/21_6.PNG)\n</center>  \nHorizontal and then vertical flips:\n<center>\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/21_7.PNG)\n</center>  \n\n### 【22/06/2018】\n#### Image rezise\nThis function is to rezise input image to a uniform size with same shortest side\n<center>\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/22_2.PNG)\n</center> \n\nAccording to set the value of shortest side, convergent-divergent or augmented another side proportion\n\nTest:\nLeft image is resized image, in this case, the orignal image amplified.\n<center>\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/22_1.png)\n</center> \n\n#### Class Balance\nWhen training the model, if we sent image with no repeating classes, it may help to improve the performance of model. Therefore, this function is to make sure no repeating classes in two closed input image.\n<center>\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/22_3.PNG)\n</center> \n\nTest:\n<center>\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/22_4.PNG)\n</center> \nRandom output 4 iamge with is function, it could find no repeating classes in two closed image. However, it may reduce the number of trainning image because skip some images.\n\n\n### 【25~26/06/2018】\n#### Region Proposal Networks(RPN)\n<center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_3.jpg)</center>\n可以看到RPN网络实际分为2条线，上面一条通过softmax分类anchors获得foreground和background（检测目标是foreground），下面一条用于计算对于anchors的bounding box regression偏移量，以获得精确的proposal。而最后的Proposal层则负责综合foreground anchors和bounding box regression偏移量获取proposals，同时剔除太小和超出边界的proposals。其实整个网络到了Proposal Layer这里，就完成了相当于目标定位的功能。\n\n#### Anchors\n对每一个点生成的矩形\n<center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_5.jpg)</center>\n其中每行的4个值 (x1,y1,x2,y2) 代表矩形左上和右下角点坐标。9个矩形共有3种形状，长宽比为大约为 width:height = \\[1:1, 1:2, 2:1\\]\n<center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_6.jpg)</center>\n通过遍历Conv layers计算获得的feature maps，为每一个点都配备这9种anchors作为初始的检测框。这样做获得检测框很不准确，不用担心，后面还有2次bounding box regression可以修正检测框位置.\n\n#### Code\n\n```python\n\"\"\" intersection of two bboxes\n@param ai: left top x,y and right bottom x,y coordinates of bbox 1\n@param bi: left top x,y and right bottom x,y coordinates of bbox 2\n\n@return: area_union: whether contain target classes\n\n\"\"\"\ndef intersection(ai, bi):\n```\n```python\n\"\"\" union of two bboxes\n@param au: left top x,y and right bottom x,y coordinates of bbox 1\n@param bu: left top x,y and right bottom x,y coordinates of bbox 2\n@param area_intersection: intersection area\n\n@return: area_union: whether contain target classes\n\n\"\"\"\ndef union(au, bu, area_intersection):\n```\n\n```python\n\"\"\" calculate ratio of intersection and union\n@param a: left top x,y and right bottom x,y coordinates of bbox 1\n@param b: left top x,y and right bottom x,y coordinates of bbox 2\n\n@return: ratio of intersection and union of two bboxes\n\n\"\"\"\ndef iou(a, b):\n```\n**IOU is used to bounding box regression**\n\n---\n** rpn calculation**\n\n1. Traversal all pre-anchors to calculate IOU with GT bboxes\n2. Set number and proprty of pre-anchors\n3. return specity number of result(Anchors)\n\n```python\n\"\"\" \n\n@param C: configuration\n@param img_data: parsered xml information\n@param width: orignal width of image\n@param hegiht: orignal height of image\n@param resized_width: resized width of image after image processing\n@param resized_heighth: resized height of image after image processing\n@param img_length_calc_function: Keras's image_dim_ordering function\n\n@return: np.copy(y_rpn_cls): whether contain target classes\n@return: np.copy(y_rpn_regr): corrspoding return of gradient\n\n\"\"\"\n\ndef calc_rpn(C, img_data, width, height, resized_width, resized_height, img_length_calc_function):\n```\n\n【注：其只会返回num_regions（这里设置为256）个有效的正负样本 】\n\n【流程】\nInitialise paramters: see [jupyter notebook](https://github.com/Trouble404/Object-Detection-System-Based-on-CNN-and-Capsule-Network/blob/master/Faster-RCNN/rpn_calculation.ipynb)\n\nCalculate the size of map feature:\n```python\n(output_width, output_height) = img_length_calc_function(resized_width, resized_height)\n```\n<br>\nGet the GT box coordinates, and resize to account for image resizing\nafter rezised functon, the coordinates of bboxes need to re-calculation:\n```python\nfor bbox_num, bbox in enumerate(img_data['bboxes']):\n\tgta[bbox_num, 0] = bbox['x1'] * (resized_width / float(width))\n\tgta[bbox_num, 1] = bbox['x2'] * (resized_width / float(width))\n\tgta[bbox_num, 2] = bbox['y1'] * (resized_height / float(height))\n\tgta[bbox_num, 3] = bbox['y2'] * (resized_height / float(height))\n```\n【注意gta的存储形式是（x1,x2,y1,y2）而不是（x1,y1,x2,y2）】\n<br>\nTraverse all possible group of sizes\nanchor box scales \\[128, 256, 512\\]\nanchor box ratios \\[1:1,1:2,2:1\\]\n```python\nfor anchor_size_idx in range(len(anchor_sizes)):\n\tfor anchor_ratio_idx in range(len(anchor_ratios)):\n\t\tanchor_x = anchor_sizes[anchor_size_idx] * anchor_ratios[anchor_ratio_idx][0]\n\t\tanchor_y = anchor_sizes[anchor_size_idx] * anchor_ratios[anchor_ratio_idx][1]\n```\nTraver one bbox group, all pre boxes generated by anchors\n\noutput_width，output_height：width and height of map feature\ndownscale：mapping ration, defualt 16\nif to delete box out of iamge\n\n```python\nfor ix in range(output_width):\n\tx1_anc = downscale * (ix + 0.5) - anchor_x / 2\n\tx2_anc = downscale * (ix + 0.5) + anchor_x / 2\n\n\tif x1_anc < 0 or x2_anc > resized_width:\n\t\tcontinue\n\n\tfor jy in range(output_height):\n\t\ty1_anc = downscale * (jy + 0.5) - anchor_y / 2\n\t\ty2_anc = downscale * (jy + 0.5) + anchor_y / 2\n\n\t\tif y1_anc < 0 or y2_anc > resized_height:\n\t\t\tcontinue\n```\n<br>\n\n【注：现在我们确定了一个预选框组合有确定了中心点那就是唯一确定一个框了，接下来就是来确定这个宽的性质了：是否包含物体、如包含物体其回归梯度是多少】\n\n要确定以上两个性质，每一个框都需要遍历图中的所有bboxes 然后计算该预选框与bbox的交并比（IOU）\n如果现在的交并比curr_iou大于该bbox最好的交并比或者大于给定的阈值则求下列参数，这些参数是后来要用的即回归梯度\n\ntx：两个框中心的宽的距离与预选框宽的比\nty:同tx\ntw:bbox的宽与预选框宽的比\nth:同理\n\n```python\nif curr_iou > best_iou_for_bbox[bbox_num] or curr_iou > C.rpn_max_overlap:\n\tcx = (gta[bbox_num, 0] + gta[bbox_num, 1]) / 2.0\n\tcy = (gta[bbox_num, 2] + gta[bbox_num, 3]) / 2.0\n\tcxa = (x1_anc + x2_anc) / 2.0\n\tcya = (y1_anc + y2_anc) / 2.0\n\n\ttx = (cx - cxa) / (x2_anc - x1_anc)\n\tty = (cy - cya) / (y2_anc - y1_anc)\n\ttw = np.log((gta[bbox_num, 1] - gta[bbox_num, 0]) / (x2_anc - x1_anc))\n\tth = np.log((gta[bbox_num, 3] - gta[bbox_num, 2])) / (y2_anc - y1_anc)\n```\n对应于Faster RCNN原文，foreground anchor与ground truth之间的平移量 $(t_x, t_y)$ 如下：\n<center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/25_2.jpg)</center>\n对于训练bouding box regression网络回归分支，输入是cnn feature Φ，监督信号是Anchor与GT的差距 $(t_x, t_y, t_w, t_h)$，即训练目标是：输入 Φ的情况下使网络输出与监督信号尽可能接近。\n那么当bouding box regression工作时，再输入Φ时，回归网络分支的输出就是每个Anchor的平移量和变换尺度 $(t_x, t_y, t_w, t_h)$，显然即可用来修正Anchor位置了。\n\n<br>\n如果相交的不是背景，那么进行一系列更新\n\n关于bbox的相关信息更新\n预选框的相关更新：如果交并比大于阈值这是pos\nbest_iou_for_loc：其记录的是有最大交并比为多少和其对应的回归梯度\nnum_anchors_for_bbox[bbox_num]：记录的是bbox拥有的pos预选框的个数\n如果小于最小阈值是neg，在这两个之间是neutral\n需要注意的是：判断一个框为neg需要其与所有的bbox的交并比都小于最小的阈值\n\n```python\nif img_data['bboxes'][bbox_num]['class'] != 'bg':\n\n\t# all GT boxes should be mapped to an anchor box, so we keep track of which anchor box was best\n\tif curr_iou > best_iou_for_bbox[bbox_num]:\n\t\tbest_anchor_for_bbox[bbox_num] = [jy, ix, anchor_ratio_idx, anchor_size_idx]\n\t\tbest_iou_for_bbox[bbox_num] = curr_iou\n\t\tbest_x_for_bbox[bbox_num, :] = [x1_anc, x2_anc, y1_anc, y2_anc]\n\t\tbest_dx_for_bbox[bbox_num, :] = [tx, ty, tw, th]\n\n\tif curr_iou > C.rpn_max_overlap:\n\t\tbbox_type = 'pos'\n\t\tnum_anchors_for_bbox[bbox_num] += 1\n\t\tif curr_iou > best_iou_for_loc:\n\t\t\tbest_iou_for_loc = curr_iou\n\t\t\tbest_regr = (tx, ty, tw, th)\n\n\tif C.rpn_min_overlap < curr_iou < C.rpn_max_overlap:\n\n\t\tif bbox_type != 'pos':\n\t\t\tbbox_type = 'neutral'\n```\n<br>\n当结束对所有的bbox的遍历时，来确定该预选宽的性质。\n\ny_is_box_valid：该预选框是否可用（nertual就是不可用的）\ny_rpn_overlap：该预选框是否包含物体\ny_rpn_regr:回归梯度\n```python\nif bbox_type == 'neg':\n    y_is_box_valid[jy, ix, anchor_ratio_idx + n_anchratios * anchor_size_idx] = 1\n    y_rpn_overlap[jy, ix, anchor_ratio_idx + n_anchratios * anchor_size_idx] = 0\nelif bbox_type == 'neutral':\n    y_is_box_valid[jy, ix, anchor_ratio_idx + n_anchratios * anchor_size_idx] = 0\n    y_rpn_overlap[jy, ix, anchor_ratio_idx + n_anchratios * anchor_size_idx] = 0\nelse:\n    y_is_box_valid[jy, ix, anchor_ratio_idx + n_anchratios * anchor_size_idx] = 1\n    y_rpn_overlap[jy, ix, anchor_ratio_idx + n_anchratios * anchor_size_idx] = 1\n    start = 4 * (anchor_ratio_idx + n_anchratios * anchor_size_idx)\n    y_rpn_regr[jy, ix, start:start+4] = best_regr\n```\n<br>\n如果有一个bbox没有pos的预选宽和其对应，这找一个与它交并比最高的anchor的设置为pos\n```python\nfor idx in range(num_anchors_for_bbox.shape[0]):\n\tif num_anchors_for_bbox[idx] == 0:\n\t\t# no box with an IOU greater than zero ...\n\t\tif best_anchor_for_bbox[idx, 0] == -1:\n\t\t\tcontinue\n\t\ty_is_box_valid[best_anchor_for_bbox[idx,0], best_anchor_for_bbox[idx,1], best_anchor_for_bbox[idx,2] + n_anchratios *best_anchor_for_bbox[idx,3]] = 1\n\t\ty_rpn_overlap[best_anchor_for_bbox[idx,0], best_anchor_for_bbox[idx,1], best_anchor_for_bbox[idx,2] + n_anchratios *best_anchor_for_bbox[idx,3]] = 1\n\t\tstart = 4 * (best_anchor_for_bbox[idx,2] + n_anchratios * best_anchor_for_bbox[idx,3])\n\t\ty_rpn_regr[best_anchor_for_bbox[idx,0], best_anchor_for_bbox[idx,1], start:start+4] = best_dx_for_bbox[idx, :]\n```\n<br>\n将深度变到第一位，给向量增加一个维度, 在Tensorflow中， 第一纬度是batch size, 此外， 变换向量位置匹配要求\n```python\n\ty_rpn_overlap = np.transpose(y_rpn_overlap, (2, 0, 1))\n\ty_rpn_overlap = np.expand_dims(y_rpn_overlap, axis=0)\n\n\ty_is_box_valid = np.transpose(y_is_box_valid, (2, 0, 1))\n\ty_is_box_valid = np.expand_dims(y_is_box_valid, axis=0)\n\n\ty_rpn_regr = np.transpose(y_rpn_regr, (2, 0, 1))\n\ty_rpn_regr = np.expand_dims(y_rpn_regr, axis=0)\n```\n<br>\n从可用的预选框中选择num_regions\n如果pos的个数大于num_regions / 2，则将多下来的地方置为不可用。如果小于pos不做处理\n接下来将pos与neg总是超过num_regions个的neg预选框置为不可用\n最后， 256个预选框，128个positive,128个negative 会生成 在一张图片里面\n```python\npos_locs = np.where(np(y_rpn_overlap[0, :, :, :] =.logical_and= 1, y_is_box_valid[0, :, :, :] == 1))\nneg_locs = np.where(np.logical_and(y_rpn_overlap[0, :, :, :] == 0, y_is_box_valid[0, :, :, :] == 1))\nnum_regions = 256\n\nif len(pos_locs[0]) > num_regions / 2:\n\tval_locs = random.sample(range(len(pos_locs[0])), len(pos_locs[0]) - num_regions / 2)\n\ty_is_box_valid[0, pos_locs[0][val_locs], pos_locs[1][val_locs], pos_locs[2][val_locs]] = 0\n\tnum_pos = num_regions / 2\n\nif len(neg_locs[0]) + num_pos > num_regions:\n\tval_locs = random.sample(range(len(neg_locs[0])), len(neg_locs[0]) - num_pos)\n\t y_is_box_valid[0, neg_locs[0][val_locs], neg_locs[1][val_locs], neg_locs[2][val_locs]] = 0\n```\n\n<br>\n\n### 【27/06/2018】\n#### project brief\nRe organization of Project plan\n\n#### Anchors Iterative\nIntegration of privous work:\nIn each anchor: config file -> rpn_stride = 16 means generate one anchor in 16 pixels\n[Jupyter Notebook address](https://github.com/Trouble404/Object-Detection-System-Based-on-CNN-and-Capsule-Network/blob/master/Faster-RCNN/anchor_rpn.ipynb)\n\n【流程】\nFunction description\n```python\n\"\"\"\n@param all_img_data: Parsered xml file  \n@param class_count: Counting of the number of all classes objects\n@param C: Configuration class\n@param img_length_calc_function: resnet's get_img_output_length() function\n@param backend: Tensorflow in this project\n#param mode: train or val\n\nyield np.copy(x_img), [np.copy(y_rpn_cls), np.copy(y_rpn_regr)], img_data_aug\n@return: np.copy(x_img): image's matrix data\n@return: [np.copy(y_rpn_cls), np.copy(y_rpn_regr)]: calculated rpn class and radient\n@return: img_data_aug: correspoding parsed xml information\n\n\"\"\"\n\ndef get_anchor_gt(all_img_data, class_count, C, img_length_calc_function, backend, mode='train'):\n```\n<br>\n**Traverse all input image based on input xml information**\n\n* Apply class balance function: \n```python\nC.balanced_classes = True\nsample_selector = image_processing.SampleSelector(class_count)\nif C.balanced_classes and sample_selector.skip_sample_for_balanced_class(img_data):\n    continue\n```\n<br>\n\n* Apply image enhance\nif input mode is train, apply image enhance to obtain augmented image xml and matrix, if mode is val, obtain image orignal xml and matrix\n```python\nif mode == 'train':\n    img_data_aug, x_img = image_enhance.augment(img_data, C, augment=True)\nelse:\n    img_data_aug, x_img = image_enhance.augment(img_data, C, augment=False)\n```\nverifacation width and hegiht in xml and matrix\n```python\n(width, height) = (img_data_aug['width'], img_data_aug['height'])\n(rows, cols, _) = x_img.shape\nassert cols == width\nassert rows == height\n```\n<br>\n\n* Apply rezise function\n```python\n(resized_width, resized_height) = image_processing.get_new_img_size(width, height, C.im_size)\nx_img = cv2.resize(x_img, (resized_width, resized_height), interpolation=cv2.INTER_CUBIC)\n```\n<br>\n\n* Apply rpn calculation\n```python\ny_rpn_cls, y_rpn_regr = rpn_calculation.calc_rpn(C, img_data_aug, width, height, resized_width, resized_height, img_length_calc_function)\n```\n<br>\n\n* Zero-center by mean pixel, and preprocess image format\nBGR -> RGB because when apply resnet, it need RGB but in cv2, it use BGR\n```python\nx_img = x_img[:,:, (2, 1, 0)]\n```\n   For using pre-trainning model, needs to mins mean channel in each dim\n```python\nx_img = x_img.astype(np.float32)\nx_img[:, :, 0] -= C.img_channel_mean[0]\nx_img[:, :, 1] -= C.img_channel_mean[1]\nx_img[:, :, 2] -= C.img_channel_mean[2]\nx_img /= C.img_scaling_factor # default to 1,so no change here\n```\n   expand for batch size\n```python\nx_img = np.expand_dims(x_img, axis=0)\n```\n  for using pre-trainning model, need to sclaling the std to match pre trained model\n```python\ny_rpn_regr[:, y_rpn_regr.shape[1]//2:, :, :] *= C.std_scaling # scaling is 4 here\n```\n  in tensorflow, sort as batch size, width, height, deep\n```python\nif backend == 'tf':\n    x_img = np.transpose(x_img, (0, 3, 2, 1))\n    y_rpn_cls = np.transpose(y_rpn_cls, (0, 3, 2, 1))\n\ty_rpn_regr = np.transpose(y_rpn_regr, (0, 3, 2, 1))\t\t\t\t\t\t\t\t\n```\n  generator to iteror, using next() to loop\n```python\nyield np.copy(x_img), [np.copy(y_rpn_cls), np.copy(y_rpn_regr)], img_data_aug  \n```\n<br>\n【执行】\n```python\ndata_gen_train = get_anchor_gt(train_imgs, classes_count, C, nn.get_img_output_length, K.image_dim_ordering(), mode='train')\ndata_gen_val = get_anchor_gt(val_imgs, classes_count, C, nn.get_img_output_length,K.image_dim_ordering(), mode='val')\n```\nTest:\n```python\nimg,rpn,img_aug = next(data_gen_train)\n```\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/26_1.PNG)\n\n### 【28/06/2018】\n#### Resnet50 structure\n论文链接: https://arxiv.org/abs/1512.03385\n\n首先，我们要问一个问题： \n**Is learning better networks as easy as stacking more layers?**\n\n很显然不是，原因有二。 \n一，**vanishing/exploding gradients**；深度会带来恶名昭著的梯度弥散/爆炸，导致系统不能收敛。然而梯度弥散/爆炸在很大程度上被normalized initialization and intermediate normalization layers处理了。 \n二、**degradation**；当深度开始增加的时候，accuracy经常会达到饱和，然后开始下降，但这并不是由于过拟合引起的。可见figure1，56-layer的error大于20-layer的error。\n\nHe kaiMing大神认为靠堆layers竟然会导致degradation，那肯定是我们堆的方式不对。因此他提出了一种基于残差块的identity mapping，通过学习残差的方式，而非直接去学习直接的映射关系。 \n\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/01_2.jpg)\n\n事实证明，靠堆积残差块能够带来很好效果提升。而不断堆积plain layer却会带来很高的训练误差 \n残差块的两个优点：\n1) Our extremely deep residual nets are easy to optimize, but the counterpart “plain” nets (that simply stack layers) exhibit higher training error when the depth increases; \n2) Our deep residual nets can easily enjoy accuracy gains from greatly increased depth, producing results substantially better than previous networks.\n\n\n### 【29/06/2018】\n#### Resnet50 image structure\n![iamge](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/1_01.png)\nResNet有2个基本的block，一个是Identity Block，输入和输出的dimension是一样的，所以可以串联多个；另外一个基本block是Conv Block，输入和输出的dimension是不一样的，所以不能连续串联，它的作用本来就是为了改变feature vector的dimension\n\n因为CNN最后都是要把image一点点的convert成很小但是depth很深的feature map，一般的套路是用统一的比较小的kernel（比如VGG都是用3x3），但是随着网络深度的增加，output的channel也增大（学到的东西越来越复杂），所以有必要在进入Identity Block之前，用Conv Block转换一下维度，这样后面就可以连续接Identity Block.\n\n可以看下Conv Block是怎么改变输出维度的:\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/01_3.png)\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/01_4.png)\n其实就是在shortcut path的地方加上一个conv2D layer（1x1 filter size），然后在main path改变dimension，并与shortcut path对应起来.\n\n## July\n### 【02/07/2018】\n#### Construct resnet by keras\n残差网络的关键步骤，跨层的合并需要保证x和F(x)的shape是完全一样的，否则它们加不起来。\n\n理解了这一点，我们开始用keras做实现，我们把输入输出大小相同的模块称为identity_block，而把输出比输入小的模块称为conv_block，首先，导入所需的模块：\n\n```python\nfrom keras.models import Model\nfrom keras.layers import Input,Dense,BatchNormalization,Conv2D,MaxPooling2D,AveragePooling2D,ZeroPadding2D\nfrom keras.layers import add,Flatten\nfrom keras.optimizers import SGD\n```\n\n我们先来编写identity_block，这是一个函数，接受一个张量为输入，并返回一个张量, 然后是conv层，是有shortcut的：\n```python\ndef Conv2d_BN(x, nb_filter,kernel_size, strides=(1,1), padding='same',name=None):\n    if name is not None:\n        bn_name = name + '_bn'\n        conv_name = name + '_conv'\n    else:\n        bn_name = None\n        conv_name = None\n \n    x = Conv2D(nb_filter,kernel_size,padding=padding,strides=strides,activation='relu',name=conv_name)(x)\n    x = BatchNormalization(axis=3,name=bn_name)(x)\n    return x\n    \ndef Conv_Block(inpt,nb_filter,kernel_size,strides=(1,1), with_conv_shortcut=False):\n    x = Conv2d_BN(inpt,nb_filter=nb_filter[0],kernel_size=(1,1),strides=strides,padding='same')\n    x = Conv2d_BN(x, nb_filter=nb_filter[1], kernel_size=(3,3), padding='same')\n    x = Conv2d_BN(x, nb_filter=nb_filter[2], kernel_size=(1,1), padding='same')\n    if with_conv_shortcut:\n        shortcut = Conv2d_BN(inpt,nb_filter=nb_filter[2],strides=strides,kernel_size=kernel_size)\n        x = add([x,shortcut])\n        return x\n    else:\n        x = add([x,inpt])\n        return x\n```\n\n剩下的事情就很简单了，数好identity_block和conv_block是如何交错的，照着网络搭就好了：\n```python\ninpt = Input(shape=(224,224,3))\nx = ZeroPadding2D((3,3))(inpt)\nx = Conv2d_BN(x,nb_filter=64,kernel_size=(7,7),strides=(2,2),padding='valid')\nx = MaxPooling2D(pool_size=(3,3),strides=(2,2),padding='same')(x)\n \nx = Conv_Block(x,nb_filter=[64,64,256],kernel_size=(3,3),strides=(1,1),with_conv_shortcut=True)\nx = Conv_Block(x,nb_filter=[64,64,256],kernel_size=(3,3))\nx = Conv_Block(x,nb_filter=[64,64,256],kernel_size=(3,3))\n \nx = Conv_Block(x,nb_filter=[128,128,512],kernel_size=(3,3),strides=(2,2),with_conv_shortcut=True)\nx = Conv_Block(x,nb_filter=[128,128,512],kernel_size=(3,3))\nx = Conv_Block(x,nb_filter=[128,128,512],kernel_size=(3,3))\nx = Conv_Block(x,nb_filter=[128,128,512],kernel_size=(3,3))\n \nx = Conv_Block(x,nb_filter=[256,256,1024],kernel_size=(3,3),strides=(2,2),with_conv_shortcut=True)\nx = Conv_Block(x,nb_filter=[256,256,1024],kernel_size=(3,3))\nx = Conv_Block(x,nb_filter=[256,256,1024],kernel_size=(3,3))\nx = Conv_Block(x,nb_filter=[256,256,1024],kernel_size=(3,3))\nx = Conv_Block(x,nb_filter=[256,256,1024],kernel_size=(3,3))\nx = Conv_Block(x,nb_filter=[256,256,1024],kernel_size=(3,3))\n \nx = Conv_Block(x,nb_filter=[512,512,2048],kernel_size=(3,3),strides=(2,2),with_conv_shortcut=True)\nx = Conv_Block(x,nb_filter=[512,512,2048],kernel_size=(3,3))\nx = Conv_Block(x,nb_filter=[512,512,2048],kernel_size=(3,3))\nx = AveragePooling2D(pool_size=(7,7))(x)\nx = Flatten()(x)\nx = Dense(1000,activation='softmax')(x)\n\nmodel = Model(inputs=inpt,outputs=x)\nsgd = SGD(decay=0.0001,momentum=0.9)\nmodel.compile(loss='categorical_crossentropy',optimizer=sgd,metrics=['accuracy'])\nmodel.summary()\n```\n\n[jupyter notebook](https://github.com/Trouble404/Object-Detection-System-Based-on-CNN-and-Capsule-Network/blob/master/Faster-RCNN/Resnet50/resnet50_keras.ipynb)\n\n\n### 【03/07/2018】\n#### load pre-trained model of resnet50\n步骤如下：\n\n* 下载ResNet50不包含全连接层的模型参数到本地（resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5）；\n* 定义好ResNet50的网络结构；\n* 将预训练的模型参数加载到我们所定义的网络结构中；\n* 更改全连接层结构，便于对我们的分类任务进行处\n* 或者根据需要解冻最后几个block，然后以很低的学习率开始训练。我们只选择最后一个block进行训练，是因为训练样本很少，而ResNet50模型层数很多，全部训练肯定不能训练好，会过拟合。 其次fine-tune时由于是在一个已经训练好的模型上进行的，故权值更新应该是一个小范围的，以免破坏预训练好的特征。\n\n[下载地址](https://github.com/fchollet/deep-learning-models/releases)\n\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/01_7.JPG)\n\n因为使用了预训练模型，参数名称需要和预训练模型一致：\nidentity层：\n```python\ndef identity_block(X, f, filters, stage, block):\n    # defining name basis\n    conv_name_base = 'res' + str(stage) + block + '_branch'\n    bn_name_base = 'bn' + str(stage) + block + '_branch'\n    \n    # Retrieve Filters\n    F1, F2, F3 = filters\n    \n    # Save the input value. You'll need this later to add back to the main path. \n    X_shortcut = X\n    \n    # First component of main path\n    X = Conv2D(filters = F1, kernel_size = (1, 1), strides = (1,1), padding = 'valid', name = conv_name_base + '2a')(X)\n    X = BatchNormalization(axis = 3, name = bn_name_base + '2a')(X)\n    X = Activation('relu')(X)\n    \n    # Second component of main path (≈3 lines)\n    X = Conv2D(filters= F2, kernel_size=(f,f),strides=(1,1),padding='same',name=conv_name_base + '2b')(X)\n    X = BatchNormalization(axis=3,name=bn_name_base+'2b')(X)\n    X = Activation('relu')(X)\n \n    # Third component of main path (≈2 lines)\n    X = Conv2D(filters=F3,kernel_size=(1,1),strides=(1,1),padding='valid',name=conv_name_base+'2c')(X)\n    X = BatchNormalization(axis=3,name=bn_name_base+'2c')(X)\n \n    # Final step: Add shortcut value to main path, and pass it through a RELU activation (≈2 lines)\n    X = Add()([X, X_shortcut])\n    X = Activation('relu')(X)\n    return X\n```\n\nconv层：\n```python\ndef convolutional_block(X, f, filters, stage, block, s = 2):\n    \n    # defining name basis\n    conv_name_base = 'res' + str(stage) + block + '_branch'\n    bn_name_base = 'bn' + str(stage) + block + '_branch'\n    \n    # Retrieve Filters\n    F1, F2, F3 = filters\n    \n    # Save the input value\n    X_shortcut = X\n \n    ##### MAIN PATH #####\n    # First component of main path \n    X = Conv2D(F1, (1, 1), strides = (s,s),padding='valid',name = conv_name_base + '2a')(X)\n    X = BatchNormalization(axis = 3, name = bn_name_base + '2a')(X)\n    X = Activation('relu')(X)\n \n    # Second component of main path (≈3 lines)\n    X = Conv2D(F2,(f,f),strides=(1,1),padding='same',name=conv_name_base+'2b')(X)\n    X = BatchNormalization(axis=3,name=bn_name_base+'2b')(X)\n    X = Activation('relu')(X)\n \n    # Third component of main path (≈2 lines)\n    X = Conv2D(F3,(1,1),strides=(1,1),padding='valid',name=conv_name_base+'2c')(X)\n    X = BatchNormalization(axis=3,name=bn_name_base+'2c')(X)\n \n    ##### SHORTCUT PATH #### (≈2 lines)\n    X_shortcut = Conv2D(F3,(1,1),strides=(s,s),padding='valid',name=conv_name_base+'1')(X_shortcut)\n    X_shortcut = BatchNormalization(axis=3,name =bn_name_base+'1')(X_shortcut)\n \n    # Final step: Add shortcut value to main path, and pass it through a RELU activation (≈2 lines)\n    X = Add()([X,X_shortcut])\n    X = Activation('relu')(X)\n    return X\n```\n\nresnet50结构：\n```python\ndef ResNet50(input_shape = (64, 64, 3), classes = 30):\n    # Define the input as a tensor with shape input_shape\n    X_input = Input(input_shape)\n \n    # Zero-Padding\n    X = ZeroPadding2D((3, 3))(X_input)\n    \n    # Stage 1\n    X = Conv2D(64, (7, 7), strides = (2, 2), name = 'conv1')(X)\n    X = BatchNormalization(axis = 3, name = 'bn_conv1')(X)\n    X = Activation('relu')(X)\n    X = MaxPooling2D((3, 3), strides=(2, 2))(X)\n \n    # Stage 2\n    X = convolutional_block(X, f = 3, filters = [64, 64, 256], stage = 2, block='a', s = 1)\n    X = identity_block(X, 3, [64, 64, 256], stage=2, block='b')\n    X = identity_block(X, 3, [64, 64, 256], stage=2, block='c')\n \n    ### START CODE HERE ###\n \n    # Stage 3 (≈4 lines)\n    X = convolutional_block(X, f = 3,filters= [128,128,512],stage=3,block='a',s=2)\n    X = identity_block(X,3,[128,128,512],stage=3,block='b')\n    X = identity_block(X,3,[128,128,512],stage=3,block='c')\n    X = identity_block(X,3,[128,128,512],stage=3,block='d')\n \n    # Stage 4 (≈6 lines)\n    X = convolutional_block(X,f=3,filters=[256,256,1024],stage=4,block='a',s=2)\n    X = identity_block(X,3,[256,256,1024],stage=4,block='b')\n    X = identity_block(X,3,[256,256,1024],stage=4,block='c')\n    X = identity_block(X,3,[256,256,1024],stage=4,block='d')\n    X = identity_block(X,3,[256,256,1024],stage=4,block='e')\n    X = identity_block(X,3,[256,256,1024],stage=4,block='f')\n \n    # Stage 5 (≈3 lines)\n    X = convolutional_block(X, f = 3,filters= [512,512,2048],stage=5,block='a',s=2)\n    X = identity_block(X,3,[512,512,2048],stage=5,block='b')\n    X = identity_block(X,3,[512,512,2048],stage=5,block='c')\n \n    # AVGPOOL (≈1 line). Use \"X = AveragePooling2D(...)(X)\"\n    X = AveragePooling2D((2,2),strides=(2,2))(X)\n \n    # output layer\n    X = Flatten()(X)\n    model = Model(inputs = X_input, outputs = X, name='ResNet50')\n \n    return model\n```\n\n构建网络并且载入权重：\n```python\nbase_model = ResNet50(input_shape=(224,224,3),classes=30) \nbase_model.load_weights('resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5')\n```\n\n无法载入\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/01_8.JPG)\n### 【04/07/2018】\n#### Loading pre-trained model\n对于keras：如果新模型和旧模型结构一样，直接调用model.load_weights读取参数就行。如果新模型中的几层和之前模型一样，也通过model.load_weights('my_model_weights.h5', by_name=True)来读取参数， 或者手动对每一层进行参数的赋值，比如x= Dense(100, weights=oldModel.layers[1].get_weights())(x)\n\n修改代码：\n```python\ntry:\n    base_model.load_weights('resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5',by_name=True)\n    print(\"load successful\")\nexcept:\n    print(\"load failed\")\n```\n载入成功：[jupyter notebook](https://github.com/Trouble404/Object-Detection-System-Based-on-CNN-and-Capsule-Network/blob/master/Faster-RCNN/Resnet50/resnet50_pre_load.ipynb)\n\n### 【05~06/07/2018】\n#### construct faster rcnn net\n**RoiPoolingConv**\n该函数的作用是对将每一个预选框框定的特征图大小规整到相同大小\n什么是ROI呢？\nROI是Region of Interest的简写，指的是在“特征图上的框”；\n1）在Fast RCNN中， RoI是指Selective Search完成后得到的“候选框”在特征图上的映射，如下图所示；\n2）在Faster RCNN中，候选框是经过RPN产生的，然后再把各个“候选框”映射到特征图上，得到RoIs\n创建一个类，这里不同的是它是要继承keras的Layer类\n```python\nclass RoiPoolingConv(Layer):\n```\n[编写自己的层](http://keras-cn.readthedocs.io/en/latest/layers/writting_layer/)\n\n定义：\n\\*\\*[kwargs](https://www.cnblogs.com/xuyuanyuan123/p/6674645.html)：表示的就是形参中按照关键字传值把多余的传值以字典的方式呈现\n[super](https://link.zhihu.com/?target=http%3A//python.jobbole.com/86787/):子类调用父类的初始化方法\n```python\n'''ROI pooling layer for 2D inputs.\n    # Arguments\n        pool_size: int\n            Size of pooling region to use. pool_size = 7 will result in a 7x7 region.\n        num_rois: number of regions of interest to be used\n    '''\n# 第一个是规整后特征图大小 第二个是预选框个数\n    def __init__(self, pool_size, num_rois, **kwargs):\n\n        self.dim_ordering = K.image_dim_ordering()\n        # print error when kernel not tensorflow or thoean\n        assert self.dim_ordering in {'tf'}, 'dim_ordering must be in tf'\n\n        self.pool_size = pool_size\n        self.num_rois = num_rois\n\n        super(RoiPoolingConv, self).__init__(**kwargs)\n```\n\n得到特征图的输出通道个数:\n```python\ndef build(self, input_shape):\n        if self.dim_ordering == 'tf':\n            self.nb_channels = input_shape[0][3]\n```\n\n定义输出特征图的形状：\n```python\ndef compute_output_shape(self, input_shape):\n        if self.dim_ordering == 'tf':\n            return None, self.num_rois, self.pool_size, self.pool_size, self.nb_channels\n```\n\n遍历提供的所有预选框,将预选宽里的特征图规整到指定大小, 并且加入到output:\n```python\ndef call(self, x, mask=None):\n\n        assert(len(x) == 2)\n\n        img = x[0]\n        rois = x[1]\n\n        input_shape = K.shape(img)\n\n        outputs = []\n\n        for roi_idx in range(self.num_rois):\n\n            x = rois[0, roi_idx, 0]\n            y = rois[0, roi_idx, 1]\n            w = rois[0, roi_idx, 2]\n            h = rois[0, roi_idx, 3]\n            \n            row_length = w / float(self.pool_size)\n            col_length = h / float(self.pool_size)\n\n            num_pool_regions = self.pool_size\n\n            if self.dim_ordering == 'tf':\n                x = K.cast(x, 'int32')\n                y = K.cast(y, 'int32')\n                w = K.cast(w, 'int32')\n                h = K.cast(h, 'int32')\n\n                # resize porposal of feature map\n                rs = tf.image.resize_images(img[:, y:y+h, x:x+w, :], (self.pool_size, self.pool_size))\n                outputs.append(rs)\n\n        # 将outputs里面的变量按照第一个维度合在一起【shape:(?, 7, 7, 512)】\n        final_output = K.concatenate(outputs, axis=0)\n        final_output = K.reshape(final_output, (1, self.num_rois, self.pool_size, self.pool_size, self.nb_channels))\n        # 将变量规整到相应的大小【shape:(1, 32, 7, 7, 512)】\n        final_output = K.permute_dimensions(final_output, (0, 1, 2, 3, 4))\n\n        return final_output\n```\n输出是batch个vector，其中batch的值等于RoI的个数，vector的大小为channel \\* w \\* h；RoI Pooling的过程就是将一个个大小不同的box矩形框，都映射成大小固定（w \\* h）的矩形框.\n\n**TimeDistributed 包装器**\nFastRcnn在做完ROIpooling后，需要将生产的所有的Roi全部送入分类和回归网络，Keras用的TimeDistributed函数：\n\nRelu激活函数本身就是逐元素计算激活值的，无论进来多少维的tensor都一样，所以不需要使用TimeDistributed。conv2D需要TimeDistributed，是因为一个ROI内的数据计算是互相依赖的，而不同ROI之间又是独立的。\n\n在最后Faster RCNN的结构中进行类别判断和bbox框的回归时，需要对设置的num_rois个感兴趣区域进行回归处理，由于每一个区域的处理是相对独立的，便等价于此时的时间步为num_rois，因此用TimeDistributed来wrap。\n\n改编之前的conv 和 identity层：\n```python\ndef conv_block_td(input_tensor, kernel_size, filters, stage, block, input_shape, strides=(2, 2), trainable=True):\n\n    # conv block time distributed\n\n    nb_filter1, nb_filter2, nb_filter3 = filters\n\n    bn_axis = 3\n\n    conv_name_base = 'res' + str(stage) + block + '_branch'\n    bn_name_base = 'bn' + str(stage) + block + '_branch'\n\n    x = TimeDistributed(Convolution2D(nb_filter1, (1, 1), strides=strides, trainable=trainable, kernel_initializer='normal'), input_shape=input_shape, name=conv_name_base + '2a')(input_tensor)\n    x = TimeDistributed(BatchNormalization(axis=bn_axis), name=bn_name_base + '2a')(x)\n    x = Activation('relu')(x)\n\n    x = TimeDistributed(Convolution2D(nb_filter2, (kernel_size, kernel_size), padding='same', trainable=trainable, kernel_initializer='normal'), name=conv_name_base + '2b')(x)\n    x = TimeDistributed(BatchNormalization(axis=bn_axis), name=bn_name_base + '2b')(x)\n    x = Activation('relu')(x)\n\n    x = TimeDistributed(Convolution2D(nb_filter3, (1, 1), kernel_initializer='normal'), name=conv_name_base + '2c', trainable=trainable)(x)\n    x = TimeDistributed(BatchNormalization(axis=bn_axis), name=bn_name_base + '2c')(x)\n\n    shortcut = TimeDistributed(Convolution2D(nb_filter3, (1, 1), strides=strides, trainable=trainable, kernel_initializer='normal'), name=conv_name_base + '1')(input_tensor)\n    shortcut = TimeDistributed(BatchNormalization(axis=bn_axis), name=bn_name_base + '1')(shortcut)\n\n    x = Add()([x, shortcut])\n    x = Activation('relu')(x)\n    return x\n```\n\n```python\ndef identity_block_td(input_tensor, kernel_size, filters, stage, block, trainable=True):\n\n    # identity block time distributed\n\n    nb_filter1, nb_filter2, nb_filter3 = filters\n\n    bn_axis = 3\n\n    conv_name_base = 'res' + str(stage) + block + '_branch'\n    bn_name_base = 'bn' + str(stage) + block + '_branch'\n\n    x = TimeDistributed(Convolution2D(nb_filter1, (1, 1), trainable=trainable, kernel_initializer='normal'), name=conv_name_base + '2a')(input_tensor)\n    x = TimeDistributed(BatchNormalization(axis=bn_axis), name=bn_name_base + '2a')(x)\n    x = Activation('relu')(x)\n\n    x = TimeDistributed(Convolution2D(nb_filter2, (kernel_size, kernel_size), trainable=trainable, kernel_initializer='normal',padding='same'), name=conv_name_base + '2b')(x)\n    x = TimeDistributed(BatchNormalization(axis=bn_axis), name=bn_name_base + '2b')(x)\n    x = Activation('relu')(x)\n\n    x = TimeDistributed(Convolution2D(nb_filter3, (1, 1), trainable=trainable, kernel_initializer='normal'), name=conv_name_base + '2c')(x)\n    x = TimeDistributed(BatchNormalization(axis=bn_axis), name=bn_name_base + '2c')(x)\n\n    x = Add()([x, input_tensor])\n    x = Activation('relu')(x)\n\n    return x\n```\n如果将时序信号看作是2D矩阵，则TimeDistributed包装后的Dense就是分别对矩阵的每一行进行全连接。\n\n**把resnet50最后一个stage拿出来做分类层：**\n```python\ndef classifier_layers(x, input_shape, trainable=False):\n\n    # Stage 5\n    x = conv_block_td(x, 3, [512, 512, 2048], stage=5, block='a', input_shape=input_shape, strides=(2, 2), trainable=trainable)\n    x = identity_block_td(x, 3, [512, 512, 2048], stage=5, block='b', trainable=trainable)\n    x = identity_block_td(x, 3, [512, 512, 2048], stage=5, block='c', trainable=trainable)\n\n    # AVGPOOL\n    x = TimeDistributed(AveragePooling2D((7, 7)), name='avg_pool')(x)\n\n    return x\n```\n\n* RoiPoolingConv：返回的shape为(1, 32, 7, 7, 512)含义是batch_size,预选框的个数，特征图宽，特征图高度，特征图深度\n* TimeDistributed：输入至少为3D张量，下标为1的维度将被认为是时间维。即对以一个维度下的变量当作一个完整变量来看待本文是32。你要实现的目的就是对32个预选宽提出的32个图片做出判断。\n* out_class的shape:(?, 32, 21); out_regr的shape:(?, 32, 80)\n```python\ndef classifier(base_layers, input_rois, num_rois, nb_classes = 21, trainable=False):\n\n    pooling_regions = 14\n    input_shape = (num_rois,14,14,1024)\n\n    out_roi_pool = RoiPoolingConv(pooling_regions, num_rois)([base_layers, input_rois])\n    out = classifier_layers(out_roi_pool, input_shape=input_shape, trainable=True)\n\n    out = TimeDistributed(Flatten())(out)\n\n    out_class = TimeDistributed(Dense(nb_classes, activation='softmax', kernel_initializer='zero'), name='dense_class_{}'.format(nb_classes))(out)\n    # note: no regression target for bg class\n    out_regr = TimeDistributed(Dense(4 * (nb_classes-1), activation='linear', kernel_initializer='zero'), name='dense_regress_{}'.format(nb_classes))(out)\n    return [out_class, out_regr]\n```\n\n**定义RPN网络：**\n* x_class:每一个锚点属于前景还是背景【注：这里使用的是sigmoid激活函数所以其输出的通道数是num_anchors】\n* x_regr：每一个锚点对应的回归梯度\n```python\ndef rpn(base_layers,num_anchors):\n\n    x = Convolution2D(512, (3, 3), padding='same', activation='relu', kernel_initializer='normal', name='rpn_conv1')(base_layers)\n\n    x_class = Convolution2D(num_anchors, (1, 1), activation='sigmoid', kernel_initializer='uniform', name='rpn_out_class')(x)\n    x_regr = Convolution2D(num_anchors * 4, (1, 1), activation='linear', kernel_initializer='zero', name='rpn_out_regress')(x)\n\n    return [x_class, x_regr, base_layers]\n```\n\n**resnet前面部分作为公共层：**\n```python\ndef nn_base(input_tensor=None, trainable=False):\n\n    # Determine proper input shape\n\n    input_shape = (None, None, 3)\n\n    if input_tensor is None:\n        img_input = Input(shape=input_shape)\n    else:\n        if not K.is_keras_tensor(input_tensor):\n            img_input = Input(tensor=input_tensor, shape=input_shape)\n        else:\n            img_input = input_tensor\n\n    bn_axis = 3\n\n    # Zero-Padding\n    x = ZeroPadding2D((3, 3))(img_input)\n\n    # Stage 1\n    x = Convolution2D(64, (7, 7), strides=(2, 2), name='conv1', trainable = trainable)(x)\n    x = BatchNormalization(axis=bn_axis, name='bn_conv1')(x)\n    x = Activation('relu')(x)\n    x = MaxPooling2D((3, 3), strides=(2, 2))(x)\n\n    # Stage 2\n    x = conv_block(x, 3, [64, 64, 256], stage=2, block='a', strides=(1, 1), trainable = trainable)\n    x = identity_block(x, 3, [64, 64, 256], stage=2, block='b', trainable = trainable)\n    x = identity_block(x, 3, [64, 64, 256], stage=2, block='c', trainable = trainable)\n\n    # Stage 3\n    x = conv_block(x, 3, [128, 128, 512], stage=3, block='a', trainable = trainable)\n    x = identity_block(x, 3, [128, 128, 512], stage=3, block='b', trainable = trainable)\n    x = identity_block(x, 3, [128, 128, 512], stage=3, block='c', trainable = trainable)\n    x = identity_block(x, 3, [128, 128, 512], stage=3, block='d', trainable = trainable)\n\n    # Stage 4\n    x = conv_block(x, 3, [256, 256, 1024], stage=4, block='a', trainable = trainable)\n    x = identity_block(x, 3, [256, 256, 1024], stage=4, block='b', trainable = trainable)\n    x = identity_block(x, 3, [256, 256, 1024], stage=4, block='c', trainable = trainable)\n    x = identity_block(x, 3, [256, 256, 1024], stage=4, block='d', trainable = trainable)\n    x = identity_block(x, 3, [256, 256, 1024], stage=4, block='e', trainable = trainable)\n    x = identity_block(x, 3, [256, 256, 1024], stage=4, block='f', trainable = trainable)\n\n    return x\n```\n\n**搭建网络：**\n```python\n# define the base network (resnet here)\nshared_layers = nn.nn_base(img_input, trainable=True)\n\n# define the RPN, built on the base layers\n# 9 types of anchors\nnum_anchors = len(C.anchor_box_scales) * len(C.anchor_box_ratios)\nrpn = nn.rpn(shared_layers, num_anchors)\n\nclassifier = nn.classifier(shared_layers, roi_input, C.num_rois, nb_classes=len(classes_count), trainable=True)\n\nmodel_rpn = Model(img_input, rpn[:2])\nmodel_classifier = Model([img_input, roi_input], classifier)\n\n# this is a model that holds both the RPN and the classifier, used to load/save weights for the models\nmodel_all = Model([img_input, roi_input], rpn[:2] + classifier)\n```\n\n[jupyter notebook](https://github.com/Trouble404/Object-Detection-System-Based-on-CNN-and-Capsule-Network/blob/master/Faster-RCNN/Resnet50/workflow_model.ipynb)\n\n### 【09/07/2018】\n#### Loss define\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/6_1.JPG)\n\n由于涉及到分类和回归，所以需要定义一个多任务损失函数(Multi-task Loss Function)，包括Softmax Classification Loss和Bounding Box Regression Loss，公式定义如下：\n\n$L(\\{p_{i}\\},\\{t_{i}\\}) = \\frac{1}{N_{cls}}\\sum_{i}L_{cls}(p_{i},p_{i}^{\\ast}) + \\lambda\\frac{1}{N_{reg}}\\sum_{i}L_{reg}(t_{i},t_{i}^{\\ast})$\n\n**Softmax Classification：**\n对于RPN网络的分类层(cls)，其向量维数为2k = 18，考虑整个特征图conv5-3，则输出大小为W×H×18，正好对应conv5-3上每个点有9个anchors，而每个anchor又有两个score(fg/bg)输出，对于单个anchor训练样本，其实是一个二分类问题。为了便于Softmax分类，需要对分类层执行reshape操作，这也是由底层数据结构决定的。\n在上式中，$p_{i}$为样本分类的概率值，$p_{i}^{\\ast}$为样本的标定值(label)，anchor为正样本时$p_{i}^{\\ast}$为1，为负样本时$p_{i}^{\\ast}$为0，$L_{cls}$为两种类别的对数损失(log loss)。\n\n**Bounding Box Regression：**\nRPN网络的回归层输出向量的维数为4k = 36，回归参数为每个样本的坐标$[x,y,w,h]$，分别为box的中心位置和宽高，考虑三组参数预测框(predicted box)坐标$[x,y,w,h]$，anchor坐标$[x_{a},y_{a},w_{a},h_{a}]$，ground truth坐标$[x^{\\ast},y^{\\ast},w^{\\ast},h^{\\ast}]$，分别计算预测框相对anchor中心位置的偏移量以及宽高的缩放量{$t$}，ground truth相对anchor的偏移量和缩放量{$t^{\\ast}$}\n\n$t_{x}=\\frac{(x-x_{a})}{w_{a}}$ , $t_{y}=\\frac{(y-y_{a})}{h_{a}}$ , $t_{w}=log(\\frac{w}{w_{a}})$ , $t_{h}=log(\\frac{h}{h_{a}})$ (1)\n$t_{x}^{\\ast}=\\frac{(x^{\\ast}-x_{a})}{w_{a}}$ , $t_{y}^{\\ast}=\\frac{(y^{\\ast}-y_{a})}{h_{a}}$ , $t_{w}^{\\ast}=log(\\frac{w^{\\ast}}{w_{a}})$ , $t_{h}^{\\ast}=log(\\frac{h^{\\ast}}{h_{a}})$ (2)\n\n回归目标就是让{t}尽可能地接近${t^{\\ast}}$，所以回归真正预测输出的是${t}$，而训练样本的标定真值为${t^{\\ast}}$。得到预测输出${t}$后，通过上式(1)即可反推获取预测框的真实坐标。\n\n在损失函数中，回归损失采用Smooth L1函数:\n\n$$ Smooth_{L1}(x) =\\left\\{\n\\begin{aligned}\n0.5x^{2} \\ \\ |x| \\leqslant 1\\\\\n|x| - 0.5 \\ \\ otherwise \n\\end{aligned}\n\\right.\n$$\n\n$L_{reg} = Smooth_{L1}(t-t^{\\ast})$\nSmooth L1损失函数曲线如下图所示，相比于L2损失函数，L1对离群点或异常值不敏感，可控制梯度的量级使训练更易收敛。\n![iamge](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/7_1.JPG)\n\n在损失函数中，$p_{i}^{\\ast}L_{reg}$这一项表示只有目标anchor$(p_{i}^{\\ast}=1)$才有回归损失，其他anchor不参与计算。这里需要注意的是，当样本bbox和ground truth比较接近时(IoU大于某一阈值)，可以认为上式的坐标变换是一种线性变换，因此可将样本用于训练线性回归模型，否则当bbox与ground truth离得较远时，就是非线性问题，用线性回归建模显然不合理，会导致模型不work。分类层(cls)和回归层(reg)的输出分别为{p}和{t}，两项损失函数分别由$N_{cls}$和$N_{reg}$以及一个平衡权重λ归一化。\n\n### 【10/07/2018】\n#### loss code\n  generator to iteror, using next() to loop\n```python\nyield np.copy(x_img), [np.copy(y_rpn_cls), np.copy(y_rpn_regr)], img_data_aug  \n```\nRpn calculation:\n```python\nimg,rpn,img_aug = next(data_gen_train)\n```\n\n ![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/7_2.JPG)\n\n连续两个def 是装饰器，\n装饰器其实也就是一个函数，一个用来包装函数的函数，返回一个修改之后的函数对象。经常被用于有切面需求的场景，较为经典的有插入日志、\n性能测试、事务处理等。装饰器是解决这类问题的绝佳设计，有了装饰器，我们就可以抽离出大量函数中与函数功能本身无关的雷同代码并继续重用。概括的讲，装\n饰器的作用就是为已经存在的对象添加额外的功能。\n\n根据：$L$ 的 cls 部分\n$L(\\{p_{i}\\},\\{t_{i}\\}) = \\frac{1}{N_{cls}}\\sum_{i}L_{cls}(p_{i},p_{i}^{\\ast})$\n\n在上式中，$p_{i}$为样本分类的概率值，$p_{i}^{\\ast}$为样本的标定值(label)，anchor为正样本时$p_{i}^{\\ast}$为1，为负样本时$p_{i}^{\\ast}$为0，$L_{cls}$为两种类别的对数损失(log loss)。\n\n因此， 定义 rpn loss cls:\n```python\ndef rpn_loss_cls(num_anchors):\n\tdef rpn_loss_cls_fixed_num(y_true, y_pred):\n            # binary_crossentropy -> logloss\n            # epsilon to increase robustness\n\t\treturn lambda_rpn_class * K.sum(y_true[:, :, :, :num_anchors] * K.binary_crossentropy(y_pred[:, :, :, :], y_true[:, :, :, num_anchors:])) / K.sum(epsilon + y_true[:, :, :, :num_anchors])\n\treturn rpn_loss_cls_fixed_num\n```\n\n根据$L$ 的 reg 部分\n$L(\\{p_{i}\\},\\{t_{i}\\}) =  \\lambda\\frac{1}{N_{reg}}\\sum_{i}L_{reg}(t_{i},t_{i}^{\\ast})$\n在损失函数中，回归损失采用Smooth L1函数:\n\n$$ Smooth_{L1}(x) =\\left\\{\n\\begin{aligned}\n0.5x^{2} \\ \\ |x| \\leqslant 1\\\\\n|x| - 0.5 \\ \\ otherwise \n\\end{aligned}\n\\right.\n$$\n$L_{reg} = Smooth_{L1}(t-t^{\\ast})$\n\n因此， 定义 rpn loss reg:\n```python\ndef rpn_loss_regr(num_anchors):\n\tdef rpn_loss_regr_fixed_num(y_true, y_pred):\n\n\t\t# difference of ture value and predicted value\n\t\tx = y_true[:, :, :, 4 * num_anchors:] - y_pred\n\t\t# absulote value of difference\n\t\tx_abs = K.abs(x)\n\t\t# if absulote value less than 1, x_bool == 1, else x_bool = 0\n\t\tx_bool = K.cast(K.less_equal(x_abs, 1.0), tf.float32)\n\n\t\treturn lambda_rpn_regr * K.sum(y_true[:, :, :, :4 * num_anchors] * (x_bool * (0.5 * x * x) + (1 - x_bool) * (x_abs - 0.5))) / K.sum(epsilon + y_true[:, :, :, :4 * num_anchors])\n\n\treturn rpn_loss_regr_fixed_num\n```\n\n对class的loss来说用一样的方程，但是class_loss_cls是无差别求loss【这个可以用K.mean，是因为其是无差别的求loss】，不用管是否可用\n```python\ndef class_loss_regr(num_classes):\n\tdef class_loss_regr_fixed_num(y_true, y_pred):\n\t\tx = y_true[:, :, 4*num_classes:] - y_pred\n\t\tx_abs = K.abs(x)\n\t\tx_bool = K.cast(K.less_equal(x_abs, 1.0), 'float32')\n\t\treturn lambda_cls_regr * K.sum(y_true[:, :, :4*num_classes] * (x_bool * (0.5 * x * x) + (1 - x_bool) * (x_abs - 0.5))) / K.sum(epsilon + y_true[:, :, :4*num_classes])\n\treturn class_loss_regr_fixed_num\n\n\ndef class_loss_cls(y_true, y_pred):\n\treturn lambda_cls_class * K.mean(categorical_crossentropy(y_true[0, :, :], y_pred[0, :, :]))\n```\n\n### 【11/07/2018】\n#### Iridis\n#### High Performance Computing (HPC)\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/8_1.jpg)\n[Introduction](https://www.southampton.ac.uk/isolutions/staff/high-performance-computing.page)\n\nIridis 5 specifications\n* #251 in hte world(Based on July 2018 TOPP500 list) with $R_{peak}\\sim1305.6\\ TFlops/s$\n* 464 2.0 GHz nodes with 40 cores per node, 192 GB memeory\n* 10 nodes with 4xGTX1080TI GPUs, 28 cores(hyper-threaded), 128 GB memeory\n* 10 nodes with 2xVolta Tesia GPUs, same as thandard compute\n* 2.2 PB disk with paraller file system (>12GB\\s)\n* £5M Project delivered by OCF/IBM\n\n[MobaXterm](https://mobaxterm.mobatek.net/)\n\n#### create my own conda envieroment\nFllowing instroduction before\n\n#### Slurm command\n\nCommand | Definition\n---- | ---\nsbatch | Submits job scripts into system for execution (queued)\nscancel |  Cancels a job\nscontrol | Used to display Slurm state, several options only available to root\nsinfo | Display state of partitions and nodes\nsqueue | Display state of jobs\nsalloc | Submit a job for execution, or initiate job in real time\n\n** Bash script**\n```bash\n#!/bin/bash\n#SBATCH -J faster_rcnn \n#SBATCH -o train_7.out\n#SBATCH --ntasks=28\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=8\n#SBATCH --time=00:05:00\n#SBATCH --gres=gpu:1\n#SBATCH -p lyceum\n\nmodule load conda\nmodule load cuda\nsource activate project\npython test_frcnn.py\n```\n\n\n### 【12~13/07/2018】\n#### change plan\n\n因为faster r-cnn的搭建过程比想象中复杂，在咨询老师的意见以后，决定砍掉capsule的测试，专心faster-rcnn并且找到一些fine turn的方法。\n\n1）基础特征提取网络\nResNet，IncRes V2，ResNeXt 都是显著超越 VGG 的特征网络，当然网络的改进带来的是计算量的增加。\n\n2）RPN\n通过更准确地  RPN 方法，减少 Proposal 个数，提高准确度。\n\n3）改进分类回归层\n分类回归层的改进，包括 通过多层来提取特征 和 判别。\n\n---\n\n@改进1：ION\n论文：Inside outside net: Detecting objects in context with skip pooling and recurrent neural networks\n提出了两个方面的贡献：\n\n1）Inside Net\n所谓 Inside 是指在 ROI 区域之内，通过连接不同 Scale 下的 Feature Map，实现多尺度特征融合。\n这里采用的是 Skip-Pooling，从 conv3-4-5-context 分别提取特征，后面会讲到。\n多尺度特征 能够提升对小目标的检测精度。\n\n2）Outside Net\n所谓 Outside 是指 ROI 区域之外，也就是目标周围的 上下文（Contextual）信息。\n作者通过添加了两个 RNN 层（修改后的 IRNN）实现上下文特征提取。\n上下文信息 对于目标遮挡有比较好的适应。\n\n---\n\n@改进2：多尺度之 HyperNet\n论文：Hypernet: Towards accurate region proposal generation and joint object detection\n基于 Region Proposal 的方法，通过多尺度的特征提取来提高对小目标的检测能力，来看网络框图：\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/8_2.JPG)\n分为 三个主要特征 来介绍（对应上面网络拓扑图的 三个红色框）：\n\n1）Hyper Feature Extraction （特征提取）\n多尺度特征提取是本文的核心点，作者的方法稍微有所不同，他是以中间的 Feature 尺度为参考，前面的层通过 Max Pooling 到对应大小，后面的层则是通过 反卷积（Deconv）进行放大。\n多尺度 Feature ConCat 的时候，作者使用了 LRN进行归一化（类似于 ION 的 L2 Norm）。\n\n2）Region Proposal Generation（建议框生成）\n作者设计了一个轻量级的 ConvNet，与 RPN 的区别不大（为写论文强创新)。\n一个 ROI Pooling层，一个 Conv 层，还有一个 FC 层。每个 Position 通过 ROI Pooling 得到一个 13\\*13 的 bin，通过 Conv（3\\*3\\*4）层得到一个 13\\*13\\*4 的 Cube，再通过 FC 层得到一个 256d 的向量。\n后面的 Score+ BBox_Reg 与 Faster并无区别，用于目标得分 和 Location OffSet。\n考虑到建议框的 Overlap，作者用了 Greedy NMS 去重，文中将 IOU参考设为 0.7，每个 Image 保留 1k 个 Region，并选择其中 Top-200 做 Detetcion。\n通过对比，要优于基于 Edge Box 重排序的 Deep Box，从多尺度上考虑比 Deep Proposal 效果更好。\n\n3）Object Detection（目标检测）\n与 Fast RCNN基本一致，在原来的检测网络基础上做了两点改进：\na）在 FC 层之前添加了一个 卷积层（3*3*63），对特征有效降维；\nb）将 DropOut 从 0.5 降到 0.25；\n另外，与 Proposal一样采用了 NMS 进行 Box抑制，但由于之前已经做了，这一步的意义不大。\n\n---\n\n@改进3：多尺度之 MSCNN\n论文：A Unified Multi-scale Deep Convolutional Neural Network for Fast Object Detection\na）原图缩放，多个Scale的原图对应不同Scale的Feature；\n该方法计算多次Scale，每个Scale提取一次Feature，计算量巨大。\n\nb）一幅输入图像对应多个分类器；\n不需要重复提取特征图，但对分类器要求很高，一般很难得到理想的结果。\n\nc）原图缩放，少量Scale原图->少量特征图->多个Model模板；\n相当于对 a）和 b）的 Trade-Off。\n\nd）原图缩放，少量Scale原图->少量特征图->特征图插值->1个Model；\n\ne）RCNN方法，Proposal直接给到CNN；\n和 a）全图计算不同，只针对Patch计算。\n\nf）RPN方法，特征图是通过CNN卷积层得到；\n和 b）类似，不过采用的是同尺度的不同模板，容易导致尺度不一致问题。\n\ng）上套路，提出我们自己的方法，多尺度特征图；\n每个尺度特征图对应一个 输出模板，每个尺度cover一个目标尺寸范围。\n\n---\n\nNMS和soft-nms算法\nRepulsion loss：遮挡下的行人检测 加入overlapping 与不同的 loss\n融合以上两个到faster rcnn中\n\n### 【16~20/07/2018】\n旅行\n\n\n### 【23/07/2018】\n#### fix boxes location by regrident\n使用regr对anchor所确定的框进行修正\n\n```python\n\"\"\" fix boxes with grident\n\n@param X: current cordinates of box\n@param T: coresspoding grident\n\n\n@return: Fixed cordinates of box\n\"\"\"\ndef apply_regr_np(X, T):\n\ttry:\n\t\tx = X[0, :, :]\n\t\ty = X[1, :, :]\n\t\tw = X[2, :, :]\n\t\th = X[3, :, :]\n\n\t\ttx = T[0, :, :]\n\t\tty = T[1, :, :]\n\t\ttw = T[2, :, :]\n\t\tth = T[3, :, :]\n```\n$t_{x}=\\frac{(x-x_{a})}{w_{a}}$ , $t_{y}=\\frac{(y-y_{a})}{h_{a}}$ , $t_{w}=log(\\frac{w}{w_{a}})$ , $t_{h}=log(\\frac{h}{h_{a}})$ (1)\n$t_{x}^{\\ast}=\\frac{(x^{\\ast}-x_{a})}{w_{a}}$ , $t_{y}^{\\ast}=\\frac{(y^{\\ast}-y_{a})}{h_{a}}$ , $t_{w}^{\\ast}=log(\\frac{w^{\\ast}}{w_{a}})$ , $t_{h}^{\\ast}=log(\\frac{h^{\\ast}}{h_{a}})$ (2)\n\n回归目标就是让{t}尽可能地接近${t^{\\ast}}$，所以回归真正预测输出的是${t}$，而训练样本的标定真值为${t^{\\ast}}$。得到预测输出${t}$后，通过上式(1)即可反推获取预测框的真实坐标。\n\n过程：\n```python\n\t\t# centre cordinate\n\t\tcx = x + w/2.\n\t\tcy = y + h/2.\n\t\t# fixed centre cordinate\n\t\tcx1 = tx * w + cx\n\t\tcy1 = ty * h + cy\n\n\t\t# fixed wdith and height\n\t\tw1 = np.exp(tw.astype(np.float64)) * w\n\t\th1 = np.exp(th.astype(np.float64)) * h\n\n\t\t# fixed left top corner's cordinate\n\t\tx1 = cx1 - w1/2.\n\t\ty1 = cy1 - h1/2.\n\n\t\t# apporximate\n\t\tx1 = np.round(x1)\n\t\ty1 = np.round(y1)\n\t\tw1 = np.round(w1)\n\t\th1 = np.round(h1)\n\t\treturn np.stack([x1, y1, w1, h1])\n\texcept Exception as e:\n\t\tprint(e)\n\t\treturn X\n```\n\n\n#### NMS no max suppression\n该函数的作用是从所给定的所有预选框中选择指定个数最合理的边框。\n\n定义：\n```python\n\"\"\" NMS , delete overlapping box\n\n@param boxes: (n,4) box and coresspoding cordinates\n@param probs: (n,) box adn coresspding possibility\n@param overlap_thresh: treshold of delet box overlapping\n@param max_boxes: maximum keeping number of boxes\n\n\n@return: boxes: boxes cordinates(x1,y1,x2,y2)\n@return: probs: coresspoding possibility\n\"\"\"\ndef non_max_suppression_fast(boxes, probs, overlap_thresh=0.9, max_boxes=300):\n```\n\n```python\nif len(boxes) == 0:\n   return []\n\n# grab the coordinates of the bounding boxes\nx1 = boxes[:, 0]\ny1 = boxes[:, 1]\nx2 = boxes[:, 2]\ny2 = boxes[:, 3]\n\nnp.testing.assert_array_less(x1, x2)\nnp.testing.assert_array_less(y1, y2)\n\n# if the bounding boxes integers, convert them to floats --\n# this is important since we'll be doing a bunch of divisions\nif boxes.dtype.kind == \"i\":\n\tboxes = boxes.astype(\"float\")\n```\n对输入的数据进行确认\n不能为空\n左上角的坐标小于右下角\n数据类型的转换\n```python\n# initialize the list of picked indexes\t\npick = []\n\n# calculate the areas\narea = (x2 - x1) * (y2 - y1)\n\n# sort the bounding boxes \nidxs = np.argsort(probs)\n```\npick（拾取）用来存放边框序号\n计算框的面积\nprobs按照概率从小到大排序\n```python\nwhile len(idxs) > 0:\n# grab the last index in the indexes list and add the\n# index value to the list of picked indexes\nlast = len(idxs) - 1\ni = idxs[last]\npick.append(i)\n```\n接下来就是按照概率从大到小取出框，且框的重合度不可以高于overlap_thresh。代码的思路是这样的：\n\n每一次取概率最大的框（即idxs最后一个）\n删除掉剩下的框中重和度高于overlap_thresh的框\n直到取满max_boxes为止\n```python\n# find the intersection\n\nxx1_int = np.maximum(x1[i], x1[idxs[:last]])\nyy1_int = np.maximum(y1[i], y1[idxs[:last]])\nxx2_int = np.minimum(x2[i], x2[idxs[:last]])\nyy2_int = np.minimum(y2[i], y2[idxs[:last]])\n\nww_int = np.maximum(0, xx2_int - xx1_int)\nhh_int = np.maximum(0, yy2_int - yy1_int)\n\narea_int = ww_int * hh_int\n```\n取出idxs队列中最大概率框的序号，将其添加到pick中\n```python\n# find the union\narea_union = area[i] + area[idxs[:last]] - area_int\n\n# compute the ratio of overlap\noverlap = area_int/(area_union + 1e-6)\n\n# delete all indexes from the index list that have\nidxs = np.delete(idxs, np.concatenate(([last],np.where(overlap > overlap_thresh)[0])))\n```\n计算取出来的框与剩下来的框区域的交集\n```python\nif len(pick) >= max_boxes:\n   break\n```\n计算重叠率，然后删除掉重叠率较高的位置\\[np.concatest\\]，是因为最后一个位置你已经用过了，就得将其从队列中删掉\n当取足max_boxes框，停止循环\n```python\nboxes = boxes[pick].astype(\"int\")\nprobs = probs[pick]\nreturn boxes, probs\n```\n返回pick内存取的边框和对应的概率\n\n### 【24/07/2018】\n#### rpn to porposal fixed\n该函数的作用是将rpn网络的预测结果转化到一个个预选框\n函数流程：\n遍历anchor_size，在遍历anchor_ratio\n\n得到框的长宽在原图上的映射\n\n得到相应尺寸的框对应的回归梯度，将深度都放到第一个维度\n注1：regr_layer\\[0, :, :, 4 \\* curr_layer:4 \\* curr_layer + 4]当某一个维度的取值为一个值时，那么新的变量就会减小一个维度\n注2：curr_layer代表的是特定长度和比例的框所代表的编号\n\n得到anchor对应的（x,y,w,h）\n\n使用regr对anchor所确定的框进行修正\n\n对修正后的边框一些不合理的地方进行矫正。\n如，边框回归后的左上角和右下角的点不能超过图片外，框的宽高不可以小于0\n注：得到框的形式是（x1,y1,x2,y2）\n\n得到all_boxes形状是（n,4），和每一个框对应的概率all_probs形状是（n,）\n\n删除掉一些不合理的点，即右下角的点值要小于左上角的点值\n注：np.where() 返回位置信息，这也是删除不符合要求点的一种方法\nnp.delete(all_boxes, idxs, 0)最后一个参数是在哪一个维度删除\n\n最后是根据要求选取指定个数的合理预选框。这一步是重要的，因为每一个点可以有9个预选框，而又拥有很多点，一张图片可能会有几万个预选框。而经过这一步预选迅速下降到几百个。\n```python\n\"\"\" rpn to porposal\n\n@param rpn_layer: porposal's coresspoding possibiliy, whether item exciseted\n@param regr_layer: porposal's coresspoding regrident\n@param C: Configuration\n@param dim_ordering: Dimensional organization\n@param use_regr=True: wether use regurident to fix proposal\n@param max_boxes=300: max boxes after apply this function\n@param overlap_thresh=0.9: threshold of overlapping\n@param C: Configuration\n\n@return: max_boxes proposal with format (x1,y1,x2,y2)\n\"\"\"\n\ndef rpn_to_roi(rpn_layer, regr_layer, C, dim_ordering, use_regr=True, max_boxes=300,overlap_thresh=0.9):\n\t# std_scaling default 4\n\tregr_layer = regr_layer / C.std_scaling\n\n\tanchor_sizes = C.anchor_box_scales\n\tanchor_ratios = C.anchor_box_ratios\n\n\tassert rpn_layer.shape[0] == 1\n\n\t# obtain img's width and height's matrix\n\t(rows, cols) = rpn_layer.shape[1:3]\n\n\tcurr_layer = 0\n\n\tA = np.zeros((4, rpn_layer.shape[1], rpn_layer.shape[2], rpn_layer.shape[3]))\n\n\t# anchor size is [128, 256, 512]\n\tfor anchor_size in anchor_sizes:\n\t\t# anchor ratio is [1,2,1]\n\t\tfor anchor_ratio in anchor_ratios:\n\t\t\t# rpn_stride = 16\n\t\t\t# obatin anchor's weidth and height on feature map\n\t\t\tanchor_x = (anchor_size * anchor_ratio[0])/C.rpn_stride\n\t\t\tanchor_y = (anchor_size * anchor_ratio[1])/C.rpn_stride\n\n\t\t\t# obtain current regrident\n\t\t\t# when one dimentional obtain a value, the new varirant will decrease one dimenttion\n\t\t\tregr = regr_layer[0, :, :, 4 * curr_layer:4 * curr_layer + 4]\n\t\t\t# put depth to first bacause tensorflow as backend\n\t\t\tregr = np.transpose(regr, (2, 0, 1))\n\n\t\t\t# The rows of the output array X are copies of the vector x; columns of the output array Y are copies of the vector y\n\t\t\t# each cordinartes of matrix cls and rows\n\t\t\tX, Y = np.meshgrid(np.arange(cols),np. arange(rows))\n\n\t\t\t# obtain anchors's (x,y,w,h)\n\t\t\tA[0, :, :, curr_layer] = X - anchor_x/2\n\t\t\tA[1, :, :, curr_layer] = Y - anchor_y/2\n\t\t\tA[2, :, :, curr_layer] = anchor_x\n\t\t\tA[3, :, :, curr_layer] = anchor_y\n\n\t\t\t# fix boxes with grident\n\t\t\tif use_regr:\n\t\t\t\t# fixed corinates of box\n\t\t\t\tA[:, :, :, curr_layer] = apply_regr_np(A[:, :, :, curr_layer], regr)\n\n\t\t\t# fix unreasonable cordinates\n\t\t\t# np.maximum(1,[]) will set the value less than 1 in [] to 1\n\t\t\t# box's width and height can't less than 0\n\t\t\tA[2, :, :, curr_layer] = np.maximum(1, A[2, :, :, curr_layer])\n\t\t\tA[3, :, :, curr_layer] = np.maximum(1, A[3, :, :, curr_layer])\n\t\t\t# fixed right bottom cordinates\n\t\t\tA[2, :, :, curr_layer] += A[0, :, :, curr_layer]\n\t\t\tA[3, :, :, curr_layer] += A[1, :, :, curr_layer]\n\n\t\t\t# left top corner cordinates can't out image\n\t\t\tA[0, :, :, curr_layer] = np.maximum(0, A[0, :, :, curr_layer])\n\t\t\tA[1, :, :, curr_layer] = np.maximum(0, A[1, :, :, curr_layer])\n\t\t\t# right bottom corner cordinates can't out img\n\t\t\tA[2, :, :, curr_layer] = np.minimum(cols-1, A[2, :, :, curr_layer])\n\t\t\tA[3, :, :, curr_layer] = np.minimum(rows-1, A[3, :, :, curr_layer])\n\n\t\t\t# next layer\n\t\t\tcurr_layer += 1\n\n\t# obtain (n,4) object and coresspoding cordinate\n\tall_boxes = np.reshape(A.transpose((0, 3, 1,2)), (4, -1)).transpose((1, 0))\n\t# obtain(n,) object and creoespdoing possibility\n\tall_probs = rpn_layer.transpose((0, 3, 1, 2)).reshape((-1))\n\n\t# cordinates of left top and right bottom of box\n\tx1 = all_boxes[:, 0]\n\ty1 = all_boxes[:, 1]\n\tx2 = all_boxes[:, 2]\n\ty2 = all_boxes[:, 3]\n\n\t# find where right cordinate bigger than left cordinate\n\tidxs = np.where((x1 - x2 >= 0) | (y1 - y2 >= 0))\n\t# delete thoese point at 0 dimentional -> all boxes\n\tall_boxes = np.delete(all_boxes, idxs, 0)\n\tall_probs = np.delete(all_probs, idxs, 0)\n\n\t# apply NMS to reduce overlapping boxes\n\tresult = non_max_suppression_fast(all_boxes, all_probs, overlap_thresh=overlap_thresh, max_boxes=max_boxes)[0]\n\n\treturn result\n```\n\n### 【25/07/2018】\n#### generate classifier's trainning data\n该函数的作用是生成classifier网络训练的数据,需要注意的是它对提供的预选框还会做一次选择就是将容易判断的背景删除\n\n代码流程：\n得到图片的基本信息，并将图片的最短边规整到相应的长度。并将bboxes的长度做相应的变化\n\n遍历所有的预选框R, 将每一个预选框与所有的bboxes求交并比，记录最大交并比。用来确定该预选框的类别。\n\n对最佳的交并比作不同的判断:\n当最佳交并比小于最小的阈值时，放弃概框。因为，交并比太低就说明是很好判断的背景没必要训练。当大于最小阈值时，则保留相关的边框信息\n当在最小和最大之间，就认为是背景。有必要进行训练。\n大于最大阈值时认为是物体，计算其边框回归梯度\n\n得到该类别对应的数字\n将该数字对应的地方置为1【one-hot】\n将该类别加入到y_class_num\ncoords是用来存储边框回归梯度的，labels来决定是否要加入计算loss中\n```python\nclass_num = 2\nclass_label = 10 * [0]\nprint(class_label)\nclass_label[class_num] = 1\nprint(class_label)\n输出：\n[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n[0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n```\n如果不是背景的话，计算相应的回归梯度\n\n返回数据\n\n```python\n\"\"\" generate classifier training data\n\n@param R: porposal -> boxes\n@param img_data: image data\n@param C: configuration\n@param class_mapping: classes and coresspoding numbers\n\n@return: np.expand_dims(X, axis=0): boxes after filter\n@return: np.expand_dims(Y1, axis=0): boxes coresspoding class\n@return: np.expand_dims(Y2, axis=0): boxes coresspoding regurident\n@return IoUs: IOU\n\n\"\"\"\ndef calc_iou(R, img_data, C, class_mapping):\n\n\t# obtain boxxes information from img data\n\tbboxes = img_data['bboxes']\n\t# obtain width and height of img\n\t(width, height) = (img_data['width'], img_data['height'])\n\t# get image dimensions for resizing\n\t# Fix image's shortest edge to config setting: eg: 600\n\t(resized_width, resized_height) = get_new_img_size(width, height, C.im_size)\n\n\t# record parameters, bboxes cordinates on feature map\n\tgta = np.zeros((len(bboxes), 4))\n\n\t# change bboxes's width and height because the img was rezised\n\tfor bbox_num, bbox in enumerate(bboxes):\n\t\t# get the GT box coordinates, and resize to account for image resizing\n\t\t# /C.rpn_stride mapping to feature map\n\t\tgta[bbox_num, 0] = int(round(bbox['x1'] * (resized_width / float(width))/C.rpn_stride))\n\t\tgta[bbox_num, 1] = int(round(bbox['x2'] * (resized_width / float(width))/C.rpn_stride))\n\t\tgta[bbox_num, 2] = int(round(bbox['y1'] * (resized_height / float(height))/C.rpn_stride))\n\t\tgta[bbox_num, 3] = int(round(bbox['y2'] * (resized_height / float(height))/C.rpn_stride))\n\n\tx_roi = []\n\ty_class_num = []\n\ty_class_regr_coords = []\n\ty_class_regr_label = []\n\tIoUs = [] # for debugging only\n\n\t# for all given proposals -> boxes\n\tfor ix in range(R.shape[0]):\n\t\t# current boxes's cordinates\n\t\t(x1, y1, x2, y2) = R[ix, :]\n\t\tx1 = int(round(x1))\n\t\ty1 = int(round(y1))\n\t\tx2 = int(round(x2))\n\t\ty2 = int(round(y2))\n\n\t\tbest_iou = 0.0\n\t\tbest_bbox = -1\n\t\t# using current proposal to compare with given xml's boxes\n\t\tfor bbox_num in range(len(bboxes)):\n\t\t\t# calculate current iou\n\t\t\tcurr_iou = iou([gta[bbox_num, 0], gta[bbox_num, 2], gta[bbox_num, 1], gta[bbox_num, 3]], [x1, y1, x2, y2])\n\t\t\t# update parameters\n\t\t\tif curr_iou > best_iou:\n\t\t\t\tbest_iou = curr_iou\n\t\t\t\tbest_bbox = bbox_num\n\n\t\t# if iou to small, we don't put it in trainning because it should be backgroud\n\t\tif best_iou < C.classifier_min_overlap:\n\t\t\t\tcontinue\n\t\telse:\n\t\t\t# saveing left top cordinates, width and height\n\t\t\tw = x2 - x1\n\t\t\th = y2 - y1\n\t\t\tx_roi.append([x1, y1, w, h])\n\t\t\t# saving this bbox's iou\n\t\t\tIoUs.append(best_iou)\n\n\t\t\t# hard to classfier -> set it to backgroud\n\t\t\tif C.classifier_min_overlap <= best_iou < C.classifier_max_overlap:\n\t\t\t\t# hard negative example\n\t\t\t\tcls_name = 'bg'\n\n\t\t\t# valid proposal\n\t\t\telif C.classifier_max_overlap <= best_iou:\n\t\t\t\t# coresspoding class name\n\t\t\t\tcls_name = bboxes[best_bbox]['class']\n\n\t\t\t\t# calculate rpn graident with true cordinates given by xml file\n\t\t\t\tcxg = (gta[best_bbox, 0] + gta[best_bbox, 1]) / 2.0\n\t\t\t\tcyg = (gta[best_bbox, 2] + gta[best_bbox, 3]) / 2.0\n\n\t\t\t\tcx = x1 + w / 2.0\n\t\t\t\tcy = y1 + h / 2.0\n\n\t\t\t\ttx = (cxg - cx) / float(w)\n\t\t\t\tty = (cyg - cy) / float(h)\n\t\t\t\ttw = np.log((gta[best_bbox, 1] - gta[best_bbox, 0]) / float(w))\n\t\t\t\tth = np.log((gta[best_bbox, 3] - gta[best_bbox, 2]) / float(h))\n\t\t\telse:\n\t\t\t\tprint('roi = {}'.format(best_iou))\n\t\t\t\traise RuntimeError\n\n\t\t# class name's mapping number\n\t\tclass_num = class_mapping[cls_name]\n\t\t# list of calss label\n\t\tclass_label = len(class_mapping) * [0]\n\t\t# set class_num's coresspoding location to 1\n\t\tclass_label[class_num] = 1\n\t\t# privous is one-hot vector\n\n\t\t# saving the one-hot vector\n\t\ty_class_num.append(copy.deepcopy(class_label))\n\n\t\t# coords used to saving calculated graident\n\t\tcoords = [0] * 4 * (len(class_mapping) - 1)\n\t\t# labels used to decide whether adding to loss calculation\n\t\tlabels = [0] * 4 * (len(class_mapping) - 1)\n\t\tif cls_name != 'bg':\n\t\t\tlabel_pos = 4 * class_num\n\t\t\tsx, sy, sw, sh = C.classifier_regr_std\n\t\t\tcoords[label_pos:4+label_pos] = [sx*tx, sy*ty, sw*tw, sh*th]\n\t\t\tlabels[label_pos:4+label_pos] = [1, 1, 1, 1]\n\t\t\ty_class_regr_coords.append(copy.deepcopy(coords))\n\t\t\ty_class_regr_label.append(copy.deepcopy(labels))\n\t\telse:\n\t\t\ty_class_regr_coords.append(copy.deepcopy(coords))\n\t\t\ty_class_regr_label.append(copy.deepcopy(labels))\n\n\t# no bboxes\n\tif len(x_roi) == 0:\n\t\treturn None, None, None, None\n\n\t# matrix with [x1, y1, w, h]\n\tX = np.array(x_roi)\n\t# boxxes coresspoding class number\n\tY1 = np.array(y_class_num)\n\t# matrix of whether adding to calculation and coresspoding regrident\n\tY2 = np.concatenate([np.array(y_class_regr_label),np.array(y_class_regr_coords)],axis=1)\n\n\t# adding batch size dimention\n\treturn np.expand_dims(X, axis=0), np.expand_dims(Y1, axis=0), np.expand_dims(Y2, axis=0), IoUs\n```\n\n### 【26~27/07/2018】\n#### model parameters\n\n```python\n# rpn optimizer\noptimizer = Adam(lr=1e-5)\n# classifier optimizer\noptimizer_classifier = Adam(lr=1e-5)\n# defined loss apply, metrics used to print accury\nmodel_rpn.compile(optimizer=optimizer, loss=[losses.rpn_loss_cls(num_anchors), losses.rpn_loss_regr(num_anchors)])\nmodel_classifier.compile(optimizer=optimizer_classifier, loss=[losses.class_loss_cls, losses.class_loss_regr(len(classes_count)-1)], metrics={'dense_class_{}'.format(len(classes_count)): 'accuracy'})\n# for saving weight\nmodel_all.compile(optimizer='sgd', loss='mae')\n\n# traing time of each epochs\nepoch_length = 1000\n# totoal epochs\nnum_epochs = 2000\n#\niter_num = 0\n# losses saving matrix\nlosses = np.zeros((epoch_length, 5))\nrpn_accuracy_rpn_monitor = []\nrpn_accuracy_for_epoch = []\nstart_time = time.time()\n# current total loss\nbest_loss = np.Inf\n# sorted classing mapping\nclass_mapping_inv = {v: k for k, v in class_mapping.items()}\n```\n\n#### Training process\n函数流程：\n**训练rpn网络并且进行预测：**\n训练RPN网络,X是图片、Y是对应类别和回归梯度【注：并不是所有的点都参与训练，只有符合条件的点才参与训练】\n\n**根据rpn网络的预测结果得到classifier网络的训练数据:**\n将预测结果转化为预选框\n计算宽属于哪一类，回归梯度是多少\n如果没有有效的预选框则结束本次循环\n得到正负样本在的位置【Y1\\[0, :, -1\\]：0指定batch的位置，：指所有框，-1指最后一个维度即背景类】\nneg_samples = neg_samples\\[0\\]：这样做的原因是将其变为一维的数组\n下面这一步是选择C.num_rois个数的框，送入classifier网络进行训练。思路是：当C.num_rois大于1的时候正负样本尽量取到各一半，小于1的时候正负样本随机取一个。需要注意的是我们这是拿到的是正负样本在的位置而不是正负样本本身，这也是随机抽取的一般方法\n\n**训练classifier网络:**\n打印Loss和accury\n如果网络有两个不同的输出，那么第一个是和损失接下来是分损失【loss_class\\[3\\]：代表是准确率在定义网络的时候定义了】\n```python\nclassifer网络的loss输出：\n[1.4640709, 1.0986123, 0.36545864, 0.15625]\n```\n还有就是这些loss都是list数据类型，所以要把它倒腾到numpy数据中\n当结束一轮的epoch时，只有当这轮epoch的loss小于最优的时候才会存储这轮的训练数据。并结束这轮epoch进入下一轮epoch.\n\n---\n\n```python\n# Training Process\nprint('Starting training')\n\nfor epoch_num in range(num_epochs):\n    #progbar is used to print % of processing\n\tprogbar = generic_utils.Progbar(epoch_length)\n    # print current process\n\tprint('Epoch {}/{}'.format(epoch_num + 1, num_epochs))\n\n\twhile True:\n\t\ttry:\n\n            # verbose True to print RPN situation, if can't generate boxes on positivate object, it will print error\n\t\t\tif len(rpn_accuracy_rpn_monitor) == epoch_length and C.verbose:\n                # postivate boxes / all boxes\n\t\t\t\tmean_overlapping_bboxes = float(sum(rpn_accuracy_rpn_monitor))/len(rpn_accuracy_rpn_monitor)\n\t\t\t\trpn_accuracy_rpn_monitor = []\n\t\t\t\tprint('Average number of overlapping bounding boxes from RPN = {} for {} previous iterations'.format(mean_overlapping_bboxes, epoch_length))\n\t\t\t\tif mean_overlapping_bboxes == 0:\n\t\t\t\t\tprint('RPN is not producing bounding boxes that overlap the ground truth boxes. Check RPN settings or keep training.')\n\n            # obtain img, rpn information and img xml format\n\t\t\tX, Y, img_data = next(data_gen_train)\n\n            # train RPN net, X is img, Y is correspoding class type and graident\n\t\t\tloss_rpn = model_rpn.train_on_batch(X, Y)\n\n            # predict new Y from privious rpn model\n\t\t\tP_rpn = model_rpn.predict_on_batch(X)\n\n            # transform predicted rpn to cordinates of boxes\n\t\t\tR = rpn_to_boxes.rpn_to_roi(P_rpn[0], P_rpn[1], C, K.image_dim_ordering(), use_regr=True, overlap_thresh=0.7, max_boxes=300)\n\t\t\t# note: calc_iou converts from (x1,y1,x2,y2) to (x,y,w,h) format\n            # X2: [x,y,w,h]\n            # Y1: coresspoding class number -> one hot vector\n            # Y2: boxes coresspoding regrident\n\t\t\tX2, Y1, Y2, IouS = rpn_to_classifier.calc_iou(R, img_data, C, class_mapping)\n\n            # no box, stop this epoch\n\t\t\tif X2 is None:\n\t\t\t\trpn_accuracy_rpn_monitor.append(0)\n\t\t\t\trpn_accuracy_for_epoch.append(0)\n\t\t\t\tcontinue\n\n            # if last position of one-hot is 1 -> is background\n\t\t\tneg_samples = np.where(Y1[0, :, -1] == 1)\n            # else is postivate sample\n\t\t\tpos_samples = np.where(Y1[0, :, -1] == 0)\n\n            # obtain backgourd samples's coresspoding rows\n\t\t\tif len(neg_samples) > 0:\n\t\t\t\tneg_samples = neg_samples[0]\n\t\t\telse:\n\t\t\t\tneg_samples = []\n\n            # obtain posivate samples's coresspoding rows\n\t\t\tif len(pos_samples) > 0:\n\t\t\t\tpos_samples = pos_samples[0]\n\t\t\telse:\n\t\t\t\tpos_samples = []\n\t\t\t# saving posivate samples's number\n\t\t\trpn_accuracy_rpn_monitor.append(len(pos_samples))\n\t\t\trpn_accuracy_for_epoch.append((len(pos_samples)))\n\n            # default 4 here\n\t\t\tif C.num_rois > 1:\n                # wehn postivate samples less than 2\n\t\t\t\tif len(pos_samples) < C.num_rois//2:\n                    # chosse all samples\n\t\t\t\t\tselected_pos_samples = pos_samples.tolist()\n\t\t\t\telse:\n                    # random choose 2 samples\n\t\t\t\t\tselected_pos_samples = np.random.choice(pos_samples, C.num_rois//2, replace=False).tolist()\n\t\t\t\ttry:\n                    # random choose num_rois - positave samples naegivate samples\n\t\t\t\t\tselected_neg_samples = np.random.choice(neg_samples, C.num_rois - len(selected_pos_samples), replace=False).tolist()\n\t\t\t\texcept:\n                    # if no enought neg samples, copy priouvs neg sample\n\t\t\t\t\tselected_neg_samples = np.random.choice(neg_samples, C.num_rois - len(selected_pos_samples), replace=True).tolist()\n\n                # samples picked to classifier network\n\t\t\t\tsel_samples = selected_pos_samples + selected_neg_samples\n\t\t\telse:\n\t\t\t\t# in the extreme case where num_rois = 1, we pick a random pos or neg sample\n\t\t\t\tselected_pos_samples = pos_samples.tolist()\n\t\t\t\tselected_neg_samples = neg_samples.tolist()\n\t\t\t\tif np.random.randint(0, 2):\n\t\t\t\t\tsel_samples = random.choice(neg_samples)\n\t\t\t\telse:\n\t\t\t\t\tsel_samples = random.choice(pos_samples)\n\n            # train classifier, img data, selceted samples' cordinates, mapping number of selected samples, coresspoding regreident\n\t\t\tloss_class = model_classifier.train_on_batch([X, X2[:, sel_samples, :]], [Y1[:, sel_samples, :], Y2[:, sel_samples, :]])\n\n            # in Keras, if loss part bigger than 1, it will return sum of part losses, each loss and accury\n            # put each losses and accury into losses\n\t\t\tlosses[iter_num, 0] = loss_rpn[1]\n\t\t\tlosses[iter_num, 1] = loss_rpn[2]\n\n\t\t\tlosses[iter_num, 2] = loss_class[1]\n\t\t\tlosses[iter_num, 3] = loss_class[2]\n\t\t\tlosses[iter_num, 4] = loss_class[3]\n\n            # next iter\n\t\t\titer_num += 1\n\n            # display and update current mean value of losses\n\t\t\tprogbar.update(iter_num, [('rpn_cls', np.mean(losses[:iter_num, 0])), ('rpn_regr', np.mean(losses[:iter_num, 1])),\n\t\t\t\t\t\t\t\t\t  ('detector_cls', np.mean(losses[:iter_num, 2])), ('detector_regr', np.mean(losses[:iter_num, 3]))])\n\n            # reach epoch_length\n\t\t\tif iter_num == epoch_length:\n\t\t\t\tloss_rpn_cls = np.mean(losses[:, 0])\n\t\t\t\tloss_rpn_regr = np.mean(losses[:, 1])\n\t\t\t\tloss_class_cls = np.mean(losses[:, 2])\n\t\t\t\tloss_class_regr = np.mean(losses[:, 3])\n\t\t\t\tclass_acc = np.mean(losses[:, 4])\n\n                # negativate samples / all samples\n\t\t\t\tmean_overlapping_bboxes = float(sum(rpn_accuracy_for_epoch)) / len(rpn_accuracy_for_epoch)\n                # reset\n\t\t\t\trpn_accuracy_for_epoch = []\n\n                # print trainning loss and accrury\n\t\t\t\tif C.verbose:\n\t\t\t\t\tprint('Mean number of bounding boxes from RPN overlapping ground truth boxes: {}'.format(mean_overlapping_bboxes))\n\t\t\t\t\tprint('Classifier accuracy for bounding boxes from RPN: {}'.format(class_acc))\n\t\t\t\t\tprint('Loss RPN classifier: {}'.format(loss_rpn_cls))\n\t\t\t\t\tprint('Loss RPN regression: {}'.format(loss_rpn_regr))\n\t\t\t\t\tprint('Loss Detector classifier: {}'.format(loss_class_cls))\n\t\t\t\t\tprint('Loss Detector regression: {}'.format(loss_class_regr))\n                    # trainng time of one epoch\n\t\t\t\t\tprint('Elapsed time: {}'.format(time.time() - start_time))\n                    \n                # total loss\n\t\t\t\tcurr_loss = loss_rpn_cls + loss_rpn_regr + loss_class_cls + loss_class_regr\n                # reset\n\t\t\t\titer_num = 0\n                # reset time\n\t\t\t\tstart_time = time.time()\n\n                # if obtain smaller total loss, save weight of current model\n\t\t\t\tif curr_loss < best_loss:\n\t\t\t\t\tif C.verbose:\n\t\t\t\t\t\tprint('Total loss decreased from {} to {}, saving weights'.format(best_loss,curr_loss))\n\t\t\t\t\tbest_loss = curr_loss\n\t\t\t\t\tmodel_all.save_weights(C.model_path)\n\n\t\t\t\tbreak\n\n\t\texcept Exception as e:\n\t\t\tprint('Exception: {}'.format(e))\n\t\t\tcontinue\n\nprint('Training complete, exiting.')\n```\n[jupyter notebook](https://github.com/Trouble404/Object-Detection-System-Based-on-CNN-and-Capsule-Network/blob/master/Faster-RCNN/workflow_train.ipynb)\n\n### 【30/07/2018】\n#### Running at GPU enviorment\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/9_1.JPG)\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/9_2.JPG)\nMeet error in GPU version tensorflow\nNo enough memory.\n\nTry to Running at Irius:\n\nSetting 3 differnet configration:\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/9_3.JPG)\nat Prjoect1 file:\nset epoch_length to number of training img\n```python\nepoch_length = 11540\nnum_epochs = 100\n```\nApply img enhance function\n```python\nC.use_horizontal_flips = True\nC.use_vertical_flips = True\nC.rot_90 = True\n```\n\n---\n\nat Prjoect file:\nset epoch_length to 1000, increase epoch\n```python\nepoch_length = 1000\nnum_epochs = 2000\n```\nApply img enhance and class balance function\n```python\nC.use_horizontal_flips = True\nC.use_vertical_flips = True\nC.rot_90 = True\nC.balanced_classes = True\n```\n---\n\nat Prjoect3 file:\nset epoch_length to 1000, increase epoch\n```python\nepoch_length = 1000\nnum_epochs = 2000\n```\nApply img enhance\n```python\nC.use_horizontal_flips = True\nC.use_vertical_flips = True\nC.rot_90 = True\n```\n\n---\n#### check irdius work\n```bash\nmyqueue\n```\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/9_4.JPG)\n\n```bash\nssh pink59\nnvidia-smi\n```\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/9_5.JPG)\n\n### 【31/07/2018】\n#### obtain trained model and log file\n因为 Iriuds 的GPU使用时长限制最高为24小时，因此，需要在下一次开始前载入上一次训练的模型。\n每次训练的粗略结果更新在LogBook最前面.\n\n#### plot rpn and classfier loss\n获取日志中每个小epoch的rpn_cls, rpn_regr, detc_cls, detc_regr\n遍历日志，用正则匹配出相应的数值添加到List中：\n```python\ndef obtain_each_batch(filename):\n    n = 0\n    rpn_cls = []\n    rpn_regr = []\n    detector_cls = []\n    detector_regr = []\n    f = open(filename,'r',buffering=-1)\n    lines = f.readlines()\n    for line in lines:\n        n = n + 1\n        match = re.match(r'.* - rpn_cls: (.*) - rpn_regr: .*', line, re.M|re.I)\n        if match is None:\n            continue\n        else: \n            rpn_cls.append(float(match.group(1)))\n\n        match = re.match(r'.* - rpn_regr: (.*) - detector_cls: .*', line, re.M|re.I)\n        if match is None:\n            continue\n        else: \n            rpn_regr.append(float(match.group(1)))            \n            \n        match = re.match(r'.* - detector_cls: (.*) - detector_regr: .*', line, re.M|re.I)\n        if match is None:\n            continue\n        else: \n            detector_cls.append(float(match.group(1))) \n            \n        match = re.match(r'.* - detector_regr: (.*)\\n', line, re.M|re.I)\n        if match is None:\n            continue\n        else: \n            det_regr = match.group(1)[0:6]\n            detector_regr.append(float(det_regr))\n\n    f.close()\n    print(n)\n    return rpn_cls, rpn_regr, detector_cls, detector_regr  \n```\n\n每个epoch都会计算accury, loss of rpn cls, loss of rpn regr, loss of detc cls, loss of detc regr\n遍历日志找到相应的数值添加到list中：\n```python\ndef obtain_batch(filename):\n    n = 0\n    accuracy = []\n    loss_rpn_cls = []\n    loss_rpn_regr = []\n    loss_detc_cls = []\n    loss_detc_regr = []\n    f = open(filename,'r',buffering=-1)\n    lines = f.readlines()\n    \n    for line in lines:\n        n = n + 1\n        if 'Classifier accuracy for bounding boxes from RPN' in line:\n            result = re.findall(r\"\\d+\\.?\\d*\",line)\n            accuracy.append(float(result[0]))\n            \n        if 'Loss RPN classifier' in line:\n            result = re.findall(r\"\\d+\\.?\\d*\",line)\n            loss_rpn_cls.append(float(result[0]))       \n\n        if 'Loss RPN regression' in line:\n            result = re.findall(r\"\\d+\\.?\\d*\",line)\n            loss_rpn_regr.append(float(result[0]))\n            \n        if 'Loss Detector classifier' in line:\n            result = re.findall(r\"\\d+\\.?\\d*\",line)\n            loss_detc_cls.append(float(result[0]))\n            \n        if 'Loss Detector regression' in line:\n            result = re.findall(r\"\\d+\\.?\\d*\",line)\n            loss_detc_regr.append(float(result[0])) \n            \n    f.close()\n    print(n)\n    return accuracy, loss_rpn_cls, loss_rpn_regr, loss_detc_cls, loss_detc_regr\n```\n\n\n#### plot epoch loss and accury\n```python\nfilename = r'F:\\desktop\\新建文件夹\\1000-no_balance\\train1.out'\naa,bb,cc,dd,ee = obtain_batch(filename)\nx_cor = np.arange(0,len(aa),1)\n\nplt.plot(x_cor,aa, c='b', label = \"Accuracy\")\nplt.plot(x_cor,bb, c='c', label = \"Loss RPN classifier\")\nplt.plot(x_cor,cc, c='g', label = \"Loss RPN regression\")\nplt.plot(x_cor,dd, c='k', label = \"Loss Detector classifier\")\nplt.plot(x_cor,ee, c='m', label = \"Loss Detector regression\")\nplt.ylabel(\"Value of Accuracy and Loss\") \nplt.xlabel(\"Number of Epoch\")\nplt.title('Loss and Accuracy for Totoal Epochs')  \nplt.legend()\nplt.ylim(0,2)\n#plt.xlim(0,11540)\nplt.savefig(\"pic1.PNG\", dpi = 600)\nplt.show()\n```\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/pic1.PNG)\n\n```python\nfilename = r'F:\\desktop\\新建文件夹\\1000-no_balance\\train1.out'\na,b,c,d = obtain_each_batch(filename)\nx_cor = np.arange(0,len(a),1)\n\nplt.plot(x_cor,a, c='b', label = \"rpn_cls\")\nplt.plot(x_cor,b, c='c', label = \"rpn_regr\")\nplt.plot(x_cor,c, c='g', label = \"detector_cls\")\nplt.plot(x_cor,d, c='k', label = \"detector_regr\")\nplt.ylabel(\"Value of Loss\") \nplt.xlabel(\"Epoch Length\")\nplt.title('Loss for Lenght of Epoch')  \nplt.legend()\n#plt.ylim(0,2)\nplt.xlim(80787,92327)\nplt.savefig(\"pic2.PNG\", dpi = 600)\nplt.show()\n```\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/pic2.PNG)\n\n[Jupyter notebook](https://github.com/Trouble404/Object-Detection-System-Based-on-CNN-and-Capsule-Network/blob/master/Plot/logbook_plot.ipynb)\n\n## August\n### 【01~02/08/2018】\n#### test network\n首先是搭建网络，用于train部分相同的设置搭建\n不过在这里图像增强就设置为关闭了\n\n**构建rpn输出**\n```python\nshared_layers = nn.nn_base(img_input, trainable=True)\nnum_anchors = len(C.anchor_box_scales)*len(C.anchor_box_ratios)\nrpn_layers = nn.rpn(shared_layers,num_anchors)\n```\n\n**构建classifier输出**，参数分别是：特征层输出，预选框，探测框的数目，多少个类，是否可训练\n```python\nclassifier = nn.classifier(feature_map_input, roi_input, C.num_rois,nb_classes=len(class_mapping), trainable=True)\n```\n\n**载入训练好的权重：**\n```python\nC.model_path = 'gpu_resnet50_weights.h5'\ntry:\n    print('Loading weights from {}'.format(C.model_path))\n    model_rpn.load_weights(C.model_path, by_name=True)\n    model_classifier.load_weights(C.model_path, by_name=True)\nexcept:\n    print('can not load')\n```\n\n**读取需要检测的图片：**\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/test/test4.jpg)\n将图片规整到制定的大小\n1. 将图片缩放到规定的大小\n    首先从配置文件夹中得到最小边的大小\n    得到图片的高度和宽度\n    根据高度和宽度谁大谁小，确定规整后图片的高宽\n    将图片缩放到指定的大小，用的是立方插值。返回的缩放后的图片img和相应的缩放的比例。\n```python\ndef format_img_size(img, C):\n    (height,width,_) = img.shape\n    if width <= height:\n        ratio = C.im_size/width\n    else:\n        ratio = C.im_size/height\n    new_width, new_height = image_processing.get_new_img_size(width,height, C.im_size)\n    img = cv2.resize(img, (new_width, new_height), interpolation=cv2.INTER_CUBIC)\n    return img, ratio\n```\n2. 对图片每一个通道的像素值做规整\n     将图片的BGR变成RGB，因为网上训练好的RESNET图片都是以此训练的\n    将图片数据类型转换为np.float32，并减去每一个通道的均值，理由同上\n    图片的像素值除一个缩放因子，此处为1\n    将图片的深度变到第一个位置\n    给图片增加一个维度\n```python\ndef format_img_channels(img, C):\n\t\"\"\" formats the image channels based on config \"\"\"\n\timg = img[:, :, (2, 1, 0)]\n\timg = img.astype(np.float32)\n\timg[:, :, 0] -= C.img_channel_mean[0]\n\timg[:, :, 1] -= C.img_channel_mean[1]\n\timg[:, :, 2] -= C.img_channel_mean[2]\n\timg /= C.img_scaling_factor\n\timg = np.transpose(img, (2, 0, 1))\n\timg = np.expand_dims(img, axis=0)\n\treturn img\n```\n如果用的是tensorflow内核，需要将图片的深度变换到最后一位。\n\n**进行区域预测**\nY1:anchor包含物体的概率\nY2:每一个anchor对应的回归梯度\nF:卷积后的特征图，接下来会有用\n\n```python\n[Y1, Y2, F] = model_rpn.predict(X)\n```\n\n获得rpn预测的结果以及对应的回归梯度，这一步就是对图片上隔16个像素的每个anchor进行rpn计算\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/pic_anchors.png)\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/25_1.jpg)\n\n**根据rpn预测的结果，得到预选框:**\n这里会返回300个预选框以及它们对应的坐标(x1,y1,x2,y2)\n```python\n# transform predicted rpn to cordinates of boxes\nR = rpn_to_boxes.rpn_to_roi(Y1, Y2, C, K.image_dim_ordering(), use_regr=True, overlap_thresh=0.7)\n```\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/pic_rois.png)\n\n将预选框的坐标由(x1,y1,x2,y2) 改到 (x,y,w,h)\n```python\nR[:, 2] -= R[:, 0]\nR[:, 3] -= R[:, 1]\n```\n\n**遍历所有的预选框**\n需要注意的是每一次遍历预选框的个数为C.num_rois\n每一次遍历32个预选框，那么总共需要300/32, 10批次\n取出32个预选框，并增加一个维度【注：当不满一个32，其自动只取到最后一个】\n当预选框被取空的时候，停止循环\n当最后一次去不足32个预选框时，补第一个框使其达到32个。\n```python\n# divided 32 bboxes as one group\nfor jk in range(R.shape[0]//C.num_rois + 1):\n    # pick num_rios(32) bboxes one time, only pick to last bboxes in last group\n    ROIs = np.expand_dims(R[C.num_rois*jk:C.num_rois*(jk+1), :], axis=0)\n    #print(ROIs.shape)\n    \n    # no proposals, out iter\n    if ROIs.shape[1] == 0:\n        break\n\n    # when last time can't obtain num_rios(32) bboxes, adding bboxes with 0 to fill to 32 bboxes\n    if jk == R.shape[0]//C.num_rois:\n        #pad R\n        curr_shape = ROIs.shape\n        target_shape = (curr_shape[0],C.num_rois,curr_shape[2])\n        ROIs_padded = np.zeros(target_shape).astype(ROIs.dtype)\n        ROIs_padded[:, :curr_shape[1], :] = ROIs\n        ROIs_padded[0, curr_shape[1]:, :] = ROIs[0, 0, :]\n        # 10 group with 320 bboxes\n        ROIs = ROIs_padded\n```\n这样就可以送入分类网络了\n\n**进行类别预测和边框回归**\n\n预测\nP\\_cls：该边框属于某一类别的概率\nP\\_regr：每一个类别对应的边框回归梯度\nF:rpn网络得到的卷积后的特征图\nROIS:处理得到的区域预选框\n```python\n[P_cls, P_regr] = model_classifier_only.predict([F, ROIs])\n```\n\n遍历每一个预选宽\n如果该预选框的最大概率小于设定的阈值（即预测的肯定程度大于一定的值，我们才认为这次的类别的概率预测是有效的，或者最大的概率出现在背景上，则认为这个预选框是无效的，进行下一次预测。\n```python\n    for ii in range(P_cls.shape[1]):\n\n        # if smaller than setting threshold, we think this bbox invalid\n        # and if this bbox's class is background, we don't need to care about it\n        if np.max(P_cls[0, ii, :]) < bbox_threshold or np.argmax(P_cls[0, ii, :]) == (P_cls.shape[2] - 1):\n            continue\n```\n\n不属于上面的两种情况，取最大的概率处为此边框的类别得到其名称。\n创建两个list，用于存放不同类别对应的边框和概率\n```python\n        # obatain max possibility's class name by class mapping\n        cls_name = class_mapping[np.argmax(P_cls[0, ii, :])]\n\n        # saving bboxes and probs\n        if cls_name not in bboxes:\n            bboxes[cls_name] = []\n            probs[cls_name] = []\n```\n\n得到该预选框的信息\n得到类别对应的编号\n```python\n        # obtain current cordinates of proposal\n        (x, y, w, h) = ROIs[0, ii, :]\n        \n        # obtain the position with max possibility\n        cls_num = np.argmax(P_cls[0, ii, :])\n```\n这样符合条件的预选框以及对应的分类类别和概率就可以画在图片上了\n![iamge](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/pic_cls1.png)\n\n根据类别编号得到该类的边框回归梯度\n对回归梯度进行规整化\n对预测的边框进行修正\n向相应的类里面添加信息【乘 C.rpn_stride，边框的预测都是在特征图上进行的要将其映射到规整后的原图上】\n```python\n        try:\n            # obtain privous position's bbox's regrient\n            (tx, ty, tw, th) = P_regr[0, ii, 4*cls_num:4*(cls_num+1)]\n            # waiting test\n            tx /= C.classifier_regr_std[0]\n            ty /= C.classifier_regr_std[1]\n            tw /= C.classifier_regr_std[2]\n            th /= C.classifier_regr_std[3]\n            # fix box with regreient\n            x, y, w, h = rpn_to_boxes.apply_regr(x, y, w, h, tx, ty, tw, th)\n        except:\n            pass\n        # cordinates of current's box on real img\n        bboxes[cls_name].append([C.rpn_stride*x, C.rpn_stride*y, C.rpn_stride*(x+w), C.rpn_stride*(y+h)])\n        # coresspoding posbility\n        probs[cls_name].append(np.max(P_cls[0, ii, :]))\n```\n这样修正过的框可以画在图上：\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/pic_cls2.png)\n\n遍历bboxes里的类，取出某一类的bbox，合并一些重合度较高的选框\nNo Max Supression\n```python\n# for all classes in current boxes\nfor key in bboxes:\n\n    # bboxes's cordinates\n    bbox = np.array(bboxes[key])\n    # apply NMX to merge some  overlapping boxes\n    new_boxes, new_probs = rpn_to_boxes.non_max_suppression_fast(bbox, np.array(probs[key]), overlap_thresh=0.5)\n```\n最终的图：\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/pic_cls3.png)\n\n[Jupyter notebook](https://github.com/Trouble404/Object-Detection-System-Based-on-CNN-and-Capsule-Network/blob/master/Faster-RCNN/test.ipynb)\n\n#### result\nSmall img, only 8k\n<div align=\"center\">\n<img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/test/test1.jpg\" height=\"200px\"  ><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/test/test1.png\" height=\"200px\"  >   \n</div>\n\n---\n\nOverlapping img\n<div align=\"center\">\n<img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/test/test2.jpg\" height=\"200px\"  ><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/test/test2.png\" height=\"200px\"  >   \n</div>\n\n---\n\nCrowed People\n<div align=\"center\">\n<img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/test/test3.jpg\" height=\"260px\"  ><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/test/test3.png\" height=\"260px\"  >   \n</div>\n\n---\n\ncow and people\n<div align=\"center\">\n<img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/test/test4.jpg\" height=\"260px\"  ><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/test/test4.png\" height=\"260px\"  >   \n</div>\n\n---\n\ncar and plane\n<div align=\"center\">\n<img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/test/test5.jpg\" height=\"220px\"  ><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/test/test5.png\" height=\"220px\"  >   \n</div>\n\n---\n\nStreet img\n<div align=\"center\">\n<img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/test/test6.jpg\" height=\"270px\"  ><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/test/test6.png\" height=\"270px\"  >   \n</div>\n\n---\n\nLots Dogs\n<div align=\"center\">\n<img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/test/test7.jpg\" height=\"250px\"  ><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/test/test7.png\" height=\"250px\"  >   \n</div>\n\n---\n\nOverlapping car and people\n<div align=\"center\">\n<img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/test/test8.jpg\" height=\"290px\"  ><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/test/test8.png\" height=\"290px\"  >   \n</div>\n\n直观看的话效果还不错，但是一些重叠的物体框会出现反复，或者取不到。而且分类有一点过拟合。\n\n\n### 【03/08/2018】\n#### evaluation\n**mAP**\nmAP是目标算法中衡量算法的精确度的指标，涉及两个概念：查准率Precision、查全率Recall。对于object detection任务，每一个object都可以计算出其Precision和Recall，多次计算/试验，每个类都 可以得到一条P-R曲线，曲线下的面积就是AP的值，这个mean的意思是对每个类的AP再求平均，得到的就是mAP的值，mAP的大小一定在[0,1]区间。 \n\n**AP**:Precision对Recall积分，可通过改变正负样本阈值求得矩形面积，进而求积分得到，也可以通过sklearn.metrics.average\\_precision\\_score函数直接得到。 \n\n传入预测值和真实值和resize比例，得到可以传入sklearn.metrics.average_precision_score函数的值，即：真实值和预测概率\n\n---\n\n首先搭建rpn和分类器网络，按照之前的train部分来就可以了\n这里注意分类网络的输入换成测试图片的feature map\n```python\nnum_features = 1024\n\ninput_shape_img = (None, None, 3)\ninput_shape_features = (None, None, num_features)\n\nimg_input = Input(shape=input_shape_img)\nroi_input = Input(shape=(C.num_rois, 4))\nfeature_map_input = Input(shape=input_shape_features)\n\nclassifier = nn.classifier(feature_map_input, roi_input, C.num_rois, nb_classes=len(class_mapping), trainable=True)\n```\n\n然后载入需要测试的模型权重\n\n按照VOC的数据集标注，把测试集分出来：\n```python\ntrain_imgs = []\ntest_imgs = []\n\nfor each in all_imgs:\n\tif each['imageset'] == 'trainval':\n\t\ttrain_imgs.append(each)\n\tif each['imageset'] == 'test':\n\t\ttest_imgs.append(each)\n```\n\n按照之前的预测方法，求出图片的预测框坐标以及对应的分类名字，然后把这些信息放入对应的字典里面，与xml解析的文件一样的格式：\n```python\n    for jk in range(new_boxes.shape[0]):\n        (x1, y1, x2, y2) = new_boxes[jk, :]\n        det = {'x1': x1, 'x2': x2, 'y1': y1, 'y2': y2, 'class': key, 'prob': new_probs[jk]}\n        all_dets.append(det)\n```\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/014_1.JPG)\n\n然后读取标注的框的真实数值：\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/014_2.JPG)\n\n遍历真实信息里面的每一个狂，将bbox_matched这个属性标注为FALSE，之后如果预测框和标注框对应上的话，这个属性就会被设置为True\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/14_3.JPG)\n\n获取预测框里面的分类对应概率，并且按照概率从大到小得到idx位置：\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/014_3.JPG)\n\n按照概率大小，对每一个对应的预测框，对比每一个标注的框，如果预测的类与当前标注框的类相同并且没有被匹配过，计算两个框的iou，如果大于0.5的话就表明预测框匹配当前标注框，保存预测概率以及对应的是否匹配：\n\n```python\n# process each bbox with hightest prob\nfor box_idx in box_idx_sorted_by_prob:\n    \n    # obtain current box's cordinates, class and prob\n    pred_box = pred[box_idx]\n    pred_class = pred_box['class']\n    pred_x1 = pred_box['x1']\n    pred_x2 = pred_box['x2']\n    pred_y1 = pred_box['y1']\n    pred_y2 = pred_box['y2']\n    pred_prob = pred_box['prob']\n    \n    # if not in P list, save current class infomration to it\n    if pred_class not in P:\n        P[pred_class] = []\n        T[pred_class] = []\n        # put porb to P\n    P[pred_class].append(pred_prob)\n    # used to check whether find current object\n    found_match = False\n\n    # compare each real bbox\n    # obtain real box's cordinates, class and prob\n    for gt_box in gt:\n        gt_class = gt_box['class']\n        # bacause the image is rezied, so calculate the real cordinates\n        gt_x1 = gt_box['x1']/fx\n        gt_x2 = gt_box['x2']/fx\n        gt_y1 = gt_box['y1']/fy\n        gt_y2 = gt_box['y2']/fy\n        \n        # obtain box_matched - all false at beginning\n        gt_seen = gt_box['bbox_matched']\n        \n        # ture class != predicted class\n        if gt_class != pred_class:\n            continue\n        # already matched\n        if gt_seen:\n            continue\n        # calculate iou of predicted bbox and real bbox \n        iou = rpn_calculation.iou((pred_x1, pred_y1, pred_x2, pred_y2), (gt_x1, gt_y1, gt_x2, gt_y2))\n        # if iou > 0.5, we will set this prediction correct\n        if iou >= 0.5:\n            found_match = True\n            gt_box['bbox_matched'] = True\n            break\n        else:\n            continue\n    # 1 means this position's bbox correct match with orignal image\n    T[pred_class].append(int(found_match))\n```\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/014_4.JPG)\n\n遍历每一个标注框，如果没有被匹配到并且diffcult属性不是true的话，说明这个框漏检了，在之前保存的概率以及对应是否有概率里面加入物体1以及对应概率0\n```python\n# adding missing object compared to orignal image\nfor gt_box in gt:\n    if not gt_box['bbox_matched'] and not gt_box['difficult']:\n        if gt_box['class'] not in P:\n            P[gt_box['class']] = []\n            T[gt_box['class']] = []\n\n        # T = 1 means there are object, P = 0 means we did't detected that\n        T[gt_box['class']].append(1)\n        P[gt_box['class']].append(0)\n```\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/014_5.JPG)\n\n把当前信息存入到总的一个词典里面，就可以使用average_precision_score这个sklearn里面的函数计算ap了。与此同时，保存得到的结果并且显示总的map：\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/014_6.JPG)\n\n[jupyter notebook](https://github.com/Trouble404/Object-Detection-System-Based-on-CNN-and-Capsule-Network/blob/master/Faster-RCNN/map.ipynb)\n\n### 【06~10/08/2018】\n#### adjust\n将计算ap的函数包装好：\n[jupyter notebook](https://github.com/Trouble404/Object-Detection-System-Based-on-CNN-and-Capsule-Network/blob/master/Faster-RCNN/map_all.ipynb)\n\n##### Project1 all: 9 models:\nALL_2 NO THRESHOLD OTHER WITH THRESHOLD MOST 0.8\n\n| Classes | ALL_1 | ALL_2 | ALL_3 | ALL_4 | ALL_5 | ALL_6 | ALL_7 | ALL_8 | ALL_9 |\n| :---: | :----: | :---: | :----: | :----: | :----: | :----: | :----: | :----: | :----: |\n| motorbike | 0.2305 | 0.2412 | 0.2132 | 0.2220 | 0.2889 | 0.2528 | 0.2204 | 0.2644 | 0.2336 |\n| person | 0.6489 | 0.6735 | 0.7107 | 0.6652 | 0.7120 | 0.7041 | 0.7238 | 0.7003 | 0.7201 |\n| car | 0.1697 | 0.1563 | 0.2032 | 0.2105 | 0.2308 | 0.2221 | 0.2436 | 0.2053 | 0.2080 |\n| aeroplane | 0.7968 | 0.7062 | 0.7941 | 0.6412 | 0.7871 | 0.7331 | 0.7648 | 0.7659 | 0.6902 |\n| bottle | 0.2213 | 0.2428 | 0.2261 | 0.1943 | 0.2570 | 0.2437 | 0.2899 | 0.1442 | 0.2265 |\n| sheep | 0.6162 | 0.5702 | 0.6876 | 0.6295 | 0.6364 | 0.5710 | 0.6536 | 0.6349 | 0.6455 |\n| tvmonitor | 0.1582 | 0.1601 | 0.2231 | 0.1748 | 0.1551 | 0.1603 | 0.1317 | 0.1584 | 0.1678 |\n| boat | 0.3842 | 0.2621 | 0.2261 | 0.1943 | 0.3499 | 0.2437 | 0.2057 | 0.2748 | 0.3509 |\n| chair | 0.2811 | 0.0563| 0.0891 | 0.0621 | 0.1353 | 0.0865 | 0.0907 | 0.0854 | 0.1282 |\n| bicycle | 0.1464 | 0.1224 | 0.1346 | 0.1781 | 0.1406 | 0.1448 | 0.1810 | 0.1071 | 0.1673 |\n| cat | 0.8901 | 0.8565 | 0.9103 | 0.8417 | 0.8289 | 0.8274 | 0.7572 | 0.9143 | 0.8118 |\n| pottedplant | 0.2075 | 0.0926 | 0.1790 | 0.0532 | 0.1517 | 0.1150 | 0.1080 | 0.1022 | 0.0939 |\n| horse | 0.1185 | 0.0588 | 0.0726 | 0.0489 | 0.0696 | 0.0695 | 0.0637 | 0.0651 | 0.0640 |\n| sofa | 0.2797 | 0.2309 | 0.2852 | 0.2966 | 0.3855 | 0.4817 | 0.3659 | 0.3132 | 0.3090 |\n| dog | 0.5359 | 0.5077 | 0.5578 | 0.4413 | 0.4832 | 0.5793 | 0.5687 | 0.4910 | 0.4598 |\n| cow | 0.7582 | 0.6229 | 0.7295 | 0.5420 | 0.5379 | 0.5312 | 0.5147 | 0.5706 | 0.6503 |\n| diningtable | 0.3979 | 0.2734 | 0.3739 | 0.2963 | 0.4715 | 0.4987 | 0.3895 | 0.4983 | 0.4666 |\n| bus | 0.6203 | 0.5572 | 0.6468 | 0.6032 | 0.6320 | 0.6096 | 0.7169 | 0.5938 | 0.5485 |\n| bird | 0.6164 | 0.6662 | 0.5692 | 0.5751 | 0.5407 | 0.4125 | 0.4925 | 0.4347 | 0.5208 |\n| train | 0.8655 | 0.6916 | 0.7141 | 0.7166 | 0.7643 | 0.8107 | 0.7100 | 0.7194 | 0.6263 |\n| **mAP** | **0.4472** | **0.3874** | **0.4341** | **0.3859** | **0.4279** | **0.4141** | **0.4096** | **0.4022** | **0.4045** |\n\n---\n\n##### Project1 epoch_lenght=1000, epoch:1041 : 7 models:\nALL WITH THRESHOLD MOST 0.51\n\n| Classes | ALL_1 | ALL_2 | ALL_3 | ALL_4 | ALL_5 | ALL_6 | ALL_7 |\n| :---: | :----: | :---: | :----: | :----: | :----: | :----: | :----: |\n| motorbike | 0.2433 | 0.2128 | 0.2232 | 0.2262 | 0.2286 | 0.2393 | 0.2279 |\n| person | 0.6560 | 0.6537 | 0.6742 | 0.6952 | 0.6852 | 0.6719 | 0.6636 |\n| car | 0.1562 | 0.1905 | 0.1479 | 0.2024 | 0.2010 | 0.1379 | 0.1583 | \n| aeroplane | 0.7359 | 0.6837 | 0.6729 | 0.6687 | 0.6957 | 0.7339 | 0.6391 |\n| bottle | 0.1913 | 0.1937 | 0.2635 | 0.1843 | 0.2570 | 0.1632 | 0.1863 | \n| sheep | 0.5429 | 0.5579 | 0.6219 | 0.5355 | 0.5881 | 0.5441 | 0.5824 |\n| tvmonitor | 0.1295 | 0.1601 | 0.1368 | 0.1407 | 0.1147 | 0.1349 | 0.1154 | \n| boat | 0.1913 | 0.2880 | 0.2635 | 0.3433 |0.3335 | 0.3422 | 0.3069 | \n| chair | 0.0587 | 0.0657| 0.0342 | 0.0680 | 0.0695 | 0.0752 | 0.0760 |\n| bicycle | 0.1013 | 0.1485 | 0.1225 | 0.1871 | 0.1685 | 0.1037 | 0.1490 | \n| cat | 0.8737 | 0.8557 | 0.8007 | 0.7982 | 0.8045 | 0.8067 | 0.7732 |\n| pottedplant | 0.0694 | 0.1059 | 0.0748 | 0.0878 | 0.0893 | 0.0690 | 0.0865 |\n| horse | 0.0556 | 0.0561 | 0.0581 | 0.0770 | 0.0575 | 0.0539 | 0.0522 |\n| sofa | 0.2177 | 0.2917 | 0.1699 | 0.1940 | 0.3177 | 0.1863 | 0.1857 |\n| dog | 0.6269 | 0.4989 | 0.5015 | 0.5333 | 0.4914 | 0.5572 | 0.4747 |\n| cow | 0.5216 | 0.6229 | 0.5283 | 0.6426 | 0.4358 | 0.4227 | 0.4589 | \n| diningtable | 0.3076 | 0.3889 | 0.3283 | 0.2404 | 0.4219 | 0.4153 | 0.2627 |\n| bus | 0.5865 | 0.5222 | 0.6312 | 0.5853 | 0.5042 | 0.4882 | 0.5576 |\n| bird | 0.5339 | 0.5039 | 0.5150 | 0.5152 | 0.5838 | 0.3890 | 0.4680 |\n| train | 0.4994 | 0.6541 | 0.6702 | 0.6920 | 0.5959 | 0.5893 | 0.6861 |\n| **mAP** | **0.3699** | **0.3765** | **0.3748** | **0.3814** | **0.3786** | **0.3562** | **0.3555** |\n\n\n在测试集上的结果不是很好，不同class的ap差距较大，可能是由于训练时候不平均或者训练集太小的原因\n\n**尝试加入VOC2007的数据进训练集当中，观察结果。**\n解析VOC2007的过程中遇到了OpenCV读取不了图片的BUG。\n（已修复）\nVOC2012的数据莫名没有了，因为之前测试过的原因，一直以为是VOC2007的数据解析有问题，大概是Irius的文件上限时间到了自动清除了数据。\n\n20个类当中的AP差距过大，其实数据集是不平衡的，有的类只有大概1000个样本，但是人这个样本就有2W多，而且之前的训练过程中每次图片都是在训练集中随机选的，所以尝试修改了流程，当所有训练集中的数据都被读取训练过以后再打乱训练集，与此同时配合class_balance的功能使用。\n\n实际上使用的时候class balance效果不是很好，后面没有开启。\n\n用了一个较大的学习率尝试训练没有载入imagenet预训练权重的版本。\n\n交叉法\n\n### 【13/08/2018】\n#### soft-NMS\n传统的非最大抑制算法首先在被检测图片中产生一系列的检测框B以及对应的分数S。当选中最大分数的检测框M，它被从集合B中移出并放入最终检测结果集合D。于此同时，集合B中任何与检测框M的重叠部分大于重叠阈值Nt的检测框也将随之移除。非最大抑制算法中的最大问题就是它将相邻检测框的分数均强制归零。在这种情况下，如果一个真实物体在重叠区域出现，则将导致对该物体的检测失败并降低了算法的平均检测率（average precision, AP）。\n\n换一种思路，如果我们只是通过一个基于与M重叠程度相关的函数来降低相邻检测框的分数而非彻底剔除。虽然分数被降低，但相邻的检测框仍在物体检测的序列中。图二中的实例可以说明这个问题。\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/018_1.JPG)\n针对NMS存在的这个问题，我们提出了一种新的Soft-NMS算法（图三），它只需改动一行代码即可有效改进传统贪心NMS算法。在该算法中，我们基于重叠部分的大小为相邻检测框设置一个衰减函数而非彻底将其分数置为零。**简单来讲，如果一个检测框与M有大部分重叠，它会有很低的分数；而如果检测框与M只有小部分重叠，那么它的原有检测分数不会受太大影响**。在标准数据集PASCAL VOC 和 MS-COCO等标准数据集上，Soft-NMS对现有物体检测算法在多个重叠物体检测的平均准确率有显著的提升。同时，Soft-NMS不需要额外的训练且易于实现，因此，它很容易被集成到当前的物体检测流程中。\n\n伪代码：\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/018_2.JPG)\n\n公式：\nNMS\n$$ s_{i}=\\left\\{\n\\begin{aligned}\ns_{i}, \\ \\ \\ \\ iou(M,b_{i}) <  N_{t} \\\\\n0, \\ \\ \\ \\ iou(M,b_{i}) \\geq  N_{t} \n\\end{aligned}\n\\right.\n$$\n\nSOFT NMS\n$$ s_{i}=\\left\\{\n\\begin{aligned}\ns_{i}, \\ \\ \\ \\ iou(M,b_{i}) <  N_{t} \\\\\n1-iou(M,b_{i}), \\ \\ \\ \\ iou(M,b_{i}) \\geq  N_{t} \n\\end{aligned}\n\\right.\n$$\n\n当相邻检测框与M的重叠度超过重叠阈值Nt后，检测框的检测分数呈线性衰减。在这种情况下，与M相邻很近的检测框衰减程度很大，而远离M的检测框并不受影响。\n\n但是，上述分数重置函数并不是一个连续函数，在重叠程度超过重叠阈值Nt时，该分数重置函数产生突变，从而可能导致检测结果序列产生大的变动，因此我们更希望找到一个连续的分数重置函数。它对没有重叠的检测框的原有检测分数不产生衰减，同时对高度重叠的检测框产生大的衰减。综合考虑这些因素，我们进一步对soft-NMS中的分数重置函数进行了改进：\n\nGaussian penalty:\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/018_3.JPG)\n\n根据这个伪代码以及公式，实现代码：\n```python\n\"\"\" NMS , delete overlapping box\n\n@param boxes: (n,4) box and coresspoding cordinates\n@param probs: (n,) box adn coresspding possibility\n@param overlap_thresh: treshold of delet box overlapping\n@param max_boxes: maximum keeping number of boxes\n@param method: 1 for linear soft NMS, 2 for gaussian soft NMS\n@param sigma: parameter of gaussian soft NMS\nprob_thresh: threshold of probs after soft NMS\n\n\n@return: boxes: boxes cordinates(x1,y1,x2,y2)\n@return: probs: coresspoding possibility\n\"\"\"\ndef soft_nms(boxes, probs, overlap_thresh=0.9, max_boxes=300, method = 1, sigma=0.5, prob_thresh=0.49):\n    # number of input boxes\n    N = boxes.shape[0]\n    # if the bounding boxes integers, convert them to floats --\n    # this is important since we'll be doing a bunch of divisions\n    if boxes.dtype.kind == \"i\":\n        boxes = boxes.astype(\"float\")\n\n    # iterate all boxes\n    for i in range(N):\n        \n        # obtain current boxes' cordinates and probs\n        maxscore = probs[i]\n        maxpos = i\n\n        tx1 = boxes[i,0]\n        ty1 = boxes[i,1]\n        tx2 = boxes[i,2]\n        ty2 = boxes[i,3]\n        ts = probs[i]\n\n        pos = i + 1\n        # get max box\n        while pos < N:\n            if maxscore < probs[pos]:\n                maxscore = probs[pos]\n                maxpos = pos\n            pos = pos + 1\n\n        # add max box as a detection \n        boxes[i,0] = boxes[maxpos,0]\n        boxes[i,1] = boxes[maxpos,1]\n        boxes[i,2] = boxes[maxpos,2]\n        boxes[i,3] = boxes[maxpos,3]\n        probs[i] = probs[maxpos]\n\n        # swap ith box with position of max box\n        boxes[maxpos,0] = tx1\n        boxes[maxpos,1] = ty1\n        boxes[maxpos,2] = tx2\n        boxes[maxpos,3] = ty2\n        probs[maxpos] = ts\n\n        # cordinates of max box\n        tx1 = boxes[i,0]\n        ty1 = boxes[i,1]\n        tx2 = boxes[i,2]\n        ty2 = boxes[i,3]\n        ts = probs[i]\n\n        pos = i + 1\n        # NMS iterations, note that N changes if detection boxes fall below threshold\n        while pos < N:\n            x1 = boxes[pos, 0]\n            y1 = boxes[pos, 1]\n            x2 = boxes[pos, 2]\n            y2 = boxes[pos, 3]\n            s = probs[pos]\n            \n            # calculate the areas, +1 for robatness\n            area = (x2 - x1 + 1) * (y2 - y1 + 1)\n            iw = (min(tx2, x2) - max(tx1, x1) + 1)\n            # # confirm left top cordinates less than top right\n            if iw > 0:\n                ih = (min(ty2, y2) - max(ty1, y1) + 1)\n                # confirm left top cordinates less than top right\n                if ih > 0:\n                    # find the union\n                    ua = float((tx2 - tx1 + 1) * (ty2 - ty1 + 1) + area - iw * ih)\n                    #iou between max box and detection box\n                    ov = iw * ih / ua\n\n                    if method == 1: # linear\n                        if ov > overlap_thresh: \n                            weight = 1 - ov\n                        else:\n                            weight = 1\n                    elif method == 2: # gaussian\n                        weight = np.exp(-(ov * ov)/sigma)\n                    else: # original NMS\n                        if ov > overlap_thresh: \n                            weight = 0\n                        else:\n                            weight = 1\n\n                    # obtain adjusted probs\n                    probs[pos] = weight*probs[pos]\n\n   \n                    # if box score falls below threshold, discard the box by swapping with last box\n                    # update N\n                    if probs[pos] < prob_thresh:\n                        boxes[pos,0] = boxes[N-1, 0]\n                        boxes[pos,1] = boxes[N-1, 1]\n                        boxes[pos,2] = boxes[N-1, 2]\n                        boxes[pos,3] = boxes[N-1, 3]\n                        probs[pos] = probs[N-1]\n                        N = N - 1\n                        pos = pos - 1\n\n            pos = pos + 1\n    # keep is the idx of current keeping objects, front ith objectes\n    keep = [i for i in range(N)]\n    return boxes[keep], probs[keep]\n```\n\n\n### 【14/08/2018】\n#### OVERLAPPING OBJECT DETECTION\n\n\n### 【15/08/2018】\n\n\n### 【16/08/2018】\n\n\n### 【17/08/2018】\n\n\n## September\n\n\n\n","source":"_posts/LogBook.md","raw":"---\ntitle: (Logbook) -- Object Detection System Based on CNN and Capsule Network\ndate: 2018-05-25 00:00:00\ntags: [Deep Learning, Object Detection]\ncategories: Msc Project\n---\n\n## Gantt chart\n![image](https://raw.githubusercontent.com/Trouble404/Object-Detection-System-Based-on-CNN-and-Capsule-Network/master/Project%20Plan/gantt%20chart%20of%20project.PNG)\n<!-- more -->\n---\n\n## Check list\n- [x] **1) preparation**\n- [x] ***1.1) Familiarization with develop tools***\n- [x] 1.1.1) Keras\n- [x] 1.1.2) Pythrch\n- [x] ***1.2) Presentation***\n- [x] 1.2.1) Poster conference\n- [x] **2) Create image database**\n- [x] 2.1) Confirmation of detected objects\n- [x] 2.2) Collect and generate the dataset\n- [x] **3) Familiarization with CNN based object detection methods**\n- [x] 3.1) R-CNN\n- [x] 3.2) SPP-net\n- [x] 3.3) Fast R-CNN\n- [x] 3.4) Faster R-CNN\n- [x] **4) Implement object detection system based on one chosen CNN method**\n- [x] 4.1) Pre-processing of images\n- [x] 4.2) Extracting features\n- [x] 4.3) Mode architecture\n- [x] 4.4) Train model and optimization\n- [x] 4.5) Models ensemble\n- [x] **5) Analysis work**\n- [x] 5.1) Evaluation of detection result.\n- [x] **6) Paperwork and bench inspection**\n- [x] 6.1) Logbook\n- [x] 6.2) Write the thesis\n- [x] 6.3) Project video\n- [x] 6.4) Speech and ppt of bench inspection\n- [x] **7) Documents**\n- [x] 7.1) Project Brief\n\n---\n\n## May\n### 【28/05/2018】\nKeras is a high-level neural networks API, written in Python and capable of running on top of [TensorFlow](https://github.com/tensorflow/tensorflow), CNTK, or Theano.\n\n* **[Keras document](https://keras.io/)**\n\n* **[Keras 文档](https://keras-cn.readthedocs.io/en/latest/#keraspython)**\n\n---\n#### Installation\n\n* **TensorFlow**\n  [Microsoft Visual Studio 2015](https://www.visualstudio.com/zh-hans/vs/older-downloads/)\n  [CUDA 9.0](http://docs.nvidia.com/cuda/cuda-installation-guide-microsoft-windows/)\n  [cuDNN7](https://developer.nvidia.com/cudnn)\n  [Anaconda](https://www.anaconda.com/download/)\n  \n  * Step 1: Install VS2015\n  * Step 2: Install CUDA 9.0 并添加环境变量\n  * Step 3: Install cuDNN7 解压后把cudnn目录下的bin目录加到PATH环境变量里\n  * Step 4: Install Anaconda 把安装路径添加到PATH里去, 在这里我用了 **Python 3.5**\n  * Step 5: 使用Anaconda的命令行 新建一个虚拟环境,激活并且关联到jupyterbook\n  {% codeblock %}\n  conda create  --name tensorflow python=3.5\n  activate tensorflow\n  conda install nb_conda\n  {% endcodeblock %}\n  * Step 6: Install GPU version TensorFlow.\n  {% codeblock %}\n  pip install -i https://pypi.tuna.tsinghua.edu.cn/simple --ignore-installed --upgrade tensorflow-gpu \n  {% endcodeblock %}\n  \n* **Keras**\n  * Step 1: 启动之前的 虚拟环境， 并且安装Keras GPU 版本\n    {% codeblock %}\n    activate tensorflow\n    pip install keras -U --pre\n    {% endcodeblock %}\n  \n#### 在硕士学习过程中，使用Keras的项目**\n* **[NBA with Machine Learning](https://github.com/Trouble404/NBA-with-Machine-Learning)**\n* **[Kaggle- Job salary prediction](https://github.com/Trouble404/kaggle-Job-Salary-Prediction)**\n \n#### TensorFlow CPU 切换\n```python\nimport tensorflow as tf  \nimport os\nimport keras.backend.tensorflow_backend as KTF  \n\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  #设置需要使用的GPU的编号\nconfig = tf.ConfigProto()\nconfig.gpu_options.per_process_gpu_memory_fraction = 0.4 #设置使用GPU容量占GPU总容量的比例\nsess = tf.Session(config=config)\nKTF.set_session(sess)\n\nwith tf.device('/cpu:0'):\n```\n这样可以在GPU版本的虚拟环境里面使用CPU计算\n\n#### Jupyter Notebook 工作目录设置\n启动命令行，切换至预设的工作目录， 运行：\n```\njupyter notebook --generate-config\n```\n![image](https://raw.githubusercontent.com/Trouble404/Object-Detection-System-Based-on-CNN-and-Capsule-Network/master/Learned%20skills/jupyter%20_setting.PNG)\n\n## June\n### 【01/06/2018】\n**[PyTorch](https://pytorch.org/about/)** is a python package that provides two high-level features:\n* Tensor computation (like numpy) with strong GPU acceleration\n* Deep Neural Networks built on a tape-based autodiff system\n\n| Package | Description |\n|:----|:----|\n|torch|a Tensor library like NumPy, with strong GPU support|\n|torch.autograd|a tape based automatic differentiation library that supports all differentiable Tensor operations in torch|\n|torch.nn|a neural networks library deeply integrated with autograd designed for maximum flexibility|\n|torch.optim|an optimization package to be used with torch.nn with standard optimization methods such as SGD, RMSProp, LBFGS, Adam etc.|\n|torch.multiprocessing|python multiprocessing, but with magical memory sharing of torch Tensors across processes. Useful for data loading and hogwild training.|\n|torch.utils|DataLoader, Trainer and other utility functions for convenience|\n|torch.legacy(.nn/.optim)|legacy code that has been ported over from torch for backward compatibility reasons|\n\n---\n\n#### Installation\n\n* Step 1: 使用Anaconda的命令行 新建一个虚拟环境,激活并且关联到jupyterbook\n  {% codeblock %}\n  conda create  --name pytorch python=3.5\n  activate pytorch\n  conda install nb_conda\n  {% endcodeblock %}\n* Step 2: Install GPU version PyTorch.\n  {% codeblock %}\n  conda install pytorch cuda90 -c pytorch \n  pip install torchvision\n  {% endcodeblock %}\n\n#### Understanding of PyTorch\n\n* **Tensors**\n  Tensors和numpy中的ndarrays较为相似, 与此同时Tensor也能够使用GPU来加速运算\n  ```python\n  from __future__ import print_function\n  import torch\n  x = torch.Tensor(5, 3)  # 构造一个未初始化的5*3的矩阵\n  x = torch.rand(5, 3)  # 构造一个随机初始化的矩阵\n  x # 此处在notebook中输出x的值来查看具体的x内容\n  x.size()\n\n  #NOTE: torch.Size 事实上是一个tuple, 所以其支持相关的操作*\n  y = torch.rand(5, 3)\n\n  #此处 将两个同形矩阵相加有两种语法结构\n  x + y # 语法一\n  torch.add(x, y) # 语法二\n\n  # 另外输出tensor也有两种写法\n  result = torch.Tensor(5, 3) # 语法一\n  torch.add(x, y, out=result) # 语法二\n  y.add_(x) # 将y与x相加\n\n  # 特别注明：任何可以改变tensor内容的操作都会在方法名后加一个下划线'_'\n  # 例如：x.copy_(y), x.t_(), 这俩都会改变x的值。\n\n  #另外python中的切片操作也是资次的。\n  x[:,1] #这一操作会输出x矩阵的第二列的所有值\n  ```\n\n* **Numpy桥**\n  将Torch的Tensor和numpy的array相互转换简，注意Torch的Tensor和numpy的array会共享他们的存储空间，修改一个会导致另外的一个也被修改。\n  \n  ```python\n# 此处演示tensor和numpy数据结构的相互转换\na = torch.ones(5)\nb = a.numpy()\n\n# 此处演示当修改numpy数组之后,与之相关联的tensor也会相应的被修改\na.add_(1)\nprint(a)\nprint(b)\n\n# 将numpy的Array转换为torch的Tensor\nimport numpy as np\na = np.ones(5)\nb = torch.from_numpy(a)\nnp.add(a, 1, out=a)\nprint(a)\nprint(b)\n\n# 另外除了CharTensor之外，所有的tensor都可以在CPU运算和GPU预算之间相互转换\n# 使用CUDA函数来将Tensor移动到GPU上\n# 当CUDA可用时会进行GPU的运算\nif torch.cuda.is_available():\n    x = x.cuda()\n    y = y.cuda() \n  ```\n* **使用PyTorch设计一个CIFAR10数据集的分类模型**\n**[code](https://github.com/Trouble404/Object-Detection-System-Based-on-CNN-and-Capsule-Network/blob/master/Learned%20skills/pytorch.ipynb)**\n\n* **MMdnn**\n  MMdnn is a set of tools to help users inter-operate among different deep learning frameworks. E.g. model conversion and visualization. Convert models between Caffe, Keras, MXNet, Tensorflow, CNTK, PyTorch Onnx and CoreML.\n  \n  ![iamge](https://raw.githubusercontent.com/Microsoft/MMdnn/master/docs/supported.jpg)\n  \n  MMdnn主要有以下特征：\n\n  * 模型文件转换器，不同的框架间转换DNN模型\n  * 模型代码片段生成器，生成适合不同框架的代码\n  * 模型可视化，DNN网络结构和框架参数可视化\n  * 模型兼容性测试（正在进行中）\n \n **[Github](https://github.com/Microsoft/MMdnn)**\n \n### 【04/06/2018】\n#### **Dataset:**\n **[VOC 2012 Dataset](http://host.robots.ox.ac.uk/pascal/VOC/voc2012/index.html)**\n \n#### **Introduce:**\n **Visual Object Classes Challenge 2012 (VOC2012)**\n[PASCAL](http://host.robots.ox.ac.uk/pascal/VOC/)'s full name is Pattern Analysis, Statistical Modelling and Computational Learning.\nVOC's full name is **Visual OBject Classes**.\nThe first competition was held in 2005 and terminated in 2012. I will use the last updated dataset which is [VOC2012](http://host.robots.ox.ac.uk/pascal/VOC/voc2012/index.html) dataset.\n\nThe main aim of this competition is object detection, there are 20 classes objects in the dataset:\n* person\n* bird, cat, cow, dog, horse, sheep\n* aeroplane, bicycle, boat, bus, car, motorbike, train\n* bottle, chair, dining table, potted plant, sofa, tv/monitor\n\n#### **Detection Task**\nReferenced: \n**The PASCAL Visual Object Classes Challenge 2012 (VOC2012) Development Kit**\n**Mark Everingham - John Winn**\nhttp://host.robots.ox.ac.uk/pascal/VOC/voc2012/htmldoc/index.html\n\n**Task:**\nFor each of the twenty classes predict the bounding boxes of each object of that class in a test image (if any). Each bounding box should be output with an associated real-valued confidence of the detection so that a precision/recall curve can be drawn. Participants may choose to tackle all, or any subset of object classes, for example 'cars only' or 'motorbikes and cars'.\n\n**Competitions**:\nTwo competitions are defined according to the choice of training data:\n*  taken from the $VOC_{trainval}$ data provided.\n*  from any source excluding the $VOC_{test}$ data provided.\n\n**Submission of Results**:\nA separate text file of results should be generated for each competition and each class e.g. \\`car'. Each line should be a detection output by the detector in the following format:\n    ```\n    <image identifier> <confidence> <left> <top> <right> <bottom>\n    ```\nwhere (left,top)-(right,bottom) defines the bounding box of the detected object. The top-left pixel in the image has coordinates $(1,1)$. Greater confidence values signify greater confidence that the detection is correct. An example file excerpt is shown below. Note that for the image 2009_000032, multiple objects are detected:\n```\ncomp3_det_test_car.txt:\n    ...\n    2009_000026 0.949297 172.000000 233.000000 191.000000 248.000000\n    2009_000032 0.013737 1.000000 147.000000 114.000000 242.000000\n    2009_000032 0.013737 1.000000 134.000000 94.000000 168.000000\n    2009_000035 0.063948 455.000000 229.000000 491.000000 243.000000\n    ...\n```\n\n**Evaluation**:\nThe detection task will be judged by the precision/recall curve. The principal quantitative measure used will be the average precision (AP). Detections are considered true or false positives based on the area of overlap with ground truth bounding boxes. To be considered a correct detection, the area of overlap $a_o$ between the predicted bounding box $B_p$ and ground truth bounding box $B_{gt}$ must exceed $50\\%$ by the formula: \n<center> $a_o = \\frac{area(B_p \\cap B_{gt})}{area(B_p \\cup B_{gt})}$ </center>\n\n #### **XML标注格式**\n 对于目标检测来说，每一张图片对应一个xml格式的标注文件。所以你会猜到，就像gemfield准备的训练集有8万张照片一样，在存放xml文件的目录里，这里也将会有8万个xml文件。下面是其中一个xml文件的示例：\n ```html\n <?xml version=\"1.0\" encoding=\"utf-8\"?>\n<annotation>\n    <folder>VOC2007</folder>\n    <filename>test100.mp4_3380.jpeg</filename>\n    <size>\n        <width>1280</width>\n        <height>720</height>\n        <depth>3</depth>\n    </size>\n    <object>\n        <name>gemfield</name>\n        <bndbox>\n            <xmin>549</xmin>\n            <xmax>715</xmax>\n            <ymin>257</ymin>\n            <ymax>289</ymax>\n        </bndbox>\n        <truncated>0</truncated>\n        <difficult>0</difficult>\n    </object>\n    <object>\n        <name>civilnet</name>\n        <bndbox>\n            <xmin>842</xmin>\n            <xmax>1009</xmax>\n            <ymin>138</ymin>\n            <ymax>171</ymax>\n        </bndbox>\n        <truncated>0</truncated>\n        <difficult>0</difficult>\n    </object>\n    <segmented>0</segmented>\n</annotation>\n```\n\n在这个测试图片上，我们标注了2个object，一个是gemfield，另一个是civilnet。\n\n在这个xml例子中：\n* bndbox是一个轴对齐的矩形，它框住的是目标在照片中的可见部分；\n* truncated表明这个目标因为各种原因没有被框完整（被截断了），比如说一辆车有一部分在画面外；\n* occluded是说一个目标的重要部分被遮挡了（不管是被背景的什么东西，还是被另一个待检测目标遮挡）；\n* difficult表明这个待检测目标很难识别，有可能是虽然视觉上很清楚，但是没有上下文的话还是很难确认它属于哪个分类；标为difficult的目标在测试成绩的评估中一般会被忽略。\n\n**注意：在一个object中，name 标签要放在前面，否则的话，目标检测的一个重要工程实现SSD会出现解析数据集错误（另一个重要工程实现py-faster-rcnn则不会）。**\n\n### 【07/06/2018】\n#### **Poster conference**\n![iamge](https://raw.githubusercontent.com/Trouble404/Object-Detection-System-Based-on-CNN-and-Capsule-Network/master/Poster/poster.png)\n\n5 People in one group to present their object.\nI present this object to my supervisor in this conference.\n\n### 【11/06/2018】\n#### **R-CNN**\nPaper: [Rich feature hierarchies for accurate object detection and semantic segmentation](https://arxiv.org/abs/1311.2524)\n\n【**论文主要特点**】（相对传统方法的改进）\n\n* 速度： 经典的目标检测算法使用滑动窗法依次判断所有可能的区域。本文则(采用Selective Search方法)预先提取一系列较可能是物体的候选区域，之后仅在这些候选区域上(采用CNN)提取特征，进行判断。\n* 训练集： 经典的目标检测算法在区域中提取人工设定的特征。本文则采用深度网络进行特征提取。使用两个数据库： 一个较大的识别库   （ImageNet ILSVC 2012）：标定每张图片中物体的类别。一千万图像，1000类。 一个较小的检测库（PASCAL VOC 2007）：标定每张   图片中，物体的类别和位置，一万图像，20类。 本文使用识别库进行预训练得到CNN（有监督预训练），而后用检测库调优参数，最后在   检测库上评测。\n\n【**流程**】\n\n1. 候选区域生成： 一张图像生成1K~2K个候选区域 （采用Selective Search 方法）\n2. 特征提取： 对每个候选区域，使用深度卷积网络提取特征 （CNN） \n3. 类别判断： 特征送入每一类的SVM 分类器，判别是否属于该类\n4. 位置精修： 使用回归器精细修正候选框位置\n<center>![image](https://raw.githubusercontent.com/Trouble404/Object-Detection-System-Based-on-CNN-and-Capsule-Network/master/Research%20Review/LaTex/1.PNG)</center>\n\n【**[Selective Search](https://www.koen.me/research/pub/uijlings-ijcv2013-draft.pdf)**】\n1. 使用一种过分割手段，将图像分割成小区域 (1k~2k 个)\n2. 查看现有小区域，按照合并规则合并可能性最高的相邻两个区域。重复直到整张图像合并成一个区域位置\n3. 输出所有曾经存在过的区域，所谓候选区域\n   其中合并规则如下： 优先合并以下四种区域：\n   * 颜色（颜色直方图）相近的\n   * 纹理（梯度直方图）相近的\n   * 合并后总面积小的： 保证合并操作的尺度较为均匀，避免一个大区域陆续“吃掉”其他小区域 （例：设有区域a-b-c-d-e-f-g-h。较好的合并方式是：ab-cd-ef-gh -> abcd-efgh -> abcdefgh。 不好的合并方法是：ab-c-d-e-f-g-h ->abcd-e-f-g-h ->abcdef-gh -> abcdefgh）\n   * 合并后，总面积在其BBOX中所占比例大的： 保证合并后形状规则。\n\n### 【12/06/2018】\n#### **SPP-CNN**\nPaper: [Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition](https://arxiv.org/abs/1406.4729)\n\n【**论文主要特点**】（相对传统方法的改进）\n\nRCNN使用CNN作为特征提取器，首次使得目标检测跨入深度学习的阶段。但是RCNN对于每一个区域候选都需要首先将图片放缩到固定的尺寸（224\\*224），然后为每个区域候选提取CNN特征。容易看出这里面存在的一些性能瓶颈：\n* 速度瓶颈：重复为每个region proposal提取特征是极其费时的，Selective Search对于每幅图片产生2K左右个region proposal，也就是意味着一幅图片需要经过2K次的完整的CNN计算得到最终的结果。\n* 性能瓶颈：对于所有的region proposal防缩到固定的尺寸会导致我们不期望看到的几何形变，而且由于速度瓶颈的存在，不可能采用多尺度或者是大量的数据增强去训练模型。\n\n\n【**流程**】\n\n1. 首先通过selective search产生一系列的region proposal\n2. 然后训练多尺寸识别网络用以提取区域特征，其中处理方法是每个尺寸的最短边大小在尺寸集合中：\n   $s \\in S = \\{480,576,688,864,1200\\}$\n   训练的时候通过上面提到的多尺寸训练方法，也就是在每个epoch中首先训练一个尺寸产生一个model，然后加载这个model并训练第二个尺寸，直到训练完所有的尺寸。空间金字塔池化使用的尺度为：1\\*1，2\\*2，3\\*3，6\\*6，一共是50个bins。\n3. 在测试时，每个region proposal选择能使其包含的像素个数最接近224\\*224的尺寸，提取相 应特征。\n4. 训练SVM，BoundingBox回归.\n<center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/SPP-NET.jpg)</center>\n\n\n### 【13/06/2018】\n#### **FAST R-CNN**\nPaper: [Fast R-CNN](https://arxiv.org/abs/1504.08083)\n\n【**论文主要特点**】（相对传统方法的改进）\n\n* 测试时速度慢：RCNN一张图像内候选框之间大量重叠，提取特征操作冗余。本文将整张图像归一化后直接送入深度网络。在邻接时，才加入候选框信息，在末尾的少数几层处理每个候选框。\n* 训练时速度慢 ：原因同上。在训练时，本文先一张图像送入网络，紧接着送入从这幅图像上提取出的候选区域。这些候选区域的前几层特征不需要再重复计算。\n* 训练所需空间大: RCNN中独立的分类器和回归器需要大量特征作为训练样本。本文把类别判断和位置精调统一用深度网络实现，不再需要额外存储。\n\n\n【**流程**】\n\n1. 网络首先用几个卷积层（conv）和最大池化层处理整个图像以产生conv特征图。\n2. 然后，对于每个对象建议框（object proposals ），感兴趣区域（region of interest——RoI）池层从特征图提取固定长度的特征向量。\n3. 每个特征向量被输送到分支成两个同级输出层的全连接（fc）层序列中：\n   其中一层进行分类，对 目标关于K个对象类（包括全部“背景background”类）产生softmax概率估计，即输出每一个RoI的概率分布；\n另一层进行bbox regression，输出K个对象类中每一个类的四个实数值。每4个值编码K个类中的每个类的精确边界盒（bounding-box）位置，即输出每一个种类的的边界盒回归偏差。整个结构是使用多任务损失的端到端训练（trained end-to-end with a multi-task loss）。\n<center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/fast-rcnn.png)</center>\n\n### 【14~18/06/2018】\n#### **FASTER R-CNN**\nI want to use **Faster R-cnn** as the first method to implement object detection system.\n\nPaper: [Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks](https://arxiv.org/abs/1506.01497)\n\n在结构上，Faster RCNN已经将特征抽取(feature extraction)，proposal提取，bounding box regression(rect refine)，classification都整合在了一个网络中，使得综合性能有较大提高，在检测速度方面尤为明显。\n<center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster-rcnn.jpg)</center>\n\n #### 流程\n\n1. Conv layers：作为一种CNN网络目标检测方法，Faster R-CNN首先使用一组基础的conv+relu+pooling层提取image的feature maps。该feature maps被共享用于后续RPN层和全连接层。\n2. Region Proposal Networks：RPN网络用于生成region proposals。该层通过softmax判断anchors属于foreground或者background，再利用bounding box regression修正anchors获得精确的proposals。\n3. Roi Pooling：该层收集输入的feature maps和proposals，综合这些信息后提取proposal feature maps，送入后续全连接层判定目标类别。\n4. Classification：利用proposal feature maps计算proposal的类别，同时再次bounding box regression获得检测框最终的精确位置。\n\n#### 解释\n\n**\\[1. Conv layers\\]**\n   <center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_1.jpg)</center>\n   Conv layers包含了conv，pooling，relu三种层。以python版本中的VGG16模型中的faster_rcnn_test.pt的网络结构为例，如图,    Conv layers部分共有13个conv层，13个relu层，4个pooling层。这里有一个非常容易被忽略但是又无比重要的信息，在Conv          layers中：\n   \n  * 所有的conv层都是： $kernel\\_size=3$ ， $pad=1$ ， $stride=1$ <br>\n  * 所有的pooling层都是： $kernel\\_size=2$ ， $pad=0$ ， $stride=2$\n  \n   为何重要？在Faster RCNN Conv layers中对所有的卷积都做了扩边处理（ $pad=1$ ，即填充一圈0），导致原图变为                $(M+2)\\times (N+2)$ 大小，再做3x3卷积后输出 $M\\times N$ 。正是这种设置，导致Conv layers中的conv层不改变输入和输出    矩阵大小。如下图：\n<center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_2.jpg)</center>\n   类似的是，Conv layers中的pooling层 $kernel\\_size=2$ ， $stride=2$ 。这样每个经过pooling层的 $M\\times N$ 矩阵，都会变为 $(M/2) \\times(N/2)$ 大小。综上所述，在整个Conv layers中，conv和relu层不改变输入输出大小，只有pooling层使输出长宽都变为输入的1/2。\n那么，一个 $M\\times N$ 大小的矩阵经过Conv layers固定变为 $(M/16)\\times (N/16)$ ！这样Conv layers生成的featuure map中都可以和原图对应起来。\n\n**\\[2. Region Proposal Networks(RPN)\\]**\n   经典的检测方法生成检测框都非常耗时，如OpenCV adaboost使用滑动窗口+图像金字塔生成检测框；或如R-CNN使用SS(Selective      Search)方法生成检测框。而Faster RCNN则抛弃了传统的滑动窗口和SS方法，直接使用RPN生成检测框，这也是Faster R-CNN的巨大    优势，能极大提升检测框的生成速度。\n   <center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_3.jpg)</center>\n   上图展示了RPN网络的具体结构。可以看到RPN网络实际分为2条线，上面一条通过softmax分类anchors获得foreground和              background（检测目标是foreground），下面一条用于计算对于anchors的bounding box regression偏移量，以获得精确的          proposal。而最后的Proposal层则负责综合foreground anchors和bounding box regression偏移量获取proposals，同时剔除太    小和超出边界的proposals。其实整个网络到了Proposal Layer这里，就完成了相当于目标定位的功能。\n   \n   **2.1 多通道图像卷积基础知识介绍**\n   * 对于单通道图像+单卷积核做卷积，之前展示了；\n   * 对于多通道图像+多卷积核做卷积，计算方式如下：\n     <center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_4.jpg)</center>\n     输入有3个通道，同时有2个卷积核。对于每个卷积核，先在输入3个通道分别作卷积，再将3个通道结果加起来得到卷积输出。所以对     于某个卷积层，无论输入图像有多少个通道，输出图像通道数总是等于卷积核数量！\n    对多通道图像做 $1\\times1$ 卷积，其实就是将输入图像于每个通道乘以卷积系数后加在一起，即相当于把原图像中本来各个独立的     通道“联通”在了一起。\n    \n   **2.2 Anchors**\n   提到RPN网络，就不能不说anchors。所谓anchors，实际上就是一组由rpn/generate_anchors.py生成的矩形。直接运行作者demo中    的generate_anchors.py可以得到以下输出：\n   [[ -84.  -40.   99.   55.]\n   [-176.  -88.  191.  103.]\n   [-360. -184.  375.  199.]\n   [ -56.  -56.   71.   71.]\n   [-120. -120.  135.  135.]\n   [-248. -248.  263.  263.]\n   [ -36.  -80.   51.   95.]\n   [ -80. -168.   95.  183.]\n   [-168. -344.  183.  359.]]\n\n   其中每行的4个值 $(x1,y1,x2,y2)$ 代表矩形左上和右下角点坐标。9个矩形共有3种形状，长宽比为大约为 $width:height = [1:1, 1:2, 2:1]$ 三种，如下图。实际上通过anchors就引入了检测中常用到的多尺度方法。\n   <center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_5.jpg)</center>\n   注：关于上面的anchors size，其实是根据检测图像设置的。在python demo中，会把任意大小的输入图像reshape成 $800\\times600$。再回头来看anchors的大小，anchors中长宽 1:2 中最大为 $352\\times704$ ，长宽 2:1 中最大 $736\\times384$ ，基本是cover了 $800\\times600$ 的各个尺度和形状。\n那么这9个anchors是做什么的呢？借用Faster RCNN论文中的原图，如下图，遍历Conv layers计算获得的feature maps，为每一个点都配备这9种anchors作为初始的检测框。这样做获得检测框很不准确，不用担心，后面还有2次bounding box regression可以修正检测框位置。\n  <center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_6.jpg)</center>\n  \n  解释一下上面这张图的数字。\n\n* 在原文中使用的是ZF model中，其Conv Layers中最后的conv5层num_output=256，对应生成256张特征图，所以相当于feature map每个点都是256-dimensions\n* 在conv5之后，做了rpn_conv/3x3卷积且num_output=256，相当于每个点又融合了周围3x3的空间信息（猜测这样做也许更鲁棒？反正我没测试），同时256-d不变（如图4和图7中的红框）\n* 假设在conv5 feature map中每个点上有k个anchor（默认k=9），而每个anhcor要分foreground和background，所以每个点由256d feature转化为cls=2k scores；而每个anchor都有\\[x, y, w, h\\]对应4个偏移量，所以reg=4k coordinates\n* 补充一点，全部anchors拿去训练太多了，训练程序会在合适的anchors中随机选取128个postive anchors+128个negative anchors进行训练（什么是合适的anchors下文5.1有解释）\n\n   **2.3 softmax判定foreground与background**\n   一副MxN大小的矩阵送入Faster RCNN网络后，到RPN网络变为(M/16)x(N/16)，不妨设 W=M/16 ， H=N/16 。在进入reshape与softmax之前，先做了1x1卷积，如下图：\n   <center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_7.jpg)</center>\n   该1x1卷积的caffe prototxt定义如下：\n   <center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_19.PNG)</center>\n可以看到其num_output=18，也就是经过该卷积的输出图像为 $W\\times H \\times 18$ 大小（注意第二章开头提到的卷积计算方式）。这也就刚好对应了feature maps每一个点都有9个anchors，同时每个anchors又有可能是foreground和background，所有这些信息都保存 $W\\times H\\times (9\\cdot2)$ 大小的矩阵。为何这样做？后面接softmax分类获得foreground anchors，也就相当于初步提取了检测目标候选区域box（一般认为目标在foreground anchors中）。\n综上所述，RPN网络中利用anchors和softmax初步提取出foreground anchors作为候选区域。\n\n   **2.4 bounding box regression原理**\n 如图所示绿色框为飞机的Ground Truth(GT)，红色为提取的foreground anchors，即便红色的框被分类器识别为飞机，但是由于红色的框定位不准，这张图相当于没有正确的检测出飞机。所以我们希望采用一种方法对红色的框进行微调，使得foreground anchors和GT更加接近。\n<center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_8.jpg)</center>\n对于窗口一般使用四维向量 (x, y, w, h) 表示，分别表示窗口的中心点坐标和宽高。对于下图，红色的框A代表原始的Foreground Anchors，绿色的框G代表目标的GT，我们的目标是寻找一种关系，使得输入原始的anchor A经过映射得到一个跟真实窗口G更接近的回归窗口G'，即：\n* 给定：$anchor A=(A_{x}, A_{y}, A_{w}, A_{h})$ 和 $GT=[G_{x}, G_{y}, G_{w}, G_{h}]$\n* 寻找一种变换F，使得：$F(A_{x}, A_{y}, A_{w}, A_{h})=(G_{x}^{'}, G_{y}^{'}, G_{w}^{'}, G_{h}^{'})$，其中 $(G_{x}^{'}, G_{y}^{'}, G_{w}^{'}, G_{h}^{'}) \\approx (G_{x}, G_{y}, G_{w}, G_{h})$\n<center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_9.jpg)</center>\n那么经过何种变换F才能从图10中的anchor A变为G'呢？ 比较简单的思路就是:\n\n* 先做平移\n<center>\n$G^{'}_{x} = A_{w} \\cdot d_{x}(A) + A_{x} $\n$G^{'}_{y} = A_{y} \\cdot d_{y}(A) + A_{y} $\n</center>\n* 再做缩放\n<center>\n$G^{'}_{w} = A_{w} \\cdot exp(d_{w}(A)) $\n$G^{'}_{h} = A_{h} \\cdot exp(d_{h}(A)) $\n</center>\n\n观察上面4个公式发现，需要学习的是 $d_{x}(A),d_{y}(A),d_{w}(A),d_{h}(A)$ 这四个变换。当输入的anchor A与GT相差较小时，可以认为这种变换是一种线性变换， 那么就可以用线性回归来建模对窗口进行微调（注意，只有当anchors A和GT比较接近时，才能使用线性回归模型，否则就是复杂的非线性问题了）。\n\n接下来的问题就是如何通过线性回归获得 $d_{x}(A),d_{y}(A),d_{w}(A),d_{h}(A)$ 了。线性回归就是给定输入的特征向量X, 学习一组参数W, 使得经过线性回归后的值跟真实值Y非常接近，即$Y=WX$。对于该问题，输入X是cnn feature map，定义为Φ；同时还有训练传入A与GT之间的变换量，即$(t_{x}, t_{y}, t_{w}, t_{h})$。输出是$d_{x}(A),d_{y}(A),d_{w}(A),d_{h}(A)$四个变换。那么目标函数可以表示为：\n<center>\n$d_{*}(A) = w^{T}_{*} \\cdot \\phi(A)$\n</center>\n\n其中Φ(A)是对应anchor的feature map组成的特征向量，w是需要学习的参数，d(A)是得到的预测值（\\*表示 x，y，w，h，也就是每一个变换对应一个上述目标函数）。为了让预测值$(t_{x}, t_{y}, t_{w}, t_{h})$与真实值差距最小，设计损失函数：\n<center>\n$Loss = \\sum^{N}_{i}(t^{i}_{*} - \\hat{w}^{T}_{*} \\cdot \\phi(A^{i}))^{2}$\n</center>\n函数优化目标为：\n<center>\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_20.jpg)\n</center>\n需要说明，只有在GT与需要回归框位置比较接近时，才可近似认为上述线性变换成立。\n说完原理，对应于Faster RCNN原文，foreground anchor与ground truth之间的平移量 $(t_x, t_y)$ 与尺度因子 $(t_w, t_h)$ 如下：\n<center>\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_21.jpg)\n</center>\n对于训练bouding box regression网络回归分支，输入是cnn feature Φ，监督信号是Anchor与GT的差距 $(t_x, t_y, t_w, t_h)$，即训练目标是：输入Φ的情况下使网络输出与监督信号尽可能接近。\n那么当bouding box regression工作时，再输入Φ时，回归网络分支的输出就是每个Anchor的平移量和变换尺度 $(t_x, t_y, t_w, t_h)$，显然即可用来修正Anchor位置了。\n\n   **2.5 对proposals进行bounding box regression**\n在了解bounding box regression后，再回头来看RPN网络第二条线路，如下图。\n<center>\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_10.jpg)\n</center>\n其 $num\\_output=36$ ，即经过该卷积输出图像为 $W\\times H\\times 36$ ，在caffe blob存储为 \\[1, 36, H, W\\] ，这里相当于feature maps每个点都有9个anchors，每个anchors又都有4个用于回归的$d_{x}(A),d_{y}(A),d_{w}(A),d_{h}(A)$变换量。\n\n   **2.6 Proposal Layer**\nProposal Layer负责综合所有 $d_{x}(A),d_{y}(A),d_{w}(A),d_{h}(A)$ 变换量和foreground anchors，计算出精准的proposal，送入后续RoI Pooling Layer。\n首先解释im_info。对于一副任意大小PxQ图像，传入Faster RCNN前首先reshape到固定 $M\\times N$ ，im_info=\\[M, N, scale_factor\\]则保存了此次缩放的所有信息。然后经过Conv Layers，经过4次pooling变为 $W\\times H=(M/16)\\times(N/16)$ 大小，其中feature_stride=16则保存了该信息，用于计算anchor偏移量。\n<center>\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_11.jpg)\n</center>\n\nProposal Layer forward（caffe layer的前传函数）按照以下顺序依次处理：\n1. 生成anchors，利用$[d_{x}(A),d_{y}(A),d_{w}(A),d_{h}(A)]$对所有的anchors做bbox regression回归（这里的anchors生成和训练时完全一致）\n2. 按照输入的foreground softmax scores由大到小排序anchors，提取前pre_nms_topN(e.g. 6000)个anchors，即提取修正位置后的foreground anchors。\n3. 限定超出图像边界的foreground anchors为图像边界（防止后续roi pooling时proposal超出图像边界）\n4. 剔除非常小（width<threshold or height<threshold）的foreground anchors\n5. 进行nonmaximum suppression\n6. 再次按照nms后的foreground softmax scores由大到小排序fg anchors，提取前post_nms_topN(e.g. 300)结果作为proposal输出。\n   \n之后输出 proposal=\\[x1, y1, x2, y2\\] ，注意，由于在第三步中将anchors映射回原图判断是否超出边界，所以这里输出的proposal是对应 $M\\times N$ 输入图像尺度的，这点在后续网络中有用。另外我认为，严格意义上的检测应该到此就结束了，后续部分应该属于识别了~   \n   \n**RPN**网络结构就介绍到这里，总结起来就是：\n**生成anchors -> softmax分类器提取fg anchors -> bbox reg回归fg anchors -> Proposal Layer生成proposals**\n\n### 【19/06/2018】\n#### 处理 XML 文档\n使用 xml.etree.ElementTree 这个包去解析XML文件， 并且整理成为list形式\n【流程】\n* 读取XML文件\n* 区分训练集测试集根据竞赛要求\n* 解析XML文档收录到PYTHON词典中\n<center>\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_23.PNG)\n</center> \nGithub 的 jupyter notebook [地址](https://github.com/Trouble404/Object-Detection-System-Based-on-CNN-and-Capsule-Network/blob/master/Faster-RCNN/parser_voc2012_xml_and_plotting.ipynb) \n\n训练集根据竞赛的 trainval.txt 文件给的图片作为训练集\n其余的作为训练集\n\n解析后， 总共有 17125 张图片，\n其中 11540 张作为训练集\n\n图片中的20个类的统计情况：\n<center>\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_24.PNG)\n</center> \n\n\n### 【20/06/2018】\n#### 根据信息画出BBOXES\n安装 cv2 这个包\n  {% codeblock %}\n  pip install opencv-python\n  {% endcodeblock %}\n注意： OpenCV-python 中颜色格式 是BGR 而不是 RGB\n\n在VOC2012数据集里面，总共有20类， 根据不同的种类用不同的颜色和唯一的编码画BBOXES。\n\n| class | class_mapping | BGR of bbox |\n| :--- | :---- | :---- |\n| Person | 0 | (0, 0, 255) | \n| Aeroplane | 1 | (0, 0, 255) | \n| Tvmonitor | 2 | (0, 128, 0) | \n| Train | 3 | (128, 128, 128) | \n| Boat | 4 | (0, 165, 255) | \n| Dog | 5 | (0, 255, 255) | \n| Chair | 6 | (80, 127, 255) | \n| Bird | 7 | (208, 224, 64) | \n| Bicycle | 8 | (235, 206, 135) | \n| Bottle | 9 | (128, 0, 0) | \n| Sheep | 10 | (140, 180, 210) | \n| Diningtable | 11 | (0, 255, 0) | \n| Horse | 12 | (133, 21, 199) | \n| Motorbike | 13 | (47, 107, 85) | \n| Sofa | 14 | (19, 69, 139) | \n| Cow | 15 | (222, 196, 176) | \n| Car | 16 | (0, 0, 0) | \n| Cat | 17 |  (225, 105, 65) | \n| Bus | 18 | (255, 255, 255) | \n| Pottedplant | 19 | (205, 250, 255) | \n\n我写了一个show_image_with_bbox函数去画出带BBOXES的图根据处理XML文件得到的list:\n<center>\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_22.PNG)\n</center>  \nGithub 的 jupyter notebook [地址](https://github.com/Trouble404/Object-Detection-System-Based-on-CNN-and-Capsule-Network/blob/master/Faster-RCNN/parser_voc2012_xml_and_plotting.ipynb) \n\nEXAMPLE:\n<center>\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/img_with_bboex_1.PNG)\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/img_with_bboex_2.PNG)\n</center>  \n\n### 【21/06/2018】\n#### config setting\nset config class:\n                 for image enhancement:\n<center>\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/21_1.PNG)\n</center>  \n\n#### image enhancement\n<center>\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/21_2.PNG)\n</center>  \nAccording to the config of three peremeters, users could augment image with 3 different ways or using them all.\nFor horizontal and vertical flips, 1/3 probability to triggle\nWith 0,90,180,270 rotation, \nThis function could increase the number of datasets.\n\nimage flips and rotation are realized by opencv and replace of height and width\nNew cordinates of bboxes are calculated acccording to different change of image\n\ndetailed in Github, jupyter notebook: [address](https://github.com/Trouble404/Object-Detection-System-Based-on-CNN-and-Capsule-Network/blob/master/Faster-RCNN/config_set_and_image_enhance.ipynb)\n\nOrignal image:\n<center>\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/21_3.PNG)\n</center>  \nhorizontal flip:\n<center>\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/21_4.PNG)\n</center>  \nVertical filp:\n<center>\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/21_5.PNG)\n</center>  \nRandom rotation:\n<center>\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/21_6.PNG)\n</center>  \nHorizontal and then vertical flips:\n<center>\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/21_7.PNG)\n</center>  \n\n### 【22/06/2018】\n#### Image rezise\nThis function is to rezise input image to a uniform size with same shortest side\n<center>\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/22_2.PNG)\n</center> \n\nAccording to set the value of shortest side, convergent-divergent or augmented another side proportion\n\nTest:\nLeft image is resized image, in this case, the orignal image amplified.\n<center>\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/22_1.png)\n</center> \n\n#### Class Balance\nWhen training the model, if we sent image with no repeating classes, it may help to improve the performance of model. Therefore, this function is to make sure no repeating classes in two closed input image.\n<center>\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/22_3.PNG)\n</center> \n\nTest:\n<center>\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/22_4.PNG)\n</center> \nRandom output 4 iamge with is function, it could find no repeating classes in two closed image. However, it may reduce the number of trainning image because skip some images.\n\n\n### 【25~26/06/2018】\n#### Region Proposal Networks(RPN)\n<center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_3.jpg)</center>\n可以看到RPN网络实际分为2条线，上面一条通过softmax分类anchors获得foreground和background（检测目标是foreground），下面一条用于计算对于anchors的bounding box regression偏移量，以获得精确的proposal。而最后的Proposal层则负责综合foreground anchors和bounding box regression偏移量获取proposals，同时剔除太小和超出边界的proposals。其实整个网络到了Proposal Layer这里，就完成了相当于目标定位的功能。\n\n#### Anchors\n对每一个点生成的矩形\n<center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_5.jpg)</center>\n其中每行的4个值 (x1,y1,x2,y2) 代表矩形左上和右下角点坐标。9个矩形共有3种形状，长宽比为大约为 width:height = \\[1:1, 1:2, 2:1\\]\n<center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_6.jpg)</center>\n通过遍历Conv layers计算获得的feature maps，为每一个点都配备这9种anchors作为初始的检测框。这样做获得检测框很不准确，不用担心，后面还有2次bounding box regression可以修正检测框位置.\n\n#### Code\n\n```python\n\"\"\" intersection of two bboxes\n@param ai: left top x,y and right bottom x,y coordinates of bbox 1\n@param bi: left top x,y and right bottom x,y coordinates of bbox 2\n\n@return: area_union: whether contain target classes\n\n\"\"\"\ndef intersection(ai, bi):\n```\n```python\n\"\"\" union of two bboxes\n@param au: left top x,y and right bottom x,y coordinates of bbox 1\n@param bu: left top x,y and right bottom x,y coordinates of bbox 2\n@param area_intersection: intersection area\n\n@return: area_union: whether contain target classes\n\n\"\"\"\ndef union(au, bu, area_intersection):\n```\n\n```python\n\"\"\" calculate ratio of intersection and union\n@param a: left top x,y and right bottom x,y coordinates of bbox 1\n@param b: left top x,y and right bottom x,y coordinates of bbox 2\n\n@return: ratio of intersection and union of two bboxes\n\n\"\"\"\ndef iou(a, b):\n```\n**IOU is used to bounding box regression**\n\n---\n** rpn calculation**\n\n1. Traversal all pre-anchors to calculate IOU with GT bboxes\n2. Set number and proprty of pre-anchors\n3. return specity number of result(Anchors)\n\n```python\n\"\"\" \n\n@param C: configuration\n@param img_data: parsered xml information\n@param width: orignal width of image\n@param hegiht: orignal height of image\n@param resized_width: resized width of image after image processing\n@param resized_heighth: resized height of image after image processing\n@param img_length_calc_function: Keras's image_dim_ordering function\n\n@return: np.copy(y_rpn_cls): whether contain target classes\n@return: np.copy(y_rpn_regr): corrspoding return of gradient\n\n\"\"\"\n\ndef calc_rpn(C, img_data, width, height, resized_width, resized_height, img_length_calc_function):\n```\n\n【注：其只会返回num_regions（这里设置为256）个有效的正负样本 】\n\n【流程】\nInitialise paramters: see [jupyter notebook](https://github.com/Trouble404/Object-Detection-System-Based-on-CNN-and-Capsule-Network/blob/master/Faster-RCNN/rpn_calculation.ipynb)\n\nCalculate the size of map feature:\n```python\n(output_width, output_height) = img_length_calc_function(resized_width, resized_height)\n```\n<br>\nGet the GT box coordinates, and resize to account for image resizing\nafter rezised functon, the coordinates of bboxes need to re-calculation:\n```python\nfor bbox_num, bbox in enumerate(img_data['bboxes']):\n\tgta[bbox_num, 0] = bbox['x1'] * (resized_width / float(width))\n\tgta[bbox_num, 1] = bbox['x2'] * (resized_width / float(width))\n\tgta[bbox_num, 2] = bbox['y1'] * (resized_height / float(height))\n\tgta[bbox_num, 3] = bbox['y2'] * (resized_height / float(height))\n```\n【注意gta的存储形式是（x1,x2,y1,y2）而不是（x1,y1,x2,y2）】\n<br>\nTraverse all possible group of sizes\nanchor box scales \\[128, 256, 512\\]\nanchor box ratios \\[1:1,1:2,2:1\\]\n```python\nfor anchor_size_idx in range(len(anchor_sizes)):\n\tfor anchor_ratio_idx in range(len(anchor_ratios)):\n\t\tanchor_x = anchor_sizes[anchor_size_idx] * anchor_ratios[anchor_ratio_idx][0]\n\t\tanchor_y = anchor_sizes[anchor_size_idx] * anchor_ratios[anchor_ratio_idx][1]\n```\nTraver one bbox group, all pre boxes generated by anchors\n\noutput_width，output_height：width and height of map feature\ndownscale：mapping ration, defualt 16\nif to delete box out of iamge\n\n```python\nfor ix in range(output_width):\n\tx1_anc = downscale * (ix + 0.5) - anchor_x / 2\n\tx2_anc = downscale * (ix + 0.5) + anchor_x / 2\n\n\tif x1_anc < 0 or x2_anc > resized_width:\n\t\tcontinue\n\n\tfor jy in range(output_height):\n\t\ty1_anc = downscale * (jy + 0.5) - anchor_y / 2\n\t\ty2_anc = downscale * (jy + 0.5) + anchor_y / 2\n\n\t\tif y1_anc < 0 or y2_anc > resized_height:\n\t\t\tcontinue\n```\n<br>\n\n【注：现在我们确定了一个预选框组合有确定了中心点那就是唯一确定一个框了，接下来就是来确定这个宽的性质了：是否包含物体、如包含物体其回归梯度是多少】\n\n要确定以上两个性质，每一个框都需要遍历图中的所有bboxes 然后计算该预选框与bbox的交并比（IOU）\n如果现在的交并比curr_iou大于该bbox最好的交并比或者大于给定的阈值则求下列参数，这些参数是后来要用的即回归梯度\n\ntx：两个框中心的宽的距离与预选框宽的比\nty:同tx\ntw:bbox的宽与预选框宽的比\nth:同理\n\n```python\nif curr_iou > best_iou_for_bbox[bbox_num] or curr_iou > C.rpn_max_overlap:\n\tcx = (gta[bbox_num, 0] + gta[bbox_num, 1]) / 2.0\n\tcy = (gta[bbox_num, 2] + gta[bbox_num, 3]) / 2.0\n\tcxa = (x1_anc + x2_anc) / 2.0\n\tcya = (y1_anc + y2_anc) / 2.0\n\n\ttx = (cx - cxa) / (x2_anc - x1_anc)\n\tty = (cy - cya) / (y2_anc - y1_anc)\n\ttw = np.log((gta[bbox_num, 1] - gta[bbox_num, 0]) / (x2_anc - x1_anc))\n\tth = np.log((gta[bbox_num, 3] - gta[bbox_num, 2])) / (y2_anc - y1_anc)\n```\n对应于Faster RCNN原文，foreground anchor与ground truth之间的平移量 $(t_x, t_y)$ 如下：\n<center>![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/25_2.jpg)</center>\n对于训练bouding box regression网络回归分支，输入是cnn feature Φ，监督信号是Anchor与GT的差距 $(t_x, t_y, t_w, t_h)$，即训练目标是：输入 Φ的情况下使网络输出与监督信号尽可能接近。\n那么当bouding box regression工作时，再输入Φ时，回归网络分支的输出就是每个Anchor的平移量和变换尺度 $(t_x, t_y, t_w, t_h)$，显然即可用来修正Anchor位置了。\n\n<br>\n如果相交的不是背景，那么进行一系列更新\n\n关于bbox的相关信息更新\n预选框的相关更新：如果交并比大于阈值这是pos\nbest_iou_for_loc：其记录的是有最大交并比为多少和其对应的回归梯度\nnum_anchors_for_bbox[bbox_num]：记录的是bbox拥有的pos预选框的个数\n如果小于最小阈值是neg，在这两个之间是neutral\n需要注意的是：判断一个框为neg需要其与所有的bbox的交并比都小于最小的阈值\n\n```python\nif img_data['bboxes'][bbox_num]['class'] != 'bg':\n\n\t# all GT boxes should be mapped to an anchor box, so we keep track of which anchor box was best\n\tif curr_iou > best_iou_for_bbox[bbox_num]:\n\t\tbest_anchor_for_bbox[bbox_num] = [jy, ix, anchor_ratio_idx, anchor_size_idx]\n\t\tbest_iou_for_bbox[bbox_num] = curr_iou\n\t\tbest_x_for_bbox[bbox_num, :] = [x1_anc, x2_anc, y1_anc, y2_anc]\n\t\tbest_dx_for_bbox[bbox_num, :] = [tx, ty, tw, th]\n\n\tif curr_iou > C.rpn_max_overlap:\n\t\tbbox_type = 'pos'\n\t\tnum_anchors_for_bbox[bbox_num] += 1\n\t\tif curr_iou > best_iou_for_loc:\n\t\t\tbest_iou_for_loc = curr_iou\n\t\t\tbest_regr = (tx, ty, tw, th)\n\n\tif C.rpn_min_overlap < curr_iou < C.rpn_max_overlap:\n\n\t\tif bbox_type != 'pos':\n\t\t\tbbox_type = 'neutral'\n```\n<br>\n当结束对所有的bbox的遍历时，来确定该预选宽的性质。\n\ny_is_box_valid：该预选框是否可用（nertual就是不可用的）\ny_rpn_overlap：该预选框是否包含物体\ny_rpn_regr:回归梯度\n```python\nif bbox_type == 'neg':\n    y_is_box_valid[jy, ix, anchor_ratio_idx + n_anchratios * anchor_size_idx] = 1\n    y_rpn_overlap[jy, ix, anchor_ratio_idx + n_anchratios * anchor_size_idx] = 0\nelif bbox_type == 'neutral':\n    y_is_box_valid[jy, ix, anchor_ratio_idx + n_anchratios * anchor_size_idx] = 0\n    y_rpn_overlap[jy, ix, anchor_ratio_idx + n_anchratios * anchor_size_idx] = 0\nelse:\n    y_is_box_valid[jy, ix, anchor_ratio_idx + n_anchratios * anchor_size_idx] = 1\n    y_rpn_overlap[jy, ix, anchor_ratio_idx + n_anchratios * anchor_size_idx] = 1\n    start = 4 * (anchor_ratio_idx + n_anchratios * anchor_size_idx)\n    y_rpn_regr[jy, ix, start:start+4] = best_regr\n```\n<br>\n如果有一个bbox没有pos的预选宽和其对应，这找一个与它交并比最高的anchor的设置为pos\n```python\nfor idx in range(num_anchors_for_bbox.shape[0]):\n\tif num_anchors_for_bbox[idx] == 0:\n\t\t# no box with an IOU greater than zero ...\n\t\tif best_anchor_for_bbox[idx, 0] == -1:\n\t\t\tcontinue\n\t\ty_is_box_valid[best_anchor_for_bbox[idx,0], best_anchor_for_bbox[idx,1], best_anchor_for_bbox[idx,2] + n_anchratios *best_anchor_for_bbox[idx,3]] = 1\n\t\ty_rpn_overlap[best_anchor_for_bbox[idx,0], best_anchor_for_bbox[idx,1], best_anchor_for_bbox[idx,2] + n_anchratios *best_anchor_for_bbox[idx,3]] = 1\n\t\tstart = 4 * (best_anchor_for_bbox[idx,2] + n_anchratios * best_anchor_for_bbox[idx,3])\n\t\ty_rpn_regr[best_anchor_for_bbox[idx,0], best_anchor_for_bbox[idx,1], start:start+4] = best_dx_for_bbox[idx, :]\n```\n<br>\n将深度变到第一位，给向量增加一个维度, 在Tensorflow中， 第一纬度是batch size, 此外， 变换向量位置匹配要求\n```python\n\ty_rpn_overlap = np.transpose(y_rpn_overlap, (2, 0, 1))\n\ty_rpn_overlap = np.expand_dims(y_rpn_overlap, axis=0)\n\n\ty_is_box_valid = np.transpose(y_is_box_valid, (2, 0, 1))\n\ty_is_box_valid = np.expand_dims(y_is_box_valid, axis=0)\n\n\ty_rpn_regr = np.transpose(y_rpn_regr, (2, 0, 1))\n\ty_rpn_regr = np.expand_dims(y_rpn_regr, axis=0)\n```\n<br>\n从可用的预选框中选择num_regions\n如果pos的个数大于num_regions / 2，则将多下来的地方置为不可用。如果小于pos不做处理\n接下来将pos与neg总是超过num_regions个的neg预选框置为不可用\n最后， 256个预选框，128个positive,128个negative 会生成 在一张图片里面\n```python\npos_locs = np.where(np(y_rpn_overlap[0, :, :, :] =.logical_and= 1, y_is_box_valid[0, :, :, :] == 1))\nneg_locs = np.where(np.logical_and(y_rpn_overlap[0, :, :, :] == 0, y_is_box_valid[0, :, :, :] == 1))\nnum_regions = 256\n\nif len(pos_locs[0]) > num_regions / 2:\n\tval_locs = random.sample(range(len(pos_locs[0])), len(pos_locs[0]) - num_regions / 2)\n\ty_is_box_valid[0, pos_locs[0][val_locs], pos_locs[1][val_locs], pos_locs[2][val_locs]] = 0\n\tnum_pos = num_regions / 2\n\nif len(neg_locs[0]) + num_pos > num_regions:\n\tval_locs = random.sample(range(len(neg_locs[0])), len(neg_locs[0]) - num_pos)\n\t y_is_box_valid[0, neg_locs[0][val_locs], neg_locs[1][val_locs], neg_locs[2][val_locs]] = 0\n```\n\n<br>\n\n### 【27/06/2018】\n#### project brief\nRe organization of Project plan\n\n#### Anchors Iterative\nIntegration of privous work:\nIn each anchor: config file -> rpn_stride = 16 means generate one anchor in 16 pixels\n[Jupyter Notebook address](https://github.com/Trouble404/Object-Detection-System-Based-on-CNN-and-Capsule-Network/blob/master/Faster-RCNN/anchor_rpn.ipynb)\n\n【流程】\nFunction description\n```python\n\"\"\"\n@param all_img_data: Parsered xml file  \n@param class_count: Counting of the number of all classes objects\n@param C: Configuration class\n@param img_length_calc_function: resnet's get_img_output_length() function\n@param backend: Tensorflow in this project\n#param mode: train or val\n\nyield np.copy(x_img), [np.copy(y_rpn_cls), np.copy(y_rpn_regr)], img_data_aug\n@return: np.copy(x_img): image's matrix data\n@return: [np.copy(y_rpn_cls), np.copy(y_rpn_regr)]: calculated rpn class and radient\n@return: img_data_aug: correspoding parsed xml information\n\n\"\"\"\n\ndef get_anchor_gt(all_img_data, class_count, C, img_length_calc_function, backend, mode='train'):\n```\n<br>\n**Traverse all input image based on input xml information**\n\n* Apply class balance function: \n```python\nC.balanced_classes = True\nsample_selector = image_processing.SampleSelector(class_count)\nif C.balanced_classes and sample_selector.skip_sample_for_balanced_class(img_data):\n    continue\n```\n<br>\n\n* Apply image enhance\nif input mode is train, apply image enhance to obtain augmented image xml and matrix, if mode is val, obtain image orignal xml and matrix\n```python\nif mode == 'train':\n    img_data_aug, x_img = image_enhance.augment(img_data, C, augment=True)\nelse:\n    img_data_aug, x_img = image_enhance.augment(img_data, C, augment=False)\n```\nverifacation width and hegiht in xml and matrix\n```python\n(width, height) = (img_data_aug['width'], img_data_aug['height'])\n(rows, cols, _) = x_img.shape\nassert cols == width\nassert rows == height\n```\n<br>\n\n* Apply rezise function\n```python\n(resized_width, resized_height) = image_processing.get_new_img_size(width, height, C.im_size)\nx_img = cv2.resize(x_img, (resized_width, resized_height), interpolation=cv2.INTER_CUBIC)\n```\n<br>\n\n* Apply rpn calculation\n```python\ny_rpn_cls, y_rpn_regr = rpn_calculation.calc_rpn(C, img_data_aug, width, height, resized_width, resized_height, img_length_calc_function)\n```\n<br>\n\n* Zero-center by mean pixel, and preprocess image format\nBGR -> RGB because when apply resnet, it need RGB but in cv2, it use BGR\n```python\nx_img = x_img[:,:, (2, 1, 0)]\n```\n   For using pre-trainning model, needs to mins mean channel in each dim\n```python\nx_img = x_img.astype(np.float32)\nx_img[:, :, 0] -= C.img_channel_mean[0]\nx_img[:, :, 1] -= C.img_channel_mean[1]\nx_img[:, :, 2] -= C.img_channel_mean[2]\nx_img /= C.img_scaling_factor # default to 1,so no change here\n```\n   expand for batch size\n```python\nx_img = np.expand_dims(x_img, axis=0)\n```\n  for using pre-trainning model, need to sclaling the std to match pre trained model\n```python\ny_rpn_regr[:, y_rpn_regr.shape[1]//2:, :, :] *= C.std_scaling # scaling is 4 here\n```\n  in tensorflow, sort as batch size, width, height, deep\n```python\nif backend == 'tf':\n    x_img = np.transpose(x_img, (0, 3, 2, 1))\n    y_rpn_cls = np.transpose(y_rpn_cls, (0, 3, 2, 1))\n\ty_rpn_regr = np.transpose(y_rpn_regr, (0, 3, 2, 1))\t\t\t\t\t\t\t\t\n```\n  generator to iteror, using next() to loop\n```python\nyield np.copy(x_img), [np.copy(y_rpn_cls), np.copy(y_rpn_regr)], img_data_aug  \n```\n<br>\n【执行】\n```python\ndata_gen_train = get_anchor_gt(train_imgs, classes_count, C, nn.get_img_output_length, K.image_dim_ordering(), mode='train')\ndata_gen_val = get_anchor_gt(val_imgs, classes_count, C, nn.get_img_output_length,K.image_dim_ordering(), mode='val')\n```\nTest:\n```python\nimg,rpn,img_aug = next(data_gen_train)\n```\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/26_1.PNG)\n\n### 【28/06/2018】\n#### Resnet50 structure\n论文链接: https://arxiv.org/abs/1512.03385\n\n首先，我们要问一个问题： \n**Is learning better networks as easy as stacking more layers?**\n\n很显然不是，原因有二。 \n一，**vanishing/exploding gradients**；深度会带来恶名昭著的梯度弥散/爆炸，导致系统不能收敛。然而梯度弥散/爆炸在很大程度上被normalized initialization and intermediate normalization layers处理了。 \n二、**degradation**；当深度开始增加的时候，accuracy经常会达到饱和，然后开始下降，但这并不是由于过拟合引起的。可见figure1，56-layer的error大于20-layer的error。\n\nHe kaiMing大神认为靠堆layers竟然会导致degradation，那肯定是我们堆的方式不对。因此他提出了一种基于残差块的identity mapping，通过学习残差的方式，而非直接去学习直接的映射关系。 \n\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/01_2.jpg)\n\n事实证明，靠堆积残差块能够带来很好效果提升。而不断堆积plain layer却会带来很高的训练误差 \n残差块的两个优点：\n1) Our extremely deep residual nets are easy to optimize, but the counterpart “plain” nets (that simply stack layers) exhibit higher training error when the depth increases; \n2) Our deep residual nets can easily enjoy accuracy gains from greatly increased depth, producing results substantially better than previous networks.\n\n\n### 【29/06/2018】\n#### Resnet50 image structure\n![iamge](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/1_01.png)\nResNet有2个基本的block，一个是Identity Block，输入和输出的dimension是一样的，所以可以串联多个；另外一个基本block是Conv Block，输入和输出的dimension是不一样的，所以不能连续串联，它的作用本来就是为了改变feature vector的dimension\n\n因为CNN最后都是要把image一点点的convert成很小但是depth很深的feature map，一般的套路是用统一的比较小的kernel（比如VGG都是用3x3），但是随着网络深度的增加，output的channel也增大（学到的东西越来越复杂），所以有必要在进入Identity Block之前，用Conv Block转换一下维度，这样后面就可以连续接Identity Block.\n\n可以看下Conv Block是怎么改变输出维度的:\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/01_3.png)\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/01_4.png)\n其实就是在shortcut path的地方加上一个conv2D layer（1x1 filter size），然后在main path改变dimension，并与shortcut path对应起来.\n\n## July\n### 【02/07/2018】\n#### Construct resnet by keras\n残差网络的关键步骤，跨层的合并需要保证x和F(x)的shape是完全一样的，否则它们加不起来。\n\n理解了这一点，我们开始用keras做实现，我们把输入输出大小相同的模块称为identity_block，而把输出比输入小的模块称为conv_block，首先，导入所需的模块：\n\n```python\nfrom keras.models import Model\nfrom keras.layers import Input,Dense,BatchNormalization,Conv2D,MaxPooling2D,AveragePooling2D,ZeroPadding2D\nfrom keras.layers import add,Flatten\nfrom keras.optimizers import SGD\n```\n\n我们先来编写identity_block，这是一个函数，接受一个张量为输入，并返回一个张量, 然后是conv层，是有shortcut的：\n```python\ndef Conv2d_BN(x, nb_filter,kernel_size, strides=(1,1), padding='same',name=None):\n    if name is not None:\n        bn_name = name + '_bn'\n        conv_name = name + '_conv'\n    else:\n        bn_name = None\n        conv_name = None\n \n    x = Conv2D(nb_filter,kernel_size,padding=padding,strides=strides,activation='relu',name=conv_name)(x)\n    x = BatchNormalization(axis=3,name=bn_name)(x)\n    return x\n    \ndef Conv_Block(inpt,nb_filter,kernel_size,strides=(1,1), with_conv_shortcut=False):\n    x = Conv2d_BN(inpt,nb_filter=nb_filter[0],kernel_size=(1,1),strides=strides,padding='same')\n    x = Conv2d_BN(x, nb_filter=nb_filter[1], kernel_size=(3,3), padding='same')\n    x = Conv2d_BN(x, nb_filter=nb_filter[2], kernel_size=(1,1), padding='same')\n    if with_conv_shortcut:\n        shortcut = Conv2d_BN(inpt,nb_filter=nb_filter[2],strides=strides,kernel_size=kernel_size)\n        x = add([x,shortcut])\n        return x\n    else:\n        x = add([x,inpt])\n        return x\n```\n\n剩下的事情就很简单了，数好identity_block和conv_block是如何交错的，照着网络搭就好了：\n```python\ninpt = Input(shape=(224,224,3))\nx = ZeroPadding2D((3,3))(inpt)\nx = Conv2d_BN(x,nb_filter=64,kernel_size=(7,7),strides=(2,2),padding='valid')\nx = MaxPooling2D(pool_size=(3,3),strides=(2,2),padding='same')(x)\n \nx = Conv_Block(x,nb_filter=[64,64,256],kernel_size=(3,3),strides=(1,1),with_conv_shortcut=True)\nx = Conv_Block(x,nb_filter=[64,64,256],kernel_size=(3,3))\nx = Conv_Block(x,nb_filter=[64,64,256],kernel_size=(3,3))\n \nx = Conv_Block(x,nb_filter=[128,128,512],kernel_size=(3,3),strides=(2,2),with_conv_shortcut=True)\nx = Conv_Block(x,nb_filter=[128,128,512],kernel_size=(3,3))\nx = Conv_Block(x,nb_filter=[128,128,512],kernel_size=(3,3))\nx = Conv_Block(x,nb_filter=[128,128,512],kernel_size=(3,3))\n \nx = Conv_Block(x,nb_filter=[256,256,1024],kernel_size=(3,3),strides=(2,2),with_conv_shortcut=True)\nx = Conv_Block(x,nb_filter=[256,256,1024],kernel_size=(3,3))\nx = Conv_Block(x,nb_filter=[256,256,1024],kernel_size=(3,3))\nx = Conv_Block(x,nb_filter=[256,256,1024],kernel_size=(3,3))\nx = Conv_Block(x,nb_filter=[256,256,1024],kernel_size=(3,3))\nx = Conv_Block(x,nb_filter=[256,256,1024],kernel_size=(3,3))\n \nx = Conv_Block(x,nb_filter=[512,512,2048],kernel_size=(3,3),strides=(2,2),with_conv_shortcut=True)\nx = Conv_Block(x,nb_filter=[512,512,2048],kernel_size=(3,3))\nx = Conv_Block(x,nb_filter=[512,512,2048],kernel_size=(3,3))\nx = AveragePooling2D(pool_size=(7,7))(x)\nx = Flatten()(x)\nx = Dense(1000,activation='softmax')(x)\n\nmodel = Model(inputs=inpt,outputs=x)\nsgd = SGD(decay=0.0001,momentum=0.9)\nmodel.compile(loss='categorical_crossentropy',optimizer=sgd,metrics=['accuracy'])\nmodel.summary()\n```\n\n[jupyter notebook](https://github.com/Trouble404/Object-Detection-System-Based-on-CNN-and-Capsule-Network/blob/master/Faster-RCNN/Resnet50/resnet50_keras.ipynb)\n\n\n### 【03/07/2018】\n#### load pre-trained model of resnet50\n步骤如下：\n\n* 下载ResNet50不包含全连接层的模型参数到本地（resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5）；\n* 定义好ResNet50的网络结构；\n* 将预训练的模型参数加载到我们所定义的网络结构中；\n* 更改全连接层结构，便于对我们的分类任务进行处\n* 或者根据需要解冻最后几个block，然后以很低的学习率开始训练。我们只选择最后一个block进行训练，是因为训练样本很少，而ResNet50模型层数很多，全部训练肯定不能训练好，会过拟合。 其次fine-tune时由于是在一个已经训练好的模型上进行的，故权值更新应该是一个小范围的，以免破坏预训练好的特征。\n\n[下载地址](https://github.com/fchollet/deep-learning-models/releases)\n\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/01_7.JPG)\n\n因为使用了预训练模型，参数名称需要和预训练模型一致：\nidentity层：\n```python\ndef identity_block(X, f, filters, stage, block):\n    # defining name basis\n    conv_name_base = 'res' + str(stage) + block + '_branch'\n    bn_name_base = 'bn' + str(stage) + block + '_branch'\n    \n    # Retrieve Filters\n    F1, F2, F3 = filters\n    \n    # Save the input value. You'll need this later to add back to the main path. \n    X_shortcut = X\n    \n    # First component of main path\n    X = Conv2D(filters = F1, kernel_size = (1, 1), strides = (1,1), padding = 'valid', name = conv_name_base + '2a')(X)\n    X = BatchNormalization(axis = 3, name = bn_name_base + '2a')(X)\n    X = Activation('relu')(X)\n    \n    # Second component of main path (≈3 lines)\n    X = Conv2D(filters= F2, kernel_size=(f,f),strides=(1,1),padding='same',name=conv_name_base + '2b')(X)\n    X = BatchNormalization(axis=3,name=bn_name_base+'2b')(X)\n    X = Activation('relu')(X)\n \n    # Third component of main path (≈2 lines)\n    X = Conv2D(filters=F3,kernel_size=(1,1),strides=(1,1),padding='valid',name=conv_name_base+'2c')(X)\n    X = BatchNormalization(axis=3,name=bn_name_base+'2c')(X)\n \n    # Final step: Add shortcut value to main path, and pass it through a RELU activation (≈2 lines)\n    X = Add()([X, X_shortcut])\n    X = Activation('relu')(X)\n    return X\n```\n\nconv层：\n```python\ndef convolutional_block(X, f, filters, stage, block, s = 2):\n    \n    # defining name basis\n    conv_name_base = 'res' + str(stage) + block + '_branch'\n    bn_name_base = 'bn' + str(stage) + block + '_branch'\n    \n    # Retrieve Filters\n    F1, F2, F3 = filters\n    \n    # Save the input value\n    X_shortcut = X\n \n    ##### MAIN PATH #####\n    # First component of main path \n    X = Conv2D(F1, (1, 1), strides = (s,s),padding='valid',name = conv_name_base + '2a')(X)\n    X = BatchNormalization(axis = 3, name = bn_name_base + '2a')(X)\n    X = Activation('relu')(X)\n \n    # Second component of main path (≈3 lines)\n    X = Conv2D(F2,(f,f),strides=(1,1),padding='same',name=conv_name_base+'2b')(X)\n    X = BatchNormalization(axis=3,name=bn_name_base+'2b')(X)\n    X = Activation('relu')(X)\n \n    # Third component of main path (≈2 lines)\n    X = Conv2D(F3,(1,1),strides=(1,1),padding='valid',name=conv_name_base+'2c')(X)\n    X = BatchNormalization(axis=3,name=bn_name_base+'2c')(X)\n \n    ##### SHORTCUT PATH #### (≈2 lines)\n    X_shortcut = Conv2D(F3,(1,1),strides=(s,s),padding='valid',name=conv_name_base+'1')(X_shortcut)\n    X_shortcut = BatchNormalization(axis=3,name =bn_name_base+'1')(X_shortcut)\n \n    # Final step: Add shortcut value to main path, and pass it through a RELU activation (≈2 lines)\n    X = Add()([X,X_shortcut])\n    X = Activation('relu')(X)\n    return X\n```\n\nresnet50结构：\n```python\ndef ResNet50(input_shape = (64, 64, 3), classes = 30):\n    # Define the input as a tensor with shape input_shape\n    X_input = Input(input_shape)\n \n    # Zero-Padding\n    X = ZeroPadding2D((3, 3))(X_input)\n    \n    # Stage 1\n    X = Conv2D(64, (7, 7), strides = (2, 2), name = 'conv1')(X)\n    X = BatchNormalization(axis = 3, name = 'bn_conv1')(X)\n    X = Activation('relu')(X)\n    X = MaxPooling2D((3, 3), strides=(2, 2))(X)\n \n    # Stage 2\n    X = convolutional_block(X, f = 3, filters = [64, 64, 256], stage = 2, block='a', s = 1)\n    X = identity_block(X, 3, [64, 64, 256], stage=2, block='b')\n    X = identity_block(X, 3, [64, 64, 256], stage=2, block='c')\n \n    ### START CODE HERE ###\n \n    # Stage 3 (≈4 lines)\n    X = convolutional_block(X, f = 3,filters= [128,128,512],stage=3,block='a',s=2)\n    X = identity_block(X,3,[128,128,512],stage=3,block='b')\n    X = identity_block(X,3,[128,128,512],stage=3,block='c')\n    X = identity_block(X,3,[128,128,512],stage=3,block='d')\n \n    # Stage 4 (≈6 lines)\n    X = convolutional_block(X,f=3,filters=[256,256,1024],stage=4,block='a',s=2)\n    X = identity_block(X,3,[256,256,1024],stage=4,block='b')\n    X = identity_block(X,3,[256,256,1024],stage=4,block='c')\n    X = identity_block(X,3,[256,256,1024],stage=4,block='d')\n    X = identity_block(X,3,[256,256,1024],stage=4,block='e')\n    X = identity_block(X,3,[256,256,1024],stage=4,block='f')\n \n    # Stage 5 (≈3 lines)\n    X = convolutional_block(X, f = 3,filters= [512,512,2048],stage=5,block='a',s=2)\n    X = identity_block(X,3,[512,512,2048],stage=5,block='b')\n    X = identity_block(X,3,[512,512,2048],stage=5,block='c')\n \n    # AVGPOOL (≈1 line). Use \"X = AveragePooling2D(...)(X)\"\n    X = AveragePooling2D((2,2),strides=(2,2))(X)\n \n    # output layer\n    X = Flatten()(X)\n    model = Model(inputs = X_input, outputs = X, name='ResNet50')\n \n    return model\n```\n\n构建网络并且载入权重：\n```python\nbase_model = ResNet50(input_shape=(224,224,3),classes=30) \nbase_model.load_weights('resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5')\n```\n\n无法载入\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/01_8.JPG)\n### 【04/07/2018】\n#### Loading pre-trained model\n对于keras：如果新模型和旧模型结构一样，直接调用model.load_weights读取参数就行。如果新模型中的几层和之前模型一样，也通过model.load_weights('my_model_weights.h5', by_name=True)来读取参数， 或者手动对每一层进行参数的赋值，比如x= Dense(100, weights=oldModel.layers[1].get_weights())(x)\n\n修改代码：\n```python\ntry:\n    base_model.load_weights('resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5',by_name=True)\n    print(\"load successful\")\nexcept:\n    print(\"load failed\")\n```\n载入成功：[jupyter notebook](https://github.com/Trouble404/Object-Detection-System-Based-on-CNN-and-Capsule-Network/blob/master/Faster-RCNN/Resnet50/resnet50_pre_load.ipynb)\n\n### 【05~06/07/2018】\n#### construct faster rcnn net\n**RoiPoolingConv**\n该函数的作用是对将每一个预选框框定的特征图大小规整到相同大小\n什么是ROI呢？\nROI是Region of Interest的简写，指的是在“特征图上的框”；\n1）在Fast RCNN中， RoI是指Selective Search完成后得到的“候选框”在特征图上的映射，如下图所示；\n2）在Faster RCNN中，候选框是经过RPN产生的，然后再把各个“候选框”映射到特征图上，得到RoIs\n创建一个类，这里不同的是它是要继承keras的Layer类\n```python\nclass RoiPoolingConv(Layer):\n```\n[编写自己的层](http://keras-cn.readthedocs.io/en/latest/layers/writting_layer/)\n\n定义：\n\\*\\*[kwargs](https://www.cnblogs.com/xuyuanyuan123/p/6674645.html)：表示的就是形参中按照关键字传值把多余的传值以字典的方式呈现\n[super](https://link.zhihu.com/?target=http%3A//python.jobbole.com/86787/):子类调用父类的初始化方法\n```python\n'''ROI pooling layer for 2D inputs.\n    # Arguments\n        pool_size: int\n            Size of pooling region to use. pool_size = 7 will result in a 7x7 region.\n        num_rois: number of regions of interest to be used\n    '''\n# 第一个是规整后特征图大小 第二个是预选框个数\n    def __init__(self, pool_size, num_rois, **kwargs):\n\n        self.dim_ordering = K.image_dim_ordering()\n        # print error when kernel not tensorflow or thoean\n        assert self.dim_ordering in {'tf'}, 'dim_ordering must be in tf'\n\n        self.pool_size = pool_size\n        self.num_rois = num_rois\n\n        super(RoiPoolingConv, self).__init__(**kwargs)\n```\n\n得到特征图的输出通道个数:\n```python\ndef build(self, input_shape):\n        if self.dim_ordering == 'tf':\n            self.nb_channels = input_shape[0][3]\n```\n\n定义输出特征图的形状：\n```python\ndef compute_output_shape(self, input_shape):\n        if self.dim_ordering == 'tf':\n            return None, self.num_rois, self.pool_size, self.pool_size, self.nb_channels\n```\n\n遍历提供的所有预选框,将预选宽里的特征图规整到指定大小, 并且加入到output:\n```python\ndef call(self, x, mask=None):\n\n        assert(len(x) == 2)\n\n        img = x[0]\n        rois = x[1]\n\n        input_shape = K.shape(img)\n\n        outputs = []\n\n        for roi_idx in range(self.num_rois):\n\n            x = rois[0, roi_idx, 0]\n            y = rois[0, roi_idx, 1]\n            w = rois[0, roi_idx, 2]\n            h = rois[0, roi_idx, 3]\n            \n            row_length = w / float(self.pool_size)\n            col_length = h / float(self.pool_size)\n\n            num_pool_regions = self.pool_size\n\n            if self.dim_ordering == 'tf':\n                x = K.cast(x, 'int32')\n                y = K.cast(y, 'int32')\n                w = K.cast(w, 'int32')\n                h = K.cast(h, 'int32')\n\n                # resize porposal of feature map\n                rs = tf.image.resize_images(img[:, y:y+h, x:x+w, :], (self.pool_size, self.pool_size))\n                outputs.append(rs)\n\n        # 将outputs里面的变量按照第一个维度合在一起【shape:(?, 7, 7, 512)】\n        final_output = K.concatenate(outputs, axis=0)\n        final_output = K.reshape(final_output, (1, self.num_rois, self.pool_size, self.pool_size, self.nb_channels))\n        # 将变量规整到相应的大小【shape:(1, 32, 7, 7, 512)】\n        final_output = K.permute_dimensions(final_output, (0, 1, 2, 3, 4))\n\n        return final_output\n```\n输出是batch个vector，其中batch的值等于RoI的个数，vector的大小为channel \\* w \\* h；RoI Pooling的过程就是将一个个大小不同的box矩形框，都映射成大小固定（w \\* h）的矩形框.\n\n**TimeDistributed 包装器**\nFastRcnn在做完ROIpooling后，需要将生产的所有的Roi全部送入分类和回归网络，Keras用的TimeDistributed函数：\n\nRelu激活函数本身就是逐元素计算激活值的，无论进来多少维的tensor都一样，所以不需要使用TimeDistributed。conv2D需要TimeDistributed，是因为一个ROI内的数据计算是互相依赖的，而不同ROI之间又是独立的。\n\n在最后Faster RCNN的结构中进行类别判断和bbox框的回归时，需要对设置的num_rois个感兴趣区域进行回归处理，由于每一个区域的处理是相对独立的，便等价于此时的时间步为num_rois，因此用TimeDistributed来wrap。\n\n改编之前的conv 和 identity层：\n```python\ndef conv_block_td(input_tensor, kernel_size, filters, stage, block, input_shape, strides=(2, 2), trainable=True):\n\n    # conv block time distributed\n\n    nb_filter1, nb_filter2, nb_filter3 = filters\n\n    bn_axis = 3\n\n    conv_name_base = 'res' + str(stage) + block + '_branch'\n    bn_name_base = 'bn' + str(stage) + block + '_branch'\n\n    x = TimeDistributed(Convolution2D(nb_filter1, (1, 1), strides=strides, trainable=trainable, kernel_initializer='normal'), input_shape=input_shape, name=conv_name_base + '2a')(input_tensor)\n    x = TimeDistributed(BatchNormalization(axis=bn_axis), name=bn_name_base + '2a')(x)\n    x = Activation('relu')(x)\n\n    x = TimeDistributed(Convolution2D(nb_filter2, (kernel_size, kernel_size), padding='same', trainable=trainable, kernel_initializer='normal'), name=conv_name_base + '2b')(x)\n    x = TimeDistributed(BatchNormalization(axis=bn_axis), name=bn_name_base + '2b')(x)\n    x = Activation('relu')(x)\n\n    x = TimeDistributed(Convolution2D(nb_filter3, (1, 1), kernel_initializer='normal'), name=conv_name_base + '2c', trainable=trainable)(x)\n    x = TimeDistributed(BatchNormalization(axis=bn_axis), name=bn_name_base + '2c')(x)\n\n    shortcut = TimeDistributed(Convolution2D(nb_filter3, (1, 1), strides=strides, trainable=trainable, kernel_initializer='normal'), name=conv_name_base + '1')(input_tensor)\n    shortcut = TimeDistributed(BatchNormalization(axis=bn_axis), name=bn_name_base + '1')(shortcut)\n\n    x = Add()([x, shortcut])\n    x = Activation('relu')(x)\n    return x\n```\n\n```python\ndef identity_block_td(input_tensor, kernel_size, filters, stage, block, trainable=True):\n\n    # identity block time distributed\n\n    nb_filter1, nb_filter2, nb_filter3 = filters\n\n    bn_axis = 3\n\n    conv_name_base = 'res' + str(stage) + block + '_branch'\n    bn_name_base = 'bn' + str(stage) + block + '_branch'\n\n    x = TimeDistributed(Convolution2D(nb_filter1, (1, 1), trainable=trainable, kernel_initializer='normal'), name=conv_name_base + '2a')(input_tensor)\n    x = TimeDistributed(BatchNormalization(axis=bn_axis), name=bn_name_base + '2a')(x)\n    x = Activation('relu')(x)\n\n    x = TimeDistributed(Convolution2D(nb_filter2, (kernel_size, kernel_size), trainable=trainable, kernel_initializer='normal',padding='same'), name=conv_name_base + '2b')(x)\n    x = TimeDistributed(BatchNormalization(axis=bn_axis), name=bn_name_base + '2b')(x)\n    x = Activation('relu')(x)\n\n    x = TimeDistributed(Convolution2D(nb_filter3, (1, 1), trainable=trainable, kernel_initializer='normal'), name=conv_name_base + '2c')(x)\n    x = TimeDistributed(BatchNormalization(axis=bn_axis), name=bn_name_base + '2c')(x)\n\n    x = Add()([x, input_tensor])\n    x = Activation('relu')(x)\n\n    return x\n```\n如果将时序信号看作是2D矩阵，则TimeDistributed包装后的Dense就是分别对矩阵的每一行进行全连接。\n\n**把resnet50最后一个stage拿出来做分类层：**\n```python\ndef classifier_layers(x, input_shape, trainable=False):\n\n    # Stage 5\n    x = conv_block_td(x, 3, [512, 512, 2048], stage=5, block='a', input_shape=input_shape, strides=(2, 2), trainable=trainable)\n    x = identity_block_td(x, 3, [512, 512, 2048], stage=5, block='b', trainable=trainable)\n    x = identity_block_td(x, 3, [512, 512, 2048], stage=5, block='c', trainable=trainable)\n\n    # AVGPOOL\n    x = TimeDistributed(AveragePooling2D((7, 7)), name='avg_pool')(x)\n\n    return x\n```\n\n* RoiPoolingConv：返回的shape为(1, 32, 7, 7, 512)含义是batch_size,预选框的个数，特征图宽，特征图高度，特征图深度\n* TimeDistributed：输入至少为3D张量，下标为1的维度将被认为是时间维。即对以一个维度下的变量当作一个完整变量来看待本文是32。你要实现的目的就是对32个预选宽提出的32个图片做出判断。\n* out_class的shape:(?, 32, 21); out_regr的shape:(?, 32, 80)\n```python\ndef classifier(base_layers, input_rois, num_rois, nb_classes = 21, trainable=False):\n\n    pooling_regions = 14\n    input_shape = (num_rois,14,14,1024)\n\n    out_roi_pool = RoiPoolingConv(pooling_regions, num_rois)([base_layers, input_rois])\n    out = classifier_layers(out_roi_pool, input_shape=input_shape, trainable=True)\n\n    out = TimeDistributed(Flatten())(out)\n\n    out_class = TimeDistributed(Dense(nb_classes, activation='softmax', kernel_initializer='zero'), name='dense_class_{}'.format(nb_classes))(out)\n    # note: no regression target for bg class\n    out_regr = TimeDistributed(Dense(4 * (nb_classes-1), activation='linear', kernel_initializer='zero'), name='dense_regress_{}'.format(nb_classes))(out)\n    return [out_class, out_regr]\n```\n\n**定义RPN网络：**\n* x_class:每一个锚点属于前景还是背景【注：这里使用的是sigmoid激活函数所以其输出的通道数是num_anchors】\n* x_regr：每一个锚点对应的回归梯度\n```python\ndef rpn(base_layers,num_anchors):\n\n    x = Convolution2D(512, (3, 3), padding='same', activation='relu', kernel_initializer='normal', name='rpn_conv1')(base_layers)\n\n    x_class = Convolution2D(num_anchors, (1, 1), activation='sigmoid', kernel_initializer='uniform', name='rpn_out_class')(x)\n    x_regr = Convolution2D(num_anchors * 4, (1, 1), activation='linear', kernel_initializer='zero', name='rpn_out_regress')(x)\n\n    return [x_class, x_regr, base_layers]\n```\n\n**resnet前面部分作为公共层：**\n```python\ndef nn_base(input_tensor=None, trainable=False):\n\n    # Determine proper input shape\n\n    input_shape = (None, None, 3)\n\n    if input_tensor is None:\n        img_input = Input(shape=input_shape)\n    else:\n        if not K.is_keras_tensor(input_tensor):\n            img_input = Input(tensor=input_tensor, shape=input_shape)\n        else:\n            img_input = input_tensor\n\n    bn_axis = 3\n\n    # Zero-Padding\n    x = ZeroPadding2D((3, 3))(img_input)\n\n    # Stage 1\n    x = Convolution2D(64, (7, 7), strides=(2, 2), name='conv1', trainable = trainable)(x)\n    x = BatchNormalization(axis=bn_axis, name='bn_conv1')(x)\n    x = Activation('relu')(x)\n    x = MaxPooling2D((3, 3), strides=(2, 2))(x)\n\n    # Stage 2\n    x = conv_block(x, 3, [64, 64, 256], stage=2, block='a', strides=(1, 1), trainable = trainable)\n    x = identity_block(x, 3, [64, 64, 256], stage=2, block='b', trainable = trainable)\n    x = identity_block(x, 3, [64, 64, 256], stage=2, block='c', trainable = trainable)\n\n    # Stage 3\n    x = conv_block(x, 3, [128, 128, 512], stage=3, block='a', trainable = trainable)\n    x = identity_block(x, 3, [128, 128, 512], stage=3, block='b', trainable = trainable)\n    x = identity_block(x, 3, [128, 128, 512], stage=3, block='c', trainable = trainable)\n    x = identity_block(x, 3, [128, 128, 512], stage=3, block='d', trainable = trainable)\n\n    # Stage 4\n    x = conv_block(x, 3, [256, 256, 1024], stage=4, block='a', trainable = trainable)\n    x = identity_block(x, 3, [256, 256, 1024], stage=4, block='b', trainable = trainable)\n    x = identity_block(x, 3, [256, 256, 1024], stage=4, block='c', trainable = trainable)\n    x = identity_block(x, 3, [256, 256, 1024], stage=4, block='d', trainable = trainable)\n    x = identity_block(x, 3, [256, 256, 1024], stage=4, block='e', trainable = trainable)\n    x = identity_block(x, 3, [256, 256, 1024], stage=4, block='f', trainable = trainable)\n\n    return x\n```\n\n**搭建网络：**\n```python\n# define the base network (resnet here)\nshared_layers = nn.nn_base(img_input, trainable=True)\n\n# define the RPN, built on the base layers\n# 9 types of anchors\nnum_anchors = len(C.anchor_box_scales) * len(C.anchor_box_ratios)\nrpn = nn.rpn(shared_layers, num_anchors)\n\nclassifier = nn.classifier(shared_layers, roi_input, C.num_rois, nb_classes=len(classes_count), trainable=True)\n\nmodel_rpn = Model(img_input, rpn[:2])\nmodel_classifier = Model([img_input, roi_input], classifier)\n\n# this is a model that holds both the RPN and the classifier, used to load/save weights for the models\nmodel_all = Model([img_input, roi_input], rpn[:2] + classifier)\n```\n\n[jupyter notebook](https://github.com/Trouble404/Object-Detection-System-Based-on-CNN-and-Capsule-Network/blob/master/Faster-RCNN/Resnet50/workflow_model.ipynb)\n\n### 【09/07/2018】\n#### Loss define\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/6_1.JPG)\n\n由于涉及到分类和回归，所以需要定义一个多任务损失函数(Multi-task Loss Function)，包括Softmax Classification Loss和Bounding Box Regression Loss，公式定义如下：\n\n$L(\\{p_{i}\\},\\{t_{i}\\}) = \\frac{1}{N_{cls}}\\sum_{i}L_{cls}(p_{i},p_{i}^{\\ast}) + \\lambda\\frac{1}{N_{reg}}\\sum_{i}L_{reg}(t_{i},t_{i}^{\\ast})$\n\n**Softmax Classification：**\n对于RPN网络的分类层(cls)，其向量维数为2k = 18，考虑整个特征图conv5-3，则输出大小为W×H×18，正好对应conv5-3上每个点有9个anchors，而每个anchor又有两个score(fg/bg)输出，对于单个anchor训练样本，其实是一个二分类问题。为了便于Softmax分类，需要对分类层执行reshape操作，这也是由底层数据结构决定的。\n在上式中，$p_{i}$为样本分类的概率值，$p_{i}^{\\ast}$为样本的标定值(label)，anchor为正样本时$p_{i}^{\\ast}$为1，为负样本时$p_{i}^{\\ast}$为0，$L_{cls}$为两种类别的对数损失(log loss)。\n\n**Bounding Box Regression：**\nRPN网络的回归层输出向量的维数为4k = 36，回归参数为每个样本的坐标$[x,y,w,h]$，分别为box的中心位置和宽高，考虑三组参数预测框(predicted box)坐标$[x,y,w,h]$，anchor坐标$[x_{a},y_{a},w_{a},h_{a}]$，ground truth坐标$[x^{\\ast},y^{\\ast},w^{\\ast},h^{\\ast}]$，分别计算预测框相对anchor中心位置的偏移量以及宽高的缩放量{$t$}，ground truth相对anchor的偏移量和缩放量{$t^{\\ast}$}\n\n$t_{x}=\\frac{(x-x_{a})}{w_{a}}$ , $t_{y}=\\frac{(y-y_{a})}{h_{a}}$ , $t_{w}=log(\\frac{w}{w_{a}})$ , $t_{h}=log(\\frac{h}{h_{a}})$ (1)\n$t_{x}^{\\ast}=\\frac{(x^{\\ast}-x_{a})}{w_{a}}$ , $t_{y}^{\\ast}=\\frac{(y^{\\ast}-y_{a})}{h_{a}}$ , $t_{w}^{\\ast}=log(\\frac{w^{\\ast}}{w_{a}})$ , $t_{h}^{\\ast}=log(\\frac{h^{\\ast}}{h_{a}})$ (2)\n\n回归目标就是让{t}尽可能地接近${t^{\\ast}}$，所以回归真正预测输出的是${t}$，而训练样本的标定真值为${t^{\\ast}}$。得到预测输出${t}$后，通过上式(1)即可反推获取预测框的真实坐标。\n\n在损失函数中，回归损失采用Smooth L1函数:\n\n$$ Smooth_{L1}(x) =\\left\\{\n\\begin{aligned}\n0.5x^{2} \\ \\ |x| \\leqslant 1\\\\\n|x| - 0.5 \\ \\ otherwise \n\\end{aligned}\n\\right.\n$$\n\n$L_{reg} = Smooth_{L1}(t-t^{\\ast})$\nSmooth L1损失函数曲线如下图所示，相比于L2损失函数，L1对离群点或异常值不敏感，可控制梯度的量级使训练更易收敛。\n![iamge](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/7_1.JPG)\n\n在损失函数中，$p_{i}^{\\ast}L_{reg}$这一项表示只有目标anchor$(p_{i}^{\\ast}=1)$才有回归损失，其他anchor不参与计算。这里需要注意的是，当样本bbox和ground truth比较接近时(IoU大于某一阈值)，可以认为上式的坐标变换是一种线性变换，因此可将样本用于训练线性回归模型，否则当bbox与ground truth离得较远时，就是非线性问题，用线性回归建模显然不合理，会导致模型不work。分类层(cls)和回归层(reg)的输出分别为{p}和{t}，两项损失函数分别由$N_{cls}$和$N_{reg}$以及一个平衡权重λ归一化。\n\n### 【10/07/2018】\n#### loss code\n  generator to iteror, using next() to loop\n```python\nyield np.copy(x_img), [np.copy(y_rpn_cls), np.copy(y_rpn_regr)], img_data_aug  \n```\nRpn calculation:\n```python\nimg,rpn,img_aug = next(data_gen_train)\n```\n\n ![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/7_2.JPG)\n\n连续两个def 是装饰器，\n装饰器其实也就是一个函数，一个用来包装函数的函数，返回一个修改之后的函数对象。经常被用于有切面需求的场景，较为经典的有插入日志、\n性能测试、事务处理等。装饰器是解决这类问题的绝佳设计，有了装饰器，我们就可以抽离出大量函数中与函数功能本身无关的雷同代码并继续重用。概括的讲，装\n饰器的作用就是为已经存在的对象添加额外的功能。\n\n根据：$L$ 的 cls 部分\n$L(\\{p_{i}\\},\\{t_{i}\\}) = \\frac{1}{N_{cls}}\\sum_{i}L_{cls}(p_{i},p_{i}^{\\ast})$\n\n在上式中，$p_{i}$为样本分类的概率值，$p_{i}^{\\ast}$为样本的标定值(label)，anchor为正样本时$p_{i}^{\\ast}$为1，为负样本时$p_{i}^{\\ast}$为0，$L_{cls}$为两种类别的对数损失(log loss)。\n\n因此， 定义 rpn loss cls:\n```python\ndef rpn_loss_cls(num_anchors):\n\tdef rpn_loss_cls_fixed_num(y_true, y_pred):\n            # binary_crossentropy -> logloss\n            # epsilon to increase robustness\n\t\treturn lambda_rpn_class * K.sum(y_true[:, :, :, :num_anchors] * K.binary_crossentropy(y_pred[:, :, :, :], y_true[:, :, :, num_anchors:])) / K.sum(epsilon + y_true[:, :, :, :num_anchors])\n\treturn rpn_loss_cls_fixed_num\n```\n\n根据$L$ 的 reg 部分\n$L(\\{p_{i}\\},\\{t_{i}\\}) =  \\lambda\\frac{1}{N_{reg}}\\sum_{i}L_{reg}(t_{i},t_{i}^{\\ast})$\n在损失函数中，回归损失采用Smooth L1函数:\n\n$$ Smooth_{L1}(x) =\\left\\{\n\\begin{aligned}\n0.5x^{2} \\ \\ |x| \\leqslant 1\\\\\n|x| - 0.5 \\ \\ otherwise \n\\end{aligned}\n\\right.\n$$\n$L_{reg} = Smooth_{L1}(t-t^{\\ast})$\n\n因此， 定义 rpn loss reg:\n```python\ndef rpn_loss_regr(num_anchors):\n\tdef rpn_loss_regr_fixed_num(y_true, y_pred):\n\n\t\t# difference of ture value and predicted value\n\t\tx = y_true[:, :, :, 4 * num_anchors:] - y_pred\n\t\t# absulote value of difference\n\t\tx_abs = K.abs(x)\n\t\t# if absulote value less than 1, x_bool == 1, else x_bool = 0\n\t\tx_bool = K.cast(K.less_equal(x_abs, 1.0), tf.float32)\n\n\t\treturn lambda_rpn_regr * K.sum(y_true[:, :, :, :4 * num_anchors] * (x_bool * (0.5 * x * x) + (1 - x_bool) * (x_abs - 0.5))) / K.sum(epsilon + y_true[:, :, :, :4 * num_anchors])\n\n\treturn rpn_loss_regr_fixed_num\n```\n\n对class的loss来说用一样的方程，但是class_loss_cls是无差别求loss【这个可以用K.mean，是因为其是无差别的求loss】，不用管是否可用\n```python\ndef class_loss_regr(num_classes):\n\tdef class_loss_regr_fixed_num(y_true, y_pred):\n\t\tx = y_true[:, :, 4*num_classes:] - y_pred\n\t\tx_abs = K.abs(x)\n\t\tx_bool = K.cast(K.less_equal(x_abs, 1.0), 'float32')\n\t\treturn lambda_cls_regr * K.sum(y_true[:, :, :4*num_classes] * (x_bool * (0.5 * x * x) + (1 - x_bool) * (x_abs - 0.5))) / K.sum(epsilon + y_true[:, :, :4*num_classes])\n\treturn class_loss_regr_fixed_num\n\n\ndef class_loss_cls(y_true, y_pred):\n\treturn lambda_cls_class * K.mean(categorical_crossentropy(y_true[0, :, :], y_pred[0, :, :]))\n```\n\n### 【11/07/2018】\n#### Iridis\n#### High Performance Computing (HPC)\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/8_1.jpg)\n[Introduction](https://www.southampton.ac.uk/isolutions/staff/high-performance-computing.page)\n\nIridis 5 specifications\n* #251 in hte world(Based on July 2018 TOPP500 list) with $R_{peak}\\sim1305.6\\ TFlops/s$\n* 464 2.0 GHz nodes with 40 cores per node, 192 GB memeory\n* 10 nodes with 4xGTX1080TI GPUs, 28 cores(hyper-threaded), 128 GB memeory\n* 10 nodes with 2xVolta Tesia GPUs, same as thandard compute\n* 2.2 PB disk with paraller file system (>12GB\\s)\n* £5M Project delivered by OCF/IBM\n\n[MobaXterm](https://mobaxterm.mobatek.net/)\n\n#### create my own conda envieroment\nFllowing instroduction before\n\n#### Slurm command\n\nCommand | Definition\n---- | ---\nsbatch | Submits job scripts into system for execution (queued)\nscancel |  Cancels a job\nscontrol | Used to display Slurm state, several options only available to root\nsinfo | Display state of partitions and nodes\nsqueue | Display state of jobs\nsalloc | Submit a job for execution, or initiate job in real time\n\n** Bash script**\n```bash\n#!/bin/bash\n#SBATCH -J faster_rcnn \n#SBATCH -o train_7.out\n#SBATCH --ntasks=28\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=8\n#SBATCH --time=00:05:00\n#SBATCH --gres=gpu:1\n#SBATCH -p lyceum\n\nmodule load conda\nmodule load cuda\nsource activate project\npython test_frcnn.py\n```\n\n\n### 【12~13/07/2018】\n#### change plan\n\n因为faster r-cnn的搭建过程比想象中复杂，在咨询老师的意见以后，决定砍掉capsule的测试，专心faster-rcnn并且找到一些fine turn的方法。\n\n1）基础特征提取网络\nResNet，IncRes V2，ResNeXt 都是显著超越 VGG 的特征网络，当然网络的改进带来的是计算量的增加。\n\n2）RPN\n通过更准确地  RPN 方法，减少 Proposal 个数，提高准确度。\n\n3）改进分类回归层\n分类回归层的改进，包括 通过多层来提取特征 和 判别。\n\n---\n\n@改进1：ION\n论文：Inside outside net: Detecting objects in context with skip pooling and recurrent neural networks\n提出了两个方面的贡献：\n\n1）Inside Net\n所谓 Inside 是指在 ROI 区域之内，通过连接不同 Scale 下的 Feature Map，实现多尺度特征融合。\n这里采用的是 Skip-Pooling，从 conv3-4-5-context 分别提取特征，后面会讲到。\n多尺度特征 能够提升对小目标的检测精度。\n\n2）Outside Net\n所谓 Outside 是指 ROI 区域之外，也就是目标周围的 上下文（Contextual）信息。\n作者通过添加了两个 RNN 层（修改后的 IRNN）实现上下文特征提取。\n上下文信息 对于目标遮挡有比较好的适应。\n\n---\n\n@改进2：多尺度之 HyperNet\n论文：Hypernet: Towards accurate region proposal generation and joint object detection\n基于 Region Proposal 的方法，通过多尺度的特征提取来提高对小目标的检测能力，来看网络框图：\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/8_2.JPG)\n分为 三个主要特征 来介绍（对应上面网络拓扑图的 三个红色框）：\n\n1）Hyper Feature Extraction （特征提取）\n多尺度特征提取是本文的核心点，作者的方法稍微有所不同，他是以中间的 Feature 尺度为参考，前面的层通过 Max Pooling 到对应大小，后面的层则是通过 反卷积（Deconv）进行放大。\n多尺度 Feature ConCat 的时候，作者使用了 LRN进行归一化（类似于 ION 的 L2 Norm）。\n\n2）Region Proposal Generation（建议框生成）\n作者设计了一个轻量级的 ConvNet，与 RPN 的区别不大（为写论文强创新)。\n一个 ROI Pooling层，一个 Conv 层，还有一个 FC 层。每个 Position 通过 ROI Pooling 得到一个 13\\*13 的 bin，通过 Conv（3\\*3\\*4）层得到一个 13\\*13\\*4 的 Cube，再通过 FC 层得到一个 256d 的向量。\n后面的 Score+ BBox_Reg 与 Faster并无区别，用于目标得分 和 Location OffSet。\n考虑到建议框的 Overlap，作者用了 Greedy NMS 去重，文中将 IOU参考设为 0.7，每个 Image 保留 1k 个 Region，并选择其中 Top-200 做 Detetcion。\n通过对比，要优于基于 Edge Box 重排序的 Deep Box，从多尺度上考虑比 Deep Proposal 效果更好。\n\n3）Object Detection（目标检测）\n与 Fast RCNN基本一致，在原来的检测网络基础上做了两点改进：\na）在 FC 层之前添加了一个 卷积层（3*3*63），对特征有效降维；\nb）将 DropOut 从 0.5 降到 0.25；\n另外，与 Proposal一样采用了 NMS 进行 Box抑制，但由于之前已经做了，这一步的意义不大。\n\n---\n\n@改进3：多尺度之 MSCNN\n论文：A Unified Multi-scale Deep Convolutional Neural Network for Fast Object Detection\na）原图缩放，多个Scale的原图对应不同Scale的Feature；\n该方法计算多次Scale，每个Scale提取一次Feature，计算量巨大。\n\nb）一幅输入图像对应多个分类器；\n不需要重复提取特征图，但对分类器要求很高，一般很难得到理想的结果。\n\nc）原图缩放，少量Scale原图->少量特征图->多个Model模板；\n相当于对 a）和 b）的 Trade-Off。\n\nd）原图缩放，少量Scale原图->少量特征图->特征图插值->1个Model；\n\ne）RCNN方法，Proposal直接给到CNN；\n和 a）全图计算不同，只针对Patch计算。\n\nf）RPN方法，特征图是通过CNN卷积层得到；\n和 b）类似，不过采用的是同尺度的不同模板，容易导致尺度不一致问题。\n\ng）上套路，提出我们自己的方法，多尺度特征图；\n每个尺度特征图对应一个 输出模板，每个尺度cover一个目标尺寸范围。\n\n---\n\nNMS和soft-nms算法\nRepulsion loss：遮挡下的行人检测 加入overlapping 与不同的 loss\n融合以上两个到faster rcnn中\n\n### 【16~20/07/2018】\n旅行\n\n\n### 【23/07/2018】\n#### fix boxes location by regrident\n使用regr对anchor所确定的框进行修正\n\n```python\n\"\"\" fix boxes with grident\n\n@param X: current cordinates of box\n@param T: coresspoding grident\n\n\n@return: Fixed cordinates of box\n\"\"\"\ndef apply_regr_np(X, T):\n\ttry:\n\t\tx = X[0, :, :]\n\t\ty = X[1, :, :]\n\t\tw = X[2, :, :]\n\t\th = X[3, :, :]\n\n\t\ttx = T[0, :, :]\n\t\tty = T[1, :, :]\n\t\ttw = T[2, :, :]\n\t\tth = T[3, :, :]\n```\n$t_{x}=\\frac{(x-x_{a})}{w_{a}}$ , $t_{y}=\\frac{(y-y_{a})}{h_{a}}$ , $t_{w}=log(\\frac{w}{w_{a}})$ , $t_{h}=log(\\frac{h}{h_{a}})$ (1)\n$t_{x}^{\\ast}=\\frac{(x^{\\ast}-x_{a})}{w_{a}}$ , $t_{y}^{\\ast}=\\frac{(y^{\\ast}-y_{a})}{h_{a}}$ , $t_{w}^{\\ast}=log(\\frac{w^{\\ast}}{w_{a}})$ , $t_{h}^{\\ast}=log(\\frac{h^{\\ast}}{h_{a}})$ (2)\n\n回归目标就是让{t}尽可能地接近${t^{\\ast}}$，所以回归真正预测输出的是${t}$，而训练样本的标定真值为${t^{\\ast}}$。得到预测输出${t}$后，通过上式(1)即可反推获取预测框的真实坐标。\n\n过程：\n```python\n\t\t# centre cordinate\n\t\tcx = x + w/2.\n\t\tcy = y + h/2.\n\t\t# fixed centre cordinate\n\t\tcx1 = tx * w + cx\n\t\tcy1 = ty * h + cy\n\n\t\t# fixed wdith and height\n\t\tw1 = np.exp(tw.astype(np.float64)) * w\n\t\th1 = np.exp(th.astype(np.float64)) * h\n\n\t\t# fixed left top corner's cordinate\n\t\tx1 = cx1 - w1/2.\n\t\ty1 = cy1 - h1/2.\n\n\t\t# apporximate\n\t\tx1 = np.round(x1)\n\t\ty1 = np.round(y1)\n\t\tw1 = np.round(w1)\n\t\th1 = np.round(h1)\n\t\treturn np.stack([x1, y1, w1, h1])\n\texcept Exception as e:\n\t\tprint(e)\n\t\treturn X\n```\n\n\n#### NMS no max suppression\n该函数的作用是从所给定的所有预选框中选择指定个数最合理的边框。\n\n定义：\n```python\n\"\"\" NMS , delete overlapping box\n\n@param boxes: (n,4) box and coresspoding cordinates\n@param probs: (n,) box adn coresspding possibility\n@param overlap_thresh: treshold of delet box overlapping\n@param max_boxes: maximum keeping number of boxes\n\n\n@return: boxes: boxes cordinates(x1,y1,x2,y2)\n@return: probs: coresspoding possibility\n\"\"\"\ndef non_max_suppression_fast(boxes, probs, overlap_thresh=0.9, max_boxes=300):\n```\n\n```python\nif len(boxes) == 0:\n   return []\n\n# grab the coordinates of the bounding boxes\nx1 = boxes[:, 0]\ny1 = boxes[:, 1]\nx2 = boxes[:, 2]\ny2 = boxes[:, 3]\n\nnp.testing.assert_array_less(x1, x2)\nnp.testing.assert_array_less(y1, y2)\n\n# if the bounding boxes integers, convert them to floats --\n# this is important since we'll be doing a bunch of divisions\nif boxes.dtype.kind == \"i\":\n\tboxes = boxes.astype(\"float\")\n```\n对输入的数据进行确认\n不能为空\n左上角的坐标小于右下角\n数据类型的转换\n```python\n# initialize the list of picked indexes\t\npick = []\n\n# calculate the areas\narea = (x2 - x1) * (y2 - y1)\n\n# sort the bounding boxes \nidxs = np.argsort(probs)\n```\npick（拾取）用来存放边框序号\n计算框的面积\nprobs按照概率从小到大排序\n```python\nwhile len(idxs) > 0:\n# grab the last index in the indexes list and add the\n# index value to the list of picked indexes\nlast = len(idxs) - 1\ni = idxs[last]\npick.append(i)\n```\n接下来就是按照概率从大到小取出框，且框的重合度不可以高于overlap_thresh。代码的思路是这样的：\n\n每一次取概率最大的框（即idxs最后一个）\n删除掉剩下的框中重和度高于overlap_thresh的框\n直到取满max_boxes为止\n```python\n# find the intersection\n\nxx1_int = np.maximum(x1[i], x1[idxs[:last]])\nyy1_int = np.maximum(y1[i], y1[idxs[:last]])\nxx2_int = np.minimum(x2[i], x2[idxs[:last]])\nyy2_int = np.minimum(y2[i], y2[idxs[:last]])\n\nww_int = np.maximum(0, xx2_int - xx1_int)\nhh_int = np.maximum(0, yy2_int - yy1_int)\n\narea_int = ww_int * hh_int\n```\n取出idxs队列中最大概率框的序号，将其添加到pick中\n```python\n# find the union\narea_union = area[i] + area[idxs[:last]] - area_int\n\n# compute the ratio of overlap\noverlap = area_int/(area_union + 1e-6)\n\n# delete all indexes from the index list that have\nidxs = np.delete(idxs, np.concatenate(([last],np.where(overlap > overlap_thresh)[0])))\n```\n计算取出来的框与剩下来的框区域的交集\n```python\nif len(pick) >= max_boxes:\n   break\n```\n计算重叠率，然后删除掉重叠率较高的位置\\[np.concatest\\]，是因为最后一个位置你已经用过了，就得将其从队列中删掉\n当取足max_boxes框，停止循环\n```python\nboxes = boxes[pick].astype(\"int\")\nprobs = probs[pick]\nreturn boxes, probs\n```\n返回pick内存取的边框和对应的概率\n\n### 【24/07/2018】\n#### rpn to porposal fixed\n该函数的作用是将rpn网络的预测结果转化到一个个预选框\n函数流程：\n遍历anchor_size，在遍历anchor_ratio\n\n得到框的长宽在原图上的映射\n\n得到相应尺寸的框对应的回归梯度，将深度都放到第一个维度\n注1：regr_layer\\[0, :, :, 4 \\* curr_layer:4 \\* curr_layer + 4]当某一个维度的取值为一个值时，那么新的变量就会减小一个维度\n注2：curr_layer代表的是特定长度和比例的框所代表的编号\n\n得到anchor对应的（x,y,w,h）\n\n使用regr对anchor所确定的框进行修正\n\n对修正后的边框一些不合理的地方进行矫正。\n如，边框回归后的左上角和右下角的点不能超过图片外，框的宽高不可以小于0\n注：得到框的形式是（x1,y1,x2,y2）\n\n得到all_boxes形状是（n,4），和每一个框对应的概率all_probs形状是（n,）\n\n删除掉一些不合理的点，即右下角的点值要小于左上角的点值\n注：np.where() 返回位置信息，这也是删除不符合要求点的一种方法\nnp.delete(all_boxes, idxs, 0)最后一个参数是在哪一个维度删除\n\n最后是根据要求选取指定个数的合理预选框。这一步是重要的，因为每一个点可以有9个预选框，而又拥有很多点，一张图片可能会有几万个预选框。而经过这一步预选迅速下降到几百个。\n```python\n\"\"\" rpn to porposal\n\n@param rpn_layer: porposal's coresspoding possibiliy, whether item exciseted\n@param regr_layer: porposal's coresspoding regrident\n@param C: Configuration\n@param dim_ordering: Dimensional organization\n@param use_regr=True: wether use regurident to fix proposal\n@param max_boxes=300: max boxes after apply this function\n@param overlap_thresh=0.9: threshold of overlapping\n@param C: Configuration\n\n@return: max_boxes proposal with format (x1,y1,x2,y2)\n\"\"\"\n\ndef rpn_to_roi(rpn_layer, regr_layer, C, dim_ordering, use_regr=True, max_boxes=300,overlap_thresh=0.9):\n\t# std_scaling default 4\n\tregr_layer = regr_layer / C.std_scaling\n\n\tanchor_sizes = C.anchor_box_scales\n\tanchor_ratios = C.anchor_box_ratios\n\n\tassert rpn_layer.shape[0] == 1\n\n\t# obtain img's width and height's matrix\n\t(rows, cols) = rpn_layer.shape[1:3]\n\n\tcurr_layer = 0\n\n\tA = np.zeros((4, rpn_layer.shape[1], rpn_layer.shape[2], rpn_layer.shape[3]))\n\n\t# anchor size is [128, 256, 512]\n\tfor anchor_size in anchor_sizes:\n\t\t# anchor ratio is [1,2,1]\n\t\tfor anchor_ratio in anchor_ratios:\n\t\t\t# rpn_stride = 16\n\t\t\t# obatin anchor's weidth and height on feature map\n\t\t\tanchor_x = (anchor_size * anchor_ratio[0])/C.rpn_stride\n\t\t\tanchor_y = (anchor_size * anchor_ratio[1])/C.rpn_stride\n\n\t\t\t# obtain current regrident\n\t\t\t# when one dimentional obtain a value, the new varirant will decrease one dimenttion\n\t\t\tregr = regr_layer[0, :, :, 4 * curr_layer:4 * curr_layer + 4]\n\t\t\t# put depth to first bacause tensorflow as backend\n\t\t\tregr = np.transpose(regr, (2, 0, 1))\n\n\t\t\t# The rows of the output array X are copies of the vector x; columns of the output array Y are copies of the vector y\n\t\t\t# each cordinartes of matrix cls and rows\n\t\t\tX, Y = np.meshgrid(np.arange(cols),np. arange(rows))\n\n\t\t\t# obtain anchors's (x,y,w,h)\n\t\t\tA[0, :, :, curr_layer] = X - anchor_x/2\n\t\t\tA[1, :, :, curr_layer] = Y - anchor_y/2\n\t\t\tA[2, :, :, curr_layer] = anchor_x\n\t\t\tA[3, :, :, curr_layer] = anchor_y\n\n\t\t\t# fix boxes with grident\n\t\t\tif use_regr:\n\t\t\t\t# fixed corinates of box\n\t\t\t\tA[:, :, :, curr_layer] = apply_regr_np(A[:, :, :, curr_layer], regr)\n\n\t\t\t# fix unreasonable cordinates\n\t\t\t# np.maximum(1,[]) will set the value less than 1 in [] to 1\n\t\t\t# box's width and height can't less than 0\n\t\t\tA[2, :, :, curr_layer] = np.maximum(1, A[2, :, :, curr_layer])\n\t\t\tA[3, :, :, curr_layer] = np.maximum(1, A[3, :, :, curr_layer])\n\t\t\t# fixed right bottom cordinates\n\t\t\tA[2, :, :, curr_layer] += A[0, :, :, curr_layer]\n\t\t\tA[3, :, :, curr_layer] += A[1, :, :, curr_layer]\n\n\t\t\t# left top corner cordinates can't out image\n\t\t\tA[0, :, :, curr_layer] = np.maximum(0, A[0, :, :, curr_layer])\n\t\t\tA[1, :, :, curr_layer] = np.maximum(0, A[1, :, :, curr_layer])\n\t\t\t# right bottom corner cordinates can't out img\n\t\t\tA[2, :, :, curr_layer] = np.minimum(cols-1, A[2, :, :, curr_layer])\n\t\t\tA[3, :, :, curr_layer] = np.minimum(rows-1, A[3, :, :, curr_layer])\n\n\t\t\t# next layer\n\t\t\tcurr_layer += 1\n\n\t# obtain (n,4) object and coresspoding cordinate\n\tall_boxes = np.reshape(A.transpose((0, 3, 1,2)), (4, -1)).transpose((1, 0))\n\t# obtain(n,) object and creoespdoing possibility\n\tall_probs = rpn_layer.transpose((0, 3, 1, 2)).reshape((-1))\n\n\t# cordinates of left top and right bottom of box\n\tx1 = all_boxes[:, 0]\n\ty1 = all_boxes[:, 1]\n\tx2 = all_boxes[:, 2]\n\ty2 = all_boxes[:, 3]\n\n\t# find where right cordinate bigger than left cordinate\n\tidxs = np.where((x1 - x2 >= 0) | (y1 - y2 >= 0))\n\t# delete thoese point at 0 dimentional -> all boxes\n\tall_boxes = np.delete(all_boxes, idxs, 0)\n\tall_probs = np.delete(all_probs, idxs, 0)\n\n\t# apply NMS to reduce overlapping boxes\n\tresult = non_max_suppression_fast(all_boxes, all_probs, overlap_thresh=overlap_thresh, max_boxes=max_boxes)[0]\n\n\treturn result\n```\n\n### 【25/07/2018】\n#### generate classifier's trainning data\n该函数的作用是生成classifier网络训练的数据,需要注意的是它对提供的预选框还会做一次选择就是将容易判断的背景删除\n\n代码流程：\n得到图片的基本信息，并将图片的最短边规整到相应的长度。并将bboxes的长度做相应的变化\n\n遍历所有的预选框R, 将每一个预选框与所有的bboxes求交并比，记录最大交并比。用来确定该预选框的类别。\n\n对最佳的交并比作不同的判断:\n当最佳交并比小于最小的阈值时，放弃概框。因为，交并比太低就说明是很好判断的背景没必要训练。当大于最小阈值时，则保留相关的边框信息\n当在最小和最大之间，就认为是背景。有必要进行训练。\n大于最大阈值时认为是物体，计算其边框回归梯度\n\n得到该类别对应的数字\n将该数字对应的地方置为1【one-hot】\n将该类别加入到y_class_num\ncoords是用来存储边框回归梯度的，labels来决定是否要加入计算loss中\n```python\nclass_num = 2\nclass_label = 10 * [0]\nprint(class_label)\nclass_label[class_num] = 1\nprint(class_label)\n输出：\n[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n[0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n```\n如果不是背景的话，计算相应的回归梯度\n\n返回数据\n\n```python\n\"\"\" generate classifier training data\n\n@param R: porposal -> boxes\n@param img_data: image data\n@param C: configuration\n@param class_mapping: classes and coresspoding numbers\n\n@return: np.expand_dims(X, axis=0): boxes after filter\n@return: np.expand_dims(Y1, axis=0): boxes coresspoding class\n@return: np.expand_dims(Y2, axis=0): boxes coresspoding regurident\n@return IoUs: IOU\n\n\"\"\"\ndef calc_iou(R, img_data, C, class_mapping):\n\n\t# obtain boxxes information from img data\n\tbboxes = img_data['bboxes']\n\t# obtain width and height of img\n\t(width, height) = (img_data['width'], img_data['height'])\n\t# get image dimensions for resizing\n\t# Fix image's shortest edge to config setting: eg: 600\n\t(resized_width, resized_height) = get_new_img_size(width, height, C.im_size)\n\n\t# record parameters, bboxes cordinates on feature map\n\tgta = np.zeros((len(bboxes), 4))\n\n\t# change bboxes's width and height because the img was rezised\n\tfor bbox_num, bbox in enumerate(bboxes):\n\t\t# get the GT box coordinates, and resize to account for image resizing\n\t\t# /C.rpn_stride mapping to feature map\n\t\tgta[bbox_num, 0] = int(round(bbox['x1'] * (resized_width / float(width))/C.rpn_stride))\n\t\tgta[bbox_num, 1] = int(round(bbox['x2'] * (resized_width / float(width))/C.rpn_stride))\n\t\tgta[bbox_num, 2] = int(round(bbox['y1'] * (resized_height / float(height))/C.rpn_stride))\n\t\tgta[bbox_num, 3] = int(round(bbox['y2'] * (resized_height / float(height))/C.rpn_stride))\n\n\tx_roi = []\n\ty_class_num = []\n\ty_class_regr_coords = []\n\ty_class_regr_label = []\n\tIoUs = [] # for debugging only\n\n\t# for all given proposals -> boxes\n\tfor ix in range(R.shape[0]):\n\t\t# current boxes's cordinates\n\t\t(x1, y1, x2, y2) = R[ix, :]\n\t\tx1 = int(round(x1))\n\t\ty1 = int(round(y1))\n\t\tx2 = int(round(x2))\n\t\ty2 = int(round(y2))\n\n\t\tbest_iou = 0.0\n\t\tbest_bbox = -1\n\t\t# using current proposal to compare with given xml's boxes\n\t\tfor bbox_num in range(len(bboxes)):\n\t\t\t# calculate current iou\n\t\t\tcurr_iou = iou([gta[bbox_num, 0], gta[bbox_num, 2], gta[bbox_num, 1], gta[bbox_num, 3]], [x1, y1, x2, y2])\n\t\t\t# update parameters\n\t\t\tif curr_iou > best_iou:\n\t\t\t\tbest_iou = curr_iou\n\t\t\t\tbest_bbox = bbox_num\n\n\t\t# if iou to small, we don't put it in trainning because it should be backgroud\n\t\tif best_iou < C.classifier_min_overlap:\n\t\t\t\tcontinue\n\t\telse:\n\t\t\t# saveing left top cordinates, width and height\n\t\t\tw = x2 - x1\n\t\t\th = y2 - y1\n\t\t\tx_roi.append([x1, y1, w, h])\n\t\t\t# saving this bbox's iou\n\t\t\tIoUs.append(best_iou)\n\n\t\t\t# hard to classfier -> set it to backgroud\n\t\t\tif C.classifier_min_overlap <= best_iou < C.classifier_max_overlap:\n\t\t\t\t# hard negative example\n\t\t\t\tcls_name = 'bg'\n\n\t\t\t# valid proposal\n\t\t\telif C.classifier_max_overlap <= best_iou:\n\t\t\t\t# coresspoding class name\n\t\t\t\tcls_name = bboxes[best_bbox]['class']\n\n\t\t\t\t# calculate rpn graident with true cordinates given by xml file\n\t\t\t\tcxg = (gta[best_bbox, 0] + gta[best_bbox, 1]) / 2.0\n\t\t\t\tcyg = (gta[best_bbox, 2] + gta[best_bbox, 3]) / 2.0\n\n\t\t\t\tcx = x1 + w / 2.0\n\t\t\t\tcy = y1 + h / 2.0\n\n\t\t\t\ttx = (cxg - cx) / float(w)\n\t\t\t\tty = (cyg - cy) / float(h)\n\t\t\t\ttw = np.log((gta[best_bbox, 1] - gta[best_bbox, 0]) / float(w))\n\t\t\t\tth = np.log((gta[best_bbox, 3] - gta[best_bbox, 2]) / float(h))\n\t\t\telse:\n\t\t\t\tprint('roi = {}'.format(best_iou))\n\t\t\t\traise RuntimeError\n\n\t\t# class name's mapping number\n\t\tclass_num = class_mapping[cls_name]\n\t\t# list of calss label\n\t\tclass_label = len(class_mapping) * [0]\n\t\t# set class_num's coresspoding location to 1\n\t\tclass_label[class_num] = 1\n\t\t# privous is one-hot vector\n\n\t\t# saving the one-hot vector\n\t\ty_class_num.append(copy.deepcopy(class_label))\n\n\t\t# coords used to saving calculated graident\n\t\tcoords = [0] * 4 * (len(class_mapping) - 1)\n\t\t# labels used to decide whether adding to loss calculation\n\t\tlabels = [0] * 4 * (len(class_mapping) - 1)\n\t\tif cls_name != 'bg':\n\t\t\tlabel_pos = 4 * class_num\n\t\t\tsx, sy, sw, sh = C.classifier_regr_std\n\t\t\tcoords[label_pos:4+label_pos] = [sx*tx, sy*ty, sw*tw, sh*th]\n\t\t\tlabels[label_pos:4+label_pos] = [1, 1, 1, 1]\n\t\t\ty_class_regr_coords.append(copy.deepcopy(coords))\n\t\t\ty_class_regr_label.append(copy.deepcopy(labels))\n\t\telse:\n\t\t\ty_class_regr_coords.append(copy.deepcopy(coords))\n\t\t\ty_class_regr_label.append(copy.deepcopy(labels))\n\n\t# no bboxes\n\tif len(x_roi) == 0:\n\t\treturn None, None, None, None\n\n\t# matrix with [x1, y1, w, h]\n\tX = np.array(x_roi)\n\t# boxxes coresspoding class number\n\tY1 = np.array(y_class_num)\n\t# matrix of whether adding to calculation and coresspoding regrident\n\tY2 = np.concatenate([np.array(y_class_regr_label),np.array(y_class_regr_coords)],axis=1)\n\n\t# adding batch size dimention\n\treturn np.expand_dims(X, axis=0), np.expand_dims(Y1, axis=0), np.expand_dims(Y2, axis=0), IoUs\n```\n\n### 【26~27/07/2018】\n#### model parameters\n\n```python\n# rpn optimizer\noptimizer = Adam(lr=1e-5)\n# classifier optimizer\noptimizer_classifier = Adam(lr=1e-5)\n# defined loss apply, metrics used to print accury\nmodel_rpn.compile(optimizer=optimizer, loss=[losses.rpn_loss_cls(num_anchors), losses.rpn_loss_regr(num_anchors)])\nmodel_classifier.compile(optimizer=optimizer_classifier, loss=[losses.class_loss_cls, losses.class_loss_regr(len(classes_count)-1)], metrics={'dense_class_{}'.format(len(classes_count)): 'accuracy'})\n# for saving weight\nmodel_all.compile(optimizer='sgd', loss='mae')\n\n# traing time of each epochs\nepoch_length = 1000\n# totoal epochs\nnum_epochs = 2000\n#\niter_num = 0\n# losses saving matrix\nlosses = np.zeros((epoch_length, 5))\nrpn_accuracy_rpn_monitor = []\nrpn_accuracy_for_epoch = []\nstart_time = time.time()\n# current total loss\nbest_loss = np.Inf\n# sorted classing mapping\nclass_mapping_inv = {v: k for k, v in class_mapping.items()}\n```\n\n#### Training process\n函数流程：\n**训练rpn网络并且进行预测：**\n训练RPN网络,X是图片、Y是对应类别和回归梯度【注：并不是所有的点都参与训练，只有符合条件的点才参与训练】\n\n**根据rpn网络的预测结果得到classifier网络的训练数据:**\n将预测结果转化为预选框\n计算宽属于哪一类，回归梯度是多少\n如果没有有效的预选框则结束本次循环\n得到正负样本在的位置【Y1\\[0, :, -1\\]：0指定batch的位置，：指所有框，-1指最后一个维度即背景类】\nneg_samples = neg_samples\\[0\\]：这样做的原因是将其变为一维的数组\n下面这一步是选择C.num_rois个数的框，送入classifier网络进行训练。思路是：当C.num_rois大于1的时候正负样本尽量取到各一半，小于1的时候正负样本随机取一个。需要注意的是我们这是拿到的是正负样本在的位置而不是正负样本本身，这也是随机抽取的一般方法\n\n**训练classifier网络:**\n打印Loss和accury\n如果网络有两个不同的输出，那么第一个是和损失接下来是分损失【loss_class\\[3\\]：代表是准确率在定义网络的时候定义了】\n```python\nclassifer网络的loss输出：\n[1.4640709, 1.0986123, 0.36545864, 0.15625]\n```\n还有就是这些loss都是list数据类型，所以要把它倒腾到numpy数据中\n当结束一轮的epoch时，只有当这轮epoch的loss小于最优的时候才会存储这轮的训练数据。并结束这轮epoch进入下一轮epoch.\n\n---\n\n```python\n# Training Process\nprint('Starting training')\n\nfor epoch_num in range(num_epochs):\n    #progbar is used to print % of processing\n\tprogbar = generic_utils.Progbar(epoch_length)\n    # print current process\n\tprint('Epoch {}/{}'.format(epoch_num + 1, num_epochs))\n\n\twhile True:\n\t\ttry:\n\n            # verbose True to print RPN situation, if can't generate boxes on positivate object, it will print error\n\t\t\tif len(rpn_accuracy_rpn_monitor) == epoch_length and C.verbose:\n                # postivate boxes / all boxes\n\t\t\t\tmean_overlapping_bboxes = float(sum(rpn_accuracy_rpn_monitor))/len(rpn_accuracy_rpn_monitor)\n\t\t\t\trpn_accuracy_rpn_monitor = []\n\t\t\t\tprint('Average number of overlapping bounding boxes from RPN = {} for {} previous iterations'.format(mean_overlapping_bboxes, epoch_length))\n\t\t\t\tif mean_overlapping_bboxes == 0:\n\t\t\t\t\tprint('RPN is not producing bounding boxes that overlap the ground truth boxes. Check RPN settings or keep training.')\n\n            # obtain img, rpn information and img xml format\n\t\t\tX, Y, img_data = next(data_gen_train)\n\n            # train RPN net, X is img, Y is correspoding class type and graident\n\t\t\tloss_rpn = model_rpn.train_on_batch(X, Y)\n\n            # predict new Y from privious rpn model\n\t\t\tP_rpn = model_rpn.predict_on_batch(X)\n\n            # transform predicted rpn to cordinates of boxes\n\t\t\tR = rpn_to_boxes.rpn_to_roi(P_rpn[0], P_rpn[1], C, K.image_dim_ordering(), use_regr=True, overlap_thresh=0.7, max_boxes=300)\n\t\t\t# note: calc_iou converts from (x1,y1,x2,y2) to (x,y,w,h) format\n            # X2: [x,y,w,h]\n            # Y1: coresspoding class number -> one hot vector\n            # Y2: boxes coresspoding regrident\n\t\t\tX2, Y1, Y2, IouS = rpn_to_classifier.calc_iou(R, img_data, C, class_mapping)\n\n            # no box, stop this epoch\n\t\t\tif X2 is None:\n\t\t\t\trpn_accuracy_rpn_monitor.append(0)\n\t\t\t\trpn_accuracy_for_epoch.append(0)\n\t\t\t\tcontinue\n\n            # if last position of one-hot is 1 -> is background\n\t\t\tneg_samples = np.where(Y1[0, :, -1] == 1)\n            # else is postivate sample\n\t\t\tpos_samples = np.where(Y1[0, :, -1] == 0)\n\n            # obtain backgourd samples's coresspoding rows\n\t\t\tif len(neg_samples) > 0:\n\t\t\t\tneg_samples = neg_samples[0]\n\t\t\telse:\n\t\t\t\tneg_samples = []\n\n            # obtain posivate samples's coresspoding rows\n\t\t\tif len(pos_samples) > 0:\n\t\t\t\tpos_samples = pos_samples[0]\n\t\t\telse:\n\t\t\t\tpos_samples = []\n\t\t\t# saving posivate samples's number\n\t\t\trpn_accuracy_rpn_monitor.append(len(pos_samples))\n\t\t\trpn_accuracy_for_epoch.append((len(pos_samples)))\n\n            # default 4 here\n\t\t\tif C.num_rois > 1:\n                # wehn postivate samples less than 2\n\t\t\t\tif len(pos_samples) < C.num_rois//2:\n                    # chosse all samples\n\t\t\t\t\tselected_pos_samples = pos_samples.tolist()\n\t\t\t\telse:\n                    # random choose 2 samples\n\t\t\t\t\tselected_pos_samples = np.random.choice(pos_samples, C.num_rois//2, replace=False).tolist()\n\t\t\t\ttry:\n                    # random choose num_rois - positave samples naegivate samples\n\t\t\t\t\tselected_neg_samples = np.random.choice(neg_samples, C.num_rois - len(selected_pos_samples), replace=False).tolist()\n\t\t\t\texcept:\n                    # if no enought neg samples, copy priouvs neg sample\n\t\t\t\t\tselected_neg_samples = np.random.choice(neg_samples, C.num_rois - len(selected_pos_samples), replace=True).tolist()\n\n                # samples picked to classifier network\n\t\t\t\tsel_samples = selected_pos_samples + selected_neg_samples\n\t\t\telse:\n\t\t\t\t# in the extreme case where num_rois = 1, we pick a random pos or neg sample\n\t\t\t\tselected_pos_samples = pos_samples.tolist()\n\t\t\t\tselected_neg_samples = neg_samples.tolist()\n\t\t\t\tif np.random.randint(0, 2):\n\t\t\t\t\tsel_samples = random.choice(neg_samples)\n\t\t\t\telse:\n\t\t\t\t\tsel_samples = random.choice(pos_samples)\n\n            # train classifier, img data, selceted samples' cordinates, mapping number of selected samples, coresspoding regreident\n\t\t\tloss_class = model_classifier.train_on_batch([X, X2[:, sel_samples, :]], [Y1[:, sel_samples, :], Y2[:, sel_samples, :]])\n\n            # in Keras, if loss part bigger than 1, it will return sum of part losses, each loss and accury\n            # put each losses and accury into losses\n\t\t\tlosses[iter_num, 0] = loss_rpn[1]\n\t\t\tlosses[iter_num, 1] = loss_rpn[2]\n\n\t\t\tlosses[iter_num, 2] = loss_class[1]\n\t\t\tlosses[iter_num, 3] = loss_class[2]\n\t\t\tlosses[iter_num, 4] = loss_class[3]\n\n            # next iter\n\t\t\titer_num += 1\n\n            # display and update current mean value of losses\n\t\t\tprogbar.update(iter_num, [('rpn_cls', np.mean(losses[:iter_num, 0])), ('rpn_regr', np.mean(losses[:iter_num, 1])),\n\t\t\t\t\t\t\t\t\t  ('detector_cls', np.mean(losses[:iter_num, 2])), ('detector_regr', np.mean(losses[:iter_num, 3]))])\n\n            # reach epoch_length\n\t\t\tif iter_num == epoch_length:\n\t\t\t\tloss_rpn_cls = np.mean(losses[:, 0])\n\t\t\t\tloss_rpn_regr = np.mean(losses[:, 1])\n\t\t\t\tloss_class_cls = np.mean(losses[:, 2])\n\t\t\t\tloss_class_regr = np.mean(losses[:, 3])\n\t\t\t\tclass_acc = np.mean(losses[:, 4])\n\n                # negativate samples / all samples\n\t\t\t\tmean_overlapping_bboxes = float(sum(rpn_accuracy_for_epoch)) / len(rpn_accuracy_for_epoch)\n                # reset\n\t\t\t\trpn_accuracy_for_epoch = []\n\n                # print trainning loss and accrury\n\t\t\t\tif C.verbose:\n\t\t\t\t\tprint('Mean number of bounding boxes from RPN overlapping ground truth boxes: {}'.format(mean_overlapping_bboxes))\n\t\t\t\t\tprint('Classifier accuracy for bounding boxes from RPN: {}'.format(class_acc))\n\t\t\t\t\tprint('Loss RPN classifier: {}'.format(loss_rpn_cls))\n\t\t\t\t\tprint('Loss RPN regression: {}'.format(loss_rpn_regr))\n\t\t\t\t\tprint('Loss Detector classifier: {}'.format(loss_class_cls))\n\t\t\t\t\tprint('Loss Detector regression: {}'.format(loss_class_regr))\n                    # trainng time of one epoch\n\t\t\t\t\tprint('Elapsed time: {}'.format(time.time() - start_time))\n                    \n                # total loss\n\t\t\t\tcurr_loss = loss_rpn_cls + loss_rpn_regr + loss_class_cls + loss_class_regr\n                # reset\n\t\t\t\titer_num = 0\n                # reset time\n\t\t\t\tstart_time = time.time()\n\n                # if obtain smaller total loss, save weight of current model\n\t\t\t\tif curr_loss < best_loss:\n\t\t\t\t\tif C.verbose:\n\t\t\t\t\t\tprint('Total loss decreased from {} to {}, saving weights'.format(best_loss,curr_loss))\n\t\t\t\t\tbest_loss = curr_loss\n\t\t\t\t\tmodel_all.save_weights(C.model_path)\n\n\t\t\t\tbreak\n\n\t\texcept Exception as e:\n\t\t\tprint('Exception: {}'.format(e))\n\t\t\tcontinue\n\nprint('Training complete, exiting.')\n```\n[jupyter notebook](https://github.com/Trouble404/Object-Detection-System-Based-on-CNN-and-Capsule-Network/blob/master/Faster-RCNN/workflow_train.ipynb)\n\n### 【30/07/2018】\n#### Running at GPU enviorment\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/9_1.JPG)\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/9_2.JPG)\nMeet error in GPU version tensorflow\nNo enough memory.\n\nTry to Running at Irius:\n\nSetting 3 differnet configration:\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/9_3.JPG)\nat Prjoect1 file:\nset epoch_length to number of training img\n```python\nepoch_length = 11540\nnum_epochs = 100\n```\nApply img enhance function\n```python\nC.use_horizontal_flips = True\nC.use_vertical_flips = True\nC.rot_90 = True\n```\n\n---\n\nat Prjoect file:\nset epoch_length to 1000, increase epoch\n```python\nepoch_length = 1000\nnum_epochs = 2000\n```\nApply img enhance and class balance function\n```python\nC.use_horizontal_flips = True\nC.use_vertical_flips = True\nC.rot_90 = True\nC.balanced_classes = True\n```\n---\n\nat Prjoect3 file:\nset epoch_length to 1000, increase epoch\n```python\nepoch_length = 1000\nnum_epochs = 2000\n```\nApply img enhance\n```python\nC.use_horizontal_flips = True\nC.use_vertical_flips = True\nC.rot_90 = True\n```\n\n---\n#### check irdius work\n```bash\nmyqueue\n```\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/9_4.JPG)\n\n```bash\nssh pink59\nnvidia-smi\n```\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/9_5.JPG)\n\n### 【31/07/2018】\n#### obtain trained model and log file\n因为 Iriuds 的GPU使用时长限制最高为24小时，因此，需要在下一次开始前载入上一次训练的模型。\n每次训练的粗略结果更新在LogBook最前面.\n\n#### plot rpn and classfier loss\n获取日志中每个小epoch的rpn_cls, rpn_regr, detc_cls, detc_regr\n遍历日志，用正则匹配出相应的数值添加到List中：\n```python\ndef obtain_each_batch(filename):\n    n = 0\n    rpn_cls = []\n    rpn_regr = []\n    detector_cls = []\n    detector_regr = []\n    f = open(filename,'r',buffering=-1)\n    lines = f.readlines()\n    for line in lines:\n        n = n + 1\n        match = re.match(r'.* - rpn_cls: (.*) - rpn_regr: .*', line, re.M|re.I)\n        if match is None:\n            continue\n        else: \n            rpn_cls.append(float(match.group(1)))\n\n        match = re.match(r'.* - rpn_regr: (.*) - detector_cls: .*', line, re.M|re.I)\n        if match is None:\n            continue\n        else: \n            rpn_regr.append(float(match.group(1)))            \n            \n        match = re.match(r'.* - detector_cls: (.*) - detector_regr: .*', line, re.M|re.I)\n        if match is None:\n            continue\n        else: \n            detector_cls.append(float(match.group(1))) \n            \n        match = re.match(r'.* - detector_regr: (.*)\\n', line, re.M|re.I)\n        if match is None:\n            continue\n        else: \n            det_regr = match.group(1)[0:6]\n            detector_regr.append(float(det_regr))\n\n    f.close()\n    print(n)\n    return rpn_cls, rpn_regr, detector_cls, detector_regr  \n```\n\n每个epoch都会计算accury, loss of rpn cls, loss of rpn regr, loss of detc cls, loss of detc regr\n遍历日志找到相应的数值添加到list中：\n```python\ndef obtain_batch(filename):\n    n = 0\n    accuracy = []\n    loss_rpn_cls = []\n    loss_rpn_regr = []\n    loss_detc_cls = []\n    loss_detc_regr = []\n    f = open(filename,'r',buffering=-1)\n    lines = f.readlines()\n    \n    for line in lines:\n        n = n + 1\n        if 'Classifier accuracy for bounding boxes from RPN' in line:\n            result = re.findall(r\"\\d+\\.?\\d*\",line)\n            accuracy.append(float(result[0]))\n            \n        if 'Loss RPN classifier' in line:\n            result = re.findall(r\"\\d+\\.?\\d*\",line)\n            loss_rpn_cls.append(float(result[0]))       \n\n        if 'Loss RPN regression' in line:\n            result = re.findall(r\"\\d+\\.?\\d*\",line)\n            loss_rpn_regr.append(float(result[0]))\n            \n        if 'Loss Detector classifier' in line:\n            result = re.findall(r\"\\d+\\.?\\d*\",line)\n            loss_detc_cls.append(float(result[0]))\n            \n        if 'Loss Detector regression' in line:\n            result = re.findall(r\"\\d+\\.?\\d*\",line)\n            loss_detc_regr.append(float(result[0])) \n            \n    f.close()\n    print(n)\n    return accuracy, loss_rpn_cls, loss_rpn_regr, loss_detc_cls, loss_detc_regr\n```\n\n\n#### plot epoch loss and accury\n```python\nfilename = r'F:\\desktop\\新建文件夹\\1000-no_balance\\train1.out'\naa,bb,cc,dd,ee = obtain_batch(filename)\nx_cor = np.arange(0,len(aa),1)\n\nplt.plot(x_cor,aa, c='b', label = \"Accuracy\")\nplt.plot(x_cor,bb, c='c', label = \"Loss RPN classifier\")\nplt.plot(x_cor,cc, c='g', label = \"Loss RPN regression\")\nplt.plot(x_cor,dd, c='k', label = \"Loss Detector classifier\")\nplt.plot(x_cor,ee, c='m', label = \"Loss Detector regression\")\nplt.ylabel(\"Value of Accuracy and Loss\") \nplt.xlabel(\"Number of Epoch\")\nplt.title('Loss and Accuracy for Totoal Epochs')  \nplt.legend()\nplt.ylim(0,2)\n#plt.xlim(0,11540)\nplt.savefig(\"pic1.PNG\", dpi = 600)\nplt.show()\n```\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/pic1.PNG)\n\n```python\nfilename = r'F:\\desktop\\新建文件夹\\1000-no_balance\\train1.out'\na,b,c,d = obtain_each_batch(filename)\nx_cor = np.arange(0,len(a),1)\n\nplt.plot(x_cor,a, c='b', label = \"rpn_cls\")\nplt.plot(x_cor,b, c='c', label = \"rpn_regr\")\nplt.plot(x_cor,c, c='g', label = \"detector_cls\")\nplt.plot(x_cor,d, c='k', label = \"detector_regr\")\nplt.ylabel(\"Value of Loss\") \nplt.xlabel(\"Epoch Length\")\nplt.title('Loss for Lenght of Epoch')  \nplt.legend()\n#plt.ylim(0,2)\nplt.xlim(80787,92327)\nplt.savefig(\"pic2.PNG\", dpi = 600)\nplt.show()\n```\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/pic2.PNG)\n\n[Jupyter notebook](https://github.com/Trouble404/Object-Detection-System-Based-on-CNN-and-Capsule-Network/blob/master/Plot/logbook_plot.ipynb)\n\n## August\n### 【01~02/08/2018】\n#### test network\n首先是搭建网络，用于train部分相同的设置搭建\n不过在这里图像增强就设置为关闭了\n\n**构建rpn输出**\n```python\nshared_layers = nn.nn_base(img_input, trainable=True)\nnum_anchors = len(C.anchor_box_scales)*len(C.anchor_box_ratios)\nrpn_layers = nn.rpn(shared_layers,num_anchors)\n```\n\n**构建classifier输出**，参数分别是：特征层输出，预选框，探测框的数目，多少个类，是否可训练\n```python\nclassifier = nn.classifier(feature_map_input, roi_input, C.num_rois,nb_classes=len(class_mapping), trainable=True)\n```\n\n**载入训练好的权重：**\n```python\nC.model_path = 'gpu_resnet50_weights.h5'\ntry:\n    print('Loading weights from {}'.format(C.model_path))\n    model_rpn.load_weights(C.model_path, by_name=True)\n    model_classifier.load_weights(C.model_path, by_name=True)\nexcept:\n    print('can not load')\n```\n\n**读取需要检测的图片：**\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/test/test4.jpg)\n将图片规整到制定的大小\n1. 将图片缩放到规定的大小\n    首先从配置文件夹中得到最小边的大小\n    得到图片的高度和宽度\n    根据高度和宽度谁大谁小，确定规整后图片的高宽\n    将图片缩放到指定的大小，用的是立方插值。返回的缩放后的图片img和相应的缩放的比例。\n```python\ndef format_img_size(img, C):\n    (height,width,_) = img.shape\n    if width <= height:\n        ratio = C.im_size/width\n    else:\n        ratio = C.im_size/height\n    new_width, new_height = image_processing.get_new_img_size(width,height, C.im_size)\n    img = cv2.resize(img, (new_width, new_height), interpolation=cv2.INTER_CUBIC)\n    return img, ratio\n```\n2. 对图片每一个通道的像素值做规整\n     将图片的BGR变成RGB，因为网上训练好的RESNET图片都是以此训练的\n    将图片数据类型转换为np.float32，并减去每一个通道的均值，理由同上\n    图片的像素值除一个缩放因子，此处为1\n    将图片的深度变到第一个位置\n    给图片增加一个维度\n```python\ndef format_img_channels(img, C):\n\t\"\"\" formats the image channels based on config \"\"\"\n\timg = img[:, :, (2, 1, 0)]\n\timg = img.astype(np.float32)\n\timg[:, :, 0] -= C.img_channel_mean[0]\n\timg[:, :, 1] -= C.img_channel_mean[1]\n\timg[:, :, 2] -= C.img_channel_mean[2]\n\timg /= C.img_scaling_factor\n\timg = np.transpose(img, (2, 0, 1))\n\timg = np.expand_dims(img, axis=0)\n\treturn img\n```\n如果用的是tensorflow内核，需要将图片的深度变换到最后一位。\n\n**进行区域预测**\nY1:anchor包含物体的概率\nY2:每一个anchor对应的回归梯度\nF:卷积后的特征图，接下来会有用\n\n```python\n[Y1, Y2, F] = model_rpn.predict(X)\n```\n\n获得rpn预测的结果以及对应的回归梯度，这一步就是对图片上隔16个像素的每个anchor进行rpn计算\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/pic_anchors.png)\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/25_1.jpg)\n\n**根据rpn预测的结果，得到预选框:**\n这里会返回300个预选框以及它们对应的坐标(x1,y1,x2,y2)\n```python\n# transform predicted rpn to cordinates of boxes\nR = rpn_to_boxes.rpn_to_roi(Y1, Y2, C, K.image_dim_ordering(), use_regr=True, overlap_thresh=0.7)\n```\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/pic_rois.png)\n\n将预选框的坐标由(x1,y1,x2,y2) 改到 (x,y,w,h)\n```python\nR[:, 2] -= R[:, 0]\nR[:, 3] -= R[:, 1]\n```\n\n**遍历所有的预选框**\n需要注意的是每一次遍历预选框的个数为C.num_rois\n每一次遍历32个预选框，那么总共需要300/32, 10批次\n取出32个预选框，并增加一个维度【注：当不满一个32，其自动只取到最后一个】\n当预选框被取空的时候，停止循环\n当最后一次去不足32个预选框时，补第一个框使其达到32个。\n```python\n# divided 32 bboxes as one group\nfor jk in range(R.shape[0]//C.num_rois + 1):\n    # pick num_rios(32) bboxes one time, only pick to last bboxes in last group\n    ROIs = np.expand_dims(R[C.num_rois*jk:C.num_rois*(jk+1), :], axis=0)\n    #print(ROIs.shape)\n    \n    # no proposals, out iter\n    if ROIs.shape[1] == 0:\n        break\n\n    # when last time can't obtain num_rios(32) bboxes, adding bboxes with 0 to fill to 32 bboxes\n    if jk == R.shape[0]//C.num_rois:\n        #pad R\n        curr_shape = ROIs.shape\n        target_shape = (curr_shape[0],C.num_rois,curr_shape[2])\n        ROIs_padded = np.zeros(target_shape).astype(ROIs.dtype)\n        ROIs_padded[:, :curr_shape[1], :] = ROIs\n        ROIs_padded[0, curr_shape[1]:, :] = ROIs[0, 0, :]\n        # 10 group with 320 bboxes\n        ROIs = ROIs_padded\n```\n这样就可以送入分类网络了\n\n**进行类别预测和边框回归**\n\n预测\nP\\_cls：该边框属于某一类别的概率\nP\\_regr：每一个类别对应的边框回归梯度\nF:rpn网络得到的卷积后的特征图\nROIS:处理得到的区域预选框\n```python\n[P_cls, P_regr] = model_classifier_only.predict([F, ROIs])\n```\n\n遍历每一个预选宽\n如果该预选框的最大概率小于设定的阈值（即预测的肯定程度大于一定的值，我们才认为这次的类别的概率预测是有效的，或者最大的概率出现在背景上，则认为这个预选框是无效的，进行下一次预测。\n```python\n    for ii in range(P_cls.shape[1]):\n\n        # if smaller than setting threshold, we think this bbox invalid\n        # and if this bbox's class is background, we don't need to care about it\n        if np.max(P_cls[0, ii, :]) < bbox_threshold or np.argmax(P_cls[0, ii, :]) == (P_cls.shape[2] - 1):\n            continue\n```\n\n不属于上面的两种情况，取最大的概率处为此边框的类别得到其名称。\n创建两个list，用于存放不同类别对应的边框和概率\n```python\n        # obatain max possibility's class name by class mapping\n        cls_name = class_mapping[np.argmax(P_cls[0, ii, :])]\n\n        # saving bboxes and probs\n        if cls_name not in bboxes:\n            bboxes[cls_name] = []\n            probs[cls_name] = []\n```\n\n得到该预选框的信息\n得到类别对应的编号\n```python\n        # obtain current cordinates of proposal\n        (x, y, w, h) = ROIs[0, ii, :]\n        \n        # obtain the position with max possibility\n        cls_num = np.argmax(P_cls[0, ii, :])\n```\n这样符合条件的预选框以及对应的分类类别和概率就可以画在图片上了\n![iamge](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/pic_cls1.png)\n\n根据类别编号得到该类的边框回归梯度\n对回归梯度进行规整化\n对预测的边框进行修正\n向相应的类里面添加信息【乘 C.rpn_stride，边框的预测都是在特征图上进行的要将其映射到规整后的原图上】\n```python\n        try:\n            # obtain privous position's bbox's regrient\n            (tx, ty, tw, th) = P_regr[0, ii, 4*cls_num:4*(cls_num+1)]\n            # waiting test\n            tx /= C.classifier_regr_std[0]\n            ty /= C.classifier_regr_std[1]\n            tw /= C.classifier_regr_std[2]\n            th /= C.classifier_regr_std[3]\n            # fix box with regreient\n            x, y, w, h = rpn_to_boxes.apply_regr(x, y, w, h, tx, ty, tw, th)\n        except:\n            pass\n        # cordinates of current's box on real img\n        bboxes[cls_name].append([C.rpn_stride*x, C.rpn_stride*y, C.rpn_stride*(x+w), C.rpn_stride*(y+h)])\n        # coresspoding posbility\n        probs[cls_name].append(np.max(P_cls[0, ii, :]))\n```\n这样修正过的框可以画在图上：\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/pic_cls2.png)\n\n遍历bboxes里的类，取出某一类的bbox，合并一些重合度较高的选框\nNo Max Supression\n```python\n# for all classes in current boxes\nfor key in bboxes:\n\n    # bboxes's cordinates\n    bbox = np.array(bboxes[key])\n    # apply NMX to merge some  overlapping boxes\n    new_boxes, new_probs = rpn_to_boxes.non_max_suppression_fast(bbox, np.array(probs[key]), overlap_thresh=0.5)\n```\n最终的图：\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/pic_cls3.png)\n\n[Jupyter notebook](https://github.com/Trouble404/Object-Detection-System-Based-on-CNN-and-Capsule-Network/blob/master/Faster-RCNN/test.ipynb)\n\n#### result\nSmall img, only 8k\n<div align=\"center\">\n<img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/test/test1.jpg\" height=\"200px\"  ><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/test/test1.png\" height=\"200px\"  >   \n</div>\n\n---\n\nOverlapping img\n<div align=\"center\">\n<img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/test/test2.jpg\" height=\"200px\"  ><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/test/test2.png\" height=\"200px\"  >   \n</div>\n\n---\n\nCrowed People\n<div align=\"center\">\n<img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/test/test3.jpg\" height=\"260px\"  ><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/test/test3.png\" height=\"260px\"  >   \n</div>\n\n---\n\ncow and people\n<div align=\"center\">\n<img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/test/test4.jpg\" height=\"260px\"  ><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/test/test4.png\" height=\"260px\"  >   \n</div>\n\n---\n\ncar and plane\n<div align=\"center\">\n<img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/test/test5.jpg\" height=\"220px\"  ><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/test/test5.png\" height=\"220px\"  >   \n</div>\n\n---\n\nStreet img\n<div align=\"center\">\n<img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/test/test6.jpg\" height=\"270px\"  ><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/test/test6.png\" height=\"270px\"  >   \n</div>\n\n---\n\nLots Dogs\n<div align=\"center\">\n<img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/test/test7.jpg\" height=\"250px\"  ><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/test/test7.png\" height=\"250px\"  >   \n</div>\n\n---\n\nOverlapping car and people\n<div align=\"center\">\n<img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/test/test8.jpg\" height=\"290px\"  ><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/test/test8.png\" height=\"290px\"  >   \n</div>\n\n直观看的话效果还不错，但是一些重叠的物体框会出现反复，或者取不到。而且分类有一点过拟合。\n\n\n### 【03/08/2018】\n#### evaluation\n**mAP**\nmAP是目标算法中衡量算法的精确度的指标，涉及两个概念：查准率Precision、查全率Recall。对于object detection任务，每一个object都可以计算出其Precision和Recall，多次计算/试验，每个类都 可以得到一条P-R曲线，曲线下的面积就是AP的值，这个mean的意思是对每个类的AP再求平均，得到的就是mAP的值，mAP的大小一定在[0,1]区间。 \n\n**AP**:Precision对Recall积分，可通过改变正负样本阈值求得矩形面积，进而求积分得到，也可以通过sklearn.metrics.average\\_precision\\_score函数直接得到。 \n\n传入预测值和真实值和resize比例，得到可以传入sklearn.metrics.average_precision_score函数的值，即：真实值和预测概率\n\n---\n\n首先搭建rpn和分类器网络，按照之前的train部分来就可以了\n这里注意分类网络的输入换成测试图片的feature map\n```python\nnum_features = 1024\n\ninput_shape_img = (None, None, 3)\ninput_shape_features = (None, None, num_features)\n\nimg_input = Input(shape=input_shape_img)\nroi_input = Input(shape=(C.num_rois, 4))\nfeature_map_input = Input(shape=input_shape_features)\n\nclassifier = nn.classifier(feature_map_input, roi_input, C.num_rois, nb_classes=len(class_mapping), trainable=True)\n```\n\n然后载入需要测试的模型权重\n\n按照VOC的数据集标注，把测试集分出来：\n```python\ntrain_imgs = []\ntest_imgs = []\n\nfor each in all_imgs:\n\tif each['imageset'] == 'trainval':\n\t\ttrain_imgs.append(each)\n\tif each['imageset'] == 'test':\n\t\ttest_imgs.append(each)\n```\n\n按照之前的预测方法，求出图片的预测框坐标以及对应的分类名字，然后把这些信息放入对应的字典里面，与xml解析的文件一样的格式：\n```python\n    for jk in range(new_boxes.shape[0]):\n        (x1, y1, x2, y2) = new_boxes[jk, :]\n        det = {'x1': x1, 'x2': x2, 'y1': y1, 'y2': y2, 'class': key, 'prob': new_probs[jk]}\n        all_dets.append(det)\n```\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/014_1.JPG)\n\n然后读取标注的框的真实数值：\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/014_2.JPG)\n\n遍历真实信息里面的每一个狂，将bbox_matched这个属性标注为FALSE，之后如果预测框和标注框对应上的话，这个属性就会被设置为True\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/14_3.JPG)\n\n获取预测框里面的分类对应概率，并且按照概率从大到小得到idx位置：\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/014_3.JPG)\n\n按照概率大小，对每一个对应的预测框，对比每一个标注的框，如果预测的类与当前标注框的类相同并且没有被匹配过，计算两个框的iou，如果大于0.5的话就表明预测框匹配当前标注框，保存预测概率以及对应的是否匹配：\n\n```python\n# process each bbox with hightest prob\nfor box_idx in box_idx_sorted_by_prob:\n    \n    # obtain current box's cordinates, class and prob\n    pred_box = pred[box_idx]\n    pred_class = pred_box['class']\n    pred_x1 = pred_box['x1']\n    pred_x2 = pred_box['x2']\n    pred_y1 = pred_box['y1']\n    pred_y2 = pred_box['y2']\n    pred_prob = pred_box['prob']\n    \n    # if not in P list, save current class infomration to it\n    if pred_class not in P:\n        P[pred_class] = []\n        T[pred_class] = []\n        # put porb to P\n    P[pred_class].append(pred_prob)\n    # used to check whether find current object\n    found_match = False\n\n    # compare each real bbox\n    # obtain real box's cordinates, class and prob\n    for gt_box in gt:\n        gt_class = gt_box['class']\n        # bacause the image is rezied, so calculate the real cordinates\n        gt_x1 = gt_box['x1']/fx\n        gt_x2 = gt_box['x2']/fx\n        gt_y1 = gt_box['y1']/fy\n        gt_y2 = gt_box['y2']/fy\n        \n        # obtain box_matched - all false at beginning\n        gt_seen = gt_box['bbox_matched']\n        \n        # ture class != predicted class\n        if gt_class != pred_class:\n            continue\n        # already matched\n        if gt_seen:\n            continue\n        # calculate iou of predicted bbox and real bbox \n        iou = rpn_calculation.iou((pred_x1, pred_y1, pred_x2, pred_y2), (gt_x1, gt_y1, gt_x2, gt_y2))\n        # if iou > 0.5, we will set this prediction correct\n        if iou >= 0.5:\n            found_match = True\n            gt_box['bbox_matched'] = True\n            break\n        else:\n            continue\n    # 1 means this position's bbox correct match with orignal image\n    T[pred_class].append(int(found_match))\n```\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/014_4.JPG)\n\n遍历每一个标注框，如果没有被匹配到并且diffcult属性不是true的话，说明这个框漏检了，在之前保存的概率以及对应是否有概率里面加入物体1以及对应概率0\n```python\n# adding missing object compared to orignal image\nfor gt_box in gt:\n    if not gt_box['bbox_matched'] and not gt_box['difficult']:\n        if gt_box['class'] not in P:\n            P[gt_box['class']] = []\n            T[gt_box['class']] = []\n\n        # T = 1 means there are object, P = 0 means we did't detected that\n        T[gt_box['class']].append(1)\n        P[gt_box['class']].append(0)\n```\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/014_5.JPG)\n\n把当前信息存入到总的一个词典里面，就可以使用average_precision_score这个sklearn里面的函数计算ap了。与此同时，保存得到的结果并且显示总的map：\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/014_6.JPG)\n\n[jupyter notebook](https://github.com/Trouble404/Object-Detection-System-Based-on-CNN-and-Capsule-Network/blob/master/Faster-RCNN/map.ipynb)\n\n### 【06~10/08/2018】\n#### adjust\n将计算ap的函数包装好：\n[jupyter notebook](https://github.com/Trouble404/Object-Detection-System-Based-on-CNN-and-Capsule-Network/blob/master/Faster-RCNN/map_all.ipynb)\n\n##### Project1 all: 9 models:\nALL_2 NO THRESHOLD OTHER WITH THRESHOLD MOST 0.8\n\n| Classes | ALL_1 | ALL_2 | ALL_3 | ALL_4 | ALL_5 | ALL_6 | ALL_7 | ALL_8 | ALL_9 |\n| :---: | :----: | :---: | :----: | :----: | :----: | :----: | :----: | :----: | :----: |\n| motorbike | 0.2305 | 0.2412 | 0.2132 | 0.2220 | 0.2889 | 0.2528 | 0.2204 | 0.2644 | 0.2336 |\n| person | 0.6489 | 0.6735 | 0.7107 | 0.6652 | 0.7120 | 0.7041 | 0.7238 | 0.7003 | 0.7201 |\n| car | 0.1697 | 0.1563 | 0.2032 | 0.2105 | 0.2308 | 0.2221 | 0.2436 | 0.2053 | 0.2080 |\n| aeroplane | 0.7968 | 0.7062 | 0.7941 | 0.6412 | 0.7871 | 0.7331 | 0.7648 | 0.7659 | 0.6902 |\n| bottle | 0.2213 | 0.2428 | 0.2261 | 0.1943 | 0.2570 | 0.2437 | 0.2899 | 0.1442 | 0.2265 |\n| sheep | 0.6162 | 0.5702 | 0.6876 | 0.6295 | 0.6364 | 0.5710 | 0.6536 | 0.6349 | 0.6455 |\n| tvmonitor | 0.1582 | 0.1601 | 0.2231 | 0.1748 | 0.1551 | 0.1603 | 0.1317 | 0.1584 | 0.1678 |\n| boat | 0.3842 | 0.2621 | 0.2261 | 0.1943 | 0.3499 | 0.2437 | 0.2057 | 0.2748 | 0.3509 |\n| chair | 0.2811 | 0.0563| 0.0891 | 0.0621 | 0.1353 | 0.0865 | 0.0907 | 0.0854 | 0.1282 |\n| bicycle | 0.1464 | 0.1224 | 0.1346 | 0.1781 | 0.1406 | 0.1448 | 0.1810 | 0.1071 | 0.1673 |\n| cat | 0.8901 | 0.8565 | 0.9103 | 0.8417 | 0.8289 | 0.8274 | 0.7572 | 0.9143 | 0.8118 |\n| pottedplant | 0.2075 | 0.0926 | 0.1790 | 0.0532 | 0.1517 | 0.1150 | 0.1080 | 0.1022 | 0.0939 |\n| horse | 0.1185 | 0.0588 | 0.0726 | 0.0489 | 0.0696 | 0.0695 | 0.0637 | 0.0651 | 0.0640 |\n| sofa | 0.2797 | 0.2309 | 0.2852 | 0.2966 | 0.3855 | 0.4817 | 0.3659 | 0.3132 | 0.3090 |\n| dog | 0.5359 | 0.5077 | 0.5578 | 0.4413 | 0.4832 | 0.5793 | 0.5687 | 0.4910 | 0.4598 |\n| cow | 0.7582 | 0.6229 | 0.7295 | 0.5420 | 0.5379 | 0.5312 | 0.5147 | 0.5706 | 0.6503 |\n| diningtable | 0.3979 | 0.2734 | 0.3739 | 0.2963 | 0.4715 | 0.4987 | 0.3895 | 0.4983 | 0.4666 |\n| bus | 0.6203 | 0.5572 | 0.6468 | 0.6032 | 0.6320 | 0.6096 | 0.7169 | 0.5938 | 0.5485 |\n| bird | 0.6164 | 0.6662 | 0.5692 | 0.5751 | 0.5407 | 0.4125 | 0.4925 | 0.4347 | 0.5208 |\n| train | 0.8655 | 0.6916 | 0.7141 | 0.7166 | 0.7643 | 0.8107 | 0.7100 | 0.7194 | 0.6263 |\n| **mAP** | **0.4472** | **0.3874** | **0.4341** | **0.3859** | **0.4279** | **0.4141** | **0.4096** | **0.4022** | **0.4045** |\n\n---\n\n##### Project1 epoch_lenght=1000, epoch:1041 : 7 models:\nALL WITH THRESHOLD MOST 0.51\n\n| Classes | ALL_1 | ALL_2 | ALL_3 | ALL_4 | ALL_5 | ALL_6 | ALL_7 |\n| :---: | :----: | :---: | :----: | :----: | :----: | :----: | :----: |\n| motorbike | 0.2433 | 0.2128 | 0.2232 | 0.2262 | 0.2286 | 0.2393 | 0.2279 |\n| person | 0.6560 | 0.6537 | 0.6742 | 0.6952 | 0.6852 | 0.6719 | 0.6636 |\n| car | 0.1562 | 0.1905 | 0.1479 | 0.2024 | 0.2010 | 0.1379 | 0.1583 | \n| aeroplane | 0.7359 | 0.6837 | 0.6729 | 0.6687 | 0.6957 | 0.7339 | 0.6391 |\n| bottle | 0.1913 | 0.1937 | 0.2635 | 0.1843 | 0.2570 | 0.1632 | 0.1863 | \n| sheep | 0.5429 | 0.5579 | 0.6219 | 0.5355 | 0.5881 | 0.5441 | 0.5824 |\n| tvmonitor | 0.1295 | 0.1601 | 0.1368 | 0.1407 | 0.1147 | 0.1349 | 0.1154 | \n| boat | 0.1913 | 0.2880 | 0.2635 | 0.3433 |0.3335 | 0.3422 | 0.3069 | \n| chair | 0.0587 | 0.0657| 0.0342 | 0.0680 | 0.0695 | 0.0752 | 0.0760 |\n| bicycle | 0.1013 | 0.1485 | 0.1225 | 0.1871 | 0.1685 | 0.1037 | 0.1490 | \n| cat | 0.8737 | 0.8557 | 0.8007 | 0.7982 | 0.8045 | 0.8067 | 0.7732 |\n| pottedplant | 0.0694 | 0.1059 | 0.0748 | 0.0878 | 0.0893 | 0.0690 | 0.0865 |\n| horse | 0.0556 | 0.0561 | 0.0581 | 0.0770 | 0.0575 | 0.0539 | 0.0522 |\n| sofa | 0.2177 | 0.2917 | 0.1699 | 0.1940 | 0.3177 | 0.1863 | 0.1857 |\n| dog | 0.6269 | 0.4989 | 0.5015 | 0.5333 | 0.4914 | 0.5572 | 0.4747 |\n| cow | 0.5216 | 0.6229 | 0.5283 | 0.6426 | 0.4358 | 0.4227 | 0.4589 | \n| diningtable | 0.3076 | 0.3889 | 0.3283 | 0.2404 | 0.4219 | 0.4153 | 0.2627 |\n| bus | 0.5865 | 0.5222 | 0.6312 | 0.5853 | 0.5042 | 0.4882 | 0.5576 |\n| bird | 0.5339 | 0.5039 | 0.5150 | 0.5152 | 0.5838 | 0.3890 | 0.4680 |\n| train | 0.4994 | 0.6541 | 0.6702 | 0.6920 | 0.5959 | 0.5893 | 0.6861 |\n| **mAP** | **0.3699** | **0.3765** | **0.3748** | **0.3814** | **0.3786** | **0.3562** | **0.3555** |\n\n\n在测试集上的结果不是很好，不同class的ap差距较大，可能是由于训练时候不平均或者训练集太小的原因\n\n**尝试加入VOC2007的数据进训练集当中，观察结果。**\n解析VOC2007的过程中遇到了OpenCV读取不了图片的BUG。\n（已修复）\nVOC2012的数据莫名没有了，因为之前测试过的原因，一直以为是VOC2007的数据解析有问题，大概是Irius的文件上限时间到了自动清除了数据。\n\n20个类当中的AP差距过大，其实数据集是不平衡的，有的类只有大概1000个样本，但是人这个样本就有2W多，而且之前的训练过程中每次图片都是在训练集中随机选的，所以尝试修改了流程，当所有训练集中的数据都被读取训练过以后再打乱训练集，与此同时配合class_balance的功能使用。\n\n实际上使用的时候class balance效果不是很好，后面没有开启。\n\n用了一个较大的学习率尝试训练没有载入imagenet预训练权重的版本。\n\n交叉法\n\n### 【13/08/2018】\n#### soft-NMS\n传统的非最大抑制算法首先在被检测图片中产生一系列的检测框B以及对应的分数S。当选中最大分数的检测框M，它被从集合B中移出并放入最终检测结果集合D。于此同时，集合B中任何与检测框M的重叠部分大于重叠阈值Nt的检测框也将随之移除。非最大抑制算法中的最大问题就是它将相邻检测框的分数均强制归零。在这种情况下，如果一个真实物体在重叠区域出现，则将导致对该物体的检测失败并降低了算法的平均检测率（average precision, AP）。\n\n换一种思路，如果我们只是通过一个基于与M重叠程度相关的函数来降低相邻检测框的分数而非彻底剔除。虽然分数被降低，但相邻的检测框仍在物体检测的序列中。图二中的实例可以说明这个问题。\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/018_1.JPG)\n针对NMS存在的这个问题，我们提出了一种新的Soft-NMS算法（图三），它只需改动一行代码即可有效改进传统贪心NMS算法。在该算法中，我们基于重叠部分的大小为相邻检测框设置一个衰减函数而非彻底将其分数置为零。**简单来讲，如果一个检测框与M有大部分重叠，它会有很低的分数；而如果检测框与M只有小部分重叠，那么它的原有检测分数不会受太大影响**。在标准数据集PASCAL VOC 和 MS-COCO等标准数据集上，Soft-NMS对现有物体检测算法在多个重叠物体检测的平均准确率有显著的提升。同时，Soft-NMS不需要额外的训练且易于实现，因此，它很容易被集成到当前的物体检测流程中。\n\n伪代码：\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/018_2.JPG)\n\n公式：\nNMS\n$$ s_{i}=\\left\\{\n\\begin{aligned}\ns_{i}, \\ \\ \\ \\ iou(M,b_{i}) <  N_{t} \\\\\n0, \\ \\ \\ \\ iou(M,b_{i}) \\geq  N_{t} \n\\end{aligned}\n\\right.\n$$\n\nSOFT NMS\n$$ s_{i}=\\left\\{\n\\begin{aligned}\ns_{i}, \\ \\ \\ \\ iou(M,b_{i}) <  N_{t} \\\\\n1-iou(M,b_{i}), \\ \\ \\ \\ iou(M,b_{i}) \\geq  N_{t} \n\\end{aligned}\n\\right.\n$$\n\n当相邻检测框与M的重叠度超过重叠阈值Nt后，检测框的检测分数呈线性衰减。在这种情况下，与M相邻很近的检测框衰减程度很大，而远离M的检测框并不受影响。\n\n但是，上述分数重置函数并不是一个连续函数，在重叠程度超过重叠阈值Nt时，该分数重置函数产生突变，从而可能导致检测结果序列产生大的变动，因此我们更希望找到一个连续的分数重置函数。它对没有重叠的检测框的原有检测分数不产生衰减，同时对高度重叠的检测框产生大的衰减。综合考虑这些因素，我们进一步对soft-NMS中的分数重置函数进行了改进：\n\nGaussian penalty:\n![image](https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/018_3.JPG)\n\n根据这个伪代码以及公式，实现代码：\n```python\n\"\"\" NMS , delete overlapping box\n\n@param boxes: (n,4) box and coresspoding cordinates\n@param probs: (n,) box adn coresspding possibility\n@param overlap_thresh: treshold of delet box overlapping\n@param max_boxes: maximum keeping number of boxes\n@param method: 1 for linear soft NMS, 2 for gaussian soft NMS\n@param sigma: parameter of gaussian soft NMS\nprob_thresh: threshold of probs after soft NMS\n\n\n@return: boxes: boxes cordinates(x1,y1,x2,y2)\n@return: probs: coresspoding possibility\n\"\"\"\ndef soft_nms(boxes, probs, overlap_thresh=0.9, max_boxes=300, method = 1, sigma=0.5, prob_thresh=0.49):\n    # number of input boxes\n    N = boxes.shape[0]\n    # if the bounding boxes integers, convert them to floats --\n    # this is important since we'll be doing a bunch of divisions\n    if boxes.dtype.kind == \"i\":\n        boxes = boxes.astype(\"float\")\n\n    # iterate all boxes\n    for i in range(N):\n        \n        # obtain current boxes' cordinates and probs\n        maxscore = probs[i]\n        maxpos = i\n\n        tx1 = boxes[i,0]\n        ty1 = boxes[i,1]\n        tx2 = boxes[i,2]\n        ty2 = boxes[i,3]\n        ts = probs[i]\n\n        pos = i + 1\n        # get max box\n        while pos < N:\n            if maxscore < probs[pos]:\n                maxscore = probs[pos]\n                maxpos = pos\n            pos = pos + 1\n\n        # add max box as a detection \n        boxes[i,0] = boxes[maxpos,0]\n        boxes[i,1] = boxes[maxpos,1]\n        boxes[i,2] = boxes[maxpos,2]\n        boxes[i,3] = boxes[maxpos,3]\n        probs[i] = probs[maxpos]\n\n        # swap ith box with position of max box\n        boxes[maxpos,0] = tx1\n        boxes[maxpos,1] = ty1\n        boxes[maxpos,2] = tx2\n        boxes[maxpos,3] = ty2\n        probs[maxpos] = ts\n\n        # cordinates of max box\n        tx1 = boxes[i,0]\n        ty1 = boxes[i,1]\n        tx2 = boxes[i,2]\n        ty2 = boxes[i,3]\n        ts = probs[i]\n\n        pos = i + 1\n        # NMS iterations, note that N changes if detection boxes fall below threshold\n        while pos < N:\n            x1 = boxes[pos, 0]\n            y1 = boxes[pos, 1]\n            x2 = boxes[pos, 2]\n            y2 = boxes[pos, 3]\n            s = probs[pos]\n            \n            # calculate the areas, +1 for robatness\n            area = (x2 - x1 + 1) * (y2 - y1 + 1)\n            iw = (min(tx2, x2) - max(tx1, x1) + 1)\n            # # confirm left top cordinates less than top right\n            if iw > 0:\n                ih = (min(ty2, y2) - max(ty1, y1) + 1)\n                # confirm left top cordinates less than top right\n                if ih > 0:\n                    # find the union\n                    ua = float((tx2 - tx1 + 1) * (ty2 - ty1 + 1) + area - iw * ih)\n                    #iou between max box and detection box\n                    ov = iw * ih / ua\n\n                    if method == 1: # linear\n                        if ov > overlap_thresh: \n                            weight = 1 - ov\n                        else:\n                            weight = 1\n                    elif method == 2: # gaussian\n                        weight = np.exp(-(ov * ov)/sigma)\n                    else: # original NMS\n                        if ov > overlap_thresh: \n                            weight = 0\n                        else:\n                            weight = 1\n\n                    # obtain adjusted probs\n                    probs[pos] = weight*probs[pos]\n\n   \n                    # if box score falls below threshold, discard the box by swapping with last box\n                    # update N\n                    if probs[pos] < prob_thresh:\n                        boxes[pos,0] = boxes[N-1, 0]\n                        boxes[pos,1] = boxes[N-1, 1]\n                        boxes[pos,2] = boxes[N-1, 2]\n                        boxes[pos,3] = boxes[N-1, 3]\n                        probs[pos] = probs[N-1]\n                        N = N - 1\n                        pos = pos - 1\n\n            pos = pos + 1\n    # keep is the idx of current keeping objects, front ith objectes\n    keep = [i for i in range(N)]\n    return boxes[keep], probs[keep]\n```\n\n\n### 【14/08/2018】\n#### OVERLAPPING OBJECT DETECTION\n\n\n### 【15/08/2018】\n\n\n### 【16/08/2018】\n\n\n### 【17/08/2018】\n\n\n## September\n\n\n\n","slug":"LogBook","published":1,"updated":"2020-04-28T12:38:55.395Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck9jwdnmb001ejlrcuz1ek2ga","content":"<h2 id=\"Gantt-chart\"><a href=\"#Gantt-chart\" class=\"headerlink\" title=\"Gantt chart\"></a>Gantt chart</h2><p><img src=\"https://raw.githubusercontent.com/Trouble404/Object-Detection-System-Based-on-CNN-and-Capsule-Network/master/Project%20Plan/gantt%20chart%20of%20project.PNG\" alt=\"image\"></p>\n<a id=\"more\"></a>\n<hr>\n<h2 id=\"Check-list\"><a href=\"#Check-list\" class=\"headerlink\" title=\"Check list\"></a>Check list</h2><ul>\n<li style=\"list-style: none\"><input type=\"checkbox\" checked> <strong>1) preparation</strong></li>\n<li style=\"list-style: none\"><input type=\"checkbox\" checked> <strong><em>1.1) Familiarization with develop tools</em></strong></li>\n<li style=\"list-style: none\"><input type=\"checkbox\" checked> 1.1.1) Keras</li>\n<li style=\"list-style: none\"><input type=\"checkbox\" checked> 1.1.2) Pythrch</li>\n<li style=\"list-style: none\"><input type=\"checkbox\" checked> <strong><em>1.2) Presentation</em></strong></li>\n<li style=\"list-style: none\"><input type=\"checkbox\" checked> 1.2.1) Poster conference</li>\n<li style=\"list-style: none\"><input type=\"checkbox\" checked> <strong>2) Create image database</strong></li>\n<li style=\"list-style: none\"><input type=\"checkbox\" checked> 2.1) Confirmation of detected objects</li>\n<li style=\"list-style: none\"><input type=\"checkbox\" checked> 2.2) Collect and generate the dataset</li>\n<li style=\"list-style: none\"><input type=\"checkbox\" checked> <strong>3) Familiarization with CNN based object detection methods</strong></li>\n<li style=\"list-style: none\"><input type=\"checkbox\" checked> 3.1) R-CNN</li>\n<li style=\"list-style: none\"><input type=\"checkbox\" checked> 3.2) SPP-net</li>\n<li style=\"list-style: none\"><input type=\"checkbox\" checked> 3.3) Fast R-CNN</li>\n<li style=\"list-style: none\"><input type=\"checkbox\" checked> 3.4) Faster R-CNN</li>\n<li style=\"list-style: none\"><input type=\"checkbox\" checked> <strong>4) Implement object detection system based on one chosen CNN method</strong></li>\n<li style=\"list-style: none\"><input type=\"checkbox\" checked> 4.1) Pre-processing of images</li>\n<li style=\"list-style: none\"><input type=\"checkbox\" checked> 4.2) Extracting features</li>\n<li style=\"list-style: none\"><input type=\"checkbox\" checked> 4.3) Mode architecture</li>\n<li style=\"list-style: none\"><input type=\"checkbox\" checked> 4.4) Train model and optimization</li>\n<li style=\"list-style: none\"><input type=\"checkbox\" checked> 4.5) Models ensemble</li>\n<li style=\"list-style: none\"><input type=\"checkbox\" checked> <strong>5) Analysis work</strong></li>\n<li style=\"list-style: none\"><input type=\"checkbox\" checked> 5.1) Evaluation of detection result.</li>\n<li style=\"list-style: none\"><input type=\"checkbox\" checked> <strong>6) Paperwork and bench inspection</strong></li>\n<li style=\"list-style: none\"><input type=\"checkbox\" checked> 6.1) Logbook</li>\n<li style=\"list-style: none\"><input type=\"checkbox\" checked> 6.2) Write the thesis</li>\n<li style=\"list-style: none\"><input type=\"checkbox\" checked> 6.3) Project video</li>\n<li style=\"list-style: none\"><input type=\"checkbox\" checked> 6.4) Speech and ppt of bench inspection</li>\n<li style=\"list-style: none\"><input type=\"checkbox\" checked> <strong>7) Documents</strong></li>\n<li style=\"list-style: none\"><input type=\"checkbox\" checked> 7.1) Project Brief</li>\n</ul>\n<hr>\n<h2 id=\"May\"><a href=\"#May\" class=\"headerlink\" title=\"May\"></a>May</h2><h3 id=\"【28-05-2018】\"><a href=\"#【28-05-2018】\" class=\"headerlink\" title=\"【28/05/2018】\"></a>【28/05/2018】</h3><p>Keras is a high-level neural networks API, written in Python and capable of running on top of <a href=\"https://github.com/tensorflow/tensorflow\" target=\"_blank\" rel=\"noopener\">TensorFlow</a>, CNTK, or Theano.</p>\n<ul>\n<li><p><strong><a href=\"https://keras.io/\" target=\"_blank\" rel=\"noopener\">Keras document</a></strong></p>\n</li>\n<li><p><strong><a href=\"https://keras-cn.readthedocs.io/en/latest/#keraspython\" target=\"_blank\" rel=\"noopener\">Keras 文档</a></strong></p>\n</li>\n</ul>\n<hr>\n<h4 id=\"Installation\"><a href=\"#Installation\" class=\"headerlink\" title=\"Installation\"></a>Installation</h4><ul>\n<li><p><strong>TensorFlow</strong><br><a href=\"https://www.visualstudio.com/zh-hans/vs/older-downloads/\" target=\"_blank\" rel=\"noopener\">Microsoft Visual Studio 2015</a><br><a href=\"http://docs.nvidia.com/cuda/cuda-installation-guide-microsoft-windows/\" target=\"_blank\" rel=\"noopener\">CUDA 9.0</a><br><a href=\"https://developer.nvidia.com/cudnn\" target=\"_blank\" rel=\"noopener\">cuDNN7</a><br><a href=\"https://www.anaconda.com/download/\" target=\"_blank\" rel=\"noopener\">Anaconda</a></p>\n<ul>\n<li>Step 1: Install VS2015</li>\n<li>Step 2: Install CUDA 9.0 并添加环境变量</li>\n<li>Step 3: Install cuDNN7 解压后把cudnn目录下的bin目录加到PATH环境变量里</li>\n<li>Step 4: Install Anaconda 把安装路径添加到PATH里去, 在这里我用了 <strong>Python 3.5</strong></li>\n<li>Step 5: 使用Anaconda的命令行 新建一个虚拟环境,激活并且关联到jupyterbook<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">conda create  --name tensorflow python=3.5</span><br><span class=\"line\">activate tensorflow</span><br><span class=\"line\">conda install nb_conda</span><br></pre></td></tr></table></figure></li>\n<li>Step 6: Install GPU version TensorFlow.<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">pip install -i https://pypi.tuna.tsinghua.edu.cn/simple --ignore-installed --upgrade tensorflow-gpu </span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n</li>\n<li><p><strong>Keras</strong></p>\n<ul>\n<li>Step 1: 启动之前的 虚拟环境， 并且安装Keras GPU 版本<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">activate tensorflow</span><br><span class=\"line\">pip install keras -U --pre</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n</li>\n</ul>\n<h4 id=\"在硕士学习过程中，使用Keras的项目\"><a href=\"#在硕士学习过程中，使用Keras的项目\" class=\"headerlink\" title=\"在硕士学习过程中，使用Keras的项目**\"></a>在硕士学习过程中，使用Keras的项目**</h4><ul>\n<li><strong><a href=\"https://github.com/Trouble404/NBA-with-Machine-Learning\" target=\"_blank\" rel=\"noopener\">NBA with Machine Learning</a></strong></li>\n<li><strong><a href=\"https://github.com/Trouble404/kaggle-Job-Salary-Prediction\" target=\"_blank\" rel=\"noopener\">Kaggle- Job salary prediction</a></strong></li>\n</ul>\n<h4 id=\"TensorFlow-CPU-切换\"><a href=\"#TensorFlow-CPU-切换\" class=\"headerlink\" title=\"TensorFlow CPU 切换\"></a>TensorFlow CPU 切换</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> tensorflow <span class=\"keyword\">as</span> tf  </span><br><span class=\"line\"><span class=\"keyword\">import</span> os</span><br><span class=\"line\"><span class=\"keyword\">import</span> keras.backend.tensorflow_backend <span class=\"keyword\">as</span> KTF  </span><br><span class=\"line\"></span><br><span class=\"line\">os.environ[<span class=\"string\">\"CUDA_VISIBLE_DEVICES\"</span>] = <span class=\"string\">\"0\"</span>  <span class=\"comment\">#设置需要使用的GPU的编号</span></span><br><span class=\"line\">config = tf.ConfigProto()</span><br><span class=\"line\">config.gpu_options.per_process_gpu_memory_fraction = <span class=\"number\">0.4</span> <span class=\"comment\">#设置使用GPU容量占GPU总容量的比例</span></span><br><span class=\"line\">sess = tf.Session(config=config)</span><br><span class=\"line\">KTF.set_session(sess)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">with</span> tf.device(<span class=\"string\">'/cpu:0'</span>):</span><br></pre></td></tr></table></figure>\n<p>这样可以在GPU版本的虚拟环境里面使用CPU计算</p>\n<h4 id=\"Jupyter-Notebook-工作目录设置\"><a href=\"#Jupyter-Notebook-工作目录设置\" class=\"headerlink\" title=\"Jupyter Notebook 工作目录设置\"></a>Jupyter Notebook 工作目录设置</h4><p>启动命令行，切换至预设的工作目录， 运行：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">jupyter notebook --generate-config</span><br></pre></td></tr></table></figure></p>\n<p><img src=\"https://raw.githubusercontent.com/Trouble404/Object-Detection-System-Based-on-CNN-and-Capsule-Network/master/Learned%20skills/jupyter%20_setting.PNG\" alt=\"image\"></p>\n<h2 id=\"June\"><a href=\"#June\" class=\"headerlink\" title=\"June\"></a>June</h2><h3 id=\"【01-06-2018】\"><a href=\"#【01-06-2018】\" class=\"headerlink\" title=\"【01/06/2018】\"></a>【01/06/2018】</h3><p><strong><a href=\"https://pytorch.org/about/\" target=\"_blank\" rel=\"noopener\">PyTorch</a></strong> is a python package that provides two high-level features:</p>\n<ul>\n<li>Tensor computation (like numpy) with strong GPU acceleration</li>\n<li>Deep Neural Networks built on a tape-based autodiff system</li>\n</ul>\n<table>\n<thead>\n<tr>\n<th style=\"text-align:left\">Package</th>\n<th style=\"text-align:left\">Description</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:left\">torch</td>\n<td style=\"text-align:left\">a Tensor library like NumPy, with strong GPU support</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">torch.autograd</td>\n<td style=\"text-align:left\">a tape based automatic differentiation library that supports all differentiable Tensor operations in torch</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">torch.nn</td>\n<td style=\"text-align:left\">a neural networks library deeply integrated with autograd designed for maximum flexibility</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">torch.optim</td>\n<td style=\"text-align:left\">an optimization package to be used with torch.nn with standard optimization methods such as SGD, RMSProp, LBFGS, Adam etc.</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">torch.multiprocessing</td>\n<td style=\"text-align:left\">python multiprocessing, but with magical memory sharing of torch Tensors across processes. Useful for data loading and hogwild training.</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">torch.utils</td>\n<td style=\"text-align:left\">DataLoader, Trainer and other utility functions for convenience</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">torch.legacy(.nn/.optim)</td>\n<td style=\"text-align:left\">legacy code that has been ported over from torch for backward compatibility reasons</td>\n</tr>\n</tbody>\n</table>\n<hr>\n<h4 id=\"Installation-1\"><a href=\"#Installation-1\" class=\"headerlink\" title=\"Installation\"></a>Installation</h4><ul>\n<li>Step 1: 使用Anaconda的命令行 新建一个虚拟环境,激活并且关联到jupyterbook<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">conda create  --name pytorch python=3.5</span><br><span class=\"line\">activate pytorch</span><br><span class=\"line\">conda install nb_conda</span><br></pre></td></tr></table></figure></li>\n<li>Step 2: Install GPU version PyTorch.<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">conda install pytorch cuda90 -c pytorch </span><br><span class=\"line\">pip install torchvision</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<h4 id=\"Understanding-of-PyTorch\"><a href=\"#Understanding-of-PyTorch\" class=\"headerlink\" title=\"Understanding of PyTorch\"></a>Understanding of PyTorch</h4><ul>\n<li><p><strong>Tensors</strong><br>Tensors和numpy中的ndarrays较为相似, 与此同时Tensor也能够使用GPU来加速运算</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> __future__ <span class=\"keyword\">import</span> print_function</span><br><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\">x = torch.Tensor(<span class=\"number\">5</span>, <span class=\"number\">3</span>)  <span class=\"comment\"># 构造一个未初始化的5*3的矩阵</span></span><br><span class=\"line\">x = torch.rand(<span class=\"number\">5</span>, <span class=\"number\">3</span>)  <span class=\"comment\"># 构造一个随机初始化的矩阵</span></span><br><span class=\"line\">x <span class=\"comment\"># 此处在notebook中输出x的值来查看具体的x内容</span></span><br><span class=\"line\">x.size()</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#<span class=\"doctag\">NOTE:</span> torch.Size 事实上是一个tuple, 所以其支持相关的操作*</span></span><br><span class=\"line\">y = torch.rand(<span class=\"number\">5</span>, <span class=\"number\">3</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#此处 将两个同形矩阵相加有两种语法结构</span></span><br><span class=\"line\">x + y <span class=\"comment\"># 语法一</span></span><br><span class=\"line\">torch.add(x, y) <span class=\"comment\"># 语法二</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 另外输出tensor也有两种写法</span></span><br><span class=\"line\">result = torch.Tensor(<span class=\"number\">5</span>, <span class=\"number\">3</span>) <span class=\"comment\"># 语法一</span></span><br><span class=\"line\">torch.add(x, y, out=result) <span class=\"comment\"># 语法二</span></span><br><span class=\"line\">y.add_(x) <span class=\"comment\"># 将y与x相加</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 特别注明：任何可以改变tensor内容的操作都会在方法名后加一个下划线'_'</span></span><br><span class=\"line\"><span class=\"comment\"># 例如：x.copy_(y), x.t_(), 这俩都会改变x的值。</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#另外python中的切片操作也是资次的。</span></span><br><span class=\"line\">x[:,<span class=\"number\">1</span>] <span class=\"comment\">#这一操作会输出x矩阵的第二列的所有值</span></span><br></pre></td></tr></table></figure>\n</li>\n<li><p><strong>Numpy桥</strong><br>将Torch的Tensor和numpy的array相互转换简，注意Torch的Tensor和numpy的array会共享他们的存储空间，修改一个会导致另外的一个也被修改。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 此处演示tensor和numpy数据结构的相互转换</span></span><br><span class=\"line\">a = torch.ones(<span class=\"number\">5</span>)</span><br><span class=\"line\">b = a.numpy()</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 此处演示当修改numpy数组之后,与之相关联的tensor也会相应的被修改</span></span><br><span class=\"line\">a.add_(<span class=\"number\">1</span>)</span><br><span class=\"line\">print(a)</span><br><span class=\"line\">print(b)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 将numpy的Array转换为torch的Tensor</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\">a = np.ones(<span class=\"number\">5</span>)</span><br><span class=\"line\">b = torch.from_numpy(a)</span><br><span class=\"line\">np.add(a, <span class=\"number\">1</span>, out=a)</span><br><span class=\"line\">print(a)</span><br><span class=\"line\">print(b)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 另外除了CharTensor之外，所有的tensor都可以在CPU运算和GPU预算之间相互转换</span></span><br><span class=\"line\"><span class=\"comment\"># 使用CUDA函数来将Tensor移动到GPU上</span></span><br><span class=\"line\"><span class=\"comment\"># 当CUDA可用时会进行GPU的运算</span></span><br><span class=\"line\"><span class=\"keyword\">if</span> torch.cuda.is_available():</span><br><span class=\"line\">    x = x.cuda()</span><br><span class=\"line\">    y = y.cuda()</span><br></pre></td></tr></table></figure>\n</li>\n<li><p><strong>使用PyTorch设计一个CIFAR10数据集的分类模型</strong><br><strong><a href=\"https://github.com/Trouble404/Object-Detection-System-Based-on-CNN-and-Capsule-Network/blob/master/Learned%20skills/pytorch.ipynb\" target=\"_blank\" rel=\"noopener\">code</a></strong></p>\n</li>\n<li><p><strong>MMdnn</strong><br>MMdnn is a set of tools to help users inter-operate among different deep learning frameworks. E.g. model conversion and visualization. Convert models between Caffe, Keras, MXNet, Tensorflow, CNTK, PyTorch Onnx and CoreML.</p>\n<p><img src=\"https://raw.githubusercontent.com/Microsoft/MMdnn/master/docs/supported.jpg\" alt=\"iamge\"></p>\n<p>MMdnn主要有以下特征：</p>\n<ul>\n<li>模型文件转换器，不同的框架间转换DNN模型</li>\n<li>模型代码片段生成器，生成适合不同框架的代码</li>\n<li>模型可视化，DNN网络结构和框架参数可视化</li>\n<li>模型兼容性测试（正在进行中）</li>\n</ul>\n<p><strong><a href=\"https://github.com/Microsoft/MMdnn\" target=\"_blank\" rel=\"noopener\">Github</a></strong></p>\n</li>\n</ul>\n<h3 id=\"【04-06-2018】\"><a href=\"#【04-06-2018】\" class=\"headerlink\" title=\"【04/06/2018】\"></a>【04/06/2018】</h3><h4 id=\"Dataset\"><a href=\"#Dataset\" class=\"headerlink\" title=\"Dataset:\"></a><strong>Dataset:</strong></h4><p> <strong><a href=\"http://host.robots.ox.ac.uk/pascal/VOC/voc2012/index.html\" target=\"_blank\" rel=\"noopener\">VOC 2012 Dataset</a></strong></p>\n<h4 id=\"Introduce\"><a href=\"#Introduce\" class=\"headerlink\" title=\"Introduce:\"></a><strong>Introduce:</strong></h4><p> <strong>Visual Object Classes Challenge 2012 (VOC2012)</strong><br><a href=\"http://host.robots.ox.ac.uk/pascal/VOC/\" target=\"_blank\" rel=\"noopener\">PASCAL</a>‘s full name is Pattern Analysis, Statistical Modelling and Computational Learning.<br>VOC’s full name is <strong>Visual OBject Classes</strong>.<br>The first competition was held in 2005 and terminated in 2012. I will use the last updated dataset which is <a href=\"http://host.robots.ox.ac.uk/pascal/VOC/voc2012/index.html\" target=\"_blank\" rel=\"noopener\">VOC2012</a> dataset.</p>\n<p>The main aim of this competition is object detection, there are 20 classes objects in the dataset:</p>\n<ul>\n<li>person</li>\n<li>bird, cat, cow, dog, horse, sheep</li>\n<li>aeroplane, bicycle, boat, bus, car, motorbike, train</li>\n<li>bottle, chair, dining table, potted plant, sofa, tv/monitor</li>\n</ul>\n<h4 id=\"Detection-Task\"><a href=\"#Detection-Task\" class=\"headerlink\" title=\"Detection Task\"></a><strong>Detection Task</strong></h4><p>Referenced:<br><strong>The PASCAL Visual Object Classes Challenge 2012 (VOC2012) Development Kit</strong><br><strong>Mark Everingham - John Winn</strong><br><a href=\"http://host.robots.ox.ac.uk/pascal/VOC/voc2012/htmldoc/index.html\" target=\"_blank\" rel=\"noopener\">http://host.robots.ox.ac.uk/pascal/VOC/voc2012/htmldoc/index.html</a></p>\n<p><strong>Task:</strong><br>For each of the twenty classes predict the bounding boxes of each object of that class in a test image (if any). Each bounding box should be output with an associated real-valued confidence of the detection so that a precision/recall curve can be drawn. Participants may choose to tackle all, or any subset of object classes, for example ‘cars only’ or ‘motorbikes and cars’.</p>\n<p><strong>Competitions</strong>:<br>Two competitions are defined according to the choice of training data:</p>\n<ul>\n<li>taken from the $VOC_{trainval}$ data provided.</li>\n<li>from any source excluding the $VOC_{test}$ data provided.</li>\n</ul>\n<p><strong>Submission of Results</strong>:<br>A separate text file of results should be generated for each competition and each class e.g. `car’. Each line should be a detection output by the detector in the following format:<br>    <figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;image identifier&gt; &lt;confidence&gt; &lt;left&gt; &lt;top&gt; &lt;right&gt; &lt;bottom&gt;</span><br></pre></td></tr></table></figure></p>\n<p>where (left,top)-(right,bottom) defines the bounding box of the detected object. The top-left pixel in the image has coordinates $(1,1)$. Greater confidence values signify greater confidence that the detection is correct. An example file excerpt is shown below. Note that for the image 2009_000032, multiple objects are detected:<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">comp3_det_test_car.txt:</span><br><span class=\"line\">    ...</span><br><span class=\"line\">    2009_000026 0.949297 172.000000 233.000000 191.000000 248.000000</span><br><span class=\"line\">    2009_000032 0.013737 1.000000 147.000000 114.000000 242.000000</span><br><span class=\"line\">    2009_000032 0.013737 1.000000 134.000000 94.000000 168.000000</span><br><span class=\"line\">    2009_000035 0.063948 455.000000 229.000000 491.000000 243.000000</span><br><span class=\"line\">    ...</span><br></pre></td></tr></table></figure></p>\n<p><strong>Evaluation</strong>:<br>The detection task will be judged by the precision/recall curve. The principal quantitative measure used will be the average precision (AP). Detections are considered true or false positives based on the area of overlap with ground truth bounding boxes. To be considered a correct detection, the area of overlap $a_o$ between the predicted bounding box $B_p$ and ground truth bounding box $B_{gt}$ must exceed $50\\%$ by the formula: </p>\n<center> $a_o = \\frac{area(B_p \\cap B_{gt})}{area(B_p \\cup B_{gt})}$ </center>\n\n<h4 id=\"XML标注格式\"><a href=\"#XML标注格式\" class=\"headerlink\" title=\"XML标注格式\"></a><strong>XML标注格式</strong></h4><p> 对于目标检测来说，每一张图片对应一个xml格式的标注文件。所以你会猜到，就像gemfield准备的训练集有8万张照片一样，在存放xml文件的目录里，这里也将会有8万个xml文件。下面是其中一个xml文件的示例：<br> <figure class=\"highlight html\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"> &lt;?xml version=\"1.0\" encoding=\"utf-8\"?&gt;</span><br><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">annotation</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">folder</span>&gt;</span>VOC2007<span class=\"tag\">&lt;/<span class=\"name\">folder</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">filename</span>&gt;</span>test100.mp4_3380.jpeg<span class=\"tag\">&lt;/<span class=\"name\">filename</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">size</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">width</span>&gt;</span>1280<span class=\"tag\">&lt;/<span class=\"name\">width</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">height</span>&gt;</span>720<span class=\"tag\">&lt;/<span class=\"name\">height</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">depth</span>&gt;</span>3<span class=\"tag\">&lt;/<span class=\"name\">depth</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;/<span class=\"name\">size</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">object</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>gemfield<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">bndbox</span>&gt;</span></span><br><span class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">xmin</span>&gt;</span>549<span class=\"tag\">&lt;/<span class=\"name\">xmin</span>&gt;</span></span><br><span class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">xmax</span>&gt;</span>715<span class=\"tag\">&lt;/<span class=\"name\">xmax</span>&gt;</span></span><br><span class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">ymin</span>&gt;</span>257<span class=\"tag\">&lt;/<span class=\"name\">ymin</span>&gt;</span></span><br><span class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">ymax</span>&gt;</span>289<span class=\"tag\">&lt;/<span class=\"name\">ymax</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;/<span class=\"name\">bndbox</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">truncated</span>&gt;</span>0<span class=\"tag\">&lt;/<span class=\"name\">truncated</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">difficult</span>&gt;</span>0<span class=\"tag\">&lt;/<span class=\"name\">difficult</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;/<span class=\"name\">object</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">object</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>civilnet<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">bndbox</span>&gt;</span></span><br><span class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">xmin</span>&gt;</span>842<span class=\"tag\">&lt;/<span class=\"name\">xmin</span>&gt;</span></span><br><span class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">xmax</span>&gt;</span>1009<span class=\"tag\">&lt;/<span class=\"name\">xmax</span>&gt;</span></span><br><span class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">ymin</span>&gt;</span>138<span class=\"tag\">&lt;/<span class=\"name\">ymin</span>&gt;</span></span><br><span class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">ymax</span>&gt;</span>171<span class=\"tag\">&lt;/<span class=\"name\">ymax</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;/<span class=\"name\">bndbox</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">truncated</span>&gt;</span>0<span class=\"tag\">&lt;/<span class=\"name\">truncated</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">difficult</span>&gt;</span>0<span class=\"tag\">&lt;/<span class=\"name\">difficult</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;/<span class=\"name\">object</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">segmented</span>&gt;</span>0<span class=\"tag\">&lt;/<span class=\"name\">segmented</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">annotation</span>&gt;</span></span><br></pre></td></tr></table></figure></p>\n<p>在这个测试图片上，我们标注了2个object，一个是gemfield，另一个是civilnet。</p>\n<p>在这个xml例子中：</p>\n<ul>\n<li>bndbox是一个轴对齐的矩形，它框住的是目标在照片中的可见部分；</li>\n<li>truncated表明这个目标因为各种原因没有被框完整（被截断了），比如说一辆车有一部分在画面外；</li>\n<li>occluded是说一个目标的重要部分被遮挡了（不管是被背景的什么东西，还是被另一个待检测目标遮挡）；</li>\n<li>difficult表明这个待检测目标很难识别，有可能是虽然视觉上很清楚，但是没有上下文的话还是很难确认它属于哪个分类；标为difficult的目标在测试成绩的评估中一般会被忽略。</li>\n</ul>\n<p><strong>注意：在一个object中，name 标签要放在前面，否则的话，目标检测的一个重要工程实现SSD会出现解析数据集错误（另一个重要工程实现py-faster-rcnn则不会）。</strong></p>\n<h3 id=\"【07-06-2018】\"><a href=\"#【07-06-2018】\" class=\"headerlink\" title=\"【07/06/2018】\"></a>【07/06/2018】</h3><h4 id=\"Poster-conference\"><a href=\"#Poster-conference\" class=\"headerlink\" title=\"Poster conference\"></a><strong>Poster conference</strong></h4><p><img src=\"https://raw.githubusercontent.com/Trouble404/Object-Detection-System-Based-on-CNN-and-Capsule-Network/master/Poster/poster.png\" alt=\"iamge\"></p>\n<p>5 People in one group to present their object.<br>I present this object to my supervisor in this conference.</p>\n<h3 id=\"【11-06-2018】\"><a href=\"#【11-06-2018】\" class=\"headerlink\" title=\"【11/06/2018】\"></a>【11/06/2018】</h3><h4 id=\"R-CNN\"><a href=\"#R-CNN\" class=\"headerlink\" title=\"R-CNN\"></a><strong>R-CNN</strong></h4><p>Paper: <a href=\"https://arxiv.org/abs/1311.2524\" target=\"_blank\" rel=\"noopener\">Rich feature hierarchies for accurate object detection and semantic segmentation</a></p>\n<p>【<strong>论文主要特点</strong>】（相对传统方法的改进）</p>\n<ul>\n<li>速度： 经典的目标检测算法使用滑动窗法依次判断所有可能的区域。本文则(采用Selective Search方法)预先提取一系列较可能是物体的候选区域，之后仅在这些候选区域上(采用CNN)提取特征，进行判断。</li>\n<li>训练集： 经典的目标检测算法在区域中提取人工设定的特征。本文则采用深度网络进行特征提取。使用两个数据库： 一个较大的识别库   （ImageNet ILSVC 2012）：标定每张图片中物体的类别。一千万图像，1000类。 一个较小的检测库（PASCAL VOC 2007）：标定每张   图片中，物体的类别和位置，一万图像，20类。 本文使用识别库进行预训练得到CNN（有监督预训练），而后用检测库调优参数，最后在   检测库上评测。</li>\n</ul>\n<p>【<strong>流程</strong>】</p>\n<ol>\n<li>候选区域生成： 一张图像生成1K~2K个候选区域 （采用Selective Search 方法）</li>\n<li>特征提取： 对每个候选区域，使用深度卷积网络提取特征 （CNN） </li>\n<li>类别判断： 特征送入每一类的SVM 分类器，判别是否属于该类</li>\n<li>位置精修： 使用回归器精细修正候选框位置<center><img src=\"https://raw.githubusercontent.com/Trouble404/Object-Detection-System-Based-on-CNN-and-Capsule-Network/master/Research%20Review/LaTex/1.PNG\" alt=\"image\"></center>\n\n</li>\n</ol>\n<p>【<strong><a href=\"https://www.koen.me/research/pub/uijlings-ijcv2013-draft.pdf\" target=\"_blank\" rel=\"noopener\">Selective Search</a></strong>】</p>\n<ol>\n<li>使用一种过分割手段，将图像分割成小区域 (1k~2k 个)</li>\n<li>查看现有小区域，按照合并规则合并可能性最高的相邻两个区域。重复直到整张图像合并成一个区域位置</li>\n<li>输出所有曾经存在过的区域，所谓候选区域<br>其中合并规则如下： 优先合并以下四种区域：<ul>\n<li>颜色（颜色直方图）相近的</li>\n<li>纹理（梯度直方图）相近的</li>\n<li>合并后总面积小的： 保证合并操作的尺度较为均匀，避免一个大区域陆续“吃掉”其他小区域 （例：设有区域a-b-c-d-e-f-g-h。较好的合并方式是：ab-cd-ef-gh -&gt; abcd-efgh -&gt; abcdefgh。 不好的合并方法是：ab-c-d-e-f-g-h -&gt;abcd-e-f-g-h -&gt;abcdef-gh -&gt; abcdefgh）</li>\n<li>合并后，总面积在其BBOX中所占比例大的： 保证合并后形状规则。</li>\n</ul>\n</li>\n</ol>\n<h3 id=\"【12-06-2018】\"><a href=\"#【12-06-2018】\" class=\"headerlink\" title=\"【12/06/2018】\"></a>【12/06/2018】</h3><h4 id=\"SPP-CNN\"><a href=\"#SPP-CNN\" class=\"headerlink\" title=\"SPP-CNN\"></a><strong>SPP-CNN</strong></h4><p>Paper: <a href=\"https://arxiv.org/abs/1406.4729\" target=\"_blank\" rel=\"noopener\">Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition</a></p>\n<p>【<strong>论文主要特点</strong>】（相对传统方法的改进）</p>\n<p>RCNN使用CNN作为特征提取器，首次使得目标检测跨入深度学习的阶段。但是RCNN对于每一个区域候选都需要首先将图片放缩到固定的尺寸（224*224），然后为每个区域候选提取CNN特征。容易看出这里面存在的一些性能瓶颈：</p>\n<ul>\n<li>速度瓶颈：重复为每个region proposal提取特征是极其费时的，Selective Search对于每幅图片产生2K左右个region proposal，也就是意味着一幅图片需要经过2K次的完整的CNN计算得到最终的结果。</li>\n<li>性能瓶颈：对于所有的region proposal防缩到固定的尺寸会导致我们不期望看到的几何形变，而且由于速度瓶颈的存在，不可能采用多尺度或者是大量的数据增强去训练模型。</li>\n</ul>\n<p>【<strong>流程</strong>】</p>\n<ol>\n<li>首先通过selective search产生一系列的region proposal</li>\n<li>然后训练多尺寸识别网络用以提取区域特征，其中处理方法是每个尺寸的最短边大小在尺寸集合中：<br>$s \\in S = {480,576,688,864,1200}$<br>训练的时候通过上面提到的多尺寸训练方法，也就是在每个epoch中首先训练一个尺寸产生一个model，然后加载这个model并训练第二个尺寸，直到训练完所有的尺寸。空间金字塔池化使用的尺度为：1*1，2*2，3*3，6*6，一共是50个bins。</li>\n<li>在测试时，每个region proposal选择能使其包含的像素个数最接近224*224的尺寸，提取相 应特征。</li>\n<li>训练SVM，BoundingBox回归.<center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/SPP-NET.jpg\" alt=\"image\"></center>\n\n\n</li>\n</ol>\n<h3 id=\"【13-06-2018】\"><a href=\"#【13-06-2018】\" class=\"headerlink\" title=\"【13/06/2018】\"></a>【13/06/2018】</h3><h4 id=\"FAST-R-CNN\"><a href=\"#FAST-R-CNN\" class=\"headerlink\" title=\"FAST R-CNN\"></a><strong>FAST R-CNN</strong></h4><p>Paper: <a href=\"https://arxiv.org/abs/1504.08083\" target=\"_blank\" rel=\"noopener\">Fast R-CNN</a></p>\n<p>【<strong>论文主要特点</strong>】（相对传统方法的改进）</p>\n<ul>\n<li>测试时速度慢：RCNN一张图像内候选框之间大量重叠，提取特征操作冗余。本文将整张图像归一化后直接送入深度网络。在邻接时，才加入候选框信息，在末尾的少数几层处理每个候选框。</li>\n<li>训练时速度慢 ：原因同上。在训练时，本文先一张图像送入网络，紧接着送入从这幅图像上提取出的候选区域。这些候选区域的前几层特征不需要再重复计算。</li>\n<li>训练所需空间大: RCNN中独立的分类器和回归器需要大量特征作为训练样本。本文把类别判断和位置精调统一用深度网络实现，不再需要额外存储。</li>\n</ul>\n<p>【<strong>流程</strong>】</p>\n<ol>\n<li>网络首先用几个卷积层（conv）和最大池化层处理整个图像以产生conv特征图。</li>\n<li>然后，对于每个对象建议框（object proposals ），感兴趣区域（region of interest——RoI）池层从特征图提取固定长度的特征向量。</li>\n<li>每个特征向量被输送到分支成两个同级输出层的全连接（fc）层序列中：<br>其中一层进行分类，对 目标关于K个对象类（包括全部“背景background”类）产生softmax概率估计，即输出每一个RoI的概率分布；<br>另一层进行bbox regression，输出K个对象类中每一个类的四个实数值。每4个值编码K个类中的每个类的精确边界盒（bounding-box）位置，即输出每一个种类的的边界盒回归偏差。整个结构是使用多任务损失的端到端训练（trained end-to-end with a multi-task loss）。<center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/fast-rcnn.png\" alt=\"image\"></center>\n\n</li>\n</ol>\n<h3 id=\"【14-18-06-2018】\"><a href=\"#【14-18-06-2018】\" class=\"headerlink\" title=\"【14~18/06/2018】\"></a>【14~18/06/2018】</h3><h4 id=\"FASTER-R-CNN\"><a href=\"#FASTER-R-CNN\" class=\"headerlink\" title=\"FASTER R-CNN\"></a><strong>FASTER R-CNN</strong></h4><p>I want to use <strong>Faster R-cnn</strong> as the first method to implement object detection system.</p>\n<p>Paper: <a href=\"https://arxiv.org/abs/1506.01497\" target=\"_blank\" rel=\"noopener\">Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks</a></p>\n<p>在结构上，Faster RCNN已经将特征抽取(feature extraction)，proposal提取，bounding box regression(rect refine)，classification都整合在了一个网络中，使得综合性能有较大提高，在检测速度方面尤为明显。</p>\n<center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster-rcnn.jpg\" alt=\"image\"></center>\n\n<h4 id=\"流程\"><a href=\"#流程\" class=\"headerlink\" title=\"流程\"></a>流程</h4><ol>\n<li>Conv layers：作为一种CNN网络目标检测方法，Faster R-CNN首先使用一组基础的conv+relu+pooling层提取image的feature maps。该feature maps被共享用于后续RPN层和全连接层。</li>\n<li>Region Proposal Networks：RPN网络用于生成region proposals。该层通过softmax判断anchors属于foreground或者background，再利用bounding box regression修正anchors获得精确的proposals。</li>\n<li>Roi Pooling：该层收集输入的feature maps和proposals，综合这些信息后提取proposal feature maps，送入后续全连接层判定目标类别。</li>\n<li>Classification：利用proposal feature maps计算proposal的类别，同时再次bounding box regression获得检测框最终的精确位置。</li>\n</ol>\n<h4 id=\"解释\"><a href=\"#解释\" class=\"headerlink\" title=\"解释\"></a>解释</h4><p><strong>[1. Conv layers]</strong><br>   <center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_1.jpg\" alt=\"image\"></center><br>   Conv layers包含了conv，pooling，relu三种层。以python版本中的VGG16模型中的faster_rcnn_test.pt的网络结构为例，如图,    Conv layers部分共有13个conv层，13个relu层，4个pooling层。这里有一个非常容易被忽略但是又无比重要的信息，在Conv          layers中：</p>\n<ul>\n<li>所有的conv层都是： $kernel_size=3$ ， $pad=1$ ， $stride=1$ <br></li>\n<li><p>所有的pooling层都是： $kernel_size=2$ ， $pad=0$ ， $stride=2$</p>\n<p>为何重要？在Faster RCNN Conv layers中对所有的卷积都做了扩边处理（ $pad=1$ ，即填充一圈0），导致原图变为                $(M+2)\\times (N+2)$ 大小，再做3x3卷积后输出 $M\\times N$ 。正是这种设置，导致Conv layers中的conv层不改变输入和输出    矩阵大小。如下图：<br><center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_2.jpg\" alt=\"image\"></center><br>类似的是，Conv layers中的pooling层 $kernel_size=2$ ， $stride=2$ 。这样每个经过pooling层的 $M\\times N$ 矩阵，都会变为 $(M/2) \\times(N/2)$ 大小。综上所述，在整个Conv layers中，conv和relu层不改变输入输出大小，只有pooling层使输出长宽都变为输入的1/2。<br>那么，一个 $M\\times N$ 大小的矩阵经过Conv layers固定变为 $(M/16)\\times (N/16)$ ！这样Conv layers生成的featuure map中都可以和原图对应起来。</p>\n</li>\n</ul>\n<p><strong>[2. Region Proposal Networks(RPN)]</strong><br>   经典的检测方法生成检测框都非常耗时，如OpenCV adaboost使用滑动窗口+图像金字塔生成检测框；或如R-CNN使用SS(Selective      Search)方法生成检测框。而Faster RCNN则抛弃了传统的滑动窗口和SS方法，直接使用RPN生成检测框，这也是Faster R-CNN的巨大    优势，能极大提升检测框的生成速度。<br>   <center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_3.jpg\" alt=\"image\"></center><br>   上图展示了RPN网络的具体结构。可以看到RPN网络实际分为2条线，上面一条通过softmax分类anchors获得foreground和              background（检测目标是foreground），下面一条用于计算对于anchors的bounding box regression偏移量，以获得精确的          proposal。而最后的Proposal层则负责综合foreground anchors和bounding box regression偏移量获取proposals，同时剔除太    小和超出边界的proposals。其实整个网络到了Proposal Layer这里，就完成了相当于目标定位的功能。</p>\n<p>   <strong>2.1 多通道图像卷积基础知识介绍</strong></p>\n<ul>\n<li>对于单通道图像+单卷积核做卷积，之前展示了；</li>\n<li><p>对于多通道图像+多卷积核做卷积，计算方式如下：</p>\n<center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_4.jpg\" alt=\"image\"></center><br>输入有3个通道，同时有2个卷积核。对于每个卷积核，先在输入3个通道分别作卷积，再将3个通道结果加起来得到卷积输出。所以对     于某个卷积层，无论输入图像有多少个通道，输出图像通道数总是等于卷积核数量！<br>对多通道图像做 $1\\times1$ 卷积，其实就是将输入图像于每个通道乘以卷积系数后加在一起，即相当于把原图像中本来各个独立的     通道“联通”在了一起。<br><br><strong>2.2 Anchors</strong><br>提到RPN网络，就不能不说anchors。所谓anchors，实际上就是一组由rpn/generate_anchors.py生成的矩形。直接运行作者demo中    的generate_anchors.py可以得到以下输出：<br>[[ -84.  -40.   99.   55.]<br>[-176.  -88.  191.  103.]<br>[-360. -184.  375.  199.]<br>[ -56.  -56.   71.   71.]<br>[-120. -120.  135.  135.]<br>[-248. -248.  263.  263.]<br>[ -36.  -80.   51.   95.]<br>[ -80. -168.   95.  183.]<br>[-168. -344.  183.  359.]]<br><br>其中每行的4个值 $(x1,y1,x2,y2)$ 代表矩形左上和右下角点坐标。9个矩形共有3种形状，长宽比为大约为 $width:height = [1:1, 1:2, 2:1]$ 三种，如下图。实际上通过anchors就引入了检测中常用到的多尺度方法。<br><center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_5.jpg\" alt=\"image\"></center><br>注：关于上面的anchors size，其实是根据检测图像设置的。在python demo中，会把任意大小的输入图像reshape成 $800\\times600$。再回头来看anchors的大小，anchors中长宽 1:2 中最大为 $352\\times704$ ，长宽 2:1 中最大 $736\\times384$ ，基本是cover了 $800\\times600$ 的各个尺度和形状。<br>那么这9个anchors是做什么的呢？借用Faster RCNN论文中的原图，如下图，遍历Conv layers计算获得的feature maps，为每一个点都配备这9种anchors作为初始的检测框。这样做获得检测框很不准确，不用担心，后面还有2次bounding box regression可以修正检测框位置。<br><center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_6.jpg\" alt=\"image\"></center>\n\n<p>解释一下上面这张图的数字。</p>\n</li>\n</ul>\n<ul>\n<li>在原文中使用的是ZF model中，其Conv Layers中最后的conv5层num_output=256，对应生成256张特征图，所以相当于feature map每个点都是256-dimensions</li>\n<li>在conv5之后，做了rpn_conv/3x3卷积且num_output=256，相当于每个点又融合了周围3x3的空间信息（猜测这样做也许更鲁棒？反正我没测试），同时256-d不变（如图4和图7中的红框）</li>\n<li>假设在conv5 feature map中每个点上有k个anchor（默认k=9），而每个anhcor要分foreground和background，所以每个点由256d feature转化为cls=2k scores；而每个anchor都有[x, y, w, h]对应4个偏移量，所以reg=4k coordinates</li>\n<li><p>补充一点，全部anchors拿去训练太多了，训练程序会在合适的anchors中随机选取128个postive anchors+128个negative anchors进行训练（什么是合适的anchors下文5.1有解释）</p>\n<p> <strong>2.3 softmax判定foreground与background</strong><br> 一副MxN大小的矩阵送入Faster RCNN网络后，到RPN网络变为(M/16)x(N/16)，不妨设 W=M/16 ， H=N/16 。在进入reshape与softmax之前，先做了1x1卷积，如下图：<br> <center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_7.jpg\" alt=\"image\"></center><br> 该1x1卷积的caffe prototxt定义如下：<br> <center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_19.PNG\" alt=\"image\"></center><br>可以看到其num_output=18，也就是经过该卷积的输出图像为 $W\\times H \\times 18$ 大小（注意第二章开头提到的卷积计算方式）。这也就刚好对应了feature maps每一个点都有9个anchors，同时每个anchors又有可能是foreground和background，所有这些信息都保存 $W\\times H\\times (9\\cdot2)$ 大小的矩阵。为何这样做？后面接softmax分类获得foreground anchors，也就相当于初步提取了检测目标候选区域box（一般认为目标在foreground anchors中）。<br>综上所述，RPN网络中利用anchors和softmax初步提取出foreground anchors作为候选区域。</p>\n<p> <strong>2.4 bounding box regression原理</strong><br>如图所示绿色框为飞机的Ground Truth(GT)，红色为提取的foreground anchors，即便红色的框被分类器识别为飞机，但是由于红色的框定位不准，这张图相当于没有正确的检测出飞机。所以我们希望采用一种方法对红色的框进行微调，使得foreground anchors和GT更加接近。<br><center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_8.jpg\" alt=\"image\"></center><br>对于窗口一般使用四维向量 (x, y, w, h) 表示，分别表示窗口的中心点坐标和宽高。对于下图，红色的框A代表原始的Foreground Anchors，绿色的框G代表目标的GT，我们的目标是寻找一种关系，使得输入原始的anchor A经过映射得到一个跟真实窗口G更接近的回归窗口G’，即：</p>\n</li>\n<li>给定：$anchor A=(A_{x}, A_{y}, A_{w}, A_{h})$ 和 $GT=[G_{x}, G_{y}, G_{w}, G_{h}]$</li>\n<li><p>寻找一种变换F，使得：$F(A_{x}, A_{y}, A_{w}, A_{h})=(G_{x}^{‘}, G_{y}^{‘}, G_{w}^{‘}, G_{h}^{‘})$，其中 $(G_{x}^{‘}, G_{y}^{‘}, G_{w}^{‘}, G_{h}^{‘}) \\approx (G_{x}, G_{y}, G_{w}, G_{h})$<br><center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_9.jpg\" alt=\"image\"></center><br>那么经过何种变换F才能从图10中的anchor A变为G’呢？ 比较简单的思路就是:</p>\n</li>\n<li><p>先做平移</p>\n<center><br>$G^{‘}<em>{x} = A</em>{w} \\cdot d_{x}(A) + A_{x} $<br>$G^{‘}<em>{y} = A</em>{y} \\cdot d_{y}(A) + A_{y} $<br></center></li>\n<li>再做缩放<center><br>$G^{‘}<em>{w} = A</em>{w} \\cdot exp(d_{w}(A)) $<br>$G^{‘}<em>{h} = A</em>{h} \\cdot exp(d_{h}(A)) $<br></center>\n\n</li>\n</ul>\n<p>观察上面4个公式发现，需要学习的是 $d_{x}(A),d_{y}(A),d_{w}(A),d_{h}(A)$ 这四个变换。当输入的anchor A与GT相差较小时，可以认为这种变换是一种线性变换， 那么就可以用线性回归来建模对窗口进行微调（注意，只有当anchors A和GT比较接近时，才能使用线性回归模型，否则就是复杂的非线性问题了）。</p>\n<p>接下来的问题就是如何通过线性回归获得 $d_{x}(A),d_{y}(A),d_{w}(A),d_{h}(A)$ 了。线性回归就是给定输入的特征向量X, 学习一组参数W, 使得经过线性回归后的值跟真实值Y非常接近，即$Y=WX$。对于该问题，输入X是cnn feature map，定义为Φ；同时还有训练传入A与GT之间的变换量，即$(t_{x}, t_{y}, t_{w}, t_{h})$。输出是$d_{x}(A),d_{y}(A),d_{w}(A),d_{h}(A)$四个变换。那么目标函数可以表示为：</p>\n<center><br>$d_{<em>}(A) = w^{T}_{</em>} \\cdot \\phi(A)$<br></center>\n\n<p>其中Φ(A)是对应anchor的feature map组成的特征向量，w是需要学习的参数，d(A)是得到的预测值（*表示 x，y，w，h，也就是每一个变换对应一个上述目标函数）。为了让预测值$(t_{x}, t_{y}, t_{w}, t_{h})$与真实值差距最小，设计损失函数：</p>\n<center><br>$Loss = \\sum^{N}<em>{i}(t^{i}</em>{<em>} - \\hat{w}^{T}_{</em>} \\cdot \\phi(A^{i}))^{2}$<br></center><br>函数优化目标为：<br><center><br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_20.jpg\" alt=\"image\"><br></center><br>需要说明，只有在GT与需要回归框位置比较接近时，才可近似认为上述线性变换成立。<br>说完原理，对应于Faster RCNN原文，foreground anchor与ground truth之间的平移量 $(t_x, t_y)$ 与尺度因子 $(t_w, t_h)$ 如下：<br><center><br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_21.jpg\" alt=\"image\"><br></center><br>对于训练bouding box regression网络回归分支，输入是cnn feature Φ，监督信号是Anchor与GT的差距 $(t_x, t_y, t_w, t_h)$，即训练目标是：输入Φ的情况下使网络输出与监督信号尽可能接近。<br>那么当bouding box regression工作时，再输入Φ时，回归网络分支的输出就是每个Anchor的平移量和变换尺度 $(t_x, t_y, t_w, t_h)$，显然即可用来修正Anchor位置了。<br><br>   <strong>2.5 对proposals进行bounding box regression</strong><br>在了解bounding box regression后，再回头来看RPN网络第二条线路，如下图。<br><center><br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_10.jpg\" alt=\"image\"><br></center><br>其 $num_output=36$ ，即经过该卷积输出图像为 $W\\times H\\times 36$ ，在caffe blob存储为 [1, 36, H, W] ，这里相当于feature maps每个点都有9个anchors，每个anchors又都有4个用于回归的$d_{x}(A),d_{y}(A),d_{w}(A),d_{h}(A)$变换量。<br><br>   <strong>2.6 Proposal Layer</strong><br>Proposal Layer负责综合所有 $d_{x}(A),d_{y}(A),d_{w}(A),d_{h}(A)$ 变换量和foreground anchors，计算出精准的proposal，送入后续RoI Pooling Layer。<br>首先解释im_info。对于一副任意大小PxQ图像，传入Faster RCNN前首先reshape到固定 $M\\times N$ ，im_info=[M, N, scale_factor]则保存了此次缩放的所有信息。然后经过Conv Layers，经过4次pooling变为 $W\\times H=(M/16)\\times(N/16)$ 大小，其中feature_stride=16则保存了该信息，用于计算anchor偏移量。<br><center><br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_11.jpg\" alt=\"image\"><br></center>\n\n<p>Proposal Layer forward（caffe layer的前传函数）按照以下顺序依次处理：</p>\n<ol>\n<li>生成anchors，利用$[d_{x}(A),d_{y}(A),d_{w}(A),d_{h}(A)]$对所有的anchors做bbox regression回归（这里的anchors生成和训练时完全一致）</li>\n<li>按照输入的foreground softmax scores由大到小排序anchors，提取前pre_nms_topN(e.g. 6000)个anchors，即提取修正位置后的foreground anchors。</li>\n<li>限定超出图像边界的foreground anchors为图像边界（防止后续roi pooling时proposal超出图像边界）</li>\n<li>剔除非常小（width&lt;threshold or height&lt;threshold）的foreground anchors</li>\n<li>进行nonmaximum suppression</li>\n<li>再次按照nms后的foreground softmax scores由大到小排序fg anchors，提取前post_nms_topN(e.g. 300)结果作为proposal输出。</li>\n</ol>\n<p>之后输出 proposal=[x1, y1, x2, y2] ，注意，由于在第三步中将anchors映射回原图判断是否超出边界，所以这里输出的proposal是对应 $M\\times N$ 输入图像尺度的，这点在后续网络中有用。另外我认为，严格意义上的检测应该到此就结束了，后续部分应该属于识别了~   </p>\n<p><strong>RPN</strong>网络结构就介绍到这里，总结起来就是：<br><strong>生成anchors -&gt; softmax分类器提取fg anchors -&gt; bbox reg回归fg anchors -&gt; Proposal Layer生成proposals</strong></p>\n<h3 id=\"【19-06-2018】\"><a href=\"#【19-06-2018】\" class=\"headerlink\" title=\"【19/06/2018】\"></a>【19/06/2018】</h3><h4 id=\"处理-XML-文档\"><a href=\"#处理-XML-文档\" class=\"headerlink\" title=\"处理 XML 文档\"></a>处理 XML 文档</h4><p>使用 xml.etree.ElementTree 这个包去解析XML文件， 并且整理成为list形式<br>【流程】</p>\n<ul>\n<li>读取XML文件</li>\n<li>区分训练集测试集根据竞赛要求</li>\n<li>解析XML文档收录到PYTHON词典中<br><center><br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_23.PNG\" alt=\"image\"><br></center><br>Github 的 jupyter notebook <a href=\"https://github.com/Trouble404/Object-Detection-System-Based-on-CNN-and-Capsule-Network/blob/master/Faster-RCNN/parser_voc2012_xml_and_plotting.ipynb\" target=\"_blank\" rel=\"noopener\">地址</a> </li>\n</ul>\n<p>训练集根据竞赛的 trainval.txt 文件给的图片作为训练集<br>其余的作为训练集</p>\n<p>解析后， 总共有 17125 张图片，<br>其中 11540 张作为训练集</p>\n<p>图片中的20个类的统计情况：</p>\n<center><br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_24.PNG\" alt=\"image\"><br></center> \n\n\n<h3 id=\"【20-06-2018】\"><a href=\"#【20-06-2018】\" class=\"headerlink\" title=\"【20/06/2018】\"></a>【20/06/2018】</h3><h4 id=\"根据信息画出BBOXES\"><a href=\"#根据信息画出BBOXES\" class=\"headerlink\" title=\"根据信息画出BBOXES\"></a>根据信息画出BBOXES</h4><p>安装 cv2 这个包<br>  <figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">pip install opencv-python</span><br></pre></td></tr></table></figure><br>注意： OpenCV-python 中颜色格式 是BGR 而不是 RGB</p>\n<p>在VOC2012数据集里面，总共有20类， 根据不同的种类用不同的颜色和唯一的编码画BBOXES。</p>\n<table>\n<thead>\n<tr>\n<th style=\"text-align:left\">class</th>\n<th style=\"text-align:left\">class_mapping</th>\n<th style=\"text-align:left\">BGR of bbox</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:left\">Person</td>\n<td style=\"text-align:left\">0</td>\n<td style=\"text-align:left\">(0, 0, 255)</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">Aeroplane</td>\n<td style=\"text-align:left\">1</td>\n<td style=\"text-align:left\">(0, 0, 255)</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">Tvmonitor</td>\n<td style=\"text-align:left\">2</td>\n<td style=\"text-align:left\">(0, 128, 0)</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">Train</td>\n<td style=\"text-align:left\">3</td>\n<td style=\"text-align:left\">(128, 128, 128)</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">Boat</td>\n<td style=\"text-align:left\">4</td>\n<td style=\"text-align:left\">(0, 165, 255)</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">Dog</td>\n<td style=\"text-align:left\">5</td>\n<td style=\"text-align:left\">(0, 255, 255)</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">Chair</td>\n<td style=\"text-align:left\">6</td>\n<td style=\"text-align:left\">(80, 127, 255)</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">Bird</td>\n<td style=\"text-align:left\">7</td>\n<td style=\"text-align:left\">(208, 224, 64)</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">Bicycle</td>\n<td style=\"text-align:left\">8</td>\n<td style=\"text-align:left\">(235, 206, 135)</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">Bottle</td>\n<td style=\"text-align:left\">9</td>\n<td style=\"text-align:left\">(128, 0, 0)</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">Sheep</td>\n<td style=\"text-align:left\">10</td>\n<td style=\"text-align:left\">(140, 180, 210)</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">Diningtable</td>\n<td style=\"text-align:left\">11</td>\n<td style=\"text-align:left\">(0, 255, 0)</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">Horse</td>\n<td style=\"text-align:left\">12</td>\n<td style=\"text-align:left\">(133, 21, 199)</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">Motorbike</td>\n<td style=\"text-align:left\">13</td>\n<td style=\"text-align:left\">(47, 107, 85)</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">Sofa</td>\n<td style=\"text-align:left\">14</td>\n<td style=\"text-align:left\">(19, 69, 139)</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">Cow</td>\n<td style=\"text-align:left\">15</td>\n<td style=\"text-align:left\">(222, 196, 176)</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">Car</td>\n<td style=\"text-align:left\">16</td>\n<td style=\"text-align:left\">(0, 0, 0)</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">Cat</td>\n<td style=\"text-align:left\">17</td>\n<td style=\"text-align:left\">(225, 105, 65)</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">Bus</td>\n<td style=\"text-align:left\">18</td>\n<td style=\"text-align:left\">(255, 255, 255)</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">Pottedplant</td>\n<td style=\"text-align:left\">19</td>\n<td style=\"text-align:left\">(205, 250, 255)</td>\n</tr>\n</tbody>\n</table>\n<p>我写了一个show_image_with_bbox函数去画出带BBOXES的图根据处理XML文件得到的list:</p>\n<center><br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_22.PNG\" alt=\"image\"><br></center><br>Github 的 jupyter notebook <a href=\"https://github.com/Trouble404/Object-Detection-System-Based-on-CNN-and-Capsule-Network/blob/master/Faster-RCNN/parser_voc2012_xml_and_plotting.ipynb\" target=\"_blank\" rel=\"noopener\">地址</a><br><br>EXAMPLE:<br><center><br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/img_with_bboex_1.PNG\" alt=\"image\"><br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/img_with_bboex_2.PNG\" alt=\"image\"><br></center>  \n\n<h3 id=\"【21-06-2018】\"><a href=\"#【21-06-2018】\" class=\"headerlink\" title=\"【21/06/2018】\"></a>【21/06/2018】</h3><h4 id=\"config-setting\"><a href=\"#config-setting\" class=\"headerlink\" title=\"config setting\"></a>config setting</h4><p>set config class:<br>                 for image enhancement:</p>\n<center><br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/21_1.PNG\" alt=\"image\"><br></center>  \n\n<h4 id=\"image-enhancement\"><a href=\"#image-enhancement\" class=\"headerlink\" title=\"image enhancement\"></a>image enhancement</h4><center><br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/21_2.PNG\" alt=\"image\"><br></center><br>According to the config of three peremeters, users could augment image with 3 different ways or using them all.<br>For horizontal and vertical flips, 1/3 probability to triggle<br>With 0,90,180,270 rotation,<br>This function could increase the number of datasets.<br><br>image flips and rotation are realized by opencv and replace of height and width<br>New cordinates of bboxes are calculated acccording to different change of image<br><br>detailed in Github, jupyter notebook: <a href=\"https://github.com/Trouble404/Object-Detection-System-Based-on-CNN-and-Capsule-Network/blob/master/Faster-RCNN/config_set_and_image_enhance.ipynb\" target=\"_blank\" rel=\"noopener\">address</a><br><br>Orignal image:<br><center><br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/21_3.PNG\" alt=\"image\"><br></center><br>horizontal flip:<br><center><br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/21_4.PNG\" alt=\"image\"><br></center><br>Vertical filp:<br><center><br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/21_5.PNG\" alt=\"image\"><br></center><br>Random rotation:<br><center><br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/21_6.PNG\" alt=\"image\"><br></center><br>Horizontal and then vertical flips:<br><center><br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/21_7.PNG\" alt=\"image\"><br></center>  \n\n<h3 id=\"【22-06-2018】\"><a href=\"#【22-06-2018】\" class=\"headerlink\" title=\"【22/06/2018】\"></a>【22/06/2018】</h3><h4 id=\"Image-rezise\"><a href=\"#Image-rezise\" class=\"headerlink\" title=\"Image rezise\"></a>Image rezise</h4><p>This function is to rezise input image to a uniform size with same shortest side</p>\n<center><br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/22_2.PNG\" alt=\"image\"><br></center> \n\n<p>According to set the value of shortest side, convergent-divergent or augmented another side proportion</p>\n<p>Test:<br>Left image is resized image, in this case, the orignal image amplified.</p>\n<center><br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/22_1.png\" alt=\"image\"><br></center> \n\n<h4 id=\"Class-Balance\"><a href=\"#Class-Balance\" class=\"headerlink\" title=\"Class Balance\"></a>Class Balance</h4><p>When training the model, if we sent image with no repeating classes, it may help to improve the performance of model. Therefore, this function is to make sure no repeating classes in two closed input image.</p>\n<center><br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/22_3.PNG\" alt=\"image\"><br></center> \n\n<p>Test:</p>\n<p><center><br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/22_4.PNG\" alt=\"image\"><br></center><br>Random output 4 iamge with is function, it could find no repeating classes in two closed image. However, it may reduce the number of trainning image because skip some images.</p>\n<h3 id=\"【25-26-06-2018】\"><a href=\"#【25-26-06-2018】\" class=\"headerlink\" title=\"【25~26/06/2018】\"></a>【25~26/06/2018】</h3><h4 id=\"Region-Proposal-Networks-RPN\"><a href=\"#Region-Proposal-Networks-RPN\" class=\"headerlink\" title=\"Region Proposal Networks(RPN)\"></a>Region Proposal Networks(RPN)</h4><p><center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_3.jpg\" alt=\"image\"></center><br>可以看到RPN网络实际分为2条线，上面一条通过softmax分类anchors获得foreground和background（检测目标是foreground），下面一条用于计算对于anchors的bounding box regression偏移量，以获得精确的proposal。而最后的Proposal层则负责综合foreground anchors和bounding box regression偏移量获取proposals，同时剔除太小和超出边界的proposals。其实整个网络到了Proposal Layer这里，就完成了相当于目标定位的功能。</p>\n<h4 id=\"Anchors\"><a href=\"#Anchors\" class=\"headerlink\" title=\"Anchors\"></a>Anchors</h4><p>对每一个点生成的矩形</p>\n<p><center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_5.jpg\" alt=\"image\"></center><br>其中每行的4个值 (x1,y1,x2,y2) 代表矩形左上和右下角点坐标。9个矩形共有3种形状，长宽比为大约为 width:height = [1:1, 1:2, 2:1]</p>\n<p><center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_6.jpg\" alt=\"image\"></center><br>通过遍历Conv layers计算获得的feature maps，为每一个点都配备这9种anchors作为初始的检测框。这样做获得检测框很不准确，不用担心，后面还有2次bounding box regression可以修正检测框位置.</p>\n<h4 id=\"Code\"><a href=\"#Code\" class=\"headerlink\" title=\"Code\"></a>Code</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"string\">\"\"\" intersection of two bboxes</span></span><br><span class=\"line\"><span class=\"string\">@param ai: left top x,y and right bottom x,y coordinates of bbox 1</span></span><br><span class=\"line\"><span class=\"string\">@param bi: left top x,y and right bottom x,y coordinates of bbox 2</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">@return: area_union: whether contain target classes</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">intersection</span><span class=\"params\">(ai, bi)</span>:</span></span><br></pre></td></tr></table></figure>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"string\">\"\"\" union of two bboxes</span></span><br><span class=\"line\"><span class=\"string\">@param au: left top x,y and right bottom x,y coordinates of bbox 1</span></span><br><span class=\"line\"><span class=\"string\">@param bu: left top x,y and right bottom x,y coordinates of bbox 2</span></span><br><span class=\"line\"><span class=\"string\">@param area_intersection: intersection area</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">@return: area_union: whether contain target classes</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">union</span><span class=\"params\">(au, bu, area_intersection)</span>:</span></span><br></pre></td></tr></table></figure>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"string\">\"\"\" calculate ratio of intersection and union</span></span><br><span class=\"line\"><span class=\"string\">@param a: left top x,y and right bottom x,y coordinates of bbox 1</span></span><br><span class=\"line\"><span class=\"string\">@param b: left top x,y and right bottom x,y coordinates of bbox 2</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">@return: ratio of intersection and union of two bboxes</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">iou</span><span class=\"params\">(a, b)</span>:</span></span><br></pre></td></tr></table></figure>\n<p><strong>IOU is used to bounding box regression</strong></p>\n<hr>\n<p><strong> rpn calculation</strong></p>\n<ol>\n<li>Traversal all pre-anchors to calculate IOU with GT bboxes</li>\n<li>Set number and proprty of pre-anchors</li>\n<li>return specity number of result(Anchors)</li>\n</ol>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"string\">\"\"\" </span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">@param C: configuration</span></span><br><span class=\"line\"><span class=\"string\">@param img_data: parsered xml information</span></span><br><span class=\"line\"><span class=\"string\">@param width: orignal width of image</span></span><br><span class=\"line\"><span class=\"string\">@param hegiht: orignal height of image</span></span><br><span class=\"line\"><span class=\"string\">@param resized_width: resized width of image after image processing</span></span><br><span class=\"line\"><span class=\"string\">@param resized_heighth: resized height of image after image processing</span></span><br><span class=\"line\"><span class=\"string\">@param img_length_calc_function: Keras's image_dim_ordering function</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">@return: np.copy(y_rpn_cls): whether contain target classes</span></span><br><span class=\"line\"><span class=\"string\">@return: np.copy(y_rpn_regr): corrspoding return of gradient</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">\"\"\"</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">calc_rpn</span><span class=\"params\">(C, img_data, width, height, resized_width, resized_height, img_length_calc_function)</span>:</span></span><br></pre></td></tr></table></figure>\n<p>【注：其只会返回num_regions（这里设置为256）个有效的正负样本 】</p>\n<p>【流程】<br>Initialise paramters: see <a href=\"https://github.com/Trouble404/Object-Detection-System-Based-on-CNN-and-Capsule-Network/blob/master/Faster-RCNN/rpn_calculation.ipynb\" target=\"_blank\" rel=\"noopener\">jupyter notebook</a></p>\n<p>Calculate the size of map feature:<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">(output_width, output_height) = img_length_calc_function(resized_width, resized_height)</span><br></pre></td></tr></table></figure></p>\n<p><br><br>Get the GT box coordinates, and resize to account for image resizing<br>after rezised functon, the coordinates of bboxes need to re-calculation:<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">for</span> bbox_num, bbox <span class=\"keyword\">in</span> enumerate(img_data[<span class=\"string\">'bboxes'</span>]):</span><br><span class=\"line\">\tgta[bbox_num, <span class=\"number\">0</span>] = bbox[<span class=\"string\">'x1'</span>] * (resized_width / float(width))</span><br><span class=\"line\">\tgta[bbox_num, <span class=\"number\">1</span>] = bbox[<span class=\"string\">'x2'</span>] * (resized_width / float(width))</span><br><span class=\"line\">\tgta[bbox_num, <span class=\"number\">2</span>] = bbox[<span class=\"string\">'y1'</span>] * (resized_height / float(height))</span><br><span class=\"line\">\tgta[bbox_num, <span class=\"number\">3</span>] = bbox[<span class=\"string\">'y2'</span>] * (resized_height / float(height))</span><br></pre></td></tr></table></figure></p>\n<p>【注意gta的存储形式是（x1,x2,y1,y2）而不是（x1,y1,x2,y2）】<br><br><br>Traverse all possible group of sizes<br>anchor box scales [128, 256, 512]<br>anchor box ratios [1:1,1:2,2:1]<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">for</span> anchor_size_idx <span class=\"keyword\">in</span> range(len(anchor_sizes)):</span><br><span class=\"line\">\t<span class=\"keyword\">for</span> anchor_ratio_idx <span class=\"keyword\">in</span> range(len(anchor_ratios)):</span><br><span class=\"line\">\t\tanchor_x = anchor_sizes[anchor_size_idx] * anchor_ratios[anchor_ratio_idx][<span class=\"number\">0</span>]</span><br><span class=\"line\">\t\tanchor_y = anchor_sizes[anchor_size_idx] * anchor_ratios[anchor_ratio_idx][<span class=\"number\">1</span>]</span><br></pre></td></tr></table></figure></p>\n<p>Traver one bbox group, all pre boxes generated by anchors</p>\n<p>output_width，output_height：width and height of map feature<br>downscale：mapping ration, defualt 16<br>if to delete box out of iamge</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">for</span> ix <span class=\"keyword\">in</span> range(output_width):</span><br><span class=\"line\">\tx1_anc = downscale * (ix + <span class=\"number\">0.5</span>) - anchor_x / <span class=\"number\">2</span></span><br><span class=\"line\">\tx2_anc = downscale * (ix + <span class=\"number\">0.5</span>) + anchor_x / <span class=\"number\">2</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"keyword\">if</span> x1_anc &lt; <span class=\"number\">0</span> <span class=\"keyword\">or</span> x2_anc &gt; resized_width:</span><br><span class=\"line\">\t\t<span class=\"keyword\">continue</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"keyword\">for</span> jy <span class=\"keyword\">in</span> range(output_height):</span><br><span class=\"line\">\t\ty1_anc = downscale * (jy + <span class=\"number\">0.5</span>) - anchor_y / <span class=\"number\">2</span></span><br><span class=\"line\">\t\ty2_anc = downscale * (jy + <span class=\"number\">0.5</span>) + anchor_y / <span class=\"number\">2</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">if</span> y1_anc &lt; <span class=\"number\">0</span> <span class=\"keyword\">or</span> y2_anc &gt; resized_height:</span><br><span class=\"line\">\t\t\t<span class=\"keyword\">continue</span></span><br></pre></td></tr></table></figure>\n<p><br></p>\n<p>【注：现在我们确定了一个预选框组合有确定了中心点那就是唯一确定一个框了，接下来就是来确定这个宽的性质了：是否包含物体、如包含物体其回归梯度是多少】</p>\n<p>要确定以上两个性质，每一个框都需要遍历图中的所有bboxes 然后计算该预选框与bbox的交并比（IOU）<br>如果现在的交并比curr_iou大于该bbox最好的交并比或者大于给定的阈值则求下列参数，这些参数是后来要用的即回归梯度</p>\n<p>tx：两个框中心的宽的距离与预选框宽的比<br>ty:同tx<br>tw:bbox的宽与预选框宽的比<br>th:同理</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">if</span> curr_iou &gt; best_iou_for_bbox[bbox_num] <span class=\"keyword\">or</span> curr_iou &gt; C.rpn_max_overlap:</span><br><span class=\"line\">\tcx = (gta[bbox_num, <span class=\"number\">0</span>] + gta[bbox_num, <span class=\"number\">1</span>]) / <span class=\"number\">2.0</span></span><br><span class=\"line\">\tcy = (gta[bbox_num, <span class=\"number\">2</span>] + gta[bbox_num, <span class=\"number\">3</span>]) / <span class=\"number\">2.0</span></span><br><span class=\"line\">\tcxa = (x1_anc + x2_anc) / <span class=\"number\">2.0</span></span><br><span class=\"line\">\tcya = (y1_anc + y2_anc) / <span class=\"number\">2.0</span></span><br><span class=\"line\"></span><br><span class=\"line\">\ttx = (cx - cxa) / (x2_anc - x1_anc)</span><br><span class=\"line\">\tty = (cy - cya) / (y2_anc - y1_anc)</span><br><span class=\"line\">\ttw = np.log((gta[bbox_num, <span class=\"number\">1</span>] - gta[bbox_num, <span class=\"number\">0</span>]) / (x2_anc - x1_anc))</span><br><span class=\"line\">\tth = np.log((gta[bbox_num, <span class=\"number\">3</span>] - gta[bbox_num, <span class=\"number\">2</span>])) / (y2_anc - y1_anc)</span><br></pre></td></tr></table></figure>\n<p>对应于Faster RCNN原文，foreground anchor与ground truth之间的平移量 $(t_x, t_y)$ 如下：</p>\n<p><center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/25_2.jpg\" alt=\"image\"></center><br>对于训练bouding box regression网络回归分支，输入是cnn feature Φ，监督信号是Anchor与GT的差距 $(t_x, t_y, t_w, t_h)$，即训练目标是：输入 Φ的情况下使网络输出与监督信号尽可能接近。<br>那么当bouding box regression工作时，再输入Φ时，回归网络分支的输出就是每个Anchor的平移量和变换尺度 $(t_x, t_y, t_w, t_h)$，显然即可用来修正Anchor位置了。</p>\n<p><br><br>如果相交的不是背景，那么进行一系列更新</p>\n<p>关于bbox的相关信息更新<br>预选框的相关更新：如果交并比大于阈值这是pos<br>best_iou_for_loc：其记录的是有最大交并比为多少和其对应的回归梯度<br>num_anchors_for_bbox[bbox_num]：记录的是bbox拥有的pos预选框的个数<br>如果小于最小阈值是neg，在这两个之间是neutral<br>需要注意的是：判断一个框为neg需要其与所有的bbox的交并比都小于最小的阈值</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">if</span> img_data[<span class=\"string\">'bboxes'</span>][bbox_num][<span class=\"string\">'class'</span>] != <span class=\"string\">'bg'</span>:</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"comment\"># all GT boxes should be mapped to an anchor box, so we keep track of which anchor box was best</span></span><br><span class=\"line\">\t<span class=\"keyword\">if</span> curr_iou &gt; best_iou_for_bbox[bbox_num]:</span><br><span class=\"line\">\t\tbest_anchor_for_bbox[bbox_num] = [jy, ix, anchor_ratio_idx, anchor_size_idx]</span><br><span class=\"line\">\t\tbest_iou_for_bbox[bbox_num] = curr_iou</span><br><span class=\"line\">\t\tbest_x_for_bbox[bbox_num, :] = [x1_anc, x2_anc, y1_anc, y2_anc]</span><br><span class=\"line\">\t\tbest_dx_for_bbox[bbox_num, :] = [tx, ty, tw, th]</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"keyword\">if</span> curr_iou &gt; C.rpn_max_overlap:</span><br><span class=\"line\">\t\tbbox_type = <span class=\"string\">'pos'</span></span><br><span class=\"line\">\t\tnum_anchors_for_bbox[bbox_num] += <span class=\"number\">1</span></span><br><span class=\"line\">\t\t<span class=\"keyword\">if</span> curr_iou &gt; best_iou_for_loc:</span><br><span class=\"line\">\t\t\tbest_iou_for_loc = curr_iou</span><br><span class=\"line\">\t\t\tbest_regr = (tx, ty, tw, th)</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"keyword\">if</span> C.rpn_min_overlap &lt; curr_iou &lt; C.rpn_max_overlap:</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">if</span> bbox_type != <span class=\"string\">'pos'</span>:</span><br><span class=\"line\">\t\t\tbbox_type = <span class=\"string\">'neutral'</span></span><br></pre></td></tr></table></figure>\n<p><br><br>当结束对所有的bbox的遍历时，来确定该预选宽的性质。</p>\n<p>y_is_box_valid：该预选框是否可用（nertual就是不可用的）<br>y_rpn_overlap：该预选框是否包含物体<br>y_rpn_regr:回归梯度<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">if</span> bbox_type == <span class=\"string\">'neg'</span>:</span><br><span class=\"line\">    y_is_box_valid[jy, ix, anchor_ratio_idx + n_anchratios * anchor_size_idx] = <span class=\"number\">1</span></span><br><span class=\"line\">    y_rpn_overlap[jy, ix, anchor_ratio_idx + n_anchratios * anchor_size_idx] = <span class=\"number\">0</span></span><br><span class=\"line\"><span class=\"keyword\">elif</span> bbox_type == <span class=\"string\">'neutral'</span>:</span><br><span class=\"line\">    y_is_box_valid[jy, ix, anchor_ratio_idx + n_anchratios * anchor_size_idx] = <span class=\"number\">0</span></span><br><span class=\"line\">    y_rpn_overlap[jy, ix, anchor_ratio_idx + n_anchratios * anchor_size_idx] = <span class=\"number\">0</span></span><br><span class=\"line\"><span class=\"keyword\">else</span>:</span><br><span class=\"line\">    y_is_box_valid[jy, ix, anchor_ratio_idx + n_anchratios * anchor_size_idx] = <span class=\"number\">1</span></span><br><span class=\"line\">    y_rpn_overlap[jy, ix, anchor_ratio_idx + n_anchratios * anchor_size_idx] = <span class=\"number\">1</span></span><br><span class=\"line\">    start = <span class=\"number\">4</span> * (anchor_ratio_idx + n_anchratios * anchor_size_idx)</span><br><span class=\"line\">    y_rpn_regr[jy, ix, start:start+<span class=\"number\">4</span>] = best_regr</span><br></pre></td></tr></table></figure></p>\n<p><br><br>如果有一个bbox没有pos的预选宽和其对应，这找一个与它交并比最高的anchor的设置为pos<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">for</span> idx <span class=\"keyword\">in</span> range(num_anchors_for_bbox.shape[<span class=\"number\">0</span>]):</span><br><span class=\"line\">\t<span class=\"keyword\">if</span> num_anchors_for_bbox[idx] == <span class=\"number\">0</span>:</span><br><span class=\"line\">\t\t<span class=\"comment\"># no box with an IOU greater than zero ...</span></span><br><span class=\"line\">\t\t<span class=\"keyword\">if</span> best_anchor_for_bbox[idx, <span class=\"number\">0</span>] == <span class=\"number\">-1</span>:</span><br><span class=\"line\">\t\t\t<span class=\"keyword\">continue</span></span><br><span class=\"line\">\t\ty_is_box_valid[best_anchor_for_bbox[idx,<span class=\"number\">0</span>], best_anchor_for_bbox[idx,<span class=\"number\">1</span>], best_anchor_for_bbox[idx,<span class=\"number\">2</span>] + n_anchratios *best_anchor_for_bbox[idx,<span class=\"number\">3</span>]] = <span class=\"number\">1</span></span><br><span class=\"line\">\t\ty_rpn_overlap[best_anchor_for_bbox[idx,<span class=\"number\">0</span>], best_anchor_for_bbox[idx,<span class=\"number\">1</span>], best_anchor_for_bbox[idx,<span class=\"number\">2</span>] + n_anchratios *best_anchor_for_bbox[idx,<span class=\"number\">3</span>]] = <span class=\"number\">1</span></span><br><span class=\"line\">\t\tstart = <span class=\"number\">4</span> * (best_anchor_for_bbox[idx,<span class=\"number\">2</span>] + n_anchratios * best_anchor_for_bbox[idx,<span class=\"number\">3</span>])</span><br><span class=\"line\">\t\ty_rpn_regr[best_anchor_for_bbox[idx,<span class=\"number\">0</span>], best_anchor_for_bbox[idx,<span class=\"number\">1</span>], start:start+<span class=\"number\">4</span>] = best_dx_for_bbox[idx, :]</span><br></pre></td></tr></table></figure></p>\n<p><br><br>将深度变到第一位，给向量增加一个维度, 在Tensorflow中， 第一纬度是batch size, 此外， 变换向量位置匹配要求<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">y_rpn_overlap = np.transpose(y_rpn_overlap, (<span class=\"number\">2</span>, <span class=\"number\">0</span>, <span class=\"number\">1</span>))</span><br><span class=\"line\">y_rpn_overlap = np.expand_dims(y_rpn_overlap, axis=<span class=\"number\">0</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">y_is_box_valid = np.transpose(y_is_box_valid, (<span class=\"number\">2</span>, <span class=\"number\">0</span>, <span class=\"number\">1</span>))</span><br><span class=\"line\">y_is_box_valid = np.expand_dims(y_is_box_valid, axis=<span class=\"number\">0</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">y_rpn_regr = np.transpose(y_rpn_regr, (<span class=\"number\">2</span>, <span class=\"number\">0</span>, <span class=\"number\">1</span>))</span><br><span class=\"line\">y_rpn_regr = np.expand_dims(y_rpn_regr, axis=<span class=\"number\">0</span>)</span><br></pre></td></tr></table></figure></p>\n<p><br><br>从可用的预选框中选择num_regions<br>如果pos的个数大于num_regions / 2，则将多下来的地方置为不可用。如果小于pos不做处理<br>接下来将pos与neg总是超过num_regions个的neg预选框置为不可用<br>最后， 256个预选框，128个positive,128个negative 会生成 在一张图片里面<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">pos_locs = np.where(np(y_rpn_overlap[<span class=\"number\">0</span>, :, :, :] =.logical_and= <span class=\"number\">1</span>, y_is_box_valid[<span class=\"number\">0</span>, :, :, :] == <span class=\"number\">1</span>))</span><br><span class=\"line\">neg_locs = np.where(np.logical_and(y_rpn_overlap[<span class=\"number\">0</span>, :, :, :] == <span class=\"number\">0</span>, y_is_box_valid[<span class=\"number\">0</span>, :, :, :] == <span class=\"number\">1</span>))</span><br><span class=\"line\">num_regions = <span class=\"number\">256</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> len(pos_locs[<span class=\"number\">0</span>]) &gt; num_regions / <span class=\"number\">2</span>:</span><br><span class=\"line\">\tval_locs = random.sample(range(len(pos_locs[<span class=\"number\">0</span>])), len(pos_locs[<span class=\"number\">0</span>]) - num_regions / <span class=\"number\">2</span>)</span><br><span class=\"line\">\ty_is_box_valid[<span class=\"number\">0</span>, pos_locs[<span class=\"number\">0</span>][val_locs], pos_locs[<span class=\"number\">1</span>][val_locs], pos_locs[<span class=\"number\">2</span>][val_locs]] = <span class=\"number\">0</span></span><br><span class=\"line\">\tnum_pos = num_regions / <span class=\"number\">2</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> len(neg_locs[<span class=\"number\">0</span>]) + num_pos &gt; num_regions:</span><br><span class=\"line\">\tval_locs = random.sample(range(len(neg_locs[<span class=\"number\">0</span>])), len(neg_locs[<span class=\"number\">0</span>]) - num_pos)</span><br><span class=\"line\">\t y_is_box_valid[<span class=\"number\">0</span>, neg_locs[<span class=\"number\">0</span>][val_locs], neg_locs[<span class=\"number\">1</span>][val_locs], neg_locs[<span class=\"number\">2</span>][val_locs]] = <span class=\"number\">0</span></span><br></pre></td></tr></table></figure></p>\n<p><br></p>\n<h3 id=\"【27-06-2018】\"><a href=\"#【27-06-2018】\" class=\"headerlink\" title=\"【27/06/2018】\"></a>【27/06/2018】</h3><h4 id=\"project-brief\"><a href=\"#project-brief\" class=\"headerlink\" title=\"project brief\"></a>project brief</h4><p>Re organization of Project plan</p>\n<h4 id=\"Anchors-Iterative\"><a href=\"#Anchors-Iterative\" class=\"headerlink\" title=\"Anchors Iterative\"></a>Anchors Iterative</h4><p>Integration of privous work:<br>In each anchor: config file -&gt; rpn_stride = 16 means generate one anchor in 16 pixels<br><a href=\"https://github.com/Trouble404/Object-Detection-System-Based-on-CNN-and-Capsule-Network/blob/master/Faster-RCNN/anchor_rpn.ipynb\" target=\"_blank\" rel=\"noopener\">Jupyter Notebook address</a></p>\n<p>【流程】<br>Function description<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">@param all_img_data: Parsered xml file  </span></span><br><span class=\"line\"><span class=\"string\">@param class_count: Counting of the number of all classes objects</span></span><br><span class=\"line\"><span class=\"string\">@param C: Configuration class</span></span><br><span class=\"line\"><span class=\"string\">@param img_length_calc_function: resnet's get_img_output_length() function</span></span><br><span class=\"line\"><span class=\"string\">@param backend: Tensorflow in this project</span></span><br><span class=\"line\"><span class=\"string\">#param mode: train or val</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">yield np.copy(x_img), [np.copy(y_rpn_cls), np.copy(y_rpn_regr)], img_data_aug</span></span><br><span class=\"line\"><span class=\"string\">@return: np.copy(x_img): image's matrix data</span></span><br><span class=\"line\"><span class=\"string\">@return: [np.copy(y_rpn_cls), np.copy(y_rpn_regr)]: calculated rpn class and radient</span></span><br><span class=\"line\"><span class=\"string\">@return: img_data_aug: correspoding parsed xml information</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">\"\"\"</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">get_anchor_gt</span><span class=\"params\">(all_img_data, class_count, C, img_length_calc_function, backend, mode=<span class=\"string\">'train'</span>)</span>:</span></span><br></pre></td></tr></table></figure></p>\n<p><br><br><strong>Traverse all input image based on input xml information</strong></p>\n<ul>\n<li>Apply class balance function: <figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">C.balanced_classes = <span class=\"keyword\">True</span></span><br><span class=\"line\">sample_selector = image_processing.SampleSelector(class_count)</span><br><span class=\"line\"><span class=\"keyword\">if</span> C.balanced_classes <span class=\"keyword\">and</span> sample_selector.skip_sample_for_balanced_class(img_data):</span><br><span class=\"line\">    <span class=\"keyword\">continue</span></span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<p><br></p>\n<ul>\n<li>Apply image enhance<br>if input mode is train, apply image enhance to obtain augmented image xml and matrix, if mode is val, obtain image orignal xml and matrix<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">if</span> mode == <span class=\"string\">'train'</span>:</span><br><span class=\"line\">    img_data_aug, x_img = image_enhance.augment(img_data, C, augment=<span class=\"keyword\">True</span>)</span><br><span class=\"line\"><span class=\"keyword\">else</span>:</span><br><span class=\"line\">    img_data_aug, x_img = image_enhance.augment(img_data, C, augment=<span class=\"keyword\">False</span>)</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<p>verifacation width and hegiht in xml and matrix<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">(width, height) = (img_data_aug[<span class=\"string\">'width'</span>], img_data_aug[<span class=\"string\">'height'</span>])</span><br><span class=\"line\">(rows, cols, _) = x_img.shape</span><br><span class=\"line\"><span class=\"keyword\">assert</span> cols == width</span><br><span class=\"line\"><span class=\"keyword\">assert</span> rows == height</span><br></pre></td></tr></table></figure></p>\n<p><br></p>\n<ul>\n<li>Apply rezise function<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">(resized_width, resized_height) = image_processing.get_new_img_size(width, height, C.im_size)</span><br><span class=\"line\">x_img = cv2.resize(x_img, (resized_width, resized_height), interpolation=cv2.INTER_CUBIC)</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<p><br></p>\n<ul>\n<li>Apply rpn calculation<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">y_rpn_cls, y_rpn_regr = rpn_calculation.calc_rpn(C, img_data_aug, width, height, resized_width, resized_height, img_length_calc_function)</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<p><br></p>\n<ul>\n<li><p>Zero-center by mean pixel, and preprocess image format<br>BGR -&gt; RGB because when apply resnet, it need RGB but in cv2, it use BGR</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">x_img = x_img[:,:, (<span class=\"number\">2</span>, <span class=\"number\">1</span>, <span class=\"number\">0</span>)]</span><br></pre></td></tr></table></figure>\n<p> For using pre-trainning model, needs to mins mean channel in each dim</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">x_img = x_img.astype(np.float32)</span><br><span class=\"line\">x_img[:, :, <span class=\"number\">0</span>] -= C.img_channel_mean[<span class=\"number\">0</span>]</span><br><span class=\"line\">x_img[:, :, <span class=\"number\">1</span>] -= C.img_channel_mean[<span class=\"number\">1</span>]</span><br><span class=\"line\">x_img[:, :, <span class=\"number\">2</span>] -= C.img_channel_mean[<span class=\"number\">2</span>]</span><br><span class=\"line\">x_img /= C.img_scaling_factor <span class=\"comment\"># default to 1,so no change here</span></span><br></pre></td></tr></table></figure>\n<p> expand for batch size</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">x_img = np.expand_dims(x_img, axis=<span class=\"number\">0</span>)</span><br></pre></td></tr></table></figure>\n<p>for using pre-trainning model, need to sclaling the std to match pre trained model</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">y_rpn_regr[:, y_rpn_regr.shape[<span class=\"number\">1</span>]//<span class=\"number\">2</span>:, :, :] *= C.std_scaling <span class=\"comment\"># scaling is 4 here</span></span><br></pre></td></tr></table></figure>\n<p>in tensorflow, sort as batch size, width, height, deep</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">if</span> backend == <span class=\"string\">'tf'</span>:</span><br><span class=\"line\">    x_img = np.transpose(x_img, (<span class=\"number\">0</span>, <span class=\"number\">3</span>, <span class=\"number\">2</span>, <span class=\"number\">1</span>))</span><br><span class=\"line\">    y_rpn_cls = np.transpose(y_rpn_cls, (<span class=\"number\">0</span>, <span class=\"number\">3</span>, <span class=\"number\">2</span>, <span class=\"number\">1</span>))</span><br><span class=\"line\">\ty_rpn_regr = np.transpose(y_rpn_regr, (<span class=\"number\">0</span>, <span class=\"number\">3</span>, <span class=\"number\">2</span>, <span class=\"number\">1</span>))</span><br></pre></td></tr></table></figure>\n<p>generator to iteror, using next() to loop</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">yield</span> np.copy(x_img), [np.copy(y_rpn_cls), np.copy(y_rpn_regr)], img_data_aug</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<p><br><br>【执行】<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">data_gen_train = get_anchor_gt(train_imgs, classes_count, C, nn.get_img_output_length, K.image_dim_ordering(), mode=<span class=\"string\">'train'</span>)</span><br><span class=\"line\">data_gen_val = get_anchor_gt(val_imgs, classes_count, C, nn.get_img_output_length,K.image_dim_ordering(), mode=<span class=\"string\">'val'</span>)</span><br></pre></td></tr></table></figure></p>\n<p>Test:<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">img,rpn,img_aug = next(data_gen_train)</span><br></pre></td></tr></table></figure></p>\n<p><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/26_1.PNG\" alt=\"image\"></p>\n<h3 id=\"【28-06-2018】\"><a href=\"#【28-06-2018】\" class=\"headerlink\" title=\"【28/06/2018】\"></a>【28/06/2018】</h3><h4 id=\"Resnet50-structure\"><a href=\"#Resnet50-structure\" class=\"headerlink\" title=\"Resnet50 structure\"></a>Resnet50 structure</h4><p>论文链接: <a href=\"https://arxiv.org/abs/1512.03385\" target=\"_blank\" rel=\"noopener\">https://arxiv.org/abs/1512.03385</a></p>\n<p>首先，我们要问一个问题：<br><strong>Is learning better networks as easy as stacking more layers?</strong></p>\n<p>很显然不是，原因有二。<br>一，<strong>vanishing/exploding gradients</strong>；深度会带来恶名昭著的梯度弥散/爆炸，导致系统不能收敛。然而梯度弥散/爆炸在很大程度上被normalized initialization and intermediate normalization layers处理了。<br>二、<strong>degradation</strong>；当深度开始增加的时候，accuracy经常会达到饱和，然后开始下降，但这并不是由于过拟合引起的。可见figure1，56-layer的error大于20-layer的error。</p>\n<p>He kaiMing大神认为靠堆layers竟然会导致degradation，那肯定是我们堆的方式不对。因此他提出了一种基于残差块的identity mapping，通过学习残差的方式，而非直接去学习直接的映射关系。 </p>\n<p><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/01_2.jpg\" alt=\"image\"></p>\n<p>事实证明，靠堆积残差块能够带来很好效果提升。而不断堆积plain layer却会带来很高的训练误差<br>残差块的两个优点：<br>1) Our extremely deep residual nets are easy to optimize, but the counterpart “plain” nets (that simply stack layers) exhibit higher training error when the depth increases;<br>2) Our deep residual nets can easily enjoy accuracy gains from greatly increased depth, producing results substantially better than previous networks.</p>\n<h3 id=\"【29-06-2018】\"><a href=\"#【29-06-2018】\" class=\"headerlink\" title=\"【29/06/2018】\"></a>【29/06/2018】</h3><h4 id=\"Resnet50-image-structure\"><a href=\"#Resnet50-image-structure\" class=\"headerlink\" title=\"Resnet50 image structure\"></a>Resnet50 image structure</h4><p><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/1_01.png\" alt=\"iamge\"><br>ResNet有2个基本的block，一个是Identity Block，输入和输出的dimension是一样的，所以可以串联多个；另外一个基本block是Conv Block，输入和输出的dimension是不一样的，所以不能连续串联，它的作用本来就是为了改变feature vector的dimension</p>\n<p>因为CNN最后都是要把image一点点的convert成很小但是depth很深的feature map，一般的套路是用统一的比较小的kernel（比如VGG都是用3x3），但是随着网络深度的增加，output的channel也增大（学到的东西越来越复杂），所以有必要在进入Identity Block之前，用Conv Block转换一下维度，这样后面就可以连续接Identity Block.</p>\n<p>可以看下Conv Block是怎么改变输出维度的:<br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/01_3.png\" alt=\"image\"><br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/01_4.png\" alt=\"image\"><br>其实就是在shortcut path的地方加上一个conv2D layer（1x1 filter size），然后在main path改变dimension，并与shortcut path对应起来.</p>\n<h2 id=\"July\"><a href=\"#July\" class=\"headerlink\" title=\"July\"></a>July</h2><h3 id=\"【02-07-2018】\"><a href=\"#【02-07-2018】\" class=\"headerlink\" title=\"【02/07/2018】\"></a>【02/07/2018】</h3><h4 id=\"Construct-resnet-by-keras\"><a href=\"#Construct-resnet-by-keras\" class=\"headerlink\" title=\"Construct resnet by keras\"></a>Construct resnet by keras</h4><p>残差网络的关键步骤，跨层的合并需要保证x和F(x)的shape是完全一样的，否则它们加不起来。</p>\n<p>理解了这一点，我们开始用keras做实现，我们把输入输出大小相同的模块称为identity_block，而把输出比输入小的模块称为conv_block，首先，导入所需的模块：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> keras.models <span class=\"keyword\">import</span> Model</span><br><span class=\"line\"><span class=\"keyword\">from</span> keras.layers <span class=\"keyword\">import</span> Input,Dense,BatchNormalization,Conv2D,MaxPooling2D,AveragePooling2D,ZeroPadding2D</span><br><span class=\"line\"><span class=\"keyword\">from</span> keras.layers <span class=\"keyword\">import</span> add,Flatten</span><br><span class=\"line\"><span class=\"keyword\">from</span> keras.optimizers <span class=\"keyword\">import</span> SGD</span><br></pre></td></tr></table></figure>\n<p>我们先来编写identity_block，这是一个函数，接受一个张量为输入，并返回一个张量, 然后是conv层，是有shortcut的：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">Conv2d_BN</span><span class=\"params\">(x, nb_filter,kernel_size, strides=<span class=\"params\">(<span class=\"number\">1</span>,<span class=\"number\">1</span>)</span>, padding=<span class=\"string\">'same'</span>,name=None)</span>:</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> name <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"keyword\">None</span>:</span><br><span class=\"line\">        bn_name = name + <span class=\"string\">'_bn'</span></span><br><span class=\"line\">        conv_name = name + <span class=\"string\">'_conv'</span></span><br><span class=\"line\">    <span class=\"keyword\">else</span>:</span><br><span class=\"line\">        bn_name = <span class=\"keyword\">None</span></span><br><span class=\"line\">        conv_name = <span class=\"keyword\">None</span></span><br><span class=\"line\"> </span><br><span class=\"line\">    x = Conv2D(nb_filter,kernel_size,padding=padding,strides=strides,activation=<span class=\"string\">'relu'</span>,name=conv_name)(x)</span><br><span class=\"line\">    x = BatchNormalization(axis=<span class=\"number\">3</span>,name=bn_name)(x)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> x</span><br><span class=\"line\">    </span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">Conv_Block</span><span class=\"params\">(inpt,nb_filter,kernel_size,strides=<span class=\"params\">(<span class=\"number\">1</span>,<span class=\"number\">1</span>)</span>, with_conv_shortcut=False)</span>:</span></span><br><span class=\"line\">    x = Conv2d_BN(inpt,nb_filter=nb_filter[<span class=\"number\">0</span>],kernel_size=(<span class=\"number\">1</span>,<span class=\"number\">1</span>),strides=strides,padding=<span class=\"string\">'same'</span>)</span><br><span class=\"line\">    x = Conv2d_BN(x, nb_filter=nb_filter[<span class=\"number\">1</span>], kernel_size=(<span class=\"number\">3</span>,<span class=\"number\">3</span>), padding=<span class=\"string\">'same'</span>)</span><br><span class=\"line\">    x = Conv2d_BN(x, nb_filter=nb_filter[<span class=\"number\">2</span>], kernel_size=(<span class=\"number\">1</span>,<span class=\"number\">1</span>), padding=<span class=\"string\">'same'</span>)</span><br><span class=\"line\">    <span class=\"keyword\">if</span> with_conv_shortcut:</span><br><span class=\"line\">        shortcut = Conv2d_BN(inpt,nb_filter=nb_filter[<span class=\"number\">2</span>],strides=strides,kernel_size=kernel_size)</span><br><span class=\"line\">        x = add([x,shortcut])</span><br><span class=\"line\">        <span class=\"keyword\">return</span> x</span><br><span class=\"line\">    <span class=\"keyword\">else</span>:</span><br><span class=\"line\">        x = add([x,inpt])</span><br><span class=\"line\">        <span class=\"keyword\">return</span> x</span><br></pre></td></tr></table></figure></p>\n<p>剩下的事情就很简单了，数好identity_block和conv_block是如何交错的，照着网络搭就好了：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">inpt = Input(shape=(<span class=\"number\">224</span>,<span class=\"number\">224</span>,<span class=\"number\">3</span>))</span><br><span class=\"line\">x = ZeroPadding2D((<span class=\"number\">3</span>,<span class=\"number\">3</span>))(inpt)</span><br><span class=\"line\">x = Conv2d_BN(x,nb_filter=<span class=\"number\">64</span>,kernel_size=(<span class=\"number\">7</span>,<span class=\"number\">7</span>),strides=(<span class=\"number\">2</span>,<span class=\"number\">2</span>),padding=<span class=\"string\">'valid'</span>)</span><br><span class=\"line\">x = MaxPooling2D(pool_size=(<span class=\"number\">3</span>,<span class=\"number\">3</span>),strides=(<span class=\"number\">2</span>,<span class=\"number\">2</span>),padding=<span class=\"string\">'same'</span>)(x)</span><br><span class=\"line\"> </span><br><span class=\"line\">x = Conv_Block(x,nb_filter=[<span class=\"number\">64</span>,<span class=\"number\">64</span>,<span class=\"number\">256</span>],kernel_size=(<span class=\"number\">3</span>,<span class=\"number\">3</span>),strides=(<span class=\"number\">1</span>,<span class=\"number\">1</span>),with_conv_shortcut=<span class=\"keyword\">True</span>)</span><br><span class=\"line\">x = Conv_Block(x,nb_filter=[<span class=\"number\">64</span>,<span class=\"number\">64</span>,<span class=\"number\">256</span>],kernel_size=(<span class=\"number\">3</span>,<span class=\"number\">3</span>))</span><br><span class=\"line\">x = Conv_Block(x,nb_filter=[<span class=\"number\">64</span>,<span class=\"number\">64</span>,<span class=\"number\">256</span>],kernel_size=(<span class=\"number\">3</span>,<span class=\"number\">3</span>))</span><br><span class=\"line\"> </span><br><span class=\"line\">x = Conv_Block(x,nb_filter=[<span class=\"number\">128</span>,<span class=\"number\">128</span>,<span class=\"number\">512</span>],kernel_size=(<span class=\"number\">3</span>,<span class=\"number\">3</span>),strides=(<span class=\"number\">2</span>,<span class=\"number\">2</span>),with_conv_shortcut=<span class=\"keyword\">True</span>)</span><br><span class=\"line\">x = Conv_Block(x,nb_filter=[<span class=\"number\">128</span>,<span class=\"number\">128</span>,<span class=\"number\">512</span>],kernel_size=(<span class=\"number\">3</span>,<span class=\"number\">3</span>))</span><br><span class=\"line\">x = Conv_Block(x,nb_filter=[<span class=\"number\">128</span>,<span class=\"number\">128</span>,<span class=\"number\">512</span>],kernel_size=(<span class=\"number\">3</span>,<span class=\"number\">3</span>))</span><br><span class=\"line\">x = Conv_Block(x,nb_filter=[<span class=\"number\">128</span>,<span class=\"number\">128</span>,<span class=\"number\">512</span>],kernel_size=(<span class=\"number\">3</span>,<span class=\"number\">3</span>))</span><br><span class=\"line\"> </span><br><span class=\"line\">x = Conv_Block(x,nb_filter=[<span class=\"number\">256</span>,<span class=\"number\">256</span>,<span class=\"number\">1024</span>],kernel_size=(<span class=\"number\">3</span>,<span class=\"number\">3</span>),strides=(<span class=\"number\">2</span>,<span class=\"number\">2</span>),with_conv_shortcut=<span class=\"keyword\">True</span>)</span><br><span class=\"line\">x = Conv_Block(x,nb_filter=[<span class=\"number\">256</span>,<span class=\"number\">256</span>,<span class=\"number\">1024</span>],kernel_size=(<span class=\"number\">3</span>,<span class=\"number\">3</span>))</span><br><span class=\"line\">x = Conv_Block(x,nb_filter=[<span class=\"number\">256</span>,<span class=\"number\">256</span>,<span class=\"number\">1024</span>],kernel_size=(<span class=\"number\">3</span>,<span class=\"number\">3</span>))</span><br><span class=\"line\">x = Conv_Block(x,nb_filter=[<span class=\"number\">256</span>,<span class=\"number\">256</span>,<span class=\"number\">1024</span>],kernel_size=(<span class=\"number\">3</span>,<span class=\"number\">3</span>))</span><br><span class=\"line\">x = Conv_Block(x,nb_filter=[<span class=\"number\">256</span>,<span class=\"number\">256</span>,<span class=\"number\">1024</span>],kernel_size=(<span class=\"number\">3</span>,<span class=\"number\">3</span>))</span><br><span class=\"line\">x = Conv_Block(x,nb_filter=[<span class=\"number\">256</span>,<span class=\"number\">256</span>,<span class=\"number\">1024</span>],kernel_size=(<span class=\"number\">3</span>,<span class=\"number\">3</span>))</span><br><span class=\"line\"> </span><br><span class=\"line\">x = Conv_Block(x,nb_filter=[<span class=\"number\">512</span>,<span class=\"number\">512</span>,<span class=\"number\">2048</span>],kernel_size=(<span class=\"number\">3</span>,<span class=\"number\">3</span>),strides=(<span class=\"number\">2</span>,<span class=\"number\">2</span>),with_conv_shortcut=<span class=\"keyword\">True</span>)</span><br><span class=\"line\">x = Conv_Block(x,nb_filter=[<span class=\"number\">512</span>,<span class=\"number\">512</span>,<span class=\"number\">2048</span>],kernel_size=(<span class=\"number\">3</span>,<span class=\"number\">3</span>))</span><br><span class=\"line\">x = Conv_Block(x,nb_filter=[<span class=\"number\">512</span>,<span class=\"number\">512</span>,<span class=\"number\">2048</span>],kernel_size=(<span class=\"number\">3</span>,<span class=\"number\">3</span>))</span><br><span class=\"line\">x = AveragePooling2D(pool_size=(<span class=\"number\">7</span>,<span class=\"number\">7</span>))(x)</span><br><span class=\"line\">x = Flatten()(x)</span><br><span class=\"line\">x = Dense(<span class=\"number\">1000</span>,activation=<span class=\"string\">'softmax'</span>)(x)</span><br><span class=\"line\"></span><br><span class=\"line\">model = Model(inputs=inpt,outputs=x)</span><br><span class=\"line\">sgd = SGD(decay=<span class=\"number\">0.0001</span>,momentum=<span class=\"number\">0.9</span>)</span><br><span class=\"line\">model.compile(loss=<span class=\"string\">'categorical_crossentropy'</span>,optimizer=sgd,metrics=[<span class=\"string\">'accuracy'</span>])</span><br><span class=\"line\">model.summary()</span><br></pre></td></tr></table></figure></p>\n<p><a href=\"https://github.com/Trouble404/Object-Detection-System-Based-on-CNN-and-Capsule-Network/blob/master/Faster-RCNN/Resnet50/resnet50_keras.ipynb\" target=\"_blank\" rel=\"noopener\">jupyter notebook</a></p>\n<h3 id=\"【03-07-2018】\"><a href=\"#【03-07-2018】\" class=\"headerlink\" title=\"【03/07/2018】\"></a>【03/07/2018】</h3><h4 id=\"load-pre-trained-model-of-resnet50\"><a href=\"#load-pre-trained-model-of-resnet50\" class=\"headerlink\" title=\"load pre-trained model of resnet50\"></a>load pre-trained model of resnet50</h4><p>步骤如下：</p>\n<ul>\n<li>下载ResNet50不包含全连接层的模型参数到本地（resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5）；</li>\n<li>定义好ResNet50的网络结构；</li>\n<li>将预训练的模型参数加载到我们所定义的网络结构中；</li>\n<li>更改全连接层结构，便于对我们的分类任务进行处</li>\n<li>或者根据需要解冻最后几个block，然后以很低的学习率开始训练。我们只选择最后一个block进行训练，是因为训练样本很少，而ResNet50模型层数很多，全部训练肯定不能训练好，会过拟合。 其次fine-tune时由于是在一个已经训练好的模型上进行的，故权值更新应该是一个小范围的，以免破坏预训练好的特征。</li>\n</ul>\n<p><a href=\"https://github.com/fchollet/deep-learning-models/releases\" target=\"_blank\" rel=\"noopener\">下载地址</a></p>\n<p><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/01_7.JPG\" alt=\"image\"></p>\n<p>因为使用了预训练模型，参数名称需要和预训练模型一致：<br>identity层：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">identity_block</span><span class=\"params\">(X, f, filters, stage, block)</span>:</span></span><br><span class=\"line\">    <span class=\"comment\"># defining name basis</span></span><br><span class=\"line\">    conv_name_base = <span class=\"string\">'res'</span> + str(stage) + block + <span class=\"string\">'_branch'</span></span><br><span class=\"line\">    bn_name_base = <span class=\"string\">'bn'</span> + str(stage) + block + <span class=\"string\">'_branch'</span></span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\"># Retrieve Filters</span></span><br><span class=\"line\">    F1, F2, F3 = filters</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\"># Save the input value. You'll need this later to add back to the main path. </span></span><br><span class=\"line\">    X_shortcut = X</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\"># First component of main path</span></span><br><span class=\"line\">    X = Conv2D(filters = F1, kernel_size = (<span class=\"number\">1</span>, <span class=\"number\">1</span>), strides = (<span class=\"number\">1</span>,<span class=\"number\">1</span>), padding = <span class=\"string\">'valid'</span>, name = conv_name_base + <span class=\"string\">'2a'</span>)(X)</span><br><span class=\"line\">    X = BatchNormalization(axis = <span class=\"number\">3</span>, name = bn_name_base + <span class=\"string\">'2a'</span>)(X)</span><br><span class=\"line\">    X = Activation(<span class=\"string\">'relu'</span>)(X)</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\"># Second component of main path (≈3 lines)</span></span><br><span class=\"line\">    X = Conv2D(filters= F2, kernel_size=(f,f),strides=(<span class=\"number\">1</span>,<span class=\"number\">1</span>),padding=<span class=\"string\">'same'</span>,name=conv_name_base + <span class=\"string\">'2b'</span>)(X)</span><br><span class=\"line\">    X = BatchNormalization(axis=<span class=\"number\">3</span>,name=bn_name_base+<span class=\"string\">'2b'</span>)(X)</span><br><span class=\"line\">    X = Activation(<span class=\"string\">'relu'</span>)(X)</span><br><span class=\"line\"> </span><br><span class=\"line\">    <span class=\"comment\"># Third component of main path (≈2 lines)</span></span><br><span class=\"line\">    X = Conv2D(filters=F3,kernel_size=(<span class=\"number\">1</span>,<span class=\"number\">1</span>),strides=(<span class=\"number\">1</span>,<span class=\"number\">1</span>),padding=<span class=\"string\">'valid'</span>,name=conv_name_base+<span class=\"string\">'2c'</span>)(X)</span><br><span class=\"line\">    X = BatchNormalization(axis=<span class=\"number\">3</span>,name=bn_name_base+<span class=\"string\">'2c'</span>)(X)</span><br><span class=\"line\"> </span><br><span class=\"line\">    <span class=\"comment\"># Final step: Add shortcut value to main path, and pass it through a RELU activation (≈2 lines)</span></span><br><span class=\"line\">    X = Add()([X, X_shortcut])</span><br><span class=\"line\">    X = Activation(<span class=\"string\">'relu'</span>)(X)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> X</span><br></pre></td></tr></table></figure></p>\n<p>conv层：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">convolutional_block</span><span class=\"params\">(X, f, filters, stage, block, s = <span class=\"number\">2</span>)</span>:</span></span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\"># defining name basis</span></span><br><span class=\"line\">    conv_name_base = <span class=\"string\">'res'</span> + str(stage) + block + <span class=\"string\">'_branch'</span></span><br><span class=\"line\">    bn_name_base = <span class=\"string\">'bn'</span> + str(stage) + block + <span class=\"string\">'_branch'</span></span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\"># Retrieve Filters</span></span><br><span class=\"line\">    F1, F2, F3 = filters</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\"># Save the input value</span></span><br><span class=\"line\">    X_shortcut = X</span><br><span class=\"line\"> </span><br><span class=\"line\">    <span class=\"comment\">##### MAIN PATH #####</span></span><br><span class=\"line\">    <span class=\"comment\"># First component of main path </span></span><br><span class=\"line\">    X = Conv2D(F1, (<span class=\"number\">1</span>, <span class=\"number\">1</span>), strides = (s,s),padding=<span class=\"string\">'valid'</span>,name = conv_name_base + <span class=\"string\">'2a'</span>)(X)</span><br><span class=\"line\">    X = BatchNormalization(axis = <span class=\"number\">3</span>, name = bn_name_base + <span class=\"string\">'2a'</span>)(X)</span><br><span class=\"line\">    X = Activation(<span class=\"string\">'relu'</span>)(X)</span><br><span class=\"line\"> </span><br><span class=\"line\">    <span class=\"comment\"># Second component of main path (≈3 lines)</span></span><br><span class=\"line\">    X = Conv2D(F2,(f,f),strides=(<span class=\"number\">1</span>,<span class=\"number\">1</span>),padding=<span class=\"string\">'same'</span>,name=conv_name_base+<span class=\"string\">'2b'</span>)(X)</span><br><span class=\"line\">    X = BatchNormalization(axis=<span class=\"number\">3</span>,name=bn_name_base+<span class=\"string\">'2b'</span>)(X)</span><br><span class=\"line\">    X = Activation(<span class=\"string\">'relu'</span>)(X)</span><br><span class=\"line\"> </span><br><span class=\"line\">    <span class=\"comment\"># Third component of main path (≈2 lines)</span></span><br><span class=\"line\">    X = Conv2D(F3,(<span class=\"number\">1</span>,<span class=\"number\">1</span>),strides=(<span class=\"number\">1</span>,<span class=\"number\">1</span>),padding=<span class=\"string\">'valid'</span>,name=conv_name_base+<span class=\"string\">'2c'</span>)(X)</span><br><span class=\"line\">    X = BatchNormalization(axis=<span class=\"number\">3</span>,name=bn_name_base+<span class=\"string\">'2c'</span>)(X)</span><br><span class=\"line\"> </span><br><span class=\"line\">    <span class=\"comment\">##### SHORTCUT PATH #### (≈2 lines)</span></span><br><span class=\"line\">    X_shortcut = Conv2D(F3,(<span class=\"number\">1</span>,<span class=\"number\">1</span>),strides=(s,s),padding=<span class=\"string\">'valid'</span>,name=conv_name_base+<span class=\"string\">'1'</span>)(X_shortcut)</span><br><span class=\"line\">    X_shortcut = BatchNormalization(axis=<span class=\"number\">3</span>,name =bn_name_base+<span class=\"string\">'1'</span>)(X_shortcut)</span><br><span class=\"line\"> </span><br><span class=\"line\">    <span class=\"comment\"># Final step: Add shortcut value to main path, and pass it through a RELU activation (≈2 lines)</span></span><br><span class=\"line\">    X = Add()([X,X_shortcut])</span><br><span class=\"line\">    X = Activation(<span class=\"string\">'relu'</span>)(X)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> X</span><br></pre></td></tr></table></figure></p>\n<p>resnet50结构：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">ResNet50</span><span class=\"params\">(input_shape = <span class=\"params\">(<span class=\"number\">64</span>, <span class=\"number\">64</span>, <span class=\"number\">3</span>)</span>, classes = <span class=\"number\">30</span>)</span>:</span></span><br><span class=\"line\">    <span class=\"comment\"># Define the input as a tensor with shape input_shape</span></span><br><span class=\"line\">    X_input = Input(input_shape)</span><br><span class=\"line\"> </span><br><span class=\"line\">    <span class=\"comment\"># Zero-Padding</span></span><br><span class=\"line\">    X = ZeroPadding2D((<span class=\"number\">3</span>, <span class=\"number\">3</span>))(X_input)</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\"># Stage 1</span></span><br><span class=\"line\">    X = Conv2D(<span class=\"number\">64</span>, (<span class=\"number\">7</span>, <span class=\"number\">7</span>), strides = (<span class=\"number\">2</span>, <span class=\"number\">2</span>), name = <span class=\"string\">'conv1'</span>)(X)</span><br><span class=\"line\">    X = BatchNormalization(axis = <span class=\"number\">3</span>, name = <span class=\"string\">'bn_conv1'</span>)(X)</span><br><span class=\"line\">    X = Activation(<span class=\"string\">'relu'</span>)(X)</span><br><span class=\"line\">    X = MaxPooling2D((<span class=\"number\">3</span>, <span class=\"number\">3</span>), strides=(<span class=\"number\">2</span>, <span class=\"number\">2</span>))(X)</span><br><span class=\"line\"> </span><br><span class=\"line\">    <span class=\"comment\"># Stage 2</span></span><br><span class=\"line\">    X = convolutional_block(X, f = <span class=\"number\">3</span>, filters = [<span class=\"number\">64</span>, <span class=\"number\">64</span>, <span class=\"number\">256</span>], stage = <span class=\"number\">2</span>, block=<span class=\"string\">'a'</span>, s = <span class=\"number\">1</span>)</span><br><span class=\"line\">    X = identity_block(X, <span class=\"number\">3</span>, [<span class=\"number\">64</span>, <span class=\"number\">64</span>, <span class=\"number\">256</span>], stage=<span class=\"number\">2</span>, block=<span class=\"string\">'b'</span>)</span><br><span class=\"line\">    X = identity_block(X, <span class=\"number\">3</span>, [<span class=\"number\">64</span>, <span class=\"number\">64</span>, <span class=\"number\">256</span>], stage=<span class=\"number\">2</span>, block=<span class=\"string\">'c'</span>)</span><br><span class=\"line\"> </span><br><span class=\"line\">    <span class=\"comment\">### START CODE HERE ###</span></span><br><span class=\"line\"> </span><br><span class=\"line\">    <span class=\"comment\"># Stage 3 (≈4 lines)</span></span><br><span class=\"line\">    X = convolutional_block(X, f = <span class=\"number\">3</span>,filters= [<span class=\"number\">128</span>,<span class=\"number\">128</span>,<span class=\"number\">512</span>],stage=<span class=\"number\">3</span>,block=<span class=\"string\">'a'</span>,s=<span class=\"number\">2</span>)</span><br><span class=\"line\">    X = identity_block(X,<span class=\"number\">3</span>,[<span class=\"number\">128</span>,<span class=\"number\">128</span>,<span class=\"number\">512</span>],stage=<span class=\"number\">3</span>,block=<span class=\"string\">'b'</span>)</span><br><span class=\"line\">    X = identity_block(X,<span class=\"number\">3</span>,[<span class=\"number\">128</span>,<span class=\"number\">128</span>,<span class=\"number\">512</span>],stage=<span class=\"number\">3</span>,block=<span class=\"string\">'c'</span>)</span><br><span class=\"line\">    X = identity_block(X,<span class=\"number\">3</span>,[<span class=\"number\">128</span>,<span class=\"number\">128</span>,<span class=\"number\">512</span>],stage=<span class=\"number\">3</span>,block=<span class=\"string\">'d'</span>)</span><br><span class=\"line\"> </span><br><span class=\"line\">    <span class=\"comment\"># Stage 4 (≈6 lines)</span></span><br><span class=\"line\">    X = convolutional_block(X,f=<span class=\"number\">3</span>,filters=[<span class=\"number\">256</span>,<span class=\"number\">256</span>,<span class=\"number\">1024</span>],stage=<span class=\"number\">4</span>,block=<span class=\"string\">'a'</span>,s=<span class=\"number\">2</span>)</span><br><span class=\"line\">    X = identity_block(X,<span class=\"number\">3</span>,[<span class=\"number\">256</span>,<span class=\"number\">256</span>,<span class=\"number\">1024</span>],stage=<span class=\"number\">4</span>,block=<span class=\"string\">'b'</span>)</span><br><span class=\"line\">    X = identity_block(X,<span class=\"number\">3</span>,[<span class=\"number\">256</span>,<span class=\"number\">256</span>,<span class=\"number\">1024</span>],stage=<span class=\"number\">4</span>,block=<span class=\"string\">'c'</span>)</span><br><span class=\"line\">    X = identity_block(X,<span class=\"number\">3</span>,[<span class=\"number\">256</span>,<span class=\"number\">256</span>,<span class=\"number\">1024</span>],stage=<span class=\"number\">4</span>,block=<span class=\"string\">'d'</span>)</span><br><span class=\"line\">    X = identity_block(X,<span class=\"number\">3</span>,[<span class=\"number\">256</span>,<span class=\"number\">256</span>,<span class=\"number\">1024</span>],stage=<span class=\"number\">4</span>,block=<span class=\"string\">'e'</span>)</span><br><span class=\"line\">    X = identity_block(X,<span class=\"number\">3</span>,[<span class=\"number\">256</span>,<span class=\"number\">256</span>,<span class=\"number\">1024</span>],stage=<span class=\"number\">4</span>,block=<span class=\"string\">'f'</span>)</span><br><span class=\"line\"> </span><br><span class=\"line\">    <span class=\"comment\"># Stage 5 (≈3 lines)</span></span><br><span class=\"line\">    X = convolutional_block(X, f = <span class=\"number\">3</span>,filters= [<span class=\"number\">512</span>,<span class=\"number\">512</span>,<span class=\"number\">2048</span>],stage=<span class=\"number\">5</span>,block=<span class=\"string\">'a'</span>,s=<span class=\"number\">2</span>)</span><br><span class=\"line\">    X = identity_block(X,<span class=\"number\">3</span>,[<span class=\"number\">512</span>,<span class=\"number\">512</span>,<span class=\"number\">2048</span>],stage=<span class=\"number\">5</span>,block=<span class=\"string\">'b'</span>)</span><br><span class=\"line\">    X = identity_block(X,<span class=\"number\">3</span>,[<span class=\"number\">512</span>,<span class=\"number\">512</span>,<span class=\"number\">2048</span>],stage=<span class=\"number\">5</span>,block=<span class=\"string\">'c'</span>)</span><br><span class=\"line\"> </span><br><span class=\"line\">    <span class=\"comment\"># AVGPOOL (≈1 line). Use \"X = AveragePooling2D(...)(X)\"</span></span><br><span class=\"line\">    X = AveragePooling2D((<span class=\"number\">2</span>,<span class=\"number\">2</span>),strides=(<span class=\"number\">2</span>,<span class=\"number\">2</span>))(X)</span><br><span class=\"line\"> </span><br><span class=\"line\">    <span class=\"comment\"># output layer</span></span><br><span class=\"line\">    X = Flatten()(X)</span><br><span class=\"line\">    model = Model(inputs = X_input, outputs = X, name=<span class=\"string\">'ResNet50'</span>)</span><br><span class=\"line\"> </span><br><span class=\"line\">    <span class=\"keyword\">return</span> model</span><br></pre></td></tr></table></figure></p>\n<p>构建网络并且载入权重：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">base_model = ResNet50(input_shape=(<span class=\"number\">224</span>,<span class=\"number\">224</span>,<span class=\"number\">3</span>),classes=<span class=\"number\">30</span>) </span><br><span class=\"line\">base_model.load_weights(<span class=\"string\">'resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5'</span>)</span><br></pre></td></tr></table></figure></p>\n<p>无法载入<br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/01_8.JPG\" alt=\"image\"></p>\n<h3 id=\"【04-07-2018】\"><a href=\"#【04-07-2018】\" class=\"headerlink\" title=\"【04/07/2018】\"></a>【04/07/2018】</h3><h4 id=\"Loading-pre-trained-model\"><a href=\"#Loading-pre-trained-model\" class=\"headerlink\" title=\"Loading pre-trained model\"></a>Loading pre-trained model</h4><p>对于keras：如果新模型和旧模型结构一样，直接调用model.load_weights读取参数就行。如果新模型中的几层和之前模型一样，也通过model.load_weights(‘my_model_weights.h5’, by_name=True)来读取参数， 或者手动对每一层进行参数的赋值，比如x= Dense(100, weights=oldModel.layers[1].get_weights())(x)</p>\n<p>修改代码：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">try</span>:</span><br><span class=\"line\">    base_model.load_weights(<span class=\"string\">'resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5'</span>,by_name=<span class=\"keyword\">True</span>)</span><br><span class=\"line\">    print(<span class=\"string\">\"load successful\"</span>)</span><br><span class=\"line\"><span class=\"keyword\">except</span>:</span><br><span class=\"line\">    print(<span class=\"string\">\"load failed\"</span>)</span><br></pre></td></tr></table></figure></p>\n<p>载入成功：<a href=\"https://github.com/Trouble404/Object-Detection-System-Based-on-CNN-and-Capsule-Network/blob/master/Faster-RCNN/Resnet50/resnet50_pre_load.ipynb\" target=\"_blank\" rel=\"noopener\">jupyter notebook</a></p>\n<h3 id=\"【05-06-07-2018】\"><a href=\"#【05-06-07-2018】\" class=\"headerlink\" title=\"【05~06/07/2018】\"></a>【05~06/07/2018】</h3><h4 id=\"construct-faster-rcnn-net\"><a href=\"#construct-faster-rcnn-net\" class=\"headerlink\" title=\"construct faster rcnn net\"></a>construct faster rcnn net</h4><p><strong>RoiPoolingConv</strong><br>该函数的作用是对将每一个预选框框定的特征图大小规整到相同大小<br>什么是ROI呢？<br>ROI是Region of Interest的简写，指的是在“特征图上的框”；<br>1）在Fast RCNN中， RoI是指Selective Search完成后得到的“候选框”在特征图上的映射，如下图所示；<br>2）在Faster RCNN中，候选框是经过RPN产生的，然后再把各个“候选框”映射到特征图上，得到RoIs<br>创建一个类，这里不同的是它是要继承keras的Layer类<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">RoiPoolingConv</span><span class=\"params\">(Layer)</span>:</span></span><br></pre></td></tr></table></figure></p>\n<p><a href=\"http://keras-cn.readthedocs.io/en/latest/layers/writting_layer/\" target=\"_blank\" rel=\"noopener\">编写自己的层</a></p>\n<p>定义：<br>**<a href=\"https://www.cnblogs.com/xuyuanyuan123/p/6674645.html\" target=\"_blank\" rel=\"noopener\">kwargs</a>：表示的就是形参中按照关键字传值把多余的传值以字典的方式呈现<br><a href=\"https://link.zhihu.com/?target=http%3A//python.jobbole.com/86787/\" target=\"_blank\" rel=\"noopener\">super</a>:子类调用父类的初始化方法<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"string\">'''ROI pooling layer for 2D inputs.</span></span><br><span class=\"line\"><span class=\"string\">    # Arguments</span></span><br><span class=\"line\"><span class=\"string\">        pool_size: int</span></span><br><span class=\"line\"><span class=\"string\">            Size of pooling region to use. pool_size = 7 will result in a 7x7 region.</span></span><br><span class=\"line\"><span class=\"string\">        num_rois: number of regions of interest to be used</span></span><br><span class=\"line\"><span class=\"string\">    '''</span></span><br><span class=\"line\"><span class=\"comment\"># 第一个是规整后特征图大小 第二个是预选框个数</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self, pool_size, num_rois, **kwargs)</span>:</span></span><br><span class=\"line\"></span><br><span class=\"line\">        self.dim_ordering = K.image_dim_ordering()</span><br><span class=\"line\">        <span class=\"comment\"># print error when kernel not tensorflow or thoean</span></span><br><span class=\"line\">        <span class=\"keyword\">assert</span> self.dim_ordering <span class=\"keyword\">in</span> &#123;<span class=\"string\">'tf'</span>&#125;, <span class=\"string\">'dim_ordering must be in tf'</span></span><br><span class=\"line\"></span><br><span class=\"line\">        self.pool_size = pool_size</span><br><span class=\"line\">        self.num_rois = num_rois</span><br><span class=\"line\"></span><br><span class=\"line\">        super(RoiPoolingConv, self).__init__(**kwargs)</span><br></pre></td></tr></table></figure></p>\n<p>得到特征图的输出通道个数:<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">build</span><span class=\"params\">(self, input_shape)</span>:</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> self.dim_ordering == <span class=\"string\">'tf'</span>:</span><br><span class=\"line\">            self.nb_channels = input_shape[<span class=\"number\">0</span>][<span class=\"number\">3</span>]</span><br></pre></td></tr></table></figure></p>\n<p>定义输出特征图的形状：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">compute_output_shape</span><span class=\"params\">(self, input_shape)</span>:</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> self.dim_ordering == <span class=\"string\">'tf'</span>:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"keyword\">None</span>, self.num_rois, self.pool_size, self.pool_size, self.nb_channels</span><br></pre></td></tr></table></figure></p>\n<p>遍历提供的所有预选框,将预选宽里的特征图规整到指定大小, 并且加入到output:<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">call</span><span class=\"params\">(self, x, mask=None)</span>:</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">assert</span>(len(x) == <span class=\"number\">2</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">        img = x[<span class=\"number\">0</span>]</span><br><span class=\"line\">        rois = x[<span class=\"number\">1</span>]</span><br><span class=\"line\"></span><br><span class=\"line\">        input_shape = K.shape(img)</span><br><span class=\"line\"></span><br><span class=\"line\">        outputs = []</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">for</span> roi_idx <span class=\"keyword\">in</span> range(self.num_rois):</span><br><span class=\"line\"></span><br><span class=\"line\">            x = rois[<span class=\"number\">0</span>, roi_idx, <span class=\"number\">0</span>]</span><br><span class=\"line\">            y = rois[<span class=\"number\">0</span>, roi_idx, <span class=\"number\">1</span>]</span><br><span class=\"line\">            w = rois[<span class=\"number\">0</span>, roi_idx, <span class=\"number\">2</span>]</span><br><span class=\"line\">            h = rois[<span class=\"number\">0</span>, roi_idx, <span class=\"number\">3</span>]</span><br><span class=\"line\">            </span><br><span class=\"line\">            row_length = w / float(self.pool_size)</span><br><span class=\"line\">            col_length = h / float(self.pool_size)</span><br><span class=\"line\"></span><br><span class=\"line\">            num_pool_regions = self.pool_size</span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"keyword\">if</span> self.dim_ordering == <span class=\"string\">'tf'</span>:</span><br><span class=\"line\">                x = K.cast(x, <span class=\"string\">'int32'</span>)</span><br><span class=\"line\">                y = K.cast(y, <span class=\"string\">'int32'</span>)</span><br><span class=\"line\">                w = K.cast(w, <span class=\"string\">'int32'</span>)</span><br><span class=\"line\">                h = K.cast(h, <span class=\"string\">'int32'</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">                <span class=\"comment\"># resize porposal of feature map</span></span><br><span class=\"line\">                rs = tf.image.resize_images(img[:, y:y+h, x:x+w, :], (self.pool_size, self.pool_size))</span><br><span class=\"line\">                outputs.append(rs)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># 将outputs里面的变量按照第一个维度合在一起【shape:(?, 7, 7, 512)】</span></span><br><span class=\"line\">        final_output = K.concatenate(outputs, axis=<span class=\"number\">0</span>)</span><br><span class=\"line\">        final_output = K.reshape(final_output, (<span class=\"number\">1</span>, self.num_rois, self.pool_size, self.pool_size, self.nb_channels))</span><br><span class=\"line\">        <span class=\"comment\"># 将变量规整到相应的大小【shape:(1, 32, 7, 7, 512)】</span></span><br><span class=\"line\">        final_output = K.permute_dimensions(final_output, (<span class=\"number\">0</span>, <span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">4</span>))</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">return</span> final_output</span><br></pre></td></tr></table></figure></p>\n<p>输出是batch个vector，其中batch的值等于RoI的个数，vector的大小为channel * w * h；RoI Pooling的过程就是将一个个大小不同的box矩形框，都映射成大小固定（w * h）的矩形框.</p>\n<p><strong>TimeDistributed 包装器</strong><br>FastRcnn在做完ROIpooling后，需要将生产的所有的Roi全部送入分类和回归网络，Keras用的TimeDistributed函数：</p>\n<p>Relu激活函数本身就是逐元素计算激活值的，无论进来多少维的tensor都一样，所以不需要使用TimeDistributed。conv2D需要TimeDistributed，是因为一个ROI内的数据计算是互相依赖的，而不同ROI之间又是独立的。</p>\n<p>在最后Faster RCNN的结构中进行类别判断和bbox框的回归时，需要对设置的num_rois个感兴趣区域进行回归处理，由于每一个区域的处理是相对独立的，便等价于此时的时间步为num_rois，因此用TimeDistributed来wrap。</p>\n<p>改编之前的conv 和 identity层：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">conv_block_td</span><span class=\"params\">(input_tensor, kernel_size, filters, stage, block, input_shape, strides=<span class=\"params\">(<span class=\"number\">2</span>, <span class=\"number\">2</span>)</span>, trainable=True)</span>:</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># conv block time distributed</span></span><br><span class=\"line\"></span><br><span class=\"line\">    nb_filter1, nb_filter2, nb_filter3 = filters</span><br><span class=\"line\"></span><br><span class=\"line\">    bn_axis = <span class=\"number\">3</span></span><br><span class=\"line\"></span><br><span class=\"line\">    conv_name_base = <span class=\"string\">'res'</span> + str(stage) + block + <span class=\"string\">'_branch'</span></span><br><span class=\"line\">    bn_name_base = <span class=\"string\">'bn'</span> + str(stage) + block + <span class=\"string\">'_branch'</span></span><br><span class=\"line\"></span><br><span class=\"line\">    x = TimeDistributed(Convolution2D(nb_filter1, (<span class=\"number\">1</span>, <span class=\"number\">1</span>), strides=strides, trainable=trainable, kernel_initializer=<span class=\"string\">'normal'</span>), input_shape=input_shape, name=conv_name_base + <span class=\"string\">'2a'</span>)(input_tensor)</span><br><span class=\"line\">    x = TimeDistributed(BatchNormalization(axis=bn_axis), name=bn_name_base + <span class=\"string\">'2a'</span>)(x)</span><br><span class=\"line\">    x = Activation(<span class=\"string\">'relu'</span>)(x)</span><br><span class=\"line\"></span><br><span class=\"line\">    x = TimeDistributed(Convolution2D(nb_filter2, (kernel_size, kernel_size), padding=<span class=\"string\">'same'</span>, trainable=trainable, kernel_initializer=<span class=\"string\">'normal'</span>), name=conv_name_base + <span class=\"string\">'2b'</span>)(x)</span><br><span class=\"line\">    x = TimeDistributed(BatchNormalization(axis=bn_axis), name=bn_name_base + <span class=\"string\">'2b'</span>)(x)</span><br><span class=\"line\">    x = Activation(<span class=\"string\">'relu'</span>)(x)</span><br><span class=\"line\"></span><br><span class=\"line\">    x = TimeDistributed(Convolution2D(nb_filter3, (<span class=\"number\">1</span>, <span class=\"number\">1</span>), kernel_initializer=<span class=\"string\">'normal'</span>), name=conv_name_base + <span class=\"string\">'2c'</span>, trainable=trainable)(x)</span><br><span class=\"line\">    x = TimeDistributed(BatchNormalization(axis=bn_axis), name=bn_name_base + <span class=\"string\">'2c'</span>)(x)</span><br><span class=\"line\"></span><br><span class=\"line\">    shortcut = TimeDistributed(Convolution2D(nb_filter3, (<span class=\"number\">1</span>, <span class=\"number\">1</span>), strides=strides, trainable=trainable, kernel_initializer=<span class=\"string\">'normal'</span>), name=conv_name_base + <span class=\"string\">'1'</span>)(input_tensor)</span><br><span class=\"line\">    shortcut = TimeDistributed(BatchNormalization(axis=bn_axis), name=bn_name_base + <span class=\"string\">'1'</span>)(shortcut)</span><br><span class=\"line\"></span><br><span class=\"line\">    x = Add()([x, shortcut])</span><br><span class=\"line\">    x = Activation(<span class=\"string\">'relu'</span>)(x)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> x</span><br></pre></td></tr></table></figure></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">identity_block_td</span><span class=\"params\">(input_tensor, kernel_size, filters, stage, block, trainable=True)</span>:</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># identity block time distributed</span></span><br><span class=\"line\"></span><br><span class=\"line\">    nb_filter1, nb_filter2, nb_filter3 = filters</span><br><span class=\"line\"></span><br><span class=\"line\">    bn_axis = <span class=\"number\">3</span></span><br><span class=\"line\"></span><br><span class=\"line\">    conv_name_base = <span class=\"string\">'res'</span> + str(stage) + block + <span class=\"string\">'_branch'</span></span><br><span class=\"line\">    bn_name_base = <span class=\"string\">'bn'</span> + str(stage) + block + <span class=\"string\">'_branch'</span></span><br><span class=\"line\"></span><br><span class=\"line\">    x = TimeDistributed(Convolution2D(nb_filter1, (<span class=\"number\">1</span>, <span class=\"number\">1</span>), trainable=trainable, kernel_initializer=<span class=\"string\">'normal'</span>), name=conv_name_base + <span class=\"string\">'2a'</span>)(input_tensor)</span><br><span class=\"line\">    x = TimeDistributed(BatchNormalization(axis=bn_axis), name=bn_name_base + <span class=\"string\">'2a'</span>)(x)</span><br><span class=\"line\">    x = Activation(<span class=\"string\">'relu'</span>)(x)</span><br><span class=\"line\"></span><br><span class=\"line\">    x = TimeDistributed(Convolution2D(nb_filter2, (kernel_size, kernel_size), trainable=trainable, kernel_initializer=<span class=\"string\">'normal'</span>,padding=<span class=\"string\">'same'</span>), name=conv_name_base + <span class=\"string\">'2b'</span>)(x)</span><br><span class=\"line\">    x = TimeDistributed(BatchNormalization(axis=bn_axis), name=bn_name_base + <span class=\"string\">'2b'</span>)(x)</span><br><span class=\"line\">    x = Activation(<span class=\"string\">'relu'</span>)(x)</span><br><span class=\"line\"></span><br><span class=\"line\">    x = TimeDistributed(Convolution2D(nb_filter3, (<span class=\"number\">1</span>, <span class=\"number\">1</span>), trainable=trainable, kernel_initializer=<span class=\"string\">'normal'</span>), name=conv_name_base + <span class=\"string\">'2c'</span>)(x)</span><br><span class=\"line\">    x = TimeDistributed(BatchNormalization(axis=bn_axis), name=bn_name_base + <span class=\"string\">'2c'</span>)(x)</span><br><span class=\"line\"></span><br><span class=\"line\">    x = Add()([x, input_tensor])</span><br><span class=\"line\">    x = Activation(<span class=\"string\">'relu'</span>)(x)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">return</span> x</span><br></pre></td></tr></table></figure>\n<p>如果将时序信号看作是2D矩阵，则TimeDistributed包装后的Dense就是分别对矩阵的每一行进行全连接。</p>\n<p><strong>把resnet50最后一个stage拿出来做分类层：</strong><br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">classifier_layers</span><span class=\"params\">(x, input_shape, trainable=False)</span>:</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># Stage 5</span></span><br><span class=\"line\">    x = conv_block_td(x, <span class=\"number\">3</span>, [<span class=\"number\">512</span>, <span class=\"number\">512</span>, <span class=\"number\">2048</span>], stage=<span class=\"number\">5</span>, block=<span class=\"string\">'a'</span>, input_shape=input_shape, strides=(<span class=\"number\">2</span>, <span class=\"number\">2</span>), trainable=trainable)</span><br><span class=\"line\">    x = identity_block_td(x, <span class=\"number\">3</span>, [<span class=\"number\">512</span>, <span class=\"number\">512</span>, <span class=\"number\">2048</span>], stage=<span class=\"number\">5</span>, block=<span class=\"string\">'b'</span>, trainable=trainable)</span><br><span class=\"line\">    x = identity_block_td(x, <span class=\"number\">3</span>, [<span class=\"number\">512</span>, <span class=\"number\">512</span>, <span class=\"number\">2048</span>], stage=<span class=\"number\">5</span>, block=<span class=\"string\">'c'</span>, trainable=trainable)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># AVGPOOL</span></span><br><span class=\"line\">    x = TimeDistributed(AveragePooling2D((<span class=\"number\">7</span>, <span class=\"number\">7</span>)), name=<span class=\"string\">'avg_pool'</span>)(x)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">return</span> x</span><br></pre></td></tr></table></figure></p>\n<ul>\n<li>RoiPoolingConv：返回的shape为(1, 32, 7, 7, 512)含义是batch_size,预选框的个数，特征图宽，特征图高度，特征图深度</li>\n<li>TimeDistributed：输入至少为3D张量，下标为1的维度将被认为是时间维。即对以一个维度下的变量当作一个完整变量来看待本文是32。你要实现的目的就是对32个预选宽提出的32个图片做出判断。</li>\n<li>out_class的shape:(?, 32, 21); out_regr的shape:(?, 32, 80)<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">classifier</span><span class=\"params\">(base_layers, input_rois, num_rois, nb_classes = <span class=\"number\">21</span>, trainable=False)</span>:</span></span><br><span class=\"line\"></span><br><span class=\"line\">    pooling_regions = <span class=\"number\">14</span></span><br><span class=\"line\">    input_shape = (num_rois,<span class=\"number\">14</span>,<span class=\"number\">14</span>,<span class=\"number\">1024</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    out_roi_pool = RoiPoolingConv(pooling_regions, num_rois)([base_layers, input_rois])</span><br><span class=\"line\">    out = classifier_layers(out_roi_pool, input_shape=input_shape, trainable=<span class=\"keyword\">True</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    out = TimeDistributed(Flatten())(out)</span><br><span class=\"line\"></span><br><span class=\"line\">    out_class = TimeDistributed(Dense(nb_classes, activation=<span class=\"string\">'softmax'</span>, kernel_initializer=<span class=\"string\">'zero'</span>), name=<span class=\"string\">'dense_class_&#123;&#125;'</span>.format(nb_classes))(out)</span><br><span class=\"line\">    <span class=\"comment\"># note: no regression target for bg class</span></span><br><span class=\"line\">    out_regr = TimeDistributed(Dense(<span class=\"number\">4</span> * (nb_classes<span class=\"number\">-1</span>), activation=<span class=\"string\">'linear'</span>, kernel_initializer=<span class=\"string\">'zero'</span>), name=<span class=\"string\">'dense_regress_&#123;&#125;'</span>.format(nb_classes))(out)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> [out_class, out_regr]</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<p><strong>定义RPN网络：</strong></p>\n<ul>\n<li>x_class:每一个锚点属于前景还是背景【注：这里使用的是sigmoid激活函数所以其输出的通道数是num_anchors】</li>\n<li>x_regr：每一个锚点对应的回归梯度<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">rpn</span><span class=\"params\">(base_layers,num_anchors)</span>:</span></span><br><span class=\"line\"></span><br><span class=\"line\">    x = Convolution2D(<span class=\"number\">512</span>, (<span class=\"number\">3</span>, <span class=\"number\">3</span>), padding=<span class=\"string\">'same'</span>, activation=<span class=\"string\">'relu'</span>, kernel_initializer=<span class=\"string\">'normal'</span>, name=<span class=\"string\">'rpn_conv1'</span>)(base_layers)</span><br><span class=\"line\"></span><br><span class=\"line\">    x_class = Convolution2D(num_anchors, (<span class=\"number\">1</span>, <span class=\"number\">1</span>), activation=<span class=\"string\">'sigmoid'</span>, kernel_initializer=<span class=\"string\">'uniform'</span>, name=<span class=\"string\">'rpn_out_class'</span>)(x)</span><br><span class=\"line\">    x_regr = Convolution2D(num_anchors * <span class=\"number\">4</span>, (<span class=\"number\">1</span>, <span class=\"number\">1</span>), activation=<span class=\"string\">'linear'</span>, kernel_initializer=<span class=\"string\">'zero'</span>, name=<span class=\"string\">'rpn_out_regress'</span>)(x)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">return</span> [x_class, x_regr, base_layers]</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<p><strong>resnet前面部分作为公共层：</strong><br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">nn_base</span><span class=\"params\">(input_tensor=None, trainable=False)</span>:</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># Determine proper input shape</span></span><br><span class=\"line\"></span><br><span class=\"line\">    input_shape = (<span class=\"keyword\">None</span>, <span class=\"keyword\">None</span>, <span class=\"number\">3</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">if</span> input_tensor <span class=\"keyword\">is</span> <span class=\"keyword\">None</span>:</span><br><span class=\"line\">        img_input = Input(shape=input_shape)</span><br><span class=\"line\">    <span class=\"keyword\">else</span>:</span><br><span class=\"line\">        <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> K.is_keras_tensor(input_tensor):</span><br><span class=\"line\">            img_input = Input(tensor=input_tensor, shape=input_shape)</span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            img_input = input_tensor</span><br><span class=\"line\"></span><br><span class=\"line\">    bn_axis = <span class=\"number\">3</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># Zero-Padding</span></span><br><span class=\"line\">    x = ZeroPadding2D((<span class=\"number\">3</span>, <span class=\"number\">3</span>))(img_input)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># Stage 1</span></span><br><span class=\"line\">    x = Convolution2D(<span class=\"number\">64</span>, (<span class=\"number\">7</span>, <span class=\"number\">7</span>), strides=(<span class=\"number\">2</span>, <span class=\"number\">2</span>), name=<span class=\"string\">'conv1'</span>, trainable = trainable)(x)</span><br><span class=\"line\">    x = BatchNormalization(axis=bn_axis, name=<span class=\"string\">'bn_conv1'</span>)(x)</span><br><span class=\"line\">    x = Activation(<span class=\"string\">'relu'</span>)(x)</span><br><span class=\"line\">    x = MaxPooling2D((<span class=\"number\">3</span>, <span class=\"number\">3</span>), strides=(<span class=\"number\">2</span>, <span class=\"number\">2</span>))(x)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># Stage 2</span></span><br><span class=\"line\">    x = conv_block(x, <span class=\"number\">3</span>, [<span class=\"number\">64</span>, <span class=\"number\">64</span>, <span class=\"number\">256</span>], stage=<span class=\"number\">2</span>, block=<span class=\"string\">'a'</span>, strides=(<span class=\"number\">1</span>, <span class=\"number\">1</span>), trainable = trainable)</span><br><span class=\"line\">    x = identity_block(x, <span class=\"number\">3</span>, [<span class=\"number\">64</span>, <span class=\"number\">64</span>, <span class=\"number\">256</span>], stage=<span class=\"number\">2</span>, block=<span class=\"string\">'b'</span>, trainable = trainable)</span><br><span class=\"line\">    x = identity_block(x, <span class=\"number\">3</span>, [<span class=\"number\">64</span>, <span class=\"number\">64</span>, <span class=\"number\">256</span>], stage=<span class=\"number\">2</span>, block=<span class=\"string\">'c'</span>, trainable = trainable)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># Stage 3</span></span><br><span class=\"line\">    x = conv_block(x, <span class=\"number\">3</span>, [<span class=\"number\">128</span>, <span class=\"number\">128</span>, <span class=\"number\">512</span>], stage=<span class=\"number\">3</span>, block=<span class=\"string\">'a'</span>, trainable = trainable)</span><br><span class=\"line\">    x = identity_block(x, <span class=\"number\">3</span>, [<span class=\"number\">128</span>, <span class=\"number\">128</span>, <span class=\"number\">512</span>], stage=<span class=\"number\">3</span>, block=<span class=\"string\">'b'</span>, trainable = trainable)</span><br><span class=\"line\">    x = identity_block(x, <span class=\"number\">3</span>, [<span class=\"number\">128</span>, <span class=\"number\">128</span>, <span class=\"number\">512</span>], stage=<span class=\"number\">3</span>, block=<span class=\"string\">'c'</span>, trainable = trainable)</span><br><span class=\"line\">    x = identity_block(x, <span class=\"number\">3</span>, [<span class=\"number\">128</span>, <span class=\"number\">128</span>, <span class=\"number\">512</span>], stage=<span class=\"number\">3</span>, block=<span class=\"string\">'d'</span>, trainable = trainable)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># Stage 4</span></span><br><span class=\"line\">    x = conv_block(x, <span class=\"number\">3</span>, [<span class=\"number\">256</span>, <span class=\"number\">256</span>, <span class=\"number\">1024</span>], stage=<span class=\"number\">4</span>, block=<span class=\"string\">'a'</span>, trainable = trainable)</span><br><span class=\"line\">    x = identity_block(x, <span class=\"number\">3</span>, [<span class=\"number\">256</span>, <span class=\"number\">256</span>, <span class=\"number\">1024</span>], stage=<span class=\"number\">4</span>, block=<span class=\"string\">'b'</span>, trainable = trainable)</span><br><span class=\"line\">    x = identity_block(x, <span class=\"number\">3</span>, [<span class=\"number\">256</span>, <span class=\"number\">256</span>, <span class=\"number\">1024</span>], stage=<span class=\"number\">4</span>, block=<span class=\"string\">'c'</span>, trainable = trainable)</span><br><span class=\"line\">    x = identity_block(x, <span class=\"number\">3</span>, [<span class=\"number\">256</span>, <span class=\"number\">256</span>, <span class=\"number\">1024</span>], stage=<span class=\"number\">4</span>, block=<span class=\"string\">'d'</span>, trainable = trainable)</span><br><span class=\"line\">    x = identity_block(x, <span class=\"number\">3</span>, [<span class=\"number\">256</span>, <span class=\"number\">256</span>, <span class=\"number\">1024</span>], stage=<span class=\"number\">4</span>, block=<span class=\"string\">'e'</span>, trainable = trainable)</span><br><span class=\"line\">    x = identity_block(x, <span class=\"number\">3</span>, [<span class=\"number\">256</span>, <span class=\"number\">256</span>, <span class=\"number\">1024</span>], stage=<span class=\"number\">4</span>, block=<span class=\"string\">'f'</span>, trainable = trainable)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">return</span> x</span><br></pre></td></tr></table></figure></p>\n<p><strong>搭建网络：</strong><br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># define the base network (resnet here)</span></span><br><span class=\"line\">shared_layers = nn.nn_base(img_input, trainable=<span class=\"keyword\">True</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># define the RPN, built on the base layers</span></span><br><span class=\"line\"><span class=\"comment\"># 9 types of anchors</span></span><br><span class=\"line\">num_anchors = len(C.anchor_box_scales) * len(C.anchor_box_ratios)</span><br><span class=\"line\">rpn = nn.rpn(shared_layers, num_anchors)</span><br><span class=\"line\"></span><br><span class=\"line\">classifier = nn.classifier(shared_layers, roi_input, C.num_rois, nb_classes=len(classes_count), trainable=<span class=\"keyword\">True</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">model_rpn = Model(img_input, rpn[:<span class=\"number\">2</span>])</span><br><span class=\"line\">model_classifier = Model([img_input, roi_input], classifier)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># this is a model that holds both the RPN and the classifier, used to load/save weights for the models</span></span><br><span class=\"line\">model_all = Model([img_input, roi_input], rpn[:<span class=\"number\">2</span>] + classifier)</span><br></pre></td></tr></table></figure></p>\n<p><a href=\"https://github.com/Trouble404/Object-Detection-System-Based-on-CNN-and-Capsule-Network/blob/master/Faster-RCNN/Resnet50/workflow_model.ipynb\" target=\"_blank\" rel=\"noopener\">jupyter notebook</a></p>\n<h3 id=\"【09-07-2018】\"><a href=\"#【09-07-2018】\" class=\"headerlink\" title=\"【09/07/2018】\"></a>【09/07/2018】</h3><h4 id=\"Loss-define\"><a href=\"#Loss-define\" class=\"headerlink\" title=\"Loss define\"></a>Loss define</h4><p><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/6_1.JPG\" alt=\"image\"></p>\n<p>由于涉及到分类和回归，所以需要定义一个多任务损失函数(Multi-task Loss Function)，包括Softmax Classification Loss和Bounding Box Regression Loss，公式定义如下：</p>\n<p>$L({p_{i}},{t_{i}}) = \\frac{1}{N_{cls}}\\sum_{i}L_{cls}(p_{i},p_{i}^{\\ast}) + \\lambda\\frac{1}{N_{reg}}\\sum_{i}L_{reg}(t_{i},t_{i}^{\\ast})$</p>\n<p><strong>Softmax Classification：</strong><br>对于RPN网络的分类层(cls)，其向量维数为2k = 18，考虑整个特征图conv5-3，则输出大小为W×H×18，正好对应conv5-3上每个点有9个anchors，而每个anchor又有两个score(fg/bg)输出，对于单个anchor训练样本，其实是一个二分类问题。为了便于Softmax分类，需要对分类层执行reshape操作，这也是由底层数据结构决定的。<br>在上式中，$p_{i}$为样本分类的概率值，$p_{i}^{\\ast}$为样本的标定值(label)，anchor为正样本时$p_{i}^{\\ast}$为1，为负样本时$p_{i}^{\\ast}$为0，$L_{cls}$为两种类别的对数损失(log loss)。</p>\n<p><strong>Bounding Box Regression：</strong><br>RPN网络的回归层输出向量的维数为4k = 36，回归参数为每个样本的坐标$[x,y,w,h]$，分别为box的中心位置和宽高，考虑三组参数预测框(predicted box)坐标$[x,y,w,h]$，anchor坐标$[x_{a},y_{a},w_{a},h_{a}]$，ground truth坐标$[x^{\\ast},y^{\\ast},w^{\\ast},h^{\\ast}]$，分别计算预测框相对anchor中心位置的偏移量以及宽高的缩放量{$t$}，ground truth相对anchor的偏移量和缩放量{$t^{\\ast}$}</p>\n<p>$t_{x}=\\frac{(x-x_{a})}{w_{a}}$ , $t_{y}=\\frac{(y-y_{a})}{h_{a}}$ , $t_{w}=log(\\frac{w}{w_{a}})$ , $t_{h}=log(\\frac{h}{h_{a}})$ (1)<br>$t_{x}^{\\ast}=\\frac{(x^{\\ast}-x_{a})}{w_{a}}$ , $t_{y}^{\\ast}=\\frac{(y^{\\ast}-y_{a})}{h_{a}}$ , $t_{w}^{\\ast}=log(\\frac{w^{\\ast}}{w_{a}})$ , $t_{h}^{\\ast}=log(\\frac{h^{\\ast}}{h_{a}})$ (2)</p>\n<p>回归目标就是让{t}尽可能地接近${t^{\\ast}}$，所以回归真正预测输出的是${t}$，而训练样本的标定真值为${t^{\\ast}}$。得到预测输出${t}$后，通过上式(1)即可反推获取预测框的真实坐标。</p>\n<p>在损失函数中，回归损失采用Smooth L1函数:</p>\n<p>$$ Smooth_{L1}(x) =\\left{<br>\\begin{aligned}<br>0.5x^{2} \\ \\ |x| \\leqslant 1\\<br>|x| - 0.5 \\ \\ otherwise<br>\\end{aligned}<br>\\right.<br>$$</p>\n<p>$L_{reg} = Smooth_{L1}(t-t^{\\ast})$<br>Smooth L1损失函数曲线如下图所示，相比于L2损失函数，L1对离群点或异常值不敏感，可控制梯度的量级使训练更易收敛。<br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/7_1.JPG\" alt=\"iamge\"></p>\n<p>在损失函数中，$p_{i}^{\\ast}L_{reg}$这一项表示只有目标anchor$(p_{i}^{\\ast}=1)$才有回归损失，其他anchor不参与计算。这里需要注意的是，当样本bbox和ground truth比较接近时(IoU大于某一阈值)，可以认为上式的坐标变换是一种线性变换，因此可将样本用于训练线性回归模型，否则当bbox与ground truth离得较远时，就是非线性问题，用线性回归建模显然不合理，会导致模型不work。分类层(cls)和回归层(reg)的输出分别为{p}和{t}，两项损失函数分别由$N_{cls}$和$N_{reg}$以及一个平衡权重λ归一化。</p>\n<h3 id=\"【10-07-2018】\"><a href=\"#【10-07-2018】\" class=\"headerlink\" title=\"【10/07/2018】\"></a>【10/07/2018】</h3><h4 id=\"loss-code\"><a href=\"#loss-code\" class=\"headerlink\" title=\"loss code\"></a>loss code</h4><p>  generator to iteror, using next() to loop<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">yield</span> np.copy(x_img), [np.copy(y_rpn_cls), np.copy(y_rpn_regr)], img_data_aug</span><br></pre></td></tr></table></figure></p>\n<p>Rpn calculation:<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">img,rpn,img_aug = next(data_gen_train)</span><br></pre></td></tr></table></figure></p>\n<p> <img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/7_2.JPG\" alt=\"image\"></p>\n<p>连续两个def 是装饰器，<br>装饰器其实也就是一个函数，一个用来包装函数的函数，返回一个修改之后的函数对象。经常被用于有切面需求的场景，较为经典的有插入日志、<br>性能测试、事务处理等。装饰器是解决这类问题的绝佳设计，有了装饰器，我们就可以抽离出大量函数中与函数功能本身无关的雷同代码并继续重用。概括的讲，装<br>饰器的作用就是为已经存在的对象添加额外的功能。</p>\n<p>根据：$L$ 的 cls 部分<br>$L({p_{i}},{t_{i}}) = \\frac{1}{N_{cls}}\\sum_{i}L_{cls}(p_{i},p_{i}^{\\ast})$</p>\n<p>在上式中，$p_{i}$为样本分类的概率值，$p_{i}^{\\ast}$为样本的标定值(label)，anchor为正样本时$p_{i}^{\\ast}$为1，为负样本时$p_{i}^{\\ast}$为0，$L_{cls}$为两种类别的对数损失(log loss)。</p>\n<p>因此， 定义 rpn loss cls:<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">rpn_loss_cls</span><span class=\"params\">(num_anchors)</span>:</span></span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">rpn_loss_cls_fixed_num</span><span class=\"params\">(y_true, y_pred)</span>:</span></span><br><span class=\"line\">            <span class=\"comment\"># binary_crossentropy -&gt; logloss</span></span><br><span class=\"line\">            <span class=\"comment\"># epsilon to increase robustness</span></span><br><span class=\"line\">\t\t<span class=\"keyword\">return</span> lambda_rpn_class * K.sum(y_true[:, :, :, :num_anchors] * K.binary_crossentropy(y_pred[:, :, :, :], y_true[:, :, :, num_anchors:])) / K.sum(epsilon + y_true[:, :, :, :num_anchors])</span><br><span class=\"line\">\t<span class=\"keyword\">return</span> rpn_loss_cls_fixed_num</span><br></pre></td></tr></table></figure></p>\n<p>根据$L$ 的 reg 部分<br>$L({p_{i}},{t_{i}}) =  \\lambda\\frac{1}{N_{reg}}\\sum_{i}L_{reg}(t_{i},t_{i}^{\\ast})$<br>在损失函数中，回归损失采用Smooth L1函数:</p>\n<p>$$ Smooth_{L1}(x) =\\left{<br>\\begin{aligned}<br>0.5x^{2} \\ \\ |x| \\leqslant 1\\<br>|x| - 0.5 \\ \\ otherwise<br>\\end{aligned}<br>\\right.<br>$$<br>$L_{reg} = Smooth_{L1}(t-t^{\\ast})$</p>\n<p>因此， 定义 rpn loss reg:<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">rpn_loss_regr</span><span class=\"params\">(num_anchors)</span>:</span></span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">rpn_loss_regr_fixed_num</span><span class=\"params\">(y_true, y_pred)</span>:</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"comment\"># difference of ture value and predicted value</span></span><br><span class=\"line\">\t\tx = y_true[:, :, :, <span class=\"number\">4</span> * num_anchors:] - y_pred</span><br><span class=\"line\">\t\t<span class=\"comment\"># absulote value of difference</span></span><br><span class=\"line\">\t\tx_abs = K.abs(x)</span><br><span class=\"line\">\t\t<span class=\"comment\"># if absulote value less than 1, x_bool == 1, else x_bool = 0</span></span><br><span class=\"line\">\t\tx_bool = K.cast(K.less_equal(x_abs, <span class=\"number\">1.0</span>), tf.float32)</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">return</span> lambda_rpn_regr * K.sum(y_true[:, :, :, :<span class=\"number\">4</span> * num_anchors] * (x_bool * (<span class=\"number\">0.5</span> * x * x) + (<span class=\"number\">1</span> - x_bool) * (x_abs - <span class=\"number\">0.5</span>))) / K.sum(epsilon + y_true[:, :, :, :<span class=\"number\">4</span> * num_anchors])</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"keyword\">return</span> rpn_loss_regr_fixed_num</span><br></pre></td></tr></table></figure></p>\n<p>对class的loss来说用一样的方程，但是class_loss_cls是无差别求loss【这个可以用K.mean，是因为其是无差别的求loss】，不用管是否可用<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">class_loss_regr</span><span class=\"params\">(num_classes)</span>:</span></span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">class_loss_regr_fixed_num</span><span class=\"params\">(y_true, y_pred)</span>:</span></span><br><span class=\"line\">\t\tx = y_true[:, :, <span class=\"number\">4</span>*num_classes:] - y_pred</span><br><span class=\"line\">\t\tx_abs = K.abs(x)</span><br><span class=\"line\">\t\tx_bool = K.cast(K.less_equal(x_abs, <span class=\"number\">1.0</span>), <span class=\"string\">'float32'</span>)</span><br><span class=\"line\">\t\t<span class=\"keyword\">return</span> lambda_cls_regr * K.sum(y_true[:, :, :<span class=\"number\">4</span>*num_classes] * (x_bool * (<span class=\"number\">0.5</span> * x * x) + (<span class=\"number\">1</span> - x_bool) * (x_abs - <span class=\"number\">0.5</span>))) / K.sum(epsilon + y_true[:, :, :<span class=\"number\">4</span>*num_classes])</span><br><span class=\"line\">\t<span class=\"keyword\">return</span> class_loss_regr_fixed_num</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">class_loss_cls</span><span class=\"params\">(y_true, y_pred)</span>:</span></span><br><span class=\"line\">\t<span class=\"keyword\">return</span> lambda_cls_class * K.mean(categorical_crossentropy(y_true[<span class=\"number\">0</span>, :, :], y_pred[<span class=\"number\">0</span>, :, :]))</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"【11-07-2018】\"><a href=\"#【11-07-2018】\" class=\"headerlink\" title=\"【11/07/2018】\"></a>【11/07/2018】</h3><h4 id=\"Iridis\"><a href=\"#Iridis\" class=\"headerlink\" title=\"Iridis\"></a>Iridis</h4><h4 id=\"High-Performance-Computing-HPC\"><a href=\"#High-Performance-Computing-HPC\" class=\"headerlink\" title=\"High Performance Computing (HPC)\"></a>High Performance Computing (HPC)</h4><p><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/8_1.jpg\" alt=\"image\"><br><a href=\"https://www.southampton.ac.uk/isolutions/staff/high-performance-computing.page\" target=\"_blank\" rel=\"noopener\">Introduction</a></p>\n<p>Iridis 5 specifications</p>\n<ul>\n<li>#251 in hte world(Based on July 2018 TOPP500 list) with $R_{peak}\\sim1305.6\\ TFlops/s$</li>\n<li>464 2.0 GHz nodes with 40 cores per node, 192 GB memeory</li>\n<li>10 nodes with 4xGTX1080TI GPUs, 28 cores(hyper-threaded), 128 GB memeory</li>\n<li>10 nodes with 2xVolta Tesia GPUs, same as thandard compute</li>\n<li>2.2 PB disk with paraller file system (&gt;12GB\\s)</li>\n<li>£5M Project delivered by OCF/IBM</li>\n</ul>\n<p><a href=\"https://mobaxterm.mobatek.net/\" target=\"_blank\" rel=\"noopener\">MobaXterm</a></p>\n<h4 id=\"create-my-own-conda-envieroment\"><a href=\"#create-my-own-conda-envieroment\" class=\"headerlink\" title=\"create my own conda envieroment\"></a>create my own conda envieroment</h4><p>Fllowing instroduction before</p>\n<h4 id=\"Slurm-command\"><a href=\"#Slurm-command\" class=\"headerlink\" title=\"Slurm command\"></a>Slurm command</h4><table>\n<thead>\n<tr>\n<th>Command</th>\n<th>Definition</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>sbatch</td>\n<td>Submits job scripts into system for execution (queued)</td>\n</tr>\n<tr>\n<td>scancel</td>\n<td>Cancels a job</td>\n</tr>\n<tr>\n<td>scontrol</td>\n<td>Used to display Slurm state, several options only available to root</td>\n</tr>\n<tr>\n<td>sinfo</td>\n<td>Display state of partitions and nodes</td>\n</tr>\n<tr>\n<td>squeue</td>\n<td>Display state of jobs</td>\n</tr>\n<tr>\n<td>salloc</td>\n<td>Submit a job for execution, or initiate job in real time</td>\n</tr>\n</tbody>\n</table>\n<p><strong> Bash script</strong><br><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#!/bin/bash</span></span><br><span class=\"line\"><span class=\"comment\">#SBATCH -J faster_rcnn </span></span><br><span class=\"line\"><span class=\"comment\">#SBATCH -o train_7.out</span></span><br><span class=\"line\"><span class=\"comment\">#SBATCH --ntasks=28</span></span><br><span class=\"line\"><span class=\"comment\">#SBATCH --nodes=1</span></span><br><span class=\"line\"><span class=\"comment\">#SBATCH --ntasks-per-node=8</span></span><br><span class=\"line\"><span class=\"comment\">#SBATCH --time=00:05:00</span></span><br><span class=\"line\"><span class=\"comment\">#SBATCH --gres=gpu:1</span></span><br><span class=\"line\"><span class=\"comment\">#SBATCH -p lyceum</span></span><br><span class=\"line\"></span><br><span class=\"line\">module load conda</span><br><span class=\"line\">module load cuda</span><br><span class=\"line\"><span class=\"built_in\">source</span> activate project</span><br><span class=\"line\">python test_frcnn.py</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"【12-13-07-2018】\"><a href=\"#【12-13-07-2018】\" class=\"headerlink\" title=\"【12~13/07/2018】\"></a>【12~13/07/2018】</h3><h4 id=\"change-plan\"><a href=\"#change-plan\" class=\"headerlink\" title=\"change plan\"></a>change plan</h4><p>因为faster r-cnn的搭建过程比想象中复杂，在咨询老师的意见以后，决定砍掉capsule的测试，专心faster-rcnn并且找到一些fine turn的方法。</p>\n<p>1）基础特征提取网络<br>ResNet，IncRes V2，ResNeXt 都是显著超越 VGG 的特征网络，当然网络的改进带来的是计算量的增加。</p>\n<p>2）RPN<br>通过更准确地  RPN 方法，减少 Proposal 个数，提高准确度。</p>\n<p>3）改进分类回归层<br>分类回归层的改进，包括 通过多层来提取特征 和 判别。</p>\n<hr>\n<p>@改进1：ION<br>论文：Inside outside net: Detecting objects in context with skip pooling and recurrent neural networks<br>提出了两个方面的贡献：</p>\n<p>1）Inside Net<br>所谓 Inside 是指在 ROI 区域之内，通过连接不同 Scale 下的 Feature Map，实现多尺度特征融合。<br>这里采用的是 Skip-Pooling，从 conv3-4-5-context 分别提取特征，后面会讲到。<br>多尺度特征 能够提升对小目标的检测精度。</p>\n<p>2）Outside Net<br>所谓 Outside 是指 ROI 区域之外，也就是目标周围的 上下文（Contextual）信息。<br>作者通过添加了两个 RNN 层（修改后的 IRNN）实现上下文特征提取。<br>上下文信息 对于目标遮挡有比较好的适应。</p>\n<hr>\n<p>@改进2：多尺度之 HyperNet<br>论文：Hypernet: Towards accurate region proposal generation and joint object detection<br>基于 Region Proposal 的方法，通过多尺度的特征提取来提高对小目标的检测能力，来看网络框图：<br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/8_2.JPG\" alt=\"image\"><br>分为 三个主要特征 来介绍（对应上面网络拓扑图的 三个红色框）：</p>\n<p>1）Hyper Feature Extraction （特征提取）<br>多尺度特征提取是本文的核心点，作者的方法稍微有所不同，他是以中间的 Feature 尺度为参考，前面的层通过 Max Pooling 到对应大小，后面的层则是通过 反卷积（Deconv）进行放大。<br>多尺度 Feature ConCat 的时候，作者使用了 LRN进行归一化（类似于 ION 的 L2 Norm）。</p>\n<p>2）Region Proposal Generation（建议框生成）<br>作者设计了一个轻量级的 ConvNet，与 RPN 的区别不大（为写论文强创新)。<br>一个 ROI Pooling层，一个 Conv 层，还有一个 FC 层。每个 Position 通过 ROI Pooling 得到一个 13*13 的 bin，通过 Conv（3*3*4）层得到一个 13*13*4 的 Cube，再通过 FC 层得到一个 256d 的向量。<br>后面的 Score+ BBox_Reg 与 Faster并无区别，用于目标得分 和 Location OffSet。<br>考虑到建议框的 Overlap，作者用了 Greedy NMS 去重，文中将 IOU参考设为 0.7，每个 Image 保留 1k 个 Region，并选择其中 Top-200 做 Detetcion。<br>通过对比，要优于基于 Edge Box 重排序的 Deep Box，从多尺度上考虑比 Deep Proposal 效果更好。</p>\n<p>3）Object Detection（目标检测）<br>与 Fast RCNN基本一致，在原来的检测网络基础上做了两点改进：<br>a）在 FC 层之前添加了一个 卷积层（3<em>3</em>63），对特征有效降维；<br>b）将 DropOut 从 0.5 降到 0.25；<br>另外，与 Proposal一样采用了 NMS 进行 Box抑制，但由于之前已经做了，这一步的意义不大。</p>\n<hr>\n<p>@改进3：多尺度之 MSCNN<br>论文：A Unified Multi-scale Deep Convolutional Neural Network for Fast Object Detection<br>a）原图缩放，多个Scale的原图对应不同Scale的Feature；<br>该方法计算多次Scale，每个Scale提取一次Feature，计算量巨大。</p>\n<p>b）一幅输入图像对应多个分类器；<br>不需要重复提取特征图，但对分类器要求很高，一般很难得到理想的结果。</p>\n<p>c）原图缩放，少量Scale原图-&gt;少量特征图-&gt;多个Model模板；<br>相当于对 a）和 b）的 Trade-Off。</p>\n<p>d）原图缩放，少量Scale原图-&gt;少量特征图-&gt;特征图插值-&gt;1个Model；</p>\n<p>e）RCNN方法，Proposal直接给到CNN；<br>和 a）全图计算不同，只针对Patch计算。</p>\n<p>f）RPN方法，特征图是通过CNN卷积层得到；<br>和 b）类似，不过采用的是同尺度的不同模板，容易导致尺度不一致问题。</p>\n<p>g）上套路，提出我们自己的方法，多尺度特征图；<br>每个尺度特征图对应一个 输出模板，每个尺度cover一个目标尺寸范围。</p>\n<hr>\n<p>NMS和soft-nms算法<br>Repulsion loss：遮挡下的行人检测 加入overlapping 与不同的 loss<br>融合以上两个到faster rcnn中</p>\n<h3 id=\"【16-20-07-2018】\"><a href=\"#【16-20-07-2018】\" class=\"headerlink\" title=\"【16~20/07/2018】\"></a>【16~20/07/2018】</h3><p>旅行</p>\n<h3 id=\"【23-07-2018】\"><a href=\"#【23-07-2018】\" class=\"headerlink\" title=\"【23/07/2018】\"></a>【23/07/2018】</h3><h4 id=\"fix-boxes-location-by-regrident\"><a href=\"#fix-boxes-location-by-regrident\" class=\"headerlink\" title=\"fix boxes location by regrident\"></a>fix boxes location by regrident</h4><p>使用regr对anchor所确定的框进行修正</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"string\">\"\"\" fix boxes with grident</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">@param X: current cordinates of box</span></span><br><span class=\"line\"><span class=\"string\">@param T: coresspoding grident</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">@return: Fixed cordinates of box</span></span><br><span class=\"line\"><span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">apply_regr_np</span><span class=\"params\">(X, T)</span>:</span></span><br><span class=\"line\">\t<span class=\"keyword\">try</span>:</span><br><span class=\"line\">\t\tx = X[<span class=\"number\">0</span>, :, :]</span><br><span class=\"line\">\t\ty = X[<span class=\"number\">1</span>, :, :]</span><br><span class=\"line\">\t\tw = X[<span class=\"number\">2</span>, :, :]</span><br><span class=\"line\">\t\th = X[<span class=\"number\">3</span>, :, :]</span><br><span class=\"line\"></span><br><span class=\"line\">\t\ttx = T[<span class=\"number\">0</span>, :, :]</span><br><span class=\"line\">\t\tty = T[<span class=\"number\">1</span>, :, :]</span><br><span class=\"line\">\t\ttw = T[<span class=\"number\">2</span>, :, :]</span><br><span class=\"line\">\t\tth = T[<span class=\"number\">3</span>, :, :]</span><br></pre></td></tr></table></figure>\n<p>$t_{x}=\\frac{(x-x_{a})}{w_{a}}$ , $t_{y}=\\frac{(y-y_{a})}{h_{a}}$ , $t_{w}=log(\\frac{w}{w_{a}})$ , $t_{h}=log(\\frac{h}{h_{a}})$ (1)<br>$t_{x}^{\\ast}=\\frac{(x^{\\ast}-x_{a})}{w_{a}}$ , $t_{y}^{\\ast}=\\frac{(y^{\\ast}-y_{a})}{h_{a}}$ , $t_{w}^{\\ast}=log(\\frac{w^{\\ast}}{w_{a}})$ , $t_{h}^{\\ast}=log(\\frac{h^{\\ast}}{h_{a}})$ (2)</p>\n<p>回归目标就是让{t}尽可能地接近${t^{\\ast}}$，所以回归真正预测输出的是${t}$，而训练样本的标定真值为${t^{\\ast}}$。得到预测输出${t}$后，通过上式(1)即可反推获取预测框的真实坐标。</p>\n<p>过程：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">\t<span class=\"comment\"># centre cordinate</span></span><br><span class=\"line\">\tcx = x + w/<span class=\"number\">2.</span></span><br><span class=\"line\">\tcy = y + h/<span class=\"number\">2.</span></span><br><span class=\"line\">\t<span class=\"comment\"># fixed centre cordinate</span></span><br><span class=\"line\">\tcx1 = tx * w + cx</span><br><span class=\"line\">\tcy1 = ty * h + cy</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"comment\"># fixed wdith and height</span></span><br><span class=\"line\">\tw1 = np.exp(tw.astype(np.float64)) * w</span><br><span class=\"line\">\th1 = np.exp(th.astype(np.float64)) * h</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"comment\"># fixed left top corner's cordinate</span></span><br><span class=\"line\">\tx1 = cx1 - w1/<span class=\"number\">2.</span></span><br><span class=\"line\">\ty1 = cy1 - h1/<span class=\"number\">2.</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"comment\"># apporximate</span></span><br><span class=\"line\">\tx1 = np.round(x1)</span><br><span class=\"line\">\ty1 = np.round(y1)</span><br><span class=\"line\">\tw1 = np.round(w1)</span><br><span class=\"line\">\th1 = np.round(h1)</span><br><span class=\"line\">\t<span class=\"keyword\">return</span> np.stack([x1, y1, w1, h1])</span><br><span class=\"line\"><span class=\"keyword\">except</span> Exception <span class=\"keyword\">as</span> e:</span><br><span class=\"line\">\tprint(e)</span><br><span class=\"line\">\t<span class=\"keyword\">return</span> X</span><br></pre></td></tr></table></figure></p>\n<h4 id=\"NMS-no-max-suppression\"><a href=\"#NMS-no-max-suppression\" class=\"headerlink\" title=\"NMS no max suppression\"></a>NMS no max suppression</h4><p>该函数的作用是从所给定的所有预选框中选择指定个数最合理的边框。</p>\n<p>定义：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"string\">\"\"\" NMS , delete overlapping box</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">@param boxes: (n,4) box and coresspoding cordinates</span></span><br><span class=\"line\"><span class=\"string\">@param probs: (n,) box adn coresspding possibility</span></span><br><span class=\"line\"><span class=\"string\">@param overlap_thresh: treshold of delet box overlapping</span></span><br><span class=\"line\"><span class=\"string\">@param max_boxes: maximum keeping number of boxes</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">@return: boxes: boxes cordinates(x1,y1,x2,y2)</span></span><br><span class=\"line\"><span class=\"string\">@return: probs: coresspoding possibility</span></span><br><span class=\"line\"><span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">non_max_suppression_fast</span><span class=\"params\">(boxes, probs, overlap_thresh=<span class=\"number\">0.9</span>, max_boxes=<span class=\"number\">300</span>)</span>:</span></span><br></pre></td></tr></table></figure></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">if</span> len(boxes) == <span class=\"number\">0</span>:</span><br><span class=\"line\">   <span class=\"keyword\">return</span> []</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># grab the coordinates of the bounding boxes</span></span><br><span class=\"line\">x1 = boxes[:, <span class=\"number\">0</span>]</span><br><span class=\"line\">y1 = boxes[:, <span class=\"number\">1</span>]</span><br><span class=\"line\">x2 = boxes[:, <span class=\"number\">2</span>]</span><br><span class=\"line\">y2 = boxes[:, <span class=\"number\">3</span>]</span><br><span class=\"line\"></span><br><span class=\"line\">np.testing.assert_array_less(x1, x2)</span><br><span class=\"line\">np.testing.assert_array_less(y1, y2)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># if the bounding boxes integers, convert them to floats --</span></span><br><span class=\"line\"><span class=\"comment\"># this is important since we'll be doing a bunch of divisions</span></span><br><span class=\"line\"><span class=\"keyword\">if</span> boxes.dtype.kind == <span class=\"string\">\"i\"</span>:</span><br><span class=\"line\">\tboxes = boxes.astype(<span class=\"string\">\"float\"</span>)</span><br></pre></td></tr></table></figure>\n<p>对输入的数据进行确认<br>不能为空<br>左上角的坐标小于右下角<br>数据类型的转换<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># initialize the list of picked indexes\t</span></span><br><span class=\"line\">pick = []</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># calculate the areas</span></span><br><span class=\"line\">area = (x2 - x1) * (y2 - y1)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># sort the bounding boxes </span></span><br><span class=\"line\">idxs = np.argsort(probs)</span><br></pre></td></tr></table></figure></p>\n<p>pick（拾取）用来存放边框序号<br>计算框的面积<br>probs按照概率从小到大排序<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">while</span> len(idxs) &gt; <span class=\"number\">0</span>:</span><br><span class=\"line\"><span class=\"comment\"># grab the last index in the indexes list and add the</span></span><br><span class=\"line\"><span class=\"comment\"># index value to the list of picked indexes</span></span><br><span class=\"line\">last = len(idxs) - <span class=\"number\">1</span></span><br><span class=\"line\">i = idxs[last]</span><br><span class=\"line\">pick.append(i)</span><br></pre></td></tr></table></figure></p>\n<p>接下来就是按照概率从大到小取出框，且框的重合度不可以高于overlap_thresh。代码的思路是这样的：</p>\n<p>每一次取概率最大的框（即idxs最后一个）<br>删除掉剩下的框中重和度高于overlap_thresh的框<br>直到取满max_boxes为止<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># find the intersection</span></span><br><span class=\"line\"></span><br><span class=\"line\">xx1_int = np.maximum(x1[i], x1[idxs[:last]])</span><br><span class=\"line\">yy1_int = np.maximum(y1[i], y1[idxs[:last]])</span><br><span class=\"line\">xx2_int = np.minimum(x2[i], x2[idxs[:last]])</span><br><span class=\"line\">yy2_int = np.minimum(y2[i], y2[idxs[:last]])</span><br><span class=\"line\"></span><br><span class=\"line\">ww_int = np.maximum(<span class=\"number\">0</span>, xx2_int - xx1_int)</span><br><span class=\"line\">hh_int = np.maximum(<span class=\"number\">0</span>, yy2_int - yy1_int)</span><br><span class=\"line\"></span><br><span class=\"line\">area_int = ww_int * hh_int</span><br></pre></td></tr></table></figure></p>\n<p>取出idxs队列中最大概率框的序号，将其添加到pick中<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># find the union</span></span><br><span class=\"line\">area_union = area[i] + area[idxs[:last]] - area_int</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># compute the ratio of overlap</span></span><br><span class=\"line\">overlap = area_int/(area_union + <span class=\"number\">1e-6</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># delete all indexes from the index list that have</span></span><br><span class=\"line\">idxs = np.delete(idxs, np.concatenate(([last],np.where(overlap &gt; overlap_thresh)[<span class=\"number\">0</span>])))</span><br></pre></td></tr></table></figure></p>\n<p>计算取出来的框与剩下来的框区域的交集<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">if</span> len(pick) &gt;= max_boxes:</span><br><span class=\"line\">   <span class=\"keyword\">break</span></span><br></pre></td></tr></table></figure></p>\n<p>计算重叠率，然后删除掉重叠率较高的位置[np.concatest]，是因为最后一个位置你已经用过了，就得将其从队列中删掉<br>当取足max_boxes框，停止循环<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">boxes = boxes[pick].astype(<span class=\"string\">\"int\"</span>)</span><br><span class=\"line\">probs = probs[pick]</span><br><span class=\"line\"><span class=\"keyword\">return</span> boxes, probs</span><br></pre></td></tr></table></figure></p>\n<p>返回pick内存取的边框和对应的概率</p>\n<h3 id=\"【24-07-2018】\"><a href=\"#【24-07-2018】\" class=\"headerlink\" title=\"【24/07/2018】\"></a>【24/07/2018】</h3><h4 id=\"rpn-to-porposal-fixed\"><a href=\"#rpn-to-porposal-fixed\" class=\"headerlink\" title=\"rpn to porposal fixed\"></a>rpn to porposal fixed</h4><p>该函数的作用是将rpn网络的预测结果转化到一个个预选框<br>函数流程：<br>遍历anchor_size，在遍历anchor_ratio</p>\n<p>得到框的长宽在原图上的映射</p>\n<p>得到相应尺寸的框对应的回归梯度，将深度都放到第一个维度<br>注1：regr_layer[0, :, :, 4 * curr_layer:4 * curr_layer + 4]当某一个维度的取值为一个值时，那么新的变量就会减小一个维度<br>注2：curr_layer代表的是特定长度和比例的框所代表的编号</p>\n<p>得到anchor对应的（x,y,w,h）</p>\n<p>使用regr对anchor所确定的框进行修正</p>\n<p>对修正后的边框一些不合理的地方进行矫正。<br>如，边框回归后的左上角和右下角的点不能超过图片外，框的宽高不可以小于0<br>注：得到框的形式是（x1,y1,x2,y2）</p>\n<p>得到all_boxes形状是（n,4），和每一个框对应的概率all_probs形状是（n,）</p>\n<p>删除掉一些不合理的点，即右下角的点值要小于左上角的点值<br>注：np.where() 返回位置信息，这也是删除不符合要求点的一种方法<br>np.delete(all_boxes, idxs, 0)最后一个参数是在哪一个维度删除</p>\n<p>最后是根据要求选取指定个数的合理预选框。这一步是重要的，因为每一个点可以有9个预选框，而又拥有很多点，一张图片可能会有几万个预选框。而经过这一步预选迅速下降到几百个。<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"string\">\"\"\" rpn to porposal</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">@param rpn_layer: porposal's coresspoding possibiliy, whether item exciseted</span></span><br><span class=\"line\"><span class=\"string\">@param regr_layer: porposal's coresspoding regrident</span></span><br><span class=\"line\"><span class=\"string\">@param C: Configuration</span></span><br><span class=\"line\"><span class=\"string\">@param dim_ordering: Dimensional organization</span></span><br><span class=\"line\"><span class=\"string\">@param use_regr=True: wether use regurident to fix proposal</span></span><br><span class=\"line\"><span class=\"string\">@param max_boxes=300: max boxes after apply this function</span></span><br><span class=\"line\"><span class=\"string\">@param overlap_thresh=0.9: threshold of overlapping</span></span><br><span class=\"line\"><span class=\"string\">@param C: Configuration</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">@return: max_boxes proposal with format (x1,y1,x2,y2)</span></span><br><span class=\"line\"><span class=\"string\">\"\"\"</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">rpn_to_roi</span><span class=\"params\">(rpn_layer, regr_layer, C, dim_ordering, use_regr=True, max_boxes=<span class=\"number\">300</span>,overlap_thresh=<span class=\"number\">0.9</span>)</span>:</span></span><br><span class=\"line\">\t<span class=\"comment\"># std_scaling default 4</span></span><br><span class=\"line\">\tregr_layer = regr_layer / C.std_scaling</span><br><span class=\"line\"></span><br><span class=\"line\">\tanchor_sizes = C.anchor_box_scales</span><br><span class=\"line\">\tanchor_ratios = C.anchor_box_ratios</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"keyword\">assert</span> rpn_layer.shape[<span class=\"number\">0</span>] == <span class=\"number\">1</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"comment\"># obtain img's width and height's matrix</span></span><br><span class=\"line\">\t(rows, cols) = rpn_layer.shape[<span class=\"number\">1</span>:<span class=\"number\">3</span>]</span><br><span class=\"line\"></span><br><span class=\"line\">\tcurr_layer = <span class=\"number\">0</span></span><br><span class=\"line\"></span><br><span class=\"line\">\tA = np.zeros((<span class=\"number\">4</span>, rpn_layer.shape[<span class=\"number\">1</span>], rpn_layer.shape[<span class=\"number\">2</span>], rpn_layer.shape[<span class=\"number\">3</span>]))</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"comment\"># anchor size is [128, 256, 512]</span></span><br><span class=\"line\">\t<span class=\"keyword\">for</span> anchor_size <span class=\"keyword\">in</span> anchor_sizes:</span><br><span class=\"line\">\t\t<span class=\"comment\"># anchor ratio is [1,2,1]</span></span><br><span class=\"line\">\t\t<span class=\"keyword\">for</span> anchor_ratio <span class=\"keyword\">in</span> anchor_ratios:</span><br><span class=\"line\">\t\t\t<span class=\"comment\"># rpn_stride = 16</span></span><br><span class=\"line\">\t\t\t<span class=\"comment\"># obatin anchor's weidth and height on feature map</span></span><br><span class=\"line\">\t\t\tanchor_x = (anchor_size * anchor_ratio[<span class=\"number\">0</span>])/C.rpn_stride</span><br><span class=\"line\">\t\t\tanchor_y = (anchor_size * anchor_ratio[<span class=\"number\">1</span>])/C.rpn_stride</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t<span class=\"comment\"># obtain current regrident</span></span><br><span class=\"line\">\t\t\t<span class=\"comment\"># when one dimentional obtain a value, the new varirant will decrease one dimenttion</span></span><br><span class=\"line\">\t\t\tregr = regr_layer[<span class=\"number\">0</span>, :, :, <span class=\"number\">4</span> * curr_layer:<span class=\"number\">4</span> * curr_layer + <span class=\"number\">4</span>]</span><br><span class=\"line\">\t\t\t<span class=\"comment\"># put depth to first bacause tensorflow as backend</span></span><br><span class=\"line\">\t\t\tregr = np.transpose(regr, (<span class=\"number\">2</span>, <span class=\"number\">0</span>, <span class=\"number\">1</span>))</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t<span class=\"comment\"># The rows of the output array X are copies of the vector x; columns of the output array Y are copies of the vector y</span></span><br><span class=\"line\">\t\t\t<span class=\"comment\"># each cordinartes of matrix cls and rows</span></span><br><span class=\"line\">\t\t\tX, Y = np.meshgrid(np.arange(cols),np. arange(rows))</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t<span class=\"comment\"># obtain anchors's (x,y,w,h)</span></span><br><span class=\"line\">\t\t\tA[<span class=\"number\">0</span>, :, :, curr_layer] = X - anchor_x/<span class=\"number\">2</span></span><br><span class=\"line\">\t\t\tA[<span class=\"number\">1</span>, :, :, curr_layer] = Y - anchor_y/<span class=\"number\">2</span></span><br><span class=\"line\">\t\t\tA[<span class=\"number\">2</span>, :, :, curr_layer] = anchor_x</span><br><span class=\"line\">\t\t\tA[<span class=\"number\">3</span>, :, :, curr_layer] = anchor_y</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t<span class=\"comment\"># fix boxes with grident</span></span><br><span class=\"line\">\t\t\t<span class=\"keyword\">if</span> use_regr:</span><br><span class=\"line\">\t\t\t\t<span class=\"comment\"># fixed corinates of box</span></span><br><span class=\"line\">\t\t\t\tA[:, :, :, curr_layer] = apply_regr_np(A[:, :, :, curr_layer], regr)</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t<span class=\"comment\"># fix unreasonable cordinates</span></span><br><span class=\"line\">\t\t\t<span class=\"comment\"># np.maximum(1,[]) will set the value less than 1 in [] to 1</span></span><br><span class=\"line\">\t\t\t<span class=\"comment\"># box's width and height can't less than 0</span></span><br><span class=\"line\">\t\t\tA[<span class=\"number\">2</span>, :, :, curr_layer] = np.maximum(<span class=\"number\">1</span>, A[<span class=\"number\">2</span>, :, :, curr_layer])</span><br><span class=\"line\">\t\t\tA[<span class=\"number\">3</span>, :, :, curr_layer] = np.maximum(<span class=\"number\">1</span>, A[<span class=\"number\">3</span>, :, :, curr_layer])</span><br><span class=\"line\">\t\t\t<span class=\"comment\"># fixed right bottom cordinates</span></span><br><span class=\"line\">\t\t\tA[<span class=\"number\">2</span>, :, :, curr_layer] += A[<span class=\"number\">0</span>, :, :, curr_layer]</span><br><span class=\"line\">\t\t\tA[<span class=\"number\">3</span>, :, :, curr_layer] += A[<span class=\"number\">1</span>, :, :, curr_layer]</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t<span class=\"comment\"># left top corner cordinates can't out image</span></span><br><span class=\"line\">\t\t\tA[<span class=\"number\">0</span>, :, :, curr_layer] = np.maximum(<span class=\"number\">0</span>, A[<span class=\"number\">0</span>, :, :, curr_layer])</span><br><span class=\"line\">\t\t\tA[<span class=\"number\">1</span>, :, :, curr_layer] = np.maximum(<span class=\"number\">0</span>, A[<span class=\"number\">1</span>, :, :, curr_layer])</span><br><span class=\"line\">\t\t\t<span class=\"comment\"># right bottom corner cordinates can't out img</span></span><br><span class=\"line\">\t\t\tA[<span class=\"number\">2</span>, :, :, curr_layer] = np.minimum(cols<span class=\"number\">-1</span>, A[<span class=\"number\">2</span>, :, :, curr_layer])</span><br><span class=\"line\">\t\t\tA[<span class=\"number\">3</span>, :, :, curr_layer] = np.minimum(rows<span class=\"number\">-1</span>, A[<span class=\"number\">3</span>, :, :, curr_layer])</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t<span class=\"comment\"># next layer</span></span><br><span class=\"line\">\t\t\tcurr_layer += <span class=\"number\">1</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"comment\"># obtain (n,4) object and coresspoding cordinate</span></span><br><span class=\"line\">\tall_boxes = np.reshape(A.transpose((<span class=\"number\">0</span>, <span class=\"number\">3</span>, <span class=\"number\">1</span>,<span class=\"number\">2</span>)), (<span class=\"number\">4</span>, <span class=\"number\">-1</span>)).transpose((<span class=\"number\">1</span>, <span class=\"number\">0</span>))</span><br><span class=\"line\">\t<span class=\"comment\"># obtain(n,) object and creoespdoing possibility</span></span><br><span class=\"line\">\tall_probs = rpn_layer.transpose((<span class=\"number\">0</span>, <span class=\"number\">3</span>, <span class=\"number\">1</span>, <span class=\"number\">2</span>)).reshape((<span class=\"number\">-1</span>))</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"comment\"># cordinates of left top and right bottom of box</span></span><br><span class=\"line\">\tx1 = all_boxes[:, <span class=\"number\">0</span>]</span><br><span class=\"line\">\ty1 = all_boxes[:, <span class=\"number\">1</span>]</span><br><span class=\"line\">\tx2 = all_boxes[:, <span class=\"number\">2</span>]</span><br><span class=\"line\">\ty2 = all_boxes[:, <span class=\"number\">3</span>]</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"comment\"># find where right cordinate bigger than left cordinate</span></span><br><span class=\"line\">\tidxs = np.where((x1 - x2 &gt;= <span class=\"number\">0</span>) | (y1 - y2 &gt;= <span class=\"number\">0</span>))</span><br><span class=\"line\">\t<span class=\"comment\"># delete thoese point at 0 dimentional -&gt; all boxes</span></span><br><span class=\"line\">\tall_boxes = np.delete(all_boxes, idxs, <span class=\"number\">0</span>)</span><br><span class=\"line\">\tall_probs = np.delete(all_probs, idxs, <span class=\"number\">0</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"comment\"># apply NMS to reduce overlapping boxes</span></span><br><span class=\"line\">\tresult = non_max_suppression_fast(all_boxes, all_probs, overlap_thresh=overlap_thresh, max_boxes=max_boxes)[<span class=\"number\">0</span>]</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"keyword\">return</span> result</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"【25-07-2018】\"><a href=\"#【25-07-2018】\" class=\"headerlink\" title=\"【25/07/2018】\"></a>【25/07/2018】</h3><h4 id=\"generate-classifier’s-trainning-data\"><a href=\"#generate-classifier’s-trainning-data\" class=\"headerlink\" title=\"generate classifier’s trainning data\"></a>generate classifier’s trainning data</h4><p>该函数的作用是生成classifier网络训练的数据,需要注意的是它对提供的预选框还会做一次选择就是将容易判断的背景删除</p>\n<p>代码流程：<br>得到图片的基本信息，并将图片的最短边规整到相应的长度。并将bboxes的长度做相应的变化</p>\n<p>遍历所有的预选框R, 将每一个预选框与所有的bboxes求交并比，记录最大交并比。用来确定该预选框的类别。</p>\n<p>对最佳的交并比作不同的判断:<br>当最佳交并比小于最小的阈值时，放弃概框。因为，交并比太低就说明是很好判断的背景没必要训练。当大于最小阈值时，则保留相关的边框信息<br>当在最小和最大之间，就认为是背景。有必要进行训练。<br>大于最大阈值时认为是物体，计算其边框回归梯度</p>\n<p>得到该类别对应的数字<br>将该数字对应的地方置为1【one-hot】<br>将该类别加入到y_class_num<br>coords是用来存储边框回归梯度的，labels来决定是否要加入计算loss中<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">class_num = <span class=\"number\">2</span></span><br><span class=\"line\">class_label = <span class=\"number\">10</span> * [<span class=\"number\">0</span>]</span><br><span class=\"line\">print(class_label)</span><br><span class=\"line\">class_label[class_num] = <span class=\"number\">1</span></span><br><span class=\"line\">print(class_label)</span><br><span class=\"line\">输出：</span><br><span class=\"line\">[<span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>]</span><br><span class=\"line\">[<span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">1</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>]</span><br></pre></td></tr></table></figure></p>\n<p>如果不是背景的话，计算相应的回归梯度</p>\n<p>返回数据</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br><span class=\"line\">107</span><br><span class=\"line\">108</span><br><span class=\"line\">109</span><br><span class=\"line\">110</span><br><span class=\"line\">111</span><br><span class=\"line\">112</span><br><span class=\"line\">113</span><br><span class=\"line\">114</span><br><span class=\"line\">115</span><br><span class=\"line\">116</span><br><span class=\"line\">117</span><br><span class=\"line\">118</span><br><span class=\"line\">119</span><br><span class=\"line\">120</span><br><span class=\"line\">121</span><br><span class=\"line\">122</span><br><span class=\"line\">123</span><br><span class=\"line\">124</span><br><span class=\"line\">125</span><br><span class=\"line\">126</span><br><span class=\"line\">127</span><br><span class=\"line\">128</span><br><span class=\"line\">129</span><br><span class=\"line\">130</span><br><span class=\"line\">131</span><br><span class=\"line\">132</span><br><span class=\"line\">133</span><br><span class=\"line\">134</span><br><span class=\"line\">135</span><br><span class=\"line\">136</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"string\">\"\"\" generate classifier training data</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">@param R: porposal -&gt; boxes</span></span><br><span class=\"line\"><span class=\"string\">@param img_data: image data</span></span><br><span class=\"line\"><span class=\"string\">@param C: configuration</span></span><br><span class=\"line\"><span class=\"string\">@param class_mapping: classes and coresspoding numbers</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">@return: np.expand_dims(X, axis=0): boxes after filter</span></span><br><span class=\"line\"><span class=\"string\">@return: np.expand_dims(Y1, axis=0): boxes coresspoding class</span></span><br><span class=\"line\"><span class=\"string\">@return: np.expand_dims(Y2, axis=0): boxes coresspoding regurident</span></span><br><span class=\"line\"><span class=\"string\">@return IoUs: IOU</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">calc_iou</span><span class=\"params\">(R, img_data, C, class_mapping)</span>:</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"comment\"># obtain boxxes information from img data</span></span><br><span class=\"line\">\tbboxes = img_data[<span class=\"string\">'bboxes'</span>]</span><br><span class=\"line\">\t<span class=\"comment\"># obtain width and height of img</span></span><br><span class=\"line\">\t(width, height) = (img_data[<span class=\"string\">'width'</span>], img_data[<span class=\"string\">'height'</span>])</span><br><span class=\"line\">\t<span class=\"comment\"># get image dimensions for resizing</span></span><br><span class=\"line\">\t<span class=\"comment\"># Fix image's shortest edge to config setting: eg: 600</span></span><br><span class=\"line\">\t(resized_width, resized_height) = get_new_img_size(width, height, C.im_size)</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"comment\"># record parameters, bboxes cordinates on feature map</span></span><br><span class=\"line\">\tgta = np.zeros((len(bboxes), <span class=\"number\">4</span>))</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"comment\"># change bboxes's width and height because the img was rezised</span></span><br><span class=\"line\">\t<span class=\"keyword\">for</span> bbox_num, bbox <span class=\"keyword\">in</span> enumerate(bboxes):</span><br><span class=\"line\">\t\t<span class=\"comment\"># get the GT box coordinates, and resize to account for image resizing</span></span><br><span class=\"line\">\t\t<span class=\"comment\"># /C.rpn_stride mapping to feature map</span></span><br><span class=\"line\">\t\tgta[bbox_num, <span class=\"number\">0</span>] = int(round(bbox[<span class=\"string\">'x1'</span>] * (resized_width / float(width))/C.rpn_stride))</span><br><span class=\"line\">\t\tgta[bbox_num, <span class=\"number\">1</span>] = int(round(bbox[<span class=\"string\">'x2'</span>] * (resized_width / float(width))/C.rpn_stride))</span><br><span class=\"line\">\t\tgta[bbox_num, <span class=\"number\">2</span>] = int(round(bbox[<span class=\"string\">'y1'</span>] * (resized_height / float(height))/C.rpn_stride))</span><br><span class=\"line\">\t\tgta[bbox_num, <span class=\"number\">3</span>] = int(round(bbox[<span class=\"string\">'y2'</span>] * (resized_height / float(height))/C.rpn_stride))</span><br><span class=\"line\"></span><br><span class=\"line\">\tx_roi = []</span><br><span class=\"line\">\ty_class_num = []</span><br><span class=\"line\">\ty_class_regr_coords = []</span><br><span class=\"line\">\ty_class_regr_label = []</span><br><span class=\"line\">\tIoUs = [] <span class=\"comment\"># for debugging only</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"comment\"># for all given proposals -&gt; boxes</span></span><br><span class=\"line\">\t<span class=\"keyword\">for</span> ix <span class=\"keyword\">in</span> range(R.shape[<span class=\"number\">0</span>]):</span><br><span class=\"line\">\t\t<span class=\"comment\"># current boxes's cordinates</span></span><br><span class=\"line\">\t\t(x1, y1, x2, y2) = R[ix, :]</span><br><span class=\"line\">\t\tx1 = int(round(x1))</span><br><span class=\"line\">\t\ty1 = int(round(y1))</span><br><span class=\"line\">\t\tx2 = int(round(x2))</span><br><span class=\"line\">\t\ty2 = int(round(y2))</span><br><span class=\"line\"></span><br><span class=\"line\">\t\tbest_iou = <span class=\"number\">0.0</span></span><br><span class=\"line\">\t\tbest_bbox = <span class=\"number\">-1</span></span><br><span class=\"line\">\t\t<span class=\"comment\"># using current proposal to compare with given xml's boxes</span></span><br><span class=\"line\">\t\t<span class=\"keyword\">for</span> bbox_num <span class=\"keyword\">in</span> range(len(bboxes)):</span><br><span class=\"line\">\t\t\t<span class=\"comment\"># calculate current iou</span></span><br><span class=\"line\">\t\t\tcurr_iou = iou([gta[bbox_num, <span class=\"number\">0</span>], gta[bbox_num, <span class=\"number\">2</span>], gta[bbox_num, <span class=\"number\">1</span>], gta[bbox_num, <span class=\"number\">3</span>]], [x1, y1, x2, y2])</span><br><span class=\"line\">\t\t\t<span class=\"comment\"># update parameters</span></span><br><span class=\"line\">\t\t\t<span class=\"keyword\">if</span> curr_iou &gt; best_iou:</span><br><span class=\"line\">\t\t\t\tbest_iou = curr_iou</span><br><span class=\"line\">\t\t\t\tbest_bbox = bbox_num</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"comment\"># if iou to small, we don't put it in trainning because it should be backgroud</span></span><br><span class=\"line\">\t\t<span class=\"keyword\">if</span> best_iou &lt; C.classifier_min_overlap:</span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">continue</span></span><br><span class=\"line\">\t\t<span class=\"keyword\">else</span>:</span><br><span class=\"line\">\t\t\t<span class=\"comment\"># saveing left top cordinates, width and height</span></span><br><span class=\"line\">\t\t\tw = x2 - x1</span><br><span class=\"line\">\t\t\th = y2 - y1</span><br><span class=\"line\">\t\t\tx_roi.append([x1, y1, w, h])</span><br><span class=\"line\">\t\t\t<span class=\"comment\"># saving this bbox's iou</span></span><br><span class=\"line\">\t\t\tIoUs.append(best_iou)</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t<span class=\"comment\"># hard to classfier -&gt; set it to backgroud</span></span><br><span class=\"line\">\t\t\t<span class=\"keyword\">if</span> C.classifier_min_overlap &lt;= best_iou &lt; C.classifier_max_overlap:</span><br><span class=\"line\">\t\t\t\t<span class=\"comment\"># hard negative example</span></span><br><span class=\"line\">\t\t\t\tcls_name = <span class=\"string\">'bg'</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t<span class=\"comment\"># valid proposal</span></span><br><span class=\"line\">\t\t\t<span class=\"keyword\">elif</span> C.classifier_max_overlap &lt;= best_iou:</span><br><span class=\"line\">\t\t\t\t<span class=\"comment\"># coresspoding class name</span></span><br><span class=\"line\">\t\t\t\tcls_name = bboxes[best_bbox][<span class=\"string\">'class'</span>]</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t<span class=\"comment\"># calculate rpn graident with true cordinates given by xml file</span></span><br><span class=\"line\">\t\t\t\tcxg = (gta[best_bbox, <span class=\"number\">0</span>] + gta[best_bbox, <span class=\"number\">1</span>]) / <span class=\"number\">2.0</span></span><br><span class=\"line\">\t\t\t\tcyg = (gta[best_bbox, <span class=\"number\">2</span>] + gta[best_bbox, <span class=\"number\">3</span>]) / <span class=\"number\">2.0</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\tcx = x1 + w / <span class=\"number\">2.0</span></span><br><span class=\"line\">\t\t\t\tcy = y1 + h / <span class=\"number\">2.0</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\ttx = (cxg - cx) / float(w)</span><br><span class=\"line\">\t\t\t\tty = (cyg - cy) / float(h)</span><br><span class=\"line\">\t\t\t\ttw = np.log((gta[best_bbox, <span class=\"number\">1</span>] - gta[best_bbox, <span class=\"number\">0</span>]) / float(w))</span><br><span class=\"line\">\t\t\t\tth = np.log((gta[best_bbox, <span class=\"number\">3</span>] - gta[best_bbox, <span class=\"number\">2</span>]) / float(h))</span><br><span class=\"line\">\t\t\t<span class=\"keyword\">else</span>:</span><br><span class=\"line\">\t\t\t\tprint(<span class=\"string\">'roi = &#123;&#125;'</span>.format(best_iou))</span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">raise</span> RuntimeError</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"comment\"># class name's mapping number</span></span><br><span class=\"line\">\t\tclass_num = class_mapping[cls_name]</span><br><span class=\"line\">\t\t<span class=\"comment\"># list of calss label</span></span><br><span class=\"line\">\t\tclass_label = len(class_mapping) * [<span class=\"number\">0</span>]</span><br><span class=\"line\">\t\t<span class=\"comment\"># set class_num's coresspoding location to 1</span></span><br><span class=\"line\">\t\tclass_label[class_num] = <span class=\"number\">1</span></span><br><span class=\"line\">\t\t<span class=\"comment\"># privous is one-hot vector</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"comment\"># saving the one-hot vector</span></span><br><span class=\"line\">\t\ty_class_num.append(copy.deepcopy(class_label))</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"comment\"># coords used to saving calculated graident</span></span><br><span class=\"line\">\t\tcoords = [<span class=\"number\">0</span>] * <span class=\"number\">4</span> * (len(class_mapping) - <span class=\"number\">1</span>)</span><br><span class=\"line\">\t\t<span class=\"comment\"># labels used to decide whether adding to loss calculation</span></span><br><span class=\"line\">\t\tlabels = [<span class=\"number\">0</span>] * <span class=\"number\">4</span> * (len(class_mapping) - <span class=\"number\">1</span>)</span><br><span class=\"line\">\t\t<span class=\"keyword\">if</span> cls_name != <span class=\"string\">'bg'</span>:</span><br><span class=\"line\">\t\t\tlabel_pos = <span class=\"number\">4</span> * class_num</span><br><span class=\"line\">\t\t\tsx, sy, sw, sh = C.classifier_regr_std</span><br><span class=\"line\">\t\t\tcoords[label_pos:<span class=\"number\">4</span>+label_pos] = [sx*tx, sy*ty, sw*tw, sh*th]</span><br><span class=\"line\">\t\t\tlabels[label_pos:<span class=\"number\">4</span>+label_pos] = [<span class=\"number\">1</span>, <span class=\"number\">1</span>, <span class=\"number\">1</span>, <span class=\"number\">1</span>]</span><br><span class=\"line\">\t\t\ty_class_regr_coords.append(copy.deepcopy(coords))</span><br><span class=\"line\">\t\t\ty_class_regr_label.append(copy.deepcopy(labels))</span><br><span class=\"line\">\t\t<span class=\"keyword\">else</span>:</span><br><span class=\"line\">\t\t\ty_class_regr_coords.append(copy.deepcopy(coords))</span><br><span class=\"line\">\t\t\ty_class_regr_label.append(copy.deepcopy(labels))</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"comment\"># no bboxes</span></span><br><span class=\"line\">\t<span class=\"keyword\">if</span> len(x_roi) == <span class=\"number\">0</span>:</span><br><span class=\"line\">\t\t<span class=\"keyword\">return</span> <span class=\"keyword\">None</span>, <span class=\"keyword\">None</span>, <span class=\"keyword\">None</span>, <span class=\"keyword\">None</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"comment\"># matrix with [x1, y1, w, h]</span></span><br><span class=\"line\">\tX = np.array(x_roi)</span><br><span class=\"line\">\t<span class=\"comment\"># boxxes coresspoding class number</span></span><br><span class=\"line\">\tY1 = np.array(y_class_num)</span><br><span class=\"line\">\t<span class=\"comment\"># matrix of whether adding to calculation and coresspoding regrident</span></span><br><span class=\"line\">\tY2 = np.concatenate([np.array(y_class_regr_label),np.array(y_class_regr_coords)],axis=<span class=\"number\">1</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"comment\"># adding batch size dimention</span></span><br><span class=\"line\">\t<span class=\"keyword\">return</span> np.expand_dims(X, axis=<span class=\"number\">0</span>), np.expand_dims(Y1, axis=<span class=\"number\">0</span>), np.expand_dims(Y2, axis=<span class=\"number\">0</span>), IoUs</span><br></pre></td></tr></table></figure>\n<h3 id=\"【26-27-07-2018】\"><a href=\"#【26-27-07-2018】\" class=\"headerlink\" title=\"【26~27/07/2018】\"></a>【26~27/07/2018】</h3><h4 id=\"model-parameters\"><a href=\"#model-parameters\" class=\"headerlink\" title=\"model parameters\"></a>model parameters</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># rpn optimizer</span></span><br><span class=\"line\">optimizer = Adam(lr=<span class=\"number\">1e-5</span>)</span><br><span class=\"line\"><span class=\"comment\"># classifier optimizer</span></span><br><span class=\"line\">optimizer_classifier = Adam(lr=<span class=\"number\">1e-5</span>)</span><br><span class=\"line\"><span class=\"comment\"># defined loss apply, metrics used to print accury</span></span><br><span class=\"line\">model_rpn.compile(optimizer=optimizer, loss=[losses.rpn_loss_cls(num_anchors), losses.rpn_loss_regr(num_anchors)])</span><br><span class=\"line\">model_classifier.compile(optimizer=optimizer_classifier, loss=[losses.class_loss_cls, losses.class_loss_regr(len(classes_count)<span class=\"number\">-1</span>)], metrics=&#123;<span class=\"string\">'dense_class_&#123;&#125;'</span>.format(len(classes_count)): <span class=\"string\">'accuracy'</span>&#125;)</span><br><span class=\"line\"><span class=\"comment\"># for saving weight</span></span><br><span class=\"line\">model_all.compile(optimizer=<span class=\"string\">'sgd'</span>, loss=<span class=\"string\">'mae'</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># traing time of each epochs</span></span><br><span class=\"line\">epoch_length = <span class=\"number\">1000</span></span><br><span class=\"line\"><span class=\"comment\"># totoal epochs</span></span><br><span class=\"line\">num_epochs = <span class=\"number\">2000</span></span><br><span class=\"line\"><span class=\"comment\">#</span></span><br><span class=\"line\">iter_num = <span class=\"number\">0</span></span><br><span class=\"line\"><span class=\"comment\"># losses saving matrix</span></span><br><span class=\"line\">losses = np.zeros((epoch_length, <span class=\"number\">5</span>))</span><br><span class=\"line\">rpn_accuracy_rpn_monitor = []</span><br><span class=\"line\">rpn_accuracy_for_epoch = []</span><br><span class=\"line\">start_time = time.time()</span><br><span class=\"line\"><span class=\"comment\"># current total loss</span></span><br><span class=\"line\">best_loss = np.Inf</span><br><span class=\"line\"><span class=\"comment\"># sorted classing mapping</span></span><br><span class=\"line\">class_mapping_inv = &#123;v: k <span class=\"keyword\">for</span> k, v <span class=\"keyword\">in</span> class_mapping.items()&#125;</span><br></pre></td></tr></table></figure>\n<h4 id=\"Training-process\"><a href=\"#Training-process\" class=\"headerlink\" title=\"Training process\"></a>Training process</h4><p>函数流程：<br><strong>训练rpn网络并且进行预测：</strong><br>训练RPN网络,X是图片、Y是对应类别和回归梯度【注：并不是所有的点都参与训练，只有符合条件的点才参与训练】</p>\n<p><strong>根据rpn网络的预测结果得到classifier网络的训练数据:</strong><br>将预测结果转化为预选框<br>计算宽属于哪一类，回归梯度是多少<br>如果没有有效的预选框则结束本次循环<br>得到正负样本在的位置【Y1[0, :, -1]：0指定batch的位置，：指所有框，-1指最后一个维度即背景类】<br>neg_samples = neg_samples[0]：这样做的原因是将其变为一维的数组<br>下面这一步是选择C.num_rois个数的框，送入classifier网络进行训练。思路是：当C.num_rois大于1的时候正负样本尽量取到各一半，小于1的时候正负样本随机取一个。需要注意的是我们这是拿到的是正负样本在的位置而不是正负样本本身，这也是随机抽取的一般方法</p>\n<p><strong>训练classifier网络:</strong><br>打印Loss和accury<br>如果网络有两个不同的输出，那么第一个是和损失接下来是分损失【loss_class[3]：代表是准确率在定义网络的时候定义了】<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">classifer网络的loss输出：</span><br><span class=\"line\">[<span class=\"number\">1.4640709</span>, <span class=\"number\">1.0986123</span>, <span class=\"number\">0.36545864</span>, <span class=\"number\">0.15625</span>]</span><br></pre></td></tr></table></figure></p>\n<p>还有就是这些loss都是list数据类型，所以要把它倒腾到numpy数据中<br>当结束一轮的epoch时，只有当这轮epoch的loss小于最优的时候才会存储这轮的训练数据。并结束这轮epoch进入下一轮epoch.</p>\n<hr>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br><span class=\"line\">107</span><br><span class=\"line\">108</span><br><span class=\"line\">109</span><br><span class=\"line\">110</span><br><span class=\"line\">111</span><br><span class=\"line\">112</span><br><span class=\"line\">113</span><br><span class=\"line\">114</span><br><span class=\"line\">115</span><br><span class=\"line\">116</span><br><span class=\"line\">117</span><br><span class=\"line\">118</span><br><span class=\"line\">119</span><br><span class=\"line\">120</span><br><span class=\"line\">121</span><br><span class=\"line\">122</span><br><span class=\"line\">123</span><br><span class=\"line\">124</span><br><span class=\"line\">125</span><br><span class=\"line\">126</span><br><span class=\"line\">127</span><br><span class=\"line\">128</span><br><span class=\"line\">129</span><br><span class=\"line\">130</span><br><span class=\"line\">131</span><br><span class=\"line\">132</span><br><span class=\"line\">133</span><br><span class=\"line\">134</span><br><span class=\"line\">135</span><br><span class=\"line\">136</span><br><span class=\"line\">137</span><br><span class=\"line\">138</span><br><span class=\"line\">139</span><br><span class=\"line\">140</span><br><span class=\"line\">141</span><br><span class=\"line\">142</span><br><span class=\"line\">143</span><br><span class=\"line\">144</span><br><span class=\"line\">145</span><br><span class=\"line\">146</span><br><span class=\"line\">147</span><br><span class=\"line\">148</span><br><span class=\"line\">149</span><br><span class=\"line\">150</span><br><span class=\"line\">151</span><br><span class=\"line\">152</span><br><span class=\"line\">153</span><br><span class=\"line\">154</span><br><span class=\"line\">155</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># Training Process</span></span><br><span class=\"line\">print(<span class=\"string\">'Starting training'</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">for</span> epoch_num <span class=\"keyword\">in</span> range(num_epochs):</span><br><span class=\"line\">    <span class=\"comment\">#progbar is used to print % of processing</span></span><br><span class=\"line\">\tprogbar = generic_utils.Progbar(epoch_length)</span><br><span class=\"line\">    <span class=\"comment\"># print current process</span></span><br><span class=\"line\">\tprint(<span class=\"string\">'Epoch &#123;&#125;/&#123;&#125;'</span>.format(epoch_num + <span class=\"number\">1</span>, num_epochs))</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"keyword\">while</span> <span class=\"keyword\">True</span>:</span><br><span class=\"line\">\t\t<span class=\"keyword\">try</span>:</span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"comment\"># verbose True to print RPN situation, if can't generate boxes on positivate object, it will print error</span></span><br><span class=\"line\">\t\t\t<span class=\"keyword\">if</span> len(rpn_accuracy_rpn_monitor) == epoch_length <span class=\"keyword\">and</span> C.verbose:</span><br><span class=\"line\">                <span class=\"comment\"># postivate boxes / all boxes</span></span><br><span class=\"line\">\t\t\t\tmean_overlapping_bboxes = float(sum(rpn_accuracy_rpn_monitor))/len(rpn_accuracy_rpn_monitor)</span><br><span class=\"line\">\t\t\t\trpn_accuracy_rpn_monitor = []</span><br><span class=\"line\">\t\t\t\tprint(<span class=\"string\">'Average number of overlapping bounding boxes from RPN = &#123;&#125; for &#123;&#125; previous iterations'</span>.format(mean_overlapping_bboxes, epoch_length))</span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">if</span> mean_overlapping_bboxes == <span class=\"number\">0</span>:</span><br><span class=\"line\">\t\t\t\t\tprint(<span class=\"string\">'RPN is not producing bounding boxes that overlap the ground truth boxes. Check RPN settings or keep training.'</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"comment\"># obtain img, rpn information and img xml format</span></span><br><span class=\"line\">\t\t\tX, Y, img_data = next(data_gen_train)</span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"comment\"># train RPN net, X is img, Y is correspoding class type and graident</span></span><br><span class=\"line\">\t\t\tloss_rpn = model_rpn.train_on_batch(X, Y)</span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"comment\"># predict new Y from privious rpn model</span></span><br><span class=\"line\">\t\t\tP_rpn = model_rpn.predict_on_batch(X)</span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"comment\"># transform predicted rpn to cordinates of boxes</span></span><br><span class=\"line\">\t\t\tR = rpn_to_boxes.rpn_to_roi(P_rpn[<span class=\"number\">0</span>], P_rpn[<span class=\"number\">1</span>], C, K.image_dim_ordering(), use_regr=<span class=\"keyword\">True</span>, overlap_thresh=<span class=\"number\">0.7</span>, max_boxes=<span class=\"number\">300</span>)</span><br><span class=\"line\">\t\t\t<span class=\"comment\"># note: calc_iou converts from (x1,y1,x2,y2) to (x,y,w,h) format</span></span><br><span class=\"line\">            <span class=\"comment\"># X2: [x,y,w,h]</span></span><br><span class=\"line\">            <span class=\"comment\"># Y1: coresspoding class number -&gt; one hot vector</span></span><br><span class=\"line\">            <span class=\"comment\"># Y2: boxes coresspoding regrident</span></span><br><span class=\"line\">\t\t\tX2, Y1, Y2, IouS = rpn_to_classifier.calc_iou(R, img_data, C, class_mapping)</span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"comment\"># no box, stop this epoch</span></span><br><span class=\"line\">\t\t\t<span class=\"keyword\">if</span> X2 <span class=\"keyword\">is</span> <span class=\"keyword\">None</span>:</span><br><span class=\"line\">\t\t\t\trpn_accuracy_rpn_monitor.append(<span class=\"number\">0</span>)</span><br><span class=\"line\">\t\t\t\trpn_accuracy_for_epoch.append(<span class=\"number\">0</span>)</span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">continue</span></span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"comment\"># if last position of one-hot is 1 -&gt; is background</span></span><br><span class=\"line\">\t\t\tneg_samples = np.where(Y1[<span class=\"number\">0</span>, :, <span class=\"number\">-1</span>] == <span class=\"number\">1</span>)</span><br><span class=\"line\">            <span class=\"comment\"># else is postivate sample</span></span><br><span class=\"line\">\t\t\tpos_samples = np.where(Y1[<span class=\"number\">0</span>, :, <span class=\"number\">-1</span>] == <span class=\"number\">0</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"comment\"># obtain backgourd samples's coresspoding rows</span></span><br><span class=\"line\">\t\t\t<span class=\"keyword\">if</span> len(neg_samples) &gt; <span class=\"number\">0</span>:</span><br><span class=\"line\">\t\t\t\tneg_samples = neg_samples[<span class=\"number\">0</span>]</span><br><span class=\"line\">\t\t\t<span class=\"keyword\">else</span>:</span><br><span class=\"line\">\t\t\t\tneg_samples = []</span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"comment\"># obtain posivate samples's coresspoding rows</span></span><br><span class=\"line\">\t\t\t<span class=\"keyword\">if</span> len(pos_samples) &gt; <span class=\"number\">0</span>:</span><br><span class=\"line\">\t\t\t\tpos_samples = pos_samples[<span class=\"number\">0</span>]</span><br><span class=\"line\">\t\t\t<span class=\"keyword\">else</span>:</span><br><span class=\"line\">\t\t\t\tpos_samples = []</span><br><span class=\"line\">\t\t\t<span class=\"comment\"># saving posivate samples's number</span></span><br><span class=\"line\">\t\t\trpn_accuracy_rpn_monitor.append(len(pos_samples))</span><br><span class=\"line\">\t\t\trpn_accuracy_for_epoch.append((len(pos_samples)))</span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"comment\"># default 4 here</span></span><br><span class=\"line\">\t\t\t<span class=\"keyword\">if</span> C.num_rois &gt; <span class=\"number\">1</span>:</span><br><span class=\"line\">                <span class=\"comment\"># wehn postivate samples less than 2</span></span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">if</span> len(pos_samples) &lt; C.num_rois//<span class=\"number\">2</span>:</span><br><span class=\"line\">                    <span class=\"comment\"># chosse all samples</span></span><br><span class=\"line\">\t\t\t\t\tselected_pos_samples = pos_samples.tolist()</span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">else</span>:</span><br><span class=\"line\">                    <span class=\"comment\"># random choose 2 samples</span></span><br><span class=\"line\">\t\t\t\t\tselected_pos_samples = np.random.choice(pos_samples, C.num_rois//<span class=\"number\">2</span>, replace=<span class=\"keyword\">False</span>).tolist()</span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">try</span>:</span><br><span class=\"line\">                    <span class=\"comment\"># random choose num_rois - positave samples naegivate samples</span></span><br><span class=\"line\">\t\t\t\t\tselected_neg_samples = np.random.choice(neg_samples, C.num_rois - len(selected_pos_samples), replace=<span class=\"keyword\">False</span>).tolist()</span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">except</span>:</span><br><span class=\"line\">                    <span class=\"comment\"># if no enought neg samples, copy priouvs neg sample</span></span><br><span class=\"line\">\t\t\t\t\tselected_neg_samples = np.random.choice(neg_samples, C.num_rois - len(selected_pos_samples), replace=<span class=\"keyword\">True</span>).tolist()</span><br><span class=\"line\"></span><br><span class=\"line\">                <span class=\"comment\"># samples picked to classifier network</span></span><br><span class=\"line\">\t\t\t\tsel_samples = selected_pos_samples + selected_neg_samples</span><br><span class=\"line\">\t\t\t<span class=\"keyword\">else</span>:</span><br><span class=\"line\">\t\t\t\t<span class=\"comment\"># in the extreme case where num_rois = 1, we pick a random pos or neg sample</span></span><br><span class=\"line\">\t\t\t\tselected_pos_samples = pos_samples.tolist()</span><br><span class=\"line\">\t\t\t\tselected_neg_samples = neg_samples.tolist()</span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">if</span> np.random.randint(<span class=\"number\">0</span>, <span class=\"number\">2</span>):</span><br><span class=\"line\">\t\t\t\t\tsel_samples = random.choice(neg_samples)</span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">else</span>:</span><br><span class=\"line\">\t\t\t\t\tsel_samples = random.choice(pos_samples)</span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"comment\"># train classifier, img data, selceted samples' cordinates, mapping number of selected samples, coresspoding regreident</span></span><br><span class=\"line\">\t\t\tloss_class = model_classifier.train_on_batch([X, X2[:, sel_samples, :]], [Y1[:, sel_samples, :], Y2[:, sel_samples, :]])</span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"comment\"># in Keras, if loss part bigger than 1, it will return sum of part losses, each loss and accury</span></span><br><span class=\"line\">            <span class=\"comment\"># put each losses and accury into losses</span></span><br><span class=\"line\">\t\t\tlosses[iter_num, <span class=\"number\">0</span>] = loss_rpn[<span class=\"number\">1</span>]</span><br><span class=\"line\">\t\t\tlosses[iter_num, <span class=\"number\">1</span>] = loss_rpn[<span class=\"number\">2</span>]</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\tlosses[iter_num, <span class=\"number\">2</span>] = loss_class[<span class=\"number\">1</span>]</span><br><span class=\"line\">\t\t\tlosses[iter_num, <span class=\"number\">3</span>] = loss_class[<span class=\"number\">2</span>]</span><br><span class=\"line\">\t\t\tlosses[iter_num, <span class=\"number\">4</span>] = loss_class[<span class=\"number\">3</span>]</span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"comment\"># next iter</span></span><br><span class=\"line\">\t\t\titer_num += <span class=\"number\">1</span></span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"comment\"># display and update current mean value of losses</span></span><br><span class=\"line\">\t\t\tprogbar.update(iter_num, [(<span class=\"string\">'rpn_cls'</span>, np.mean(losses[:iter_num, <span class=\"number\">0</span>])), (<span class=\"string\">'rpn_regr'</span>, np.mean(losses[:iter_num, <span class=\"number\">1</span>])),</span><br><span class=\"line\">\t\t\t\t\t\t\t\t\t  (<span class=\"string\">'detector_cls'</span>, np.mean(losses[:iter_num, <span class=\"number\">2</span>])), (<span class=\"string\">'detector_regr'</span>, np.mean(losses[:iter_num, <span class=\"number\">3</span>]))])</span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"comment\"># reach epoch_length</span></span><br><span class=\"line\">\t\t\t<span class=\"keyword\">if</span> iter_num == epoch_length:</span><br><span class=\"line\">\t\t\t\tloss_rpn_cls = np.mean(losses[:, <span class=\"number\">0</span>])</span><br><span class=\"line\">\t\t\t\tloss_rpn_regr = np.mean(losses[:, <span class=\"number\">1</span>])</span><br><span class=\"line\">\t\t\t\tloss_class_cls = np.mean(losses[:, <span class=\"number\">2</span>])</span><br><span class=\"line\">\t\t\t\tloss_class_regr = np.mean(losses[:, <span class=\"number\">3</span>])</span><br><span class=\"line\">\t\t\t\tclass_acc = np.mean(losses[:, <span class=\"number\">4</span>])</span><br><span class=\"line\"></span><br><span class=\"line\">                <span class=\"comment\"># negativate samples / all samples</span></span><br><span class=\"line\">\t\t\t\tmean_overlapping_bboxes = float(sum(rpn_accuracy_for_epoch)) / len(rpn_accuracy_for_epoch)</span><br><span class=\"line\">                <span class=\"comment\"># reset</span></span><br><span class=\"line\">\t\t\t\trpn_accuracy_for_epoch = []</span><br><span class=\"line\"></span><br><span class=\"line\">                <span class=\"comment\"># print trainning loss and accrury</span></span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">if</span> C.verbose:</span><br><span class=\"line\">\t\t\t\t\tprint(<span class=\"string\">'Mean number of bounding boxes from RPN overlapping ground truth boxes: &#123;&#125;'</span>.format(mean_overlapping_bboxes))</span><br><span class=\"line\">\t\t\t\t\tprint(<span class=\"string\">'Classifier accuracy for bounding boxes from RPN: &#123;&#125;'</span>.format(class_acc))</span><br><span class=\"line\">\t\t\t\t\tprint(<span class=\"string\">'Loss RPN classifier: &#123;&#125;'</span>.format(loss_rpn_cls))</span><br><span class=\"line\">\t\t\t\t\tprint(<span class=\"string\">'Loss RPN regression: &#123;&#125;'</span>.format(loss_rpn_regr))</span><br><span class=\"line\">\t\t\t\t\tprint(<span class=\"string\">'Loss Detector classifier: &#123;&#125;'</span>.format(loss_class_cls))</span><br><span class=\"line\">\t\t\t\t\tprint(<span class=\"string\">'Loss Detector regression: &#123;&#125;'</span>.format(loss_class_regr))</span><br><span class=\"line\">                    <span class=\"comment\"># trainng time of one epoch</span></span><br><span class=\"line\">\t\t\t\t\tprint(<span class=\"string\">'Elapsed time: &#123;&#125;'</span>.format(time.time() - start_time))</span><br><span class=\"line\">                    </span><br><span class=\"line\">                <span class=\"comment\"># total loss</span></span><br><span class=\"line\">\t\t\t\tcurr_loss = loss_rpn_cls + loss_rpn_regr + loss_class_cls + loss_class_regr</span><br><span class=\"line\">                <span class=\"comment\"># reset</span></span><br><span class=\"line\">\t\t\t\titer_num = <span class=\"number\">0</span></span><br><span class=\"line\">                <span class=\"comment\"># reset time</span></span><br><span class=\"line\">\t\t\t\tstart_time = time.time()</span><br><span class=\"line\"></span><br><span class=\"line\">                <span class=\"comment\"># if obtain smaller total loss, save weight of current model</span></span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">if</span> curr_loss &lt; best_loss:</span><br><span class=\"line\">\t\t\t\t\t<span class=\"keyword\">if</span> C.verbose:</span><br><span class=\"line\">\t\t\t\t\t\tprint(<span class=\"string\">'Total loss decreased from &#123;&#125; to &#123;&#125;, saving weights'</span>.format(best_loss,curr_loss))</span><br><span class=\"line\">\t\t\t\t\tbest_loss = curr_loss</span><br><span class=\"line\">\t\t\t\t\tmodel_all.save_weights(C.model_path)</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">break</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">except</span> Exception <span class=\"keyword\">as</span> e:</span><br><span class=\"line\">\t\t\tprint(<span class=\"string\">'Exception: &#123;&#125;'</span>.format(e))</span><br><span class=\"line\">\t\t\t<span class=\"keyword\">continue</span></span><br><span class=\"line\"></span><br><span class=\"line\">print(<span class=\"string\">'Training complete, exiting.'</span>)</span><br></pre></td></tr></table></figure>\n<p><a href=\"https://github.com/Trouble404/Object-Detection-System-Based-on-CNN-and-Capsule-Network/blob/master/Faster-RCNN/workflow_train.ipynb\" target=\"_blank\" rel=\"noopener\">jupyter notebook</a></p>\n<h3 id=\"【30-07-2018】\"><a href=\"#【30-07-2018】\" class=\"headerlink\" title=\"【30/07/2018】\"></a>【30/07/2018】</h3><h4 id=\"Running-at-GPU-enviorment\"><a href=\"#Running-at-GPU-enviorment\" class=\"headerlink\" title=\"Running at GPU enviorment\"></a>Running at GPU enviorment</h4><p><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/9_1.JPG\" alt=\"image\"><br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/9_2.JPG\" alt=\"image\"><br>Meet error in GPU version tensorflow<br>No enough memory.</p>\n<p>Try to Running at Irius:</p>\n<p>Setting 3 differnet configration:<br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/9_3.JPG\" alt=\"image\"><br>at Prjoect1 file:<br>set epoch_length to number of training img<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">epoch_length = <span class=\"number\">11540</span></span><br><span class=\"line\">num_epochs = <span class=\"number\">100</span></span><br></pre></td></tr></table></figure></p>\n<p>Apply img enhance function<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">C.use_horizontal_flips = <span class=\"keyword\">True</span></span><br><span class=\"line\">C.use_vertical_flips = <span class=\"keyword\">True</span></span><br><span class=\"line\">C.rot_90 = <span class=\"keyword\">True</span></span><br></pre></td></tr></table></figure></p>\n<hr>\n<p>at Prjoect file:<br>set epoch_length to 1000, increase epoch<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">epoch_length = <span class=\"number\">1000</span></span><br><span class=\"line\">num_epochs = <span class=\"number\">2000</span></span><br></pre></td></tr></table></figure></p>\n<p>Apply img enhance and class balance function<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">C.use_horizontal_flips = <span class=\"keyword\">True</span></span><br><span class=\"line\">C.use_vertical_flips = <span class=\"keyword\">True</span></span><br><span class=\"line\">C.rot_90 = <span class=\"keyword\">True</span></span><br><span class=\"line\">C.balanced_classes = <span class=\"keyword\">True</span></span><br></pre></td></tr></table></figure></p>\n<hr>\n<p>at Prjoect3 file:<br>set epoch_length to 1000, increase epoch<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">epoch_length = <span class=\"number\">1000</span></span><br><span class=\"line\">num_epochs = <span class=\"number\">2000</span></span><br></pre></td></tr></table></figure></p>\n<p>Apply img enhance<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">C.use_horizontal_flips = <span class=\"keyword\">True</span></span><br><span class=\"line\">C.use_vertical_flips = <span class=\"keyword\">True</span></span><br><span class=\"line\">C.rot_90 = <span class=\"keyword\">True</span></span><br></pre></td></tr></table></figure></p>\n<hr>\n<h4 id=\"check-irdius-work\"><a href=\"#check-irdius-work\" class=\"headerlink\" title=\"check irdius work\"></a>check irdius work</h4><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">myqueue</span><br></pre></td></tr></table></figure>\n<p><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/9_4.JPG\" alt=\"image\"></p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ssh pink59</span><br><span class=\"line\">nvidia-smi</span><br></pre></td></tr></table></figure>\n<p><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/9_5.JPG\" alt=\"image\"></p>\n<h3 id=\"【31-07-2018】\"><a href=\"#【31-07-2018】\" class=\"headerlink\" title=\"【31/07/2018】\"></a>【31/07/2018】</h3><h4 id=\"obtain-trained-model-and-log-file\"><a href=\"#obtain-trained-model-and-log-file\" class=\"headerlink\" title=\"obtain trained model and log file\"></a>obtain trained model and log file</h4><p>因为 Iriuds 的GPU使用时长限制最高为24小时，因此，需要在下一次开始前载入上一次训练的模型。<br>每次训练的粗略结果更新在LogBook最前面.</p>\n<h4 id=\"plot-rpn-and-classfier-loss\"><a href=\"#plot-rpn-and-classfier-loss\" class=\"headerlink\" title=\"plot rpn and classfier loss\"></a>plot rpn and classfier loss</h4><p>获取日志中每个小epoch的rpn_cls, rpn_regr, detc_cls, detc_regr<br>遍历日志，用正则匹配出相应的数值添加到List中：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">obtain_each_batch</span><span class=\"params\">(filename)</span>:</span></span><br><span class=\"line\">    n = <span class=\"number\">0</span></span><br><span class=\"line\">    rpn_cls = []</span><br><span class=\"line\">    rpn_regr = []</span><br><span class=\"line\">    detector_cls = []</span><br><span class=\"line\">    detector_regr = []</span><br><span class=\"line\">    f = open(filename,<span class=\"string\">'r'</span>,buffering=<span class=\"number\">-1</span>)</span><br><span class=\"line\">    lines = f.readlines()</span><br><span class=\"line\">    <span class=\"keyword\">for</span> line <span class=\"keyword\">in</span> lines:</span><br><span class=\"line\">        n = n + <span class=\"number\">1</span></span><br><span class=\"line\">        match = re.match(<span class=\"string\">r'.* - rpn_cls: (.*) - rpn_regr: .*'</span>, line, re.M|re.I)</span><br><span class=\"line\">        <span class=\"keyword\">if</span> match <span class=\"keyword\">is</span> <span class=\"keyword\">None</span>:</span><br><span class=\"line\">            <span class=\"keyword\">continue</span></span><br><span class=\"line\">        <span class=\"keyword\">else</span>: </span><br><span class=\"line\">            rpn_cls.append(float(match.group(<span class=\"number\">1</span>)))</span><br><span class=\"line\"></span><br><span class=\"line\">        match = re.match(<span class=\"string\">r'.* - rpn_regr: (.*) - detector_cls: .*'</span>, line, re.M|re.I)</span><br><span class=\"line\">        <span class=\"keyword\">if</span> match <span class=\"keyword\">is</span> <span class=\"keyword\">None</span>:</span><br><span class=\"line\">            <span class=\"keyword\">continue</span></span><br><span class=\"line\">        <span class=\"keyword\">else</span>: </span><br><span class=\"line\">            rpn_regr.append(float(match.group(<span class=\"number\">1</span>)))            </span><br><span class=\"line\">            </span><br><span class=\"line\">        match = re.match(<span class=\"string\">r'.* - detector_cls: (.*) - detector_regr: .*'</span>, line, re.M|re.I)</span><br><span class=\"line\">        <span class=\"keyword\">if</span> match <span class=\"keyword\">is</span> <span class=\"keyword\">None</span>:</span><br><span class=\"line\">            <span class=\"keyword\">continue</span></span><br><span class=\"line\">        <span class=\"keyword\">else</span>: </span><br><span class=\"line\">            detector_cls.append(float(match.group(<span class=\"number\">1</span>))) </span><br><span class=\"line\">            </span><br><span class=\"line\">        match = re.match(<span class=\"string\">r'.* - detector_regr: (.*)\\n'</span>, line, re.M|re.I)</span><br><span class=\"line\">        <span class=\"keyword\">if</span> match <span class=\"keyword\">is</span> <span class=\"keyword\">None</span>:</span><br><span class=\"line\">            <span class=\"keyword\">continue</span></span><br><span class=\"line\">        <span class=\"keyword\">else</span>: </span><br><span class=\"line\">            det_regr = match.group(<span class=\"number\">1</span>)[<span class=\"number\">0</span>:<span class=\"number\">6</span>]</span><br><span class=\"line\">            detector_regr.append(float(det_regr))</span><br><span class=\"line\"></span><br><span class=\"line\">    f.close()</span><br><span class=\"line\">    print(n)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> rpn_cls, rpn_regr, detector_cls, detector_regr</span><br></pre></td></tr></table></figure></p>\n<p>每个epoch都会计算accury, loss of rpn cls, loss of rpn regr, loss of detc cls, loss of detc regr<br>遍历日志找到相应的数值添加到list中：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">obtain_batch</span><span class=\"params\">(filename)</span>:</span></span><br><span class=\"line\">    n = <span class=\"number\">0</span></span><br><span class=\"line\">    accuracy = []</span><br><span class=\"line\">    loss_rpn_cls = []</span><br><span class=\"line\">    loss_rpn_regr = []</span><br><span class=\"line\">    loss_detc_cls = []</span><br><span class=\"line\">    loss_detc_regr = []</span><br><span class=\"line\">    f = open(filename,<span class=\"string\">'r'</span>,buffering=<span class=\"number\">-1</span>)</span><br><span class=\"line\">    lines = f.readlines()</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"keyword\">for</span> line <span class=\"keyword\">in</span> lines:</span><br><span class=\"line\">        n = n + <span class=\"number\">1</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> <span class=\"string\">'Classifier accuracy for bounding boxes from RPN'</span> <span class=\"keyword\">in</span> line:</span><br><span class=\"line\">            result = re.findall(<span class=\"string\">r\"\\d+\\.?\\d*\"</span>,line)</span><br><span class=\"line\">            accuracy.append(float(result[<span class=\"number\">0</span>]))</span><br><span class=\"line\">            </span><br><span class=\"line\">        <span class=\"keyword\">if</span> <span class=\"string\">'Loss RPN classifier'</span> <span class=\"keyword\">in</span> line:</span><br><span class=\"line\">            result = re.findall(<span class=\"string\">r\"\\d+\\.?\\d*\"</span>,line)</span><br><span class=\"line\">            loss_rpn_cls.append(float(result[<span class=\"number\">0</span>]))       </span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">if</span> <span class=\"string\">'Loss RPN regression'</span> <span class=\"keyword\">in</span> line:</span><br><span class=\"line\">            result = re.findall(<span class=\"string\">r\"\\d+\\.?\\d*\"</span>,line)</span><br><span class=\"line\">            loss_rpn_regr.append(float(result[<span class=\"number\">0</span>]))</span><br><span class=\"line\">            </span><br><span class=\"line\">        <span class=\"keyword\">if</span> <span class=\"string\">'Loss Detector classifier'</span> <span class=\"keyword\">in</span> line:</span><br><span class=\"line\">            result = re.findall(<span class=\"string\">r\"\\d+\\.?\\d*\"</span>,line)</span><br><span class=\"line\">            loss_detc_cls.append(float(result[<span class=\"number\">0</span>]))</span><br><span class=\"line\">            </span><br><span class=\"line\">        <span class=\"keyword\">if</span> <span class=\"string\">'Loss Detector regression'</span> <span class=\"keyword\">in</span> line:</span><br><span class=\"line\">            result = re.findall(<span class=\"string\">r\"\\d+\\.?\\d*\"</span>,line)</span><br><span class=\"line\">            loss_detc_regr.append(float(result[<span class=\"number\">0</span>])) </span><br><span class=\"line\">            </span><br><span class=\"line\">    f.close()</span><br><span class=\"line\">    print(n)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> accuracy, loss_rpn_cls, loss_rpn_regr, loss_detc_cls, loss_detc_regr</span><br></pre></td></tr></table></figure></p>\n<h4 id=\"plot-epoch-loss-and-accury\"><a href=\"#plot-epoch-loss-and-accury\" class=\"headerlink\" title=\"plot epoch loss and accury\"></a>plot epoch loss and accury</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">filename = <span class=\"string\">r'F:\\desktop\\新建文件夹\\1000-no_balance\\train1.out'</span></span><br><span class=\"line\">aa,bb,cc,dd,ee = obtain_batch(filename)</span><br><span class=\"line\">x_cor = np.arange(<span class=\"number\">0</span>,len(aa),<span class=\"number\">1</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">plt.plot(x_cor,aa, c=<span class=\"string\">'b'</span>, label = <span class=\"string\">\"Accuracy\"</span>)</span><br><span class=\"line\">plt.plot(x_cor,bb, c=<span class=\"string\">'c'</span>, label = <span class=\"string\">\"Loss RPN classifier\"</span>)</span><br><span class=\"line\">plt.plot(x_cor,cc, c=<span class=\"string\">'g'</span>, label = <span class=\"string\">\"Loss RPN regression\"</span>)</span><br><span class=\"line\">plt.plot(x_cor,dd, c=<span class=\"string\">'k'</span>, label = <span class=\"string\">\"Loss Detector classifier\"</span>)</span><br><span class=\"line\">plt.plot(x_cor,ee, c=<span class=\"string\">'m'</span>, label = <span class=\"string\">\"Loss Detector regression\"</span>)</span><br><span class=\"line\">plt.ylabel(<span class=\"string\">\"Value of Accuracy and Loss\"</span>) </span><br><span class=\"line\">plt.xlabel(<span class=\"string\">\"Number of Epoch\"</span>)</span><br><span class=\"line\">plt.title(<span class=\"string\">'Loss and Accuracy for Totoal Epochs'</span>)  </span><br><span class=\"line\">plt.legend()</span><br><span class=\"line\">plt.ylim(<span class=\"number\">0</span>,<span class=\"number\">2</span>)</span><br><span class=\"line\"><span class=\"comment\">#plt.xlim(0,11540)</span></span><br><span class=\"line\">plt.savefig(<span class=\"string\">\"pic1.PNG\"</span>, dpi = <span class=\"number\">600</span>)</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n<p><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/pic1.PNG\" alt=\"image\"></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">filename = <span class=\"string\">r'F:\\desktop\\新建文件夹\\1000-no_balance\\train1.out'</span></span><br><span class=\"line\">a,b,c,d = obtain_each_batch(filename)</span><br><span class=\"line\">x_cor = np.arange(<span class=\"number\">0</span>,len(a),<span class=\"number\">1</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">plt.plot(x_cor,a, c=<span class=\"string\">'b'</span>, label = <span class=\"string\">\"rpn_cls\"</span>)</span><br><span class=\"line\">plt.plot(x_cor,b, c=<span class=\"string\">'c'</span>, label = <span class=\"string\">\"rpn_regr\"</span>)</span><br><span class=\"line\">plt.plot(x_cor,c, c=<span class=\"string\">'g'</span>, label = <span class=\"string\">\"detector_cls\"</span>)</span><br><span class=\"line\">plt.plot(x_cor,d, c=<span class=\"string\">'k'</span>, label = <span class=\"string\">\"detector_regr\"</span>)</span><br><span class=\"line\">plt.ylabel(<span class=\"string\">\"Value of Loss\"</span>) </span><br><span class=\"line\">plt.xlabel(<span class=\"string\">\"Epoch Length\"</span>)</span><br><span class=\"line\">plt.title(<span class=\"string\">'Loss for Lenght of Epoch'</span>)  </span><br><span class=\"line\">plt.legend()</span><br><span class=\"line\"><span class=\"comment\">#plt.ylim(0,2)</span></span><br><span class=\"line\">plt.xlim(<span class=\"number\">80787</span>,<span class=\"number\">92327</span>)</span><br><span class=\"line\">plt.savefig(<span class=\"string\">\"pic2.PNG\"</span>, dpi = <span class=\"number\">600</span>)</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n<p><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/pic2.PNG\" alt=\"image\"></p>\n<p><a href=\"https://github.com/Trouble404/Object-Detection-System-Based-on-CNN-and-Capsule-Network/blob/master/Plot/logbook_plot.ipynb\" target=\"_blank\" rel=\"noopener\">Jupyter notebook</a></p>\n<h2 id=\"August\"><a href=\"#August\" class=\"headerlink\" title=\"August\"></a>August</h2><h3 id=\"【01-02-08-2018】\"><a href=\"#【01-02-08-2018】\" class=\"headerlink\" title=\"【01~02/08/2018】\"></a>【01~02/08/2018】</h3><h4 id=\"test-network\"><a href=\"#test-network\" class=\"headerlink\" title=\"test network\"></a>test network</h4><p>首先是搭建网络，用于train部分相同的设置搭建<br>不过在这里图像增强就设置为关闭了</p>\n<p><strong>构建rpn输出</strong><br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">shared_layers = nn.nn_base(img_input, trainable=<span class=\"keyword\">True</span>)</span><br><span class=\"line\">num_anchors = len(C.anchor_box_scales)*len(C.anchor_box_ratios)</span><br><span class=\"line\">rpn_layers = nn.rpn(shared_layers,num_anchors)</span><br></pre></td></tr></table></figure></p>\n<p><strong>构建classifier输出</strong>，参数分别是：特征层输出，预选框，探测框的数目，多少个类，是否可训练<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">classifier = nn.classifier(feature_map_input, roi_input, C.num_rois,nb_classes=len(class_mapping), trainable=<span class=\"keyword\">True</span>)</span><br></pre></td></tr></table></figure></p>\n<p><strong>载入训练好的权重：</strong><br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">C.model_path = <span class=\"string\">'gpu_resnet50_weights.h5'</span></span><br><span class=\"line\"><span class=\"keyword\">try</span>:</span><br><span class=\"line\">    print(<span class=\"string\">'Loading weights from &#123;&#125;'</span>.format(C.model_path))</span><br><span class=\"line\">    model_rpn.load_weights(C.model_path, by_name=<span class=\"keyword\">True</span>)</span><br><span class=\"line\">    model_classifier.load_weights(C.model_path, by_name=<span class=\"keyword\">True</span>)</span><br><span class=\"line\"><span class=\"keyword\">except</span>:</span><br><span class=\"line\">    print(<span class=\"string\">'can not load'</span>)</span><br></pre></td></tr></table></figure></p>\n<p><strong>读取需要检测的图片：</strong><br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/test/test4.jpg\" alt=\"image\"><br>将图片规整到制定的大小</p>\n<ol>\n<li><p>将图片缩放到规定的大小<br> 首先从配置文件夹中得到最小边的大小<br> 得到图片的高度和宽度<br> 根据高度和宽度谁大谁小，确定规整后图片的高宽<br> 将图片缩放到指定的大小，用的是立方插值。返回的缩放后的图片img和相应的缩放的比例。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">format_img_size</span><span class=\"params\">(img, C)</span>:</span></span><br><span class=\"line\">    (height,width,_) = img.shape</span><br><span class=\"line\">    <span class=\"keyword\">if</span> width &lt;= height:</span><br><span class=\"line\">        ratio = C.im_size/width</span><br><span class=\"line\">    <span class=\"keyword\">else</span>:</span><br><span class=\"line\">        ratio = C.im_size/height</span><br><span class=\"line\">    new_width, new_height = image_processing.get_new_img_size(width,height, C.im_size)</span><br><span class=\"line\">    img = cv2.resize(img, (new_width, new_height), interpolation=cv2.INTER_CUBIC)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> img, ratio</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>对图片每一个通道的像素值做规整<br>  将图片的BGR变成RGB，因为网上训练好的RESNET图片都是以此训练的<br> 将图片数据类型转换为np.float32，并减去每一个通道的均值，理由同上<br> 图片的像素值除一个缩放因子，此处为1<br> 将图片的深度变到第一个位置<br> 给图片增加一个维度</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">format_img_channels</span><span class=\"params\">(img, C)</span>:</span></span><br><span class=\"line\">\t<span class=\"string\">\"\"\" formats the image channels based on config \"\"\"</span></span><br><span class=\"line\">\timg = img[:, :, (<span class=\"number\">2</span>, <span class=\"number\">1</span>, <span class=\"number\">0</span>)]</span><br><span class=\"line\">\timg = img.astype(np.float32)</span><br><span class=\"line\">\timg[:, :, <span class=\"number\">0</span>] -= C.img_channel_mean[<span class=\"number\">0</span>]</span><br><span class=\"line\">\timg[:, :, <span class=\"number\">1</span>] -= C.img_channel_mean[<span class=\"number\">1</span>]</span><br><span class=\"line\">\timg[:, :, <span class=\"number\">2</span>] -= C.img_channel_mean[<span class=\"number\">2</span>]</span><br><span class=\"line\">\timg /= C.img_scaling_factor</span><br><span class=\"line\">\timg = np.transpose(img, (<span class=\"number\">2</span>, <span class=\"number\">0</span>, <span class=\"number\">1</span>))</span><br><span class=\"line\">\timg = np.expand_dims(img, axis=<span class=\"number\">0</span>)</span><br><span class=\"line\">\t<span class=\"keyword\">return</span> img</span><br></pre></td></tr></table></figure>\n</li>\n</ol>\n<p>如果用的是tensorflow内核，需要将图片的深度变换到最后一位。</p>\n<p><strong>进行区域预测</strong><br>Y1:anchor包含物体的概率<br>Y2:每一个anchor对应的回归梯度<br>F:卷积后的特征图，接下来会有用</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[Y1, Y2, F] = model_rpn.predict(X)</span><br></pre></td></tr></table></figure>\n<p>获得rpn预测的结果以及对应的回归梯度，这一步就是对图片上隔16个像素的每个anchor进行rpn计算<br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/pic_anchors.png\" alt=\"image\"><br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/25_1.jpg\" alt=\"image\"></p>\n<p><strong>根据rpn预测的结果，得到预选框:</strong><br>这里会返回300个预选框以及它们对应的坐标(x1,y1,x2,y2)<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># transform predicted rpn to cordinates of boxes</span></span><br><span class=\"line\">R = rpn_to_boxes.rpn_to_roi(Y1, Y2, C, K.image_dim_ordering(), use_regr=<span class=\"keyword\">True</span>, overlap_thresh=<span class=\"number\">0.7</span>)</span><br></pre></td></tr></table></figure></p>\n<p><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/pic_rois.png\" alt=\"image\"></p>\n<p>将预选框的坐标由(x1,y1,x2,y2) 改到 (x,y,w,h)<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">R[:, <span class=\"number\">2</span>] -= R[:, <span class=\"number\">0</span>]</span><br><span class=\"line\">R[:, <span class=\"number\">3</span>] -= R[:, <span class=\"number\">1</span>]</span><br></pre></td></tr></table></figure></p>\n<p><strong>遍历所有的预选框</strong><br>需要注意的是每一次遍历预选框的个数为C.num_rois<br>每一次遍历32个预选框，那么总共需要300/32, 10批次<br>取出32个预选框，并增加一个维度【注：当不满一个32，其自动只取到最后一个】<br>当预选框被取空的时候，停止循环<br>当最后一次去不足32个预选框时，补第一个框使其达到32个。<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># divided 32 bboxes as one group</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> jk <span class=\"keyword\">in</span> range(R.shape[<span class=\"number\">0</span>]//C.num_rois + <span class=\"number\">1</span>):</span><br><span class=\"line\">    <span class=\"comment\"># pick num_rios(32) bboxes one time, only pick to last bboxes in last group</span></span><br><span class=\"line\">    ROIs = np.expand_dims(R[C.num_rois*jk:C.num_rois*(jk+<span class=\"number\">1</span>), :], axis=<span class=\"number\">0</span>)</span><br><span class=\"line\">    <span class=\"comment\">#print(ROIs.shape)</span></span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\"># no proposals, out iter</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> ROIs.shape[<span class=\"number\">1</span>] == <span class=\"number\">0</span>:</span><br><span class=\"line\">        <span class=\"keyword\">break</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># when last time can't obtain num_rios(32) bboxes, adding bboxes with 0 to fill to 32 bboxes</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> jk == R.shape[<span class=\"number\">0</span>]//C.num_rois:</span><br><span class=\"line\">        <span class=\"comment\">#pad R</span></span><br><span class=\"line\">        curr_shape = ROIs.shape</span><br><span class=\"line\">        target_shape = (curr_shape[<span class=\"number\">0</span>],C.num_rois,curr_shape[<span class=\"number\">2</span>])</span><br><span class=\"line\">        ROIs_padded = np.zeros(target_shape).astype(ROIs.dtype)</span><br><span class=\"line\">        ROIs_padded[:, :curr_shape[<span class=\"number\">1</span>], :] = ROIs</span><br><span class=\"line\">        ROIs_padded[<span class=\"number\">0</span>, curr_shape[<span class=\"number\">1</span>]:, :] = ROIs[<span class=\"number\">0</span>, <span class=\"number\">0</span>, :]</span><br><span class=\"line\">        <span class=\"comment\"># 10 group with 320 bboxes</span></span><br><span class=\"line\">        ROIs = ROIs_padded</span><br></pre></td></tr></table></figure></p>\n<p>这样就可以送入分类网络了</p>\n<p><strong>进行类别预测和边框回归</strong></p>\n<p>预测<br>P_cls：该边框属于某一类别的概率<br>P_regr：每一个类别对应的边框回归梯度<br>F:rpn网络得到的卷积后的特征图<br>ROIS:处理得到的区域预选框<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[P_cls, P_regr] = model_classifier_only.predict([F, ROIs])</span><br></pre></td></tr></table></figure></p>\n<p>遍历每一个预选宽<br>如果该预选框的最大概率小于设定的阈值（即预测的肯定程度大于一定的值，我们才认为这次的类别的概率预测是有效的，或者最大的概率出现在背景上，则认为这个预选框是无效的，进行下一次预测。<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">for</span> ii <span class=\"keyword\">in</span> range(P_cls.shape[<span class=\"number\">1</span>]):</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># if smaller than setting threshold, we think this bbox invalid</span></span><br><span class=\"line\">    <span class=\"comment\"># and if this bbox's class is background, we don't need to care about it</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> np.max(P_cls[<span class=\"number\">0</span>, ii, :]) &lt; bbox_threshold <span class=\"keyword\">or</span> np.argmax(P_cls[<span class=\"number\">0</span>, ii, :]) == (P_cls.shape[<span class=\"number\">2</span>] - <span class=\"number\">1</span>):</span><br><span class=\"line\">        <span class=\"keyword\">continue</span></span><br></pre></td></tr></table></figure></p>\n<p>不属于上面的两种情况，取最大的概率处为此边框的类别得到其名称。<br>创建两个list，用于存放不同类别对应的边框和概率<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># obatain max possibility's class name by class mapping</span></span><br><span class=\"line\">cls_name = class_mapping[np.argmax(P_cls[<span class=\"number\">0</span>, ii, :])]</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># saving bboxes and probs</span></span><br><span class=\"line\"><span class=\"keyword\">if</span> cls_name <span class=\"keyword\">not</span> <span class=\"keyword\">in</span> bboxes:</span><br><span class=\"line\">    bboxes[cls_name] = []</span><br><span class=\"line\">    probs[cls_name] = []</span><br></pre></td></tr></table></figure></p>\n<p>得到该预选框的信息<br>得到类别对应的编号<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># obtain current cordinates of proposal</span></span><br><span class=\"line\">(x, y, w, h) = ROIs[<span class=\"number\">0</span>, ii, :]</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># obtain the position with max possibility</span></span><br><span class=\"line\">cls_num = np.argmax(P_cls[<span class=\"number\">0</span>, ii, :])</span><br></pre></td></tr></table></figure></p>\n<p>这样符合条件的预选框以及对应的分类类别和概率就可以画在图片上了<br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/pic_cls1.png\" alt=\"iamge\"></p>\n<p>根据类别编号得到该类的边框回归梯度<br>对回归梯度进行规整化<br>对预测的边框进行修正<br>向相应的类里面添加信息【乘 C.rpn_stride，边框的预测都是在特征图上进行的要将其映射到规整后的原图上】<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">try</span>:</span><br><span class=\"line\">    <span class=\"comment\"># obtain privous position's bbox's regrient</span></span><br><span class=\"line\">    (tx, ty, tw, th) = P_regr[<span class=\"number\">0</span>, ii, <span class=\"number\">4</span>*cls_num:<span class=\"number\">4</span>*(cls_num+<span class=\"number\">1</span>)]</span><br><span class=\"line\">    <span class=\"comment\"># waiting test</span></span><br><span class=\"line\">    tx /= C.classifier_regr_std[<span class=\"number\">0</span>]</span><br><span class=\"line\">    ty /= C.classifier_regr_std[<span class=\"number\">1</span>]</span><br><span class=\"line\">    tw /= C.classifier_regr_std[<span class=\"number\">2</span>]</span><br><span class=\"line\">    th /= C.classifier_regr_std[<span class=\"number\">3</span>]</span><br><span class=\"line\">    <span class=\"comment\"># fix box with regreient</span></span><br><span class=\"line\">    x, y, w, h = rpn_to_boxes.apply_regr(x, y, w, h, tx, ty, tw, th)</span><br><span class=\"line\"><span class=\"keyword\">except</span>:</span><br><span class=\"line\">    <span class=\"keyword\">pass</span></span><br><span class=\"line\"><span class=\"comment\"># cordinates of current's box on real img</span></span><br><span class=\"line\">bboxes[cls_name].append([C.rpn_stride*x, C.rpn_stride*y, C.rpn_stride*(x+w), C.rpn_stride*(y+h)])</span><br><span class=\"line\"><span class=\"comment\"># coresspoding posbility</span></span><br><span class=\"line\">probs[cls_name].append(np.max(P_cls[<span class=\"number\">0</span>, ii, :]))</span><br></pre></td></tr></table></figure></p>\n<p>这样修正过的框可以画在图上：<br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/pic_cls2.png\" alt=\"image\"></p>\n<p>遍历bboxes里的类，取出某一类的bbox，合并一些重合度较高的选框<br>No Max Supression<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># for all classes in current boxes</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> key <span class=\"keyword\">in</span> bboxes:</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># bboxes's cordinates</span></span><br><span class=\"line\">    bbox = np.array(bboxes[key])</span><br><span class=\"line\">    <span class=\"comment\"># apply NMX to merge some  overlapping boxes</span></span><br><span class=\"line\">    new_boxes, new_probs = rpn_to_boxes.non_max_suppression_fast(bbox, np.array(probs[key]), overlap_thresh=<span class=\"number\">0.5</span>)</span><br></pre></td></tr></table></figure></p>\n<p>最终的图：<br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/pic_cls3.png\" alt=\"image\"></p>\n<p><a href=\"https://github.com/Trouble404/Object-Detection-System-Based-on-CNN-and-Capsule-Network/blob/master/Faster-RCNN/test.ipynb\" target=\"_blank\" rel=\"noopener\">Jupyter notebook</a></p>\n<h4 id=\"result\"><a href=\"#result\" class=\"headerlink\" title=\"result\"></a>result</h4><p>Small img, only 8k</p>\n<div align=\"center\"><br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/test/test1.jpg\" height=\"200px\"><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/test/test1.png\" height=\"200px\"><br></div>\n\n<hr>\n<p>Overlapping img</p>\n<div align=\"center\"><br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/test/test2.jpg\" height=\"200px\"><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/test/test2.png\" height=\"200px\"><br></div>\n\n<hr>\n<p>Crowed People</p>\n<div align=\"center\"><br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/test/test3.jpg\" height=\"260px\"><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/test/test3.png\" height=\"260px\"><br></div>\n\n<hr>\n<p>cow and people</p>\n<div align=\"center\"><br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/test/test4.jpg\" height=\"260px\"><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/test/test4.png\" height=\"260px\"><br></div>\n\n<hr>\n<p>car and plane</p>\n<div align=\"center\"><br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/test/test5.jpg\" height=\"220px\"><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/test/test5.png\" height=\"220px\"><br></div>\n\n<hr>\n<p>Street img</p>\n<div align=\"center\"><br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/test/test6.jpg\" height=\"270px\"><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/test/test6.png\" height=\"270px\"><br></div>\n\n<hr>\n<p>Lots Dogs</p>\n<div align=\"center\"><br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/test/test7.jpg\" height=\"250px\"><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/test/test7.png\" height=\"250px\"><br></div>\n\n<hr>\n<p>Overlapping car and people</p>\n<div align=\"center\"><br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/test/test8.jpg\" height=\"290px\"><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/test/test8.png\" height=\"290px\"><br></div>\n\n<p>直观看的话效果还不错，但是一些重叠的物体框会出现反复，或者取不到。而且分类有一点过拟合。</p>\n<h3 id=\"【03-08-2018】\"><a href=\"#【03-08-2018】\" class=\"headerlink\" title=\"【03/08/2018】\"></a>【03/08/2018】</h3><h4 id=\"evaluation\"><a href=\"#evaluation\" class=\"headerlink\" title=\"evaluation\"></a>evaluation</h4><p><strong>mAP</strong><br>mAP是目标算法中衡量算法的精确度的指标，涉及两个概念：查准率Precision、查全率Recall。对于object detection任务，每一个object都可以计算出其Precision和Recall，多次计算/试验，每个类都 可以得到一条P-R曲线，曲线下的面积就是AP的值，这个mean的意思是对每个类的AP再求平均，得到的就是mAP的值，mAP的大小一定在[0,1]区间。 </p>\n<p><strong>AP</strong>:Precision对Recall积分，可通过改变正负样本阈值求得矩形面积，进而求积分得到，也可以通过sklearn.metrics.average_precision_score函数直接得到。 </p>\n<p>传入预测值和真实值和resize比例，得到可以传入sklearn.metrics.average_precision_score函数的值，即：真实值和预测概率</p>\n<hr>\n<p>首先搭建rpn和分类器网络，按照之前的train部分来就可以了<br>这里注意分类网络的输入换成测试图片的feature map<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">num_features = <span class=\"number\">1024</span></span><br><span class=\"line\"></span><br><span class=\"line\">input_shape_img = (<span class=\"keyword\">None</span>, <span class=\"keyword\">None</span>, <span class=\"number\">3</span>)</span><br><span class=\"line\">input_shape_features = (<span class=\"keyword\">None</span>, <span class=\"keyword\">None</span>, num_features)</span><br><span class=\"line\"></span><br><span class=\"line\">img_input = Input(shape=input_shape_img)</span><br><span class=\"line\">roi_input = Input(shape=(C.num_rois, <span class=\"number\">4</span>))</span><br><span class=\"line\">feature_map_input = Input(shape=input_shape_features)</span><br><span class=\"line\"></span><br><span class=\"line\">classifier = nn.classifier(feature_map_input, roi_input, C.num_rois, nb_classes=len(class_mapping), trainable=<span class=\"keyword\">True</span>)</span><br></pre></td></tr></table></figure></p>\n<p>然后载入需要测试的模型权重</p>\n<p>按照VOC的数据集标注，把测试集分出来：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">train_imgs = []</span><br><span class=\"line\">test_imgs = []</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">for</span> each <span class=\"keyword\">in</span> all_imgs:</span><br><span class=\"line\">\t<span class=\"keyword\">if</span> each[<span class=\"string\">'imageset'</span>] == <span class=\"string\">'trainval'</span>:</span><br><span class=\"line\">\t\ttrain_imgs.append(each)</span><br><span class=\"line\">\t<span class=\"keyword\">if</span> each[<span class=\"string\">'imageset'</span>] == <span class=\"string\">'test'</span>:</span><br><span class=\"line\">\t\ttest_imgs.append(each)</span><br></pre></td></tr></table></figure></p>\n<p>按照之前的预测方法，求出图片的预测框坐标以及对应的分类名字，然后把这些信息放入对应的字典里面，与xml解析的文件一样的格式：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">for</span> jk <span class=\"keyword\">in</span> range(new_boxes.shape[<span class=\"number\">0</span>]):</span><br><span class=\"line\">    (x1, y1, x2, y2) = new_boxes[jk, :]</span><br><span class=\"line\">    det = &#123;<span class=\"string\">'x1'</span>: x1, <span class=\"string\">'x2'</span>: x2, <span class=\"string\">'y1'</span>: y1, <span class=\"string\">'y2'</span>: y2, <span class=\"string\">'class'</span>: key, <span class=\"string\">'prob'</span>: new_probs[jk]&#125;</span><br><span class=\"line\">    all_dets.append(det)</span><br></pre></td></tr></table></figure></p>\n<p><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/014_1.JPG\" alt=\"image\"></p>\n<p>然后读取标注的框的真实数值：<br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/014_2.JPG\" alt=\"image\"></p>\n<p>遍历真实信息里面的每一个狂，将bbox_matched这个属性标注为FALSE，之后如果预测框和标注框对应上的话，这个属性就会被设置为True<br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/14_3.JPG\" alt=\"image\"></p>\n<p>获取预测框里面的分类对应概率，并且按照概率从大到小得到idx位置：<br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/014_3.JPG\" alt=\"image\"></p>\n<p>按照概率大小，对每一个对应的预测框，对比每一个标注的框，如果预测的类与当前标注框的类相同并且没有被匹配过，计算两个框的iou，如果大于0.5的话就表明预测框匹配当前标注框，保存预测概率以及对应的是否匹配：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># process each bbox with hightest prob</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> box_idx <span class=\"keyword\">in</span> box_idx_sorted_by_prob:</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\"># obtain current box's cordinates, class and prob</span></span><br><span class=\"line\">    pred_box = pred[box_idx]</span><br><span class=\"line\">    pred_class = pred_box[<span class=\"string\">'class'</span>]</span><br><span class=\"line\">    pred_x1 = pred_box[<span class=\"string\">'x1'</span>]</span><br><span class=\"line\">    pred_x2 = pred_box[<span class=\"string\">'x2'</span>]</span><br><span class=\"line\">    pred_y1 = pred_box[<span class=\"string\">'y1'</span>]</span><br><span class=\"line\">    pred_y2 = pred_box[<span class=\"string\">'y2'</span>]</span><br><span class=\"line\">    pred_prob = pred_box[<span class=\"string\">'prob'</span>]</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\"># if not in P list, save current class infomration to it</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> pred_class <span class=\"keyword\">not</span> <span class=\"keyword\">in</span> P:</span><br><span class=\"line\">        P[pred_class] = []</span><br><span class=\"line\">        T[pred_class] = []</span><br><span class=\"line\">        <span class=\"comment\"># put porb to P</span></span><br><span class=\"line\">    P[pred_class].append(pred_prob)</span><br><span class=\"line\">    <span class=\"comment\"># used to check whether find current object</span></span><br><span class=\"line\">    found_match = <span class=\"keyword\">False</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># compare each real bbox</span></span><br><span class=\"line\">    <span class=\"comment\"># obtain real box's cordinates, class and prob</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> gt_box <span class=\"keyword\">in</span> gt:</span><br><span class=\"line\">        gt_class = gt_box[<span class=\"string\">'class'</span>]</span><br><span class=\"line\">        <span class=\"comment\"># bacause the image is rezied, so calculate the real cordinates</span></span><br><span class=\"line\">        gt_x1 = gt_box[<span class=\"string\">'x1'</span>]/fx</span><br><span class=\"line\">        gt_x2 = gt_box[<span class=\"string\">'x2'</span>]/fx</span><br><span class=\"line\">        gt_y1 = gt_box[<span class=\"string\">'y1'</span>]/fy</span><br><span class=\"line\">        gt_y2 = gt_box[<span class=\"string\">'y2'</span>]/fy</span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"comment\"># obtain box_matched - all false at beginning</span></span><br><span class=\"line\">        gt_seen = gt_box[<span class=\"string\">'bbox_matched'</span>]</span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"comment\"># ture class != predicted class</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> gt_class != pred_class:</span><br><span class=\"line\">            <span class=\"keyword\">continue</span></span><br><span class=\"line\">        <span class=\"comment\"># already matched</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> gt_seen:</span><br><span class=\"line\">            <span class=\"keyword\">continue</span></span><br><span class=\"line\">        <span class=\"comment\"># calculate iou of predicted bbox and real bbox </span></span><br><span class=\"line\">        iou = rpn_calculation.iou((pred_x1, pred_y1, pred_x2, pred_y2), (gt_x1, gt_y1, gt_x2, gt_y2))</span><br><span class=\"line\">        <span class=\"comment\"># if iou &gt; 0.5, we will set this prediction correct</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> iou &gt;= <span class=\"number\">0.5</span>:</span><br><span class=\"line\">            found_match = <span class=\"keyword\">True</span></span><br><span class=\"line\">            gt_box[<span class=\"string\">'bbox_matched'</span>] = <span class=\"keyword\">True</span></span><br><span class=\"line\">            <span class=\"keyword\">break</span></span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            <span class=\"keyword\">continue</span></span><br><span class=\"line\">    <span class=\"comment\"># 1 means this position's bbox correct match with orignal image</span></span><br><span class=\"line\">    T[pred_class].append(int(found_match))</span><br></pre></td></tr></table></figure>\n<p><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/014_4.JPG\" alt=\"image\"></p>\n<p>遍历每一个标注框，如果没有被匹配到并且diffcult属性不是true的话，说明这个框漏检了，在之前保存的概率以及对应是否有概率里面加入物体1以及对应概率0<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># adding missing object compared to orignal image</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> gt_box <span class=\"keyword\">in</span> gt:</span><br><span class=\"line\">    <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> gt_box[<span class=\"string\">'bbox_matched'</span>] <span class=\"keyword\">and</span> <span class=\"keyword\">not</span> gt_box[<span class=\"string\">'difficult'</span>]:</span><br><span class=\"line\">        <span class=\"keyword\">if</span> gt_box[<span class=\"string\">'class'</span>] <span class=\"keyword\">not</span> <span class=\"keyword\">in</span> P:</span><br><span class=\"line\">            P[gt_box[<span class=\"string\">'class'</span>]] = []</span><br><span class=\"line\">            T[gt_box[<span class=\"string\">'class'</span>]] = []</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># T = 1 means there are object, P = 0 means we did't detected that</span></span><br><span class=\"line\">        T[gt_box[<span class=\"string\">'class'</span>]].append(<span class=\"number\">1</span>)</span><br><span class=\"line\">        P[gt_box[<span class=\"string\">'class'</span>]].append(<span class=\"number\">0</span>)</span><br></pre></td></tr></table></figure></p>\n<p><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/014_5.JPG\" alt=\"image\"></p>\n<p>把当前信息存入到总的一个词典里面，就可以使用average_precision_score这个sklearn里面的函数计算ap了。与此同时，保存得到的结果并且显示总的map：<br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/014_6.JPG\" alt=\"image\"></p>\n<p><a href=\"https://github.com/Trouble404/Object-Detection-System-Based-on-CNN-and-Capsule-Network/blob/master/Faster-RCNN/map.ipynb\" target=\"_blank\" rel=\"noopener\">jupyter notebook</a></p>\n<h3 id=\"【06-10-08-2018】\"><a href=\"#【06-10-08-2018】\" class=\"headerlink\" title=\"【06~10/08/2018】\"></a>【06~10/08/2018】</h3><h4 id=\"adjust\"><a href=\"#adjust\" class=\"headerlink\" title=\"adjust\"></a>adjust</h4><p>将计算ap的函数包装好：<br><a href=\"https://github.com/Trouble404/Object-Detection-System-Based-on-CNN-and-Capsule-Network/blob/master/Faster-RCNN/map_all.ipynb\" target=\"_blank\" rel=\"noopener\">jupyter notebook</a></p>\n<h5 id=\"Project1-all-9-models\"><a href=\"#Project1-all-9-models\" class=\"headerlink\" title=\"Project1 all: 9 models:\"></a>Project1 all: 9 models:</h5><p>ALL_2 NO THRESHOLD OTHER WITH THRESHOLD MOST 0.8</p>\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">Classes</th>\n<th style=\"text-align:center\">ALL_1</th>\n<th style=\"text-align:center\">ALL_2</th>\n<th style=\"text-align:center\">ALL_3</th>\n<th style=\"text-align:center\">ALL_4</th>\n<th style=\"text-align:center\">ALL_5</th>\n<th style=\"text-align:center\">ALL_6</th>\n<th style=\"text-align:center\">ALL_7</th>\n<th style=\"text-align:center\">ALL_8</th>\n<th style=\"text-align:center\">ALL_9</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">motorbike</td>\n<td style=\"text-align:center\">0.2305</td>\n<td style=\"text-align:center\">0.2412</td>\n<td style=\"text-align:center\">0.2132</td>\n<td style=\"text-align:center\">0.2220</td>\n<td style=\"text-align:center\">0.2889</td>\n<td style=\"text-align:center\">0.2528</td>\n<td style=\"text-align:center\">0.2204</td>\n<td style=\"text-align:center\">0.2644</td>\n<td style=\"text-align:center\">0.2336</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">person</td>\n<td style=\"text-align:center\">0.6489</td>\n<td style=\"text-align:center\">0.6735</td>\n<td style=\"text-align:center\">0.7107</td>\n<td style=\"text-align:center\">0.6652</td>\n<td style=\"text-align:center\">0.7120</td>\n<td style=\"text-align:center\">0.7041</td>\n<td style=\"text-align:center\">0.7238</td>\n<td style=\"text-align:center\">0.7003</td>\n<td style=\"text-align:center\">0.7201</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">car</td>\n<td style=\"text-align:center\">0.1697</td>\n<td style=\"text-align:center\">0.1563</td>\n<td style=\"text-align:center\">0.2032</td>\n<td style=\"text-align:center\">0.2105</td>\n<td style=\"text-align:center\">0.2308</td>\n<td style=\"text-align:center\">0.2221</td>\n<td style=\"text-align:center\">0.2436</td>\n<td style=\"text-align:center\">0.2053</td>\n<td style=\"text-align:center\">0.2080</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">aeroplane</td>\n<td style=\"text-align:center\">0.7968</td>\n<td style=\"text-align:center\">0.7062</td>\n<td style=\"text-align:center\">0.7941</td>\n<td style=\"text-align:center\">0.6412</td>\n<td style=\"text-align:center\">0.7871</td>\n<td style=\"text-align:center\">0.7331</td>\n<td style=\"text-align:center\">0.7648</td>\n<td style=\"text-align:center\">0.7659</td>\n<td style=\"text-align:center\">0.6902</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">bottle</td>\n<td style=\"text-align:center\">0.2213</td>\n<td style=\"text-align:center\">0.2428</td>\n<td style=\"text-align:center\">0.2261</td>\n<td style=\"text-align:center\">0.1943</td>\n<td style=\"text-align:center\">0.2570</td>\n<td style=\"text-align:center\">0.2437</td>\n<td style=\"text-align:center\">0.2899</td>\n<td style=\"text-align:center\">0.1442</td>\n<td style=\"text-align:center\">0.2265</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">sheep</td>\n<td style=\"text-align:center\">0.6162</td>\n<td style=\"text-align:center\">0.5702</td>\n<td style=\"text-align:center\">0.6876</td>\n<td style=\"text-align:center\">0.6295</td>\n<td style=\"text-align:center\">0.6364</td>\n<td style=\"text-align:center\">0.5710</td>\n<td style=\"text-align:center\">0.6536</td>\n<td style=\"text-align:center\">0.6349</td>\n<td style=\"text-align:center\">0.6455</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">tvmonitor</td>\n<td style=\"text-align:center\">0.1582</td>\n<td style=\"text-align:center\">0.1601</td>\n<td style=\"text-align:center\">0.2231</td>\n<td style=\"text-align:center\">0.1748</td>\n<td style=\"text-align:center\">0.1551</td>\n<td style=\"text-align:center\">0.1603</td>\n<td style=\"text-align:center\">0.1317</td>\n<td style=\"text-align:center\">0.1584</td>\n<td style=\"text-align:center\">0.1678</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">boat</td>\n<td style=\"text-align:center\">0.3842</td>\n<td style=\"text-align:center\">0.2621</td>\n<td style=\"text-align:center\">0.2261</td>\n<td style=\"text-align:center\">0.1943</td>\n<td style=\"text-align:center\">0.3499</td>\n<td style=\"text-align:center\">0.2437</td>\n<td style=\"text-align:center\">0.2057</td>\n<td style=\"text-align:center\">0.2748</td>\n<td style=\"text-align:center\">0.3509</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">chair</td>\n<td style=\"text-align:center\">0.2811</td>\n<td style=\"text-align:center\">0.0563</td>\n<td style=\"text-align:center\">0.0891</td>\n<td style=\"text-align:center\">0.0621</td>\n<td style=\"text-align:center\">0.1353</td>\n<td style=\"text-align:center\">0.0865</td>\n<td style=\"text-align:center\">0.0907</td>\n<td style=\"text-align:center\">0.0854</td>\n<td style=\"text-align:center\">0.1282</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">bicycle</td>\n<td style=\"text-align:center\">0.1464</td>\n<td style=\"text-align:center\">0.1224</td>\n<td style=\"text-align:center\">0.1346</td>\n<td style=\"text-align:center\">0.1781</td>\n<td style=\"text-align:center\">0.1406</td>\n<td style=\"text-align:center\">0.1448</td>\n<td style=\"text-align:center\">0.1810</td>\n<td style=\"text-align:center\">0.1071</td>\n<td style=\"text-align:center\">0.1673</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">cat</td>\n<td style=\"text-align:center\">0.8901</td>\n<td style=\"text-align:center\">0.8565</td>\n<td style=\"text-align:center\">0.9103</td>\n<td style=\"text-align:center\">0.8417</td>\n<td style=\"text-align:center\">0.8289</td>\n<td style=\"text-align:center\">0.8274</td>\n<td style=\"text-align:center\">0.7572</td>\n<td style=\"text-align:center\">0.9143</td>\n<td style=\"text-align:center\">0.8118</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">pottedplant</td>\n<td style=\"text-align:center\">0.2075</td>\n<td style=\"text-align:center\">0.0926</td>\n<td style=\"text-align:center\">0.1790</td>\n<td style=\"text-align:center\">0.0532</td>\n<td style=\"text-align:center\">0.1517</td>\n<td style=\"text-align:center\">0.1150</td>\n<td style=\"text-align:center\">0.1080</td>\n<td style=\"text-align:center\">0.1022</td>\n<td style=\"text-align:center\">0.0939</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">horse</td>\n<td style=\"text-align:center\">0.1185</td>\n<td style=\"text-align:center\">0.0588</td>\n<td style=\"text-align:center\">0.0726</td>\n<td style=\"text-align:center\">0.0489</td>\n<td style=\"text-align:center\">0.0696</td>\n<td style=\"text-align:center\">0.0695</td>\n<td style=\"text-align:center\">0.0637</td>\n<td style=\"text-align:center\">0.0651</td>\n<td style=\"text-align:center\">0.0640</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">sofa</td>\n<td style=\"text-align:center\">0.2797</td>\n<td style=\"text-align:center\">0.2309</td>\n<td style=\"text-align:center\">0.2852</td>\n<td style=\"text-align:center\">0.2966</td>\n<td style=\"text-align:center\">0.3855</td>\n<td style=\"text-align:center\">0.4817</td>\n<td style=\"text-align:center\">0.3659</td>\n<td style=\"text-align:center\">0.3132</td>\n<td style=\"text-align:center\">0.3090</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">dog</td>\n<td style=\"text-align:center\">0.5359</td>\n<td style=\"text-align:center\">0.5077</td>\n<td style=\"text-align:center\">0.5578</td>\n<td style=\"text-align:center\">0.4413</td>\n<td style=\"text-align:center\">0.4832</td>\n<td style=\"text-align:center\">0.5793</td>\n<td style=\"text-align:center\">0.5687</td>\n<td style=\"text-align:center\">0.4910</td>\n<td style=\"text-align:center\">0.4598</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">cow</td>\n<td style=\"text-align:center\">0.7582</td>\n<td style=\"text-align:center\">0.6229</td>\n<td style=\"text-align:center\">0.7295</td>\n<td style=\"text-align:center\">0.5420</td>\n<td style=\"text-align:center\">0.5379</td>\n<td style=\"text-align:center\">0.5312</td>\n<td style=\"text-align:center\">0.5147</td>\n<td style=\"text-align:center\">0.5706</td>\n<td style=\"text-align:center\">0.6503</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">diningtable</td>\n<td style=\"text-align:center\">0.3979</td>\n<td style=\"text-align:center\">0.2734</td>\n<td style=\"text-align:center\">0.3739</td>\n<td style=\"text-align:center\">0.2963</td>\n<td style=\"text-align:center\">0.4715</td>\n<td style=\"text-align:center\">0.4987</td>\n<td style=\"text-align:center\">0.3895</td>\n<td style=\"text-align:center\">0.4983</td>\n<td style=\"text-align:center\">0.4666</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">bus</td>\n<td style=\"text-align:center\">0.6203</td>\n<td style=\"text-align:center\">0.5572</td>\n<td style=\"text-align:center\">0.6468</td>\n<td style=\"text-align:center\">0.6032</td>\n<td style=\"text-align:center\">0.6320</td>\n<td style=\"text-align:center\">0.6096</td>\n<td style=\"text-align:center\">0.7169</td>\n<td style=\"text-align:center\">0.5938</td>\n<td style=\"text-align:center\">0.5485</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">bird</td>\n<td style=\"text-align:center\">0.6164</td>\n<td style=\"text-align:center\">0.6662</td>\n<td style=\"text-align:center\">0.5692</td>\n<td style=\"text-align:center\">0.5751</td>\n<td style=\"text-align:center\">0.5407</td>\n<td style=\"text-align:center\">0.4125</td>\n<td style=\"text-align:center\">0.4925</td>\n<td style=\"text-align:center\">0.4347</td>\n<td style=\"text-align:center\">0.5208</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">train</td>\n<td style=\"text-align:center\">0.8655</td>\n<td style=\"text-align:center\">0.6916</td>\n<td style=\"text-align:center\">0.7141</td>\n<td style=\"text-align:center\">0.7166</td>\n<td style=\"text-align:center\">0.7643</td>\n<td style=\"text-align:center\">0.8107</td>\n<td style=\"text-align:center\">0.7100</td>\n<td style=\"text-align:center\">0.7194</td>\n<td style=\"text-align:center\">0.6263</td>\n</tr>\n<tr>\n<td style=\"text-align:center\"><strong>mAP</strong></td>\n<td style=\"text-align:center\"><strong>0.4472</strong></td>\n<td style=\"text-align:center\"><strong>0.3874</strong></td>\n<td style=\"text-align:center\"><strong>0.4341</strong></td>\n<td style=\"text-align:center\"><strong>0.3859</strong></td>\n<td style=\"text-align:center\"><strong>0.4279</strong></td>\n<td style=\"text-align:center\"><strong>0.4141</strong></td>\n<td style=\"text-align:center\"><strong>0.4096</strong></td>\n<td style=\"text-align:center\"><strong>0.4022</strong></td>\n<td style=\"text-align:center\"><strong>0.4045</strong></td>\n</tr>\n</tbody>\n</table>\n<hr>\n<h5 id=\"Project1-epoch-lenght-1000-epoch-1041-7-models\"><a href=\"#Project1-epoch-lenght-1000-epoch-1041-7-models\" class=\"headerlink\" title=\"Project1 epoch_lenght=1000, epoch:1041 : 7 models:\"></a>Project1 epoch_lenght=1000, epoch:1041 : 7 models:</h5><p>ALL WITH THRESHOLD MOST 0.51</p>\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">Classes</th>\n<th style=\"text-align:center\">ALL_1</th>\n<th style=\"text-align:center\">ALL_2</th>\n<th style=\"text-align:center\">ALL_3</th>\n<th style=\"text-align:center\">ALL_4</th>\n<th style=\"text-align:center\">ALL_5</th>\n<th style=\"text-align:center\">ALL_6</th>\n<th style=\"text-align:center\">ALL_7</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">motorbike</td>\n<td style=\"text-align:center\">0.2433</td>\n<td style=\"text-align:center\">0.2128</td>\n<td style=\"text-align:center\">0.2232</td>\n<td style=\"text-align:center\">0.2262</td>\n<td style=\"text-align:center\">0.2286</td>\n<td style=\"text-align:center\">0.2393</td>\n<td style=\"text-align:center\">0.2279</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">person</td>\n<td style=\"text-align:center\">0.6560</td>\n<td style=\"text-align:center\">0.6537</td>\n<td style=\"text-align:center\">0.6742</td>\n<td style=\"text-align:center\">0.6952</td>\n<td style=\"text-align:center\">0.6852</td>\n<td style=\"text-align:center\">0.6719</td>\n<td style=\"text-align:center\">0.6636</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">car</td>\n<td style=\"text-align:center\">0.1562</td>\n<td style=\"text-align:center\">0.1905</td>\n<td style=\"text-align:center\">0.1479</td>\n<td style=\"text-align:center\">0.2024</td>\n<td style=\"text-align:center\">0.2010</td>\n<td style=\"text-align:center\">0.1379</td>\n<td style=\"text-align:center\">0.1583</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">aeroplane</td>\n<td style=\"text-align:center\">0.7359</td>\n<td style=\"text-align:center\">0.6837</td>\n<td style=\"text-align:center\">0.6729</td>\n<td style=\"text-align:center\">0.6687</td>\n<td style=\"text-align:center\">0.6957</td>\n<td style=\"text-align:center\">0.7339</td>\n<td style=\"text-align:center\">0.6391</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">bottle</td>\n<td style=\"text-align:center\">0.1913</td>\n<td style=\"text-align:center\">0.1937</td>\n<td style=\"text-align:center\">0.2635</td>\n<td style=\"text-align:center\">0.1843</td>\n<td style=\"text-align:center\">0.2570</td>\n<td style=\"text-align:center\">0.1632</td>\n<td style=\"text-align:center\">0.1863</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">sheep</td>\n<td style=\"text-align:center\">0.5429</td>\n<td style=\"text-align:center\">0.5579</td>\n<td style=\"text-align:center\">0.6219</td>\n<td style=\"text-align:center\">0.5355</td>\n<td style=\"text-align:center\">0.5881</td>\n<td style=\"text-align:center\">0.5441</td>\n<td style=\"text-align:center\">0.5824</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">tvmonitor</td>\n<td style=\"text-align:center\">0.1295</td>\n<td style=\"text-align:center\">0.1601</td>\n<td style=\"text-align:center\">0.1368</td>\n<td style=\"text-align:center\">0.1407</td>\n<td style=\"text-align:center\">0.1147</td>\n<td style=\"text-align:center\">0.1349</td>\n<td style=\"text-align:center\">0.1154</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">boat</td>\n<td style=\"text-align:center\">0.1913</td>\n<td style=\"text-align:center\">0.2880</td>\n<td style=\"text-align:center\">0.2635</td>\n<td style=\"text-align:center\">0.3433</td>\n<td style=\"text-align:center\">0.3335</td>\n<td style=\"text-align:center\">0.3422</td>\n<td style=\"text-align:center\">0.3069</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">chair</td>\n<td style=\"text-align:center\">0.0587</td>\n<td style=\"text-align:center\">0.0657</td>\n<td style=\"text-align:center\">0.0342</td>\n<td style=\"text-align:center\">0.0680</td>\n<td style=\"text-align:center\">0.0695</td>\n<td style=\"text-align:center\">0.0752</td>\n<td style=\"text-align:center\">0.0760</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">bicycle</td>\n<td style=\"text-align:center\">0.1013</td>\n<td style=\"text-align:center\">0.1485</td>\n<td style=\"text-align:center\">0.1225</td>\n<td style=\"text-align:center\">0.1871</td>\n<td style=\"text-align:center\">0.1685</td>\n<td style=\"text-align:center\">0.1037</td>\n<td style=\"text-align:center\">0.1490</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">cat</td>\n<td style=\"text-align:center\">0.8737</td>\n<td style=\"text-align:center\">0.8557</td>\n<td style=\"text-align:center\">0.8007</td>\n<td style=\"text-align:center\">0.7982</td>\n<td style=\"text-align:center\">0.8045</td>\n<td style=\"text-align:center\">0.8067</td>\n<td style=\"text-align:center\">0.7732</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">pottedplant</td>\n<td style=\"text-align:center\">0.0694</td>\n<td style=\"text-align:center\">0.1059</td>\n<td style=\"text-align:center\">0.0748</td>\n<td style=\"text-align:center\">0.0878</td>\n<td style=\"text-align:center\">0.0893</td>\n<td style=\"text-align:center\">0.0690</td>\n<td style=\"text-align:center\">0.0865</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">horse</td>\n<td style=\"text-align:center\">0.0556</td>\n<td style=\"text-align:center\">0.0561</td>\n<td style=\"text-align:center\">0.0581</td>\n<td style=\"text-align:center\">0.0770</td>\n<td style=\"text-align:center\">0.0575</td>\n<td style=\"text-align:center\">0.0539</td>\n<td style=\"text-align:center\">0.0522</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">sofa</td>\n<td style=\"text-align:center\">0.2177</td>\n<td style=\"text-align:center\">0.2917</td>\n<td style=\"text-align:center\">0.1699</td>\n<td style=\"text-align:center\">0.1940</td>\n<td style=\"text-align:center\">0.3177</td>\n<td style=\"text-align:center\">0.1863</td>\n<td style=\"text-align:center\">0.1857</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">dog</td>\n<td style=\"text-align:center\">0.6269</td>\n<td style=\"text-align:center\">0.4989</td>\n<td style=\"text-align:center\">0.5015</td>\n<td style=\"text-align:center\">0.5333</td>\n<td style=\"text-align:center\">0.4914</td>\n<td style=\"text-align:center\">0.5572</td>\n<td style=\"text-align:center\">0.4747</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">cow</td>\n<td style=\"text-align:center\">0.5216</td>\n<td style=\"text-align:center\">0.6229</td>\n<td style=\"text-align:center\">0.5283</td>\n<td style=\"text-align:center\">0.6426</td>\n<td style=\"text-align:center\">0.4358</td>\n<td style=\"text-align:center\">0.4227</td>\n<td style=\"text-align:center\">0.4589</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">diningtable</td>\n<td style=\"text-align:center\">0.3076</td>\n<td style=\"text-align:center\">0.3889</td>\n<td style=\"text-align:center\">0.3283</td>\n<td style=\"text-align:center\">0.2404</td>\n<td style=\"text-align:center\">0.4219</td>\n<td style=\"text-align:center\">0.4153</td>\n<td style=\"text-align:center\">0.2627</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">bus</td>\n<td style=\"text-align:center\">0.5865</td>\n<td style=\"text-align:center\">0.5222</td>\n<td style=\"text-align:center\">0.6312</td>\n<td style=\"text-align:center\">0.5853</td>\n<td style=\"text-align:center\">0.5042</td>\n<td style=\"text-align:center\">0.4882</td>\n<td style=\"text-align:center\">0.5576</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">bird</td>\n<td style=\"text-align:center\">0.5339</td>\n<td style=\"text-align:center\">0.5039</td>\n<td style=\"text-align:center\">0.5150</td>\n<td style=\"text-align:center\">0.5152</td>\n<td style=\"text-align:center\">0.5838</td>\n<td style=\"text-align:center\">0.3890</td>\n<td style=\"text-align:center\">0.4680</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">train</td>\n<td style=\"text-align:center\">0.4994</td>\n<td style=\"text-align:center\">0.6541</td>\n<td style=\"text-align:center\">0.6702</td>\n<td style=\"text-align:center\">0.6920</td>\n<td style=\"text-align:center\">0.5959</td>\n<td style=\"text-align:center\">0.5893</td>\n<td style=\"text-align:center\">0.6861</td>\n</tr>\n<tr>\n<td style=\"text-align:center\"><strong>mAP</strong></td>\n<td style=\"text-align:center\"><strong>0.3699</strong></td>\n<td style=\"text-align:center\"><strong>0.3765</strong></td>\n<td style=\"text-align:center\"><strong>0.3748</strong></td>\n<td style=\"text-align:center\"><strong>0.3814</strong></td>\n<td style=\"text-align:center\"><strong>0.3786</strong></td>\n<td style=\"text-align:center\"><strong>0.3562</strong></td>\n<td style=\"text-align:center\"><strong>0.3555</strong></td>\n</tr>\n</tbody>\n</table>\n<p>在测试集上的结果不是很好，不同class的ap差距较大，可能是由于训练时候不平均或者训练集太小的原因</p>\n<p><strong>尝试加入VOC2007的数据进训练集当中，观察结果。</strong><br>解析VOC2007的过程中遇到了OpenCV读取不了图片的BUG。<br>（已修复）<br>VOC2012的数据莫名没有了，因为之前测试过的原因，一直以为是VOC2007的数据解析有问题，大概是Irius的文件上限时间到了自动清除了数据。</p>\n<p>20个类当中的AP差距过大，其实数据集是不平衡的，有的类只有大概1000个样本，但是人这个样本就有2W多，而且之前的训练过程中每次图片都是在训练集中随机选的，所以尝试修改了流程，当所有训练集中的数据都被读取训练过以后再打乱训练集，与此同时配合class_balance的功能使用。</p>\n<p>实际上使用的时候class balance效果不是很好，后面没有开启。</p>\n<p>用了一个较大的学习率尝试训练没有载入imagenet预训练权重的版本。</p>\n<p>交叉法</p>\n<h3 id=\"【13-08-2018】\"><a href=\"#【13-08-2018】\" class=\"headerlink\" title=\"【13/08/2018】\"></a>【13/08/2018】</h3><h4 id=\"soft-NMS\"><a href=\"#soft-NMS\" class=\"headerlink\" title=\"soft-NMS\"></a>soft-NMS</h4><p>传统的非最大抑制算法首先在被检测图片中产生一系列的检测框B以及对应的分数S。当选中最大分数的检测框M，它被从集合B中移出并放入最终检测结果集合D。于此同时，集合B中任何与检测框M的重叠部分大于重叠阈值Nt的检测框也将随之移除。非最大抑制算法中的最大问题就是它将相邻检测框的分数均强制归零。在这种情况下，如果一个真实物体在重叠区域出现，则将导致对该物体的检测失败并降低了算法的平均检测率（average precision, AP）。</p>\n<p>换一种思路，如果我们只是通过一个基于与M重叠程度相关的函数来降低相邻检测框的分数而非彻底剔除。虽然分数被降低，但相邻的检测框仍在物体检测的序列中。图二中的实例可以说明这个问题。<br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/018_1.JPG\" alt=\"image\"><br>针对NMS存在的这个问题，我们提出了一种新的Soft-NMS算法（图三），它只需改动一行代码即可有效改进传统贪心NMS算法。在该算法中，我们基于重叠部分的大小为相邻检测框设置一个衰减函数而非彻底将其分数置为零。<strong>简单来讲，如果一个检测框与M有大部分重叠，它会有很低的分数；而如果检测框与M只有小部分重叠，那么它的原有检测分数不会受太大影响</strong>。在标准数据集PASCAL VOC 和 MS-COCO等标准数据集上，Soft-NMS对现有物体检测算法在多个重叠物体检测的平均准确率有显著的提升。同时，Soft-NMS不需要额外的训练且易于实现，因此，它很容易被集成到当前的物体检测流程中。</p>\n<p>伪代码：<br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/018_2.JPG\" alt=\"image\"></p>\n<p>公式：<br>NMS<br>$$ s_{i}=\\left{<br>\\begin{aligned}<br>s_{i}, \\ \\ \\ \\ iou(M,b_{i}) &lt;  N_{t} \\<br>0, \\ \\ \\ \\ iou(M,b_{i}) \\geq  N_{t}<br>\\end{aligned}<br>\\right.<br>$$</p>\n<p>SOFT NMS<br>$$ s_{i}=\\left{<br>\\begin{aligned}<br>s_{i}, \\ \\ \\ \\ iou(M,b_{i}) &lt;  N_{t} \\<br>1-iou(M,b_{i}), \\ \\ \\ \\ iou(M,b_{i}) \\geq  N_{t}<br>\\end{aligned}<br>\\right.<br>$$</p>\n<p>当相邻检测框与M的重叠度超过重叠阈值Nt后，检测框的检测分数呈线性衰减。在这种情况下，与M相邻很近的检测框衰减程度很大，而远离M的检测框并不受影响。</p>\n<p>但是，上述分数重置函数并不是一个连续函数，在重叠程度超过重叠阈值Nt时，该分数重置函数产生突变，从而可能导致检测结果序列产生大的变动，因此我们更希望找到一个连续的分数重置函数。它对没有重叠的检测框的原有检测分数不产生衰减，同时对高度重叠的检测框产生大的衰减。综合考虑这些因素，我们进一步对soft-NMS中的分数重置函数进行了改进：</p>\n<p>Gaussian penalty:<br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/018_3.JPG\" alt=\"image\"></p>\n<p>根据这个伪代码以及公式，实现代码：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br><span class=\"line\">107</span><br><span class=\"line\">108</span><br><span class=\"line\">109</span><br><span class=\"line\">110</span><br><span class=\"line\">111</span><br><span class=\"line\">112</span><br><span class=\"line\">113</span><br><span class=\"line\">114</span><br><span class=\"line\">115</span><br><span class=\"line\">116</span><br><span class=\"line\">117</span><br><span class=\"line\">118</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"string\">\"\"\" NMS , delete overlapping box</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">@param boxes: (n,4) box and coresspoding cordinates</span></span><br><span class=\"line\"><span class=\"string\">@param probs: (n,) box adn coresspding possibility</span></span><br><span class=\"line\"><span class=\"string\">@param overlap_thresh: treshold of delet box overlapping</span></span><br><span class=\"line\"><span class=\"string\">@param max_boxes: maximum keeping number of boxes</span></span><br><span class=\"line\"><span class=\"string\">@param method: 1 for linear soft NMS, 2 for gaussian soft NMS</span></span><br><span class=\"line\"><span class=\"string\">@param sigma: parameter of gaussian soft NMS</span></span><br><span class=\"line\"><span class=\"string\">prob_thresh: threshold of probs after soft NMS</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">@return: boxes: boxes cordinates(x1,y1,x2,y2)</span></span><br><span class=\"line\"><span class=\"string\">@return: probs: coresspoding possibility</span></span><br><span class=\"line\"><span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">soft_nms</span><span class=\"params\">(boxes, probs, overlap_thresh=<span class=\"number\">0.9</span>, max_boxes=<span class=\"number\">300</span>, method = <span class=\"number\">1</span>, sigma=<span class=\"number\">0.5</span>, prob_thresh=<span class=\"number\">0.49</span>)</span>:</span></span><br><span class=\"line\">    <span class=\"comment\"># number of input boxes</span></span><br><span class=\"line\">    N = boxes.shape[<span class=\"number\">0</span>]</span><br><span class=\"line\">    <span class=\"comment\"># if the bounding boxes integers, convert them to floats --</span></span><br><span class=\"line\">    <span class=\"comment\"># this is important since we'll be doing a bunch of divisions</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> boxes.dtype.kind == <span class=\"string\">\"i\"</span>:</span><br><span class=\"line\">        boxes = boxes.astype(<span class=\"string\">\"float\"</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># iterate all boxes</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(N):</span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"comment\"># obtain current boxes' cordinates and probs</span></span><br><span class=\"line\">        maxscore = probs[i]</span><br><span class=\"line\">        maxpos = i</span><br><span class=\"line\"></span><br><span class=\"line\">        tx1 = boxes[i,<span class=\"number\">0</span>]</span><br><span class=\"line\">        ty1 = boxes[i,<span class=\"number\">1</span>]</span><br><span class=\"line\">        tx2 = boxes[i,<span class=\"number\">2</span>]</span><br><span class=\"line\">        ty2 = boxes[i,<span class=\"number\">3</span>]</span><br><span class=\"line\">        ts = probs[i]</span><br><span class=\"line\"></span><br><span class=\"line\">        pos = i + <span class=\"number\">1</span></span><br><span class=\"line\">        <span class=\"comment\"># get max box</span></span><br><span class=\"line\">        <span class=\"keyword\">while</span> pos &lt; N:</span><br><span class=\"line\">            <span class=\"keyword\">if</span> maxscore &lt; probs[pos]:</span><br><span class=\"line\">                maxscore = probs[pos]</span><br><span class=\"line\">                maxpos = pos</span><br><span class=\"line\">            pos = pos + <span class=\"number\">1</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># add max box as a detection </span></span><br><span class=\"line\">        boxes[i,<span class=\"number\">0</span>] = boxes[maxpos,<span class=\"number\">0</span>]</span><br><span class=\"line\">        boxes[i,<span class=\"number\">1</span>] = boxes[maxpos,<span class=\"number\">1</span>]</span><br><span class=\"line\">        boxes[i,<span class=\"number\">2</span>] = boxes[maxpos,<span class=\"number\">2</span>]</span><br><span class=\"line\">        boxes[i,<span class=\"number\">3</span>] = boxes[maxpos,<span class=\"number\">3</span>]</span><br><span class=\"line\">        probs[i] = probs[maxpos]</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># swap ith box with position of max box</span></span><br><span class=\"line\">        boxes[maxpos,<span class=\"number\">0</span>] = tx1</span><br><span class=\"line\">        boxes[maxpos,<span class=\"number\">1</span>] = ty1</span><br><span class=\"line\">        boxes[maxpos,<span class=\"number\">2</span>] = tx2</span><br><span class=\"line\">        boxes[maxpos,<span class=\"number\">3</span>] = ty2</span><br><span class=\"line\">        probs[maxpos] = ts</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># cordinates of max box</span></span><br><span class=\"line\">        tx1 = boxes[i,<span class=\"number\">0</span>]</span><br><span class=\"line\">        ty1 = boxes[i,<span class=\"number\">1</span>]</span><br><span class=\"line\">        tx2 = boxes[i,<span class=\"number\">2</span>]</span><br><span class=\"line\">        ty2 = boxes[i,<span class=\"number\">3</span>]</span><br><span class=\"line\">        ts = probs[i]</span><br><span class=\"line\"></span><br><span class=\"line\">        pos = i + <span class=\"number\">1</span></span><br><span class=\"line\">        <span class=\"comment\"># NMS iterations, note that N changes if detection boxes fall below threshold</span></span><br><span class=\"line\">        <span class=\"keyword\">while</span> pos &lt; N:</span><br><span class=\"line\">            x1 = boxes[pos, <span class=\"number\">0</span>]</span><br><span class=\"line\">            y1 = boxes[pos, <span class=\"number\">1</span>]</span><br><span class=\"line\">            x2 = boxes[pos, <span class=\"number\">2</span>]</span><br><span class=\"line\">            y2 = boxes[pos, <span class=\"number\">3</span>]</span><br><span class=\"line\">            s = probs[pos]</span><br><span class=\"line\">            </span><br><span class=\"line\">            <span class=\"comment\"># calculate the areas, +1 for robatness</span></span><br><span class=\"line\">            area = (x2 - x1 + <span class=\"number\">1</span>) * (y2 - y1 + <span class=\"number\">1</span>)</span><br><span class=\"line\">            iw = (min(tx2, x2) - max(tx1, x1) + <span class=\"number\">1</span>)</span><br><span class=\"line\">            <span class=\"comment\"># # confirm left top cordinates less than top right</span></span><br><span class=\"line\">            <span class=\"keyword\">if</span> iw &gt; <span class=\"number\">0</span>:</span><br><span class=\"line\">                ih = (min(ty2, y2) - max(ty1, y1) + <span class=\"number\">1</span>)</span><br><span class=\"line\">                <span class=\"comment\"># confirm left top cordinates less than top right</span></span><br><span class=\"line\">                <span class=\"keyword\">if</span> ih &gt; <span class=\"number\">0</span>:</span><br><span class=\"line\">                    <span class=\"comment\"># find the union</span></span><br><span class=\"line\">                    ua = float((tx2 - tx1 + <span class=\"number\">1</span>) * (ty2 - ty1 + <span class=\"number\">1</span>) + area - iw * ih)</span><br><span class=\"line\">                    <span class=\"comment\">#iou between max box and detection box</span></span><br><span class=\"line\">                    ov = iw * ih / ua</span><br><span class=\"line\"></span><br><span class=\"line\">                    <span class=\"keyword\">if</span> method == <span class=\"number\">1</span>: <span class=\"comment\"># linear</span></span><br><span class=\"line\">                        <span class=\"keyword\">if</span> ov &gt; overlap_thresh: </span><br><span class=\"line\">                            weight = <span class=\"number\">1</span> - ov</span><br><span class=\"line\">                        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">                            weight = <span class=\"number\">1</span></span><br><span class=\"line\">                    <span class=\"keyword\">elif</span> method == <span class=\"number\">2</span>: <span class=\"comment\"># gaussian</span></span><br><span class=\"line\">                        weight = np.exp(-(ov * ov)/sigma)</span><br><span class=\"line\">                    <span class=\"keyword\">else</span>: <span class=\"comment\"># original NMS</span></span><br><span class=\"line\">                        <span class=\"keyword\">if</span> ov &gt; overlap_thresh: </span><br><span class=\"line\">                            weight = <span class=\"number\">0</span></span><br><span class=\"line\">                        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">                            weight = <span class=\"number\">1</span></span><br><span class=\"line\"></span><br><span class=\"line\">                    <span class=\"comment\"># obtain adjusted probs</span></span><br><span class=\"line\">                    probs[pos] = weight*probs[pos]</span><br><span class=\"line\"></span><br><span class=\"line\">   </span><br><span class=\"line\">                    <span class=\"comment\"># if box score falls below threshold, discard the box by swapping with last box</span></span><br><span class=\"line\">                    <span class=\"comment\"># update N</span></span><br><span class=\"line\">                    <span class=\"keyword\">if</span> probs[pos] &lt; prob_thresh:</span><br><span class=\"line\">                        boxes[pos,<span class=\"number\">0</span>] = boxes[N<span class=\"number\">-1</span>, <span class=\"number\">0</span>]</span><br><span class=\"line\">                        boxes[pos,<span class=\"number\">1</span>] = boxes[N<span class=\"number\">-1</span>, <span class=\"number\">1</span>]</span><br><span class=\"line\">                        boxes[pos,<span class=\"number\">2</span>] = boxes[N<span class=\"number\">-1</span>, <span class=\"number\">2</span>]</span><br><span class=\"line\">                        boxes[pos,<span class=\"number\">3</span>] = boxes[N<span class=\"number\">-1</span>, <span class=\"number\">3</span>]</span><br><span class=\"line\">                        probs[pos] = probs[N<span class=\"number\">-1</span>]</span><br><span class=\"line\">                        N = N - <span class=\"number\">1</span></span><br><span class=\"line\">                        pos = pos - <span class=\"number\">1</span></span><br><span class=\"line\"></span><br><span class=\"line\">            pos = pos + <span class=\"number\">1</span></span><br><span class=\"line\">    <span class=\"comment\"># keep is the idx of current keeping objects, front ith objectes</span></span><br><span class=\"line\">    keep = [i <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(N)]</span><br><span class=\"line\">    <span class=\"keyword\">return</span> boxes[keep], probs[keep]</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"【14-08-2018】\"><a href=\"#【14-08-2018】\" class=\"headerlink\" title=\"【14/08/2018】\"></a>【14/08/2018】</h3><h4 id=\"OVERLAPPING-OBJECT-DETECTION\"><a href=\"#OVERLAPPING-OBJECT-DETECTION\" class=\"headerlink\" title=\"OVERLAPPING OBJECT DETECTION\"></a>OVERLAPPING OBJECT DETECTION</h4><h3 id=\"【15-08-2018】\"><a href=\"#【15-08-2018】\" class=\"headerlink\" title=\"【15/08/2018】\"></a>【15/08/2018】</h3><h3 id=\"【16-08-2018】\"><a href=\"#【16-08-2018】\" class=\"headerlink\" title=\"【16/08/2018】\"></a>【16/08/2018】</h3><h3 id=\"【17-08-2018】\"><a href=\"#【17-08-2018】\" class=\"headerlink\" title=\"【17/08/2018】\"></a>【17/08/2018】</h3><h2 id=\"September\"><a href=\"#September\" class=\"headerlink\" title=\"September\"></a>September</h2>","site":{"data":{}},"excerpt":"<h2 id=\"Gantt-chart\"><a href=\"#Gantt-chart\" class=\"headerlink\" title=\"Gantt chart\"></a>Gantt chart</h2><p><img src=\"https://raw.githubusercontent.com/Trouble404/Object-Detection-System-Based-on-CNN-and-Capsule-Network/master/Project%20Plan/gantt%20chart%20of%20project.PNG\" alt=\"image\"></p>","more":"<hr>\n<h2 id=\"Check-list\"><a href=\"#Check-list\" class=\"headerlink\" title=\"Check list\"></a>Check list</h2><ul>\n<li style=\"list-style: none\"><input type=\"checkbox\" checked> <strong>1) preparation</strong></li>\n<li style=\"list-style: none\"><input type=\"checkbox\" checked> <strong><em>1.1) Familiarization with develop tools</em></strong></li>\n<li style=\"list-style: none\"><input type=\"checkbox\" checked> 1.1.1) Keras</li>\n<li style=\"list-style: none\"><input type=\"checkbox\" checked> 1.1.2) Pythrch</li>\n<li style=\"list-style: none\"><input type=\"checkbox\" checked> <strong><em>1.2) Presentation</em></strong></li>\n<li style=\"list-style: none\"><input type=\"checkbox\" checked> 1.2.1) Poster conference</li>\n<li style=\"list-style: none\"><input type=\"checkbox\" checked> <strong>2) Create image database</strong></li>\n<li style=\"list-style: none\"><input type=\"checkbox\" checked> 2.1) Confirmation of detected objects</li>\n<li style=\"list-style: none\"><input type=\"checkbox\" checked> 2.2) Collect and generate the dataset</li>\n<li style=\"list-style: none\"><input type=\"checkbox\" checked> <strong>3) Familiarization with CNN based object detection methods</strong></li>\n<li style=\"list-style: none\"><input type=\"checkbox\" checked> 3.1) R-CNN</li>\n<li style=\"list-style: none\"><input type=\"checkbox\" checked> 3.2) SPP-net</li>\n<li style=\"list-style: none\"><input type=\"checkbox\" checked> 3.3) Fast R-CNN</li>\n<li style=\"list-style: none\"><input type=\"checkbox\" checked> 3.4) Faster R-CNN</li>\n<li style=\"list-style: none\"><input type=\"checkbox\" checked> <strong>4) Implement object detection system based on one chosen CNN method</strong></li>\n<li style=\"list-style: none\"><input type=\"checkbox\" checked> 4.1) Pre-processing of images</li>\n<li style=\"list-style: none\"><input type=\"checkbox\" checked> 4.2) Extracting features</li>\n<li style=\"list-style: none\"><input type=\"checkbox\" checked> 4.3) Mode architecture</li>\n<li style=\"list-style: none\"><input type=\"checkbox\" checked> 4.4) Train model and optimization</li>\n<li style=\"list-style: none\"><input type=\"checkbox\" checked> 4.5) Models ensemble</li>\n<li style=\"list-style: none\"><input type=\"checkbox\" checked> <strong>5) Analysis work</strong></li>\n<li style=\"list-style: none\"><input type=\"checkbox\" checked> 5.1) Evaluation of detection result.</li>\n<li style=\"list-style: none\"><input type=\"checkbox\" checked> <strong>6) Paperwork and bench inspection</strong></li>\n<li style=\"list-style: none\"><input type=\"checkbox\" checked> 6.1) Logbook</li>\n<li style=\"list-style: none\"><input type=\"checkbox\" checked> 6.2) Write the thesis</li>\n<li style=\"list-style: none\"><input type=\"checkbox\" checked> 6.3) Project video</li>\n<li style=\"list-style: none\"><input type=\"checkbox\" checked> 6.4) Speech and ppt of bench inspection</li>\n<li style=\"list-style: none\"><input type=\"checkbox\" checked> <strong>7) Documents</strong></li>\n<li style=\"list-style: none\"><input type=\"checkbox\" checked> 7.1) Project Brief</li>\n</ul>\n<hr>\n<h2 id=\"May\"><a href=\"#May\" class=\"headerlink\" title=\"May\"></a>May</h2><h3 id=\"【28-05-2018】\"><a href=\"#【28-05-2018】\" class=\"headerlink\" title=\"【28/05/2018】\"></a>【28/05/2018】</h3><p>Keras is a high-level neural networks API, written in Python and capable of running on top of <a href=\"https://github.com/tensorflow/tensorflow\" target=\"_blank\" rel=\"noopener\">TensorFlow</a>, CNTK, or Theano.</p>\n<ul>\n<li><p><strong><a href=\"https://keras.io/\" target=\"_blank\" rel=\"noopener\">Keras document</a></strong></p>\n</li>\n<li><p><strong><a href=\"https://keras-cn.readthedocs.io/en/latest/#keraspython\" target=\"_blank\" rel=\"noopener\">Keras 文档</a></strong></p>\n</li>\n</ul>\n<hr>\n<h4 id=\"Installation\"><a href=\"#Installation\" class=\"headerlink\" title=\"Installation\"></a>Installation</h4><ul>\n<li><p><strong>TensorFlow</strong><br><a href=\"https://www.visualstudio.com/zh-hans/vs/older-downloads/\" target=\"_blank\" rel=\"noopener\">Microsoft Visual Studio 2015</a><br><a href=\"http://docs.nvidia.com/cuda/cuda-installation-guide-microsoft-windows/\" target=\"_blank\" rel=\"noopener\">CUDA 9.0</a><br><a href=\"https://developer.nvidia.com/cudnn\" target=\"_blank\" rel=\"noopener\">cuDNN7</a><br><a href=\"https://www.anaconda.com/download/\" target=\"_blank\" rel=\"noopener\">Anaconda</a></p>\n<ul>\n<li>Step 1: Install VS2015</li>\n<li>Step 2: Install CUDA 9.0 并添加环境变量</li>\n<li>Step 3: Install cuDNN7 解压后把cudnn目录下的bin目录加到PATH环境变量里</li>\n<li>Step 4: Install Anaconda 把安装路径添加到PATH里去, 在这里我用了 <strong>Python 3.5</strong></li>\n<li>Step 5: 使用Anaconda的命令行 新建一个虚拟环境,激活并且关联到jupyterbook<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">conda create  --name tensorflow python=3.5</span><br><span class=\"line\">activate tensorflow</span><br><span class=\"line\">conda install nb_conda</span><br></pre></td></tr></table></figure></li>\n<li>Step 6: Install GPU version TensorFlow.<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">pip install -i https://pypi.tuna.tsinghua.edu.cn/simple --ignore-installed --upgrade tensorflow-gpu </span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n</li>\n<li><p><strong>Keras</strong></p>\n<ul>\n<li>Step 1: 启动之前的 虚拟环境， 并且安装Keras GPU 版本<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">activate tensorflow</span><br><span class=\"line\">pip install keras -U --pre</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n</li>\n</ul>\n<h4 id=\"在硕士学习过程中，使用Keras的项目\"><a href=\"#在硕士学习过程中，使用Keras的项目\" class=\"headerlink\" title=\"在硕士学习过程中，使用Keras的项目**\"></a>在硕士学习过程中，使用Keras的项目**</h4><ul>\n<li><strong><a href=\"https://github.com/Trouble404/NBA-with-Machine-Learning\" target=\"_blank\" rel=\"noopener\">NBA with Machine Learning</a></strong></li>\n<li><strong><a href=\"https://github.com/Trouble404/kaggle-Job-Salary-Prediction\" target=\"_blank\" rel=\"noopener\">Kaggle- Job salary prediction</a></strong></li>\n</ul>\n<h4 id=\"TensorFlow-CPU-切换\"><a href=\"#TensorFlow-CPU-切换\" class=\"headerlink\" title=\"TensorFlow CPU 切换\"></a>TensorFlow CPU 切换</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> tensorflow <span class=\"keyword\">as</span> tf  </span><br><span class=\"line\"><span class=\"keyword\">import</span> os</span><br><span class=\"line\"><span class=\"keyword\">import</span> keras.backend.tensorflow_backend <span class=\"keyword\">as</span> KTF  </span><br><span class=\"line\"></span><br><span class=\"line\">os.environ[<span class=\"string\">\"CUDA_VISIBLE_DEVICES\"</span>] = <span class=\"string\">\"0\"</span>  <span class=\"comment\">#设置需要使用的GPU的编号</span></span><br><span class=\"line\">config = tf.ConfigProto()</span><br><span class=\"line\">config.gpu_options.per_process_gpu_memory_fraction = <span class=\"number\">0.4</span> <span class=\"comment\">#设置使用GPU容量占GPU总容量的比例</span></span><br><span class=\"line\">sess = tf.Session(config=config)</span><br><span class=\"line\">KTF.set_session(sess)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">with</span> tf.device(<span class=\"string\">'/cpu:0'</span>):</span><br></pre></td></tr></table></figure>\n<p>这样可以在GPU版本的虚拟环境里面使用CPU计算</p>\n<h4 id=\"Jupyter-Notebook-工作目录设置\"><a href=\"#Jupyter-Notebook-工作目录设置\" class=\"headerlink\" title=\"Jupyter Notebook 工作目录设置\"></a>Jupyter Notebook 工作目录设置</h4><p>启动命令行，切换至预设的工作目录， 运行：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">jupyter notebook --generate-config</span><br></pre></td></tr></table></figure></p>\n<p><img src=\"https://raw.githubusercontent.com/Trouble404/Object-Detection-System-Based-on-CNN-and-Capsule-Network/master/Learned%20skills/jupyter%20_setting.PNG\" alt=\"image\"></p>\n<h2 id=\"June\"><a href=\"#June\" class=\"headerlink\" title=\"June\"></a>June</h2><h3 id=\"【01-06-2018】\"><a href=\"#【01-06-2018】\" class=\"headerlink\" title=\"【01/06/2018】\"></a>【01/06/2018】</h3><p><strong><a href=\"https://pytorch.org/about/\" target=\"_blank\" rel=\"noopener\">PyTorch</a></strong> is a python package that provides two high-level features:</p>\n<ul>\n<li>Tensor computation (like numpy) with strong GPU acceleration</li>\n<li>Deep Neural Networks built on a tape-based autodiff system</li>\n</ul>\n<table>\n<thead>\n<tr>\n<th style=\"text-align:left\">Package</th>\n<th style=\"text-align:left\">Description</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:left\">torch</td>\n<td style=\"text-align:left\">a Tensor library like NumPy, with strong GPU support</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">torch.autograd</td>\n<td style=\"text-align:left\">a tape based automatic differentiation library that supports all differentiable Tensor operations in torch</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">torch.nn</td>\n<td style=\"text-align:left\">a neural networks library deeply integrated with autograd designed for maximum flexibility</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">torch.optim</td>\n<td style=\"text-align:left\">an optimization package to be used with torch.nn with standard optimization methods such as SGD, RMSProp, LBFGS, Adam etc.</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">torch.multiprocessing</td>\n<td style=\"text-align:left\">python multiprocessing, but with magical memory sharing of torch Tensors across processes. Useful for data loading and hogwild training.</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">torch.utils</td>\n<td style=\"text-align:left\">DataLoader, Trainer and other utility functions for convenience</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">torch.legacy(.nn/.optim)</td>\n<td style=\"text-align:left\">legacy code that has been ported over from torch for backward compatibility reasons</td>\n</tr>\n</tbody>\n</table>\n<hr>\n<h4 id=\"Installation-1\"><a href=\"#Installation-1\" class=\"headerlink\" title=\"Installation\"></a>Installation</h4><ul>\n<li>Step 1: 使用Anaconda的命令行 新建一个虚拟环境,激活并且关联到jupyterbook<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">conda create  --name pytorch python=3.5</span><br><span class=\"line\">activate pytorch</span><br><span class=\"line\">conda install nb_conda</span><br></pre></td></tr></table></figure></li>\n<li>Step 2: Install GPU version PyTorch.<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">conda install pytorch cuda90 -c pytorch </span><br><span class=\"line\">pip install torchvision</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<h4 id=\"Understanding-of-PyTorch\"><a href=\"#Understanding-of-PyTorch\" class=\"headerlink\" title=\"Understanding of PyTorch\"></a>Understanding of PyTorch</h4><ul>\n<li><p><strong>Tensors</strong><br>Tensors和numpy中的ndarrays较为相似, 与此同时Tensor也能够使用GPU来加速运算</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> __future__ <span class=\"keyword\">import</span> print_function</span><br><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\">x = torch.Tensor(<span class=\"number\">5</span>, <span class=\"number\">3</span>)  <span class=\"comment\"># 构造一个未初始化的5*3的矩阵</span></span><br><span class=\"line\">x = torch.rand(<span class=\"number\">5</span>, <span class=\"number\">3</span>)  <span class=\"comment\"># 构造一个随机初始化的矩阵</span></span><br><span class=\"line\">x <span class=\"comment\"># 此处在notebook中输出x的值来查看具体的x内容</span></span><br><span class=\"line\">x.size()</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#<span class=\"doctag\">NOTE:</span> torch.Size 事实上是一个tuple, 所以其支持相关的操作*</span></span><br><span class=\"line\">y = torch.rand(<span class=\"number\">5</span>, <span class=\"number\">3</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#此处 将两个同形矩阵相加有两种语法结构</span></span><br><span class=\"line\">x + y <span class=\"comment\"># 语法一</span></span><br><span class=\"line\">torch.add(x, y) <span class=\"comment\"># 语法二</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 另外输出tensor也有两种写法</span></span><br><span class=\"line\">result = torch.Tensor(<span class=\"number\">5</span>, <span class=\"number\">3</span>) <span class=\"comment\"># 语法一</span></span><br><span class=\"line\">torch.add(x, y, out=result) <span class=\"comment\"># 语法二</span></span><br><span class=\"line\">y.add_(x) <span class=\"comment\"># 将y与x相加</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 特别注明：任何可以改变tensor内容的操作都会在方法名后加一个下划线'_'</span></span><br><span class=\"line\"><span class=\"comment\"># 例如：x.copy_(y), x.t_(), 这俩都会改变x的值。</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#另外python中的切片操作也是资次的。</span></span><br><span class=\"line\">x[:,<span class=\"number\">1</span>] <span class=\"comment\">#这一操作会输出x矩阵的第二列的所有值</span></span><br></pre></td></tr></table></figure>\n</li>\n<li><p><strong>Numpy桥</strong><br>将Torch的Tensor和numpy的array相互转换简，注意Torch的Tensor和numpy的array会共享他们的存储空间，修改一个会导致另外的一个也被修改。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 此处演示tensor和numpy数据结构的相互转换</span></span><br><span class=\"line\">a = torch.ones(<span class=\"number\">5</span>)</span><br><span class=\"line\">b = a.numpy()</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 此处演示当修改numpy数组之后,与之相关联的tensor也会相应的被修改</span></span><br><span class=\"line\">a.add_(<span class=\"number\">1</span>)</span><br><span class=\"line\">print(a)</span><br><span class=\"line\">print(b)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 将numpy的Array转换为torch的Tensor</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\">a = np.ones(<span class=\"number\">5</span>)</span><br><span class=\"line\">b = torch.from_numpy(a)</span><br><span class=\"line\">np.add(a, <span class=\"number\">1</span>, out=a)</span><br><span class=\"line\">print(a)</span><br><span class=\"line\">print(b)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 另外除了CharTensor之外，所有的tensor都可以在CPU运算和GPU预算之间相互转换</span></span><br><span class=\"line\"><span class=\"comment\"># 使用CUDA函数来将Tensor移动到GPU上</span></span><br><span class=\"line\"><span class=\"comment\"># 当CUDA可用时会进行GPU的运算</span></span><br><span class=\"line\"><span class=\"keyword\">if</span> torch.cuda.is_available():</span><br><span class=\"line\">    x = x.cuda()</span><br><span class=\"line\">    y = y.cuda()</span><br></pre></td></tr></table></figure>\n</li>\n<li><p><strong>使用PyTorch设计一个CIFAR10数据集的分类模型</strong><br><strong><a href=\"https://github.com/Trouble404/Object-Detection-System-Based-on-CNN-and-Capsule-Network/blob/master/Learned%20skills/pytorch.ipynb\" target=\"_blank\" rel=\"noopener\">code</a></strong></p>\n</li>\n<li><p><strong>MMdnn</strong><br>MMdnn is a set of tools to help users inter-operate among different deep learning frameworks. E.g. model conversion and visualization. Convert models between Caffe, Keras, MXNet, Tensorflow, CNTK, PyTorch Onnx and CoreML.</p>\n<p><img src=\"https://raw.githubusercontent.com/Microsoft/MMdnn/master/docs/supported.jpg\" alt=\"iamge\"></p>\n<p>MMdnn主要有以下特征：</p>\n<ul>\n<li>模型文件转换器，不同的框架间转换DNN模型</li>\n<li>模型代码片段生成器，生成适合不同框架的代码</li>\n<li>模型可视化，DNN网络结构和框架参数可视化</li>\n<li>模型兼容性测试（正在进行中）</li>\n</ul>\n<p><strong><a href=\"https://github.com/Microsoft/MMdnn\" target=\"_blank\" rel=\"noopener\">Github</a></strong></p>\n</li>\n</ul>\n<h3 id=\"【04-06-2018】\"><a href=\"#【04-06-2018】\" class=\"headerlink\" title=\"【04/06/2018】\"></a>【04/06/2018】</h3><h4 id=\"Dataset\"><a href=\"#Dataset\" class=\"headerlink\" title=\"Dataset:\"></a><strong>Dataset:</strong></h4><p> <strong><a href=\"http://host.robots.ox.ac.uk/pascal/VOC/voc2012/index.html\" target=\"_blank\" rel=\"noopener\">VOC 2012 Dataset</a></strong></p>\n<h4 id=\"Introduce\"><a href=\"#Introduce\" class=\"headerlink\" title=\"Introduce:\"></a><strong>Introduce:</strong></h4><p> <strong>Visual Object Classes Challenge 2012 (VOC2012)</strong><br><a href=\"http://host.robots.ox.ac.uk/pascal/VOC/\" target=\"_blank\" rel=\"noopener\">PASCAL</a>‘s full name is Pattern Analysis, Statistical Modelling and Computational Learning.<br>VOC’s full name is <strong>Visual OBject Classes</strong>.<br>The first competition was held in 2005 and terminated in 2012. I will use the last updated dataset which is <a href=\"http://host.robots.ox.ac.uk/pascal/VOC/voc2012/index.html\" target=\"_blank\" rel=\"noopener\">VOC2012</a> dataset.</p>\n<p>The main aim of this competition is object detection, there are 20 classes objects in the dataset:</p>\n<ul>\n<li>person</li>\n<li>bird, cat, cow, dog, horse, sheep</li>\n<li>aeroplane, bicycle, boat, bus, car, motorbike, train</li>\n<li>bottle, chair, dining table, potted plant, sofa, tv/monitor</li>\n</ul>\n<h4 id=\"Detection-Task\"><a href=\"#Detection-Task\" class=\"headerlink\" title=\"Detection Task\"></a><strong>Detection Task</strong></h4><p>Referenced:<br><strong>The PASCAL Visual Object Classes Challenge 2012 (VOC2012) Development Kit</strong><br><strong>Mark Everingham - John Winn</strong><br><a href=\"http://host.robots.ox.ac.uk/pascal/VOC/voc2012/htmldoc/index.html\" target=\"_blank\" rel=\"noopener\">http://host.robots.ox.ac.uk/pascal/VOC/voc2012/htmldoc/index.html</a></p>\n<p><strong>Task:</strong><br>For each of the twenty classes predict the bounding boxes of each object of that class in a test image (if any). Each bounding box should be output with an associated real-valued confidence of the detection so that a precision/recall curve can be drawn. Participants may choose to tackle all, or any subset of object classes, for example ‘cars only’ or ‘motorbikes and cars’.</p>\n<p><strong>Competitions</strong>:<br>Two competitions are defined according to the choice of training data:</p>\n<ul>\n<li>taken from the $VOC_{trainval}$ data provided.</li>\n<li>from any source excluding the $VOC_{test}$ data provided.</li>\n</ul>\n<p><strong>Submission of Results</strong>:<br>A separate text file of results should be generated for each competition and each class e.g. `car’. Each line should be a detection output by the detector in the following format:<br>    <figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;image identifier&gt; &lt;confidence&gt; &lt;left&gt; &lt;top&gt; &lt;right&gt; &lt;bottom&gt;</span><br></pre></td></tr></table></figure></p>\n<p>where (left,top)-(right,bottom) defines the bounding box of the detected object. The top-left pixel in the image has coordinates $(1,1)$. Greater confidence values signify greater confidence that the detection is correct. An example file excerpt is shown below. Note that for the image 2009_000032, multiple objects are detected:<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">comp3_det_test_car.txt:</span><br><span class=\"line\">    ...</span><br><span class=\"line\">    2009_000026 0.949297 172.000000 233.000000 191.000000 248.000000</span><br><span class=\"line\">    2009_000032 0.013737 1.000000 147.000000 114.000000 242.000000</span><br><span class=\"line\">    2009_000032 0.013737 1.000000 134.000000 94.000000 168.000000</span><br><span class=\"line\">    2009_000035 0.063948 455.000000 229.000000 491.000000 243.000000</span><br><span class=\"line\">    ...</span><br></pre></td></tr></table></figure></p>\n<p><strong>Evaluation</strong>:<br>The detection task will be judged by the precision/recall curve. The principal quantitative measure used will be the average precision (AP). Detections are considered true or false positives based on the area of overlap with ground truth bounding boxes. To be considered a correct detection, the area of overlap $a_o$ between the predicted bounding box $B_p$ and ground truth bounding box $B_{gt}$ must exceed $50\\%$ by the formula: </p>\n<center> $a_o = \\frac{area(B_p \\cap B_{gt})}{area(B_p \\cup B_{gt})}$ </center>\n\n<h4 id=\"XML标注格式\"><a href=\"#XML标注格式\" class=\"headerlink\" title=\"XML标注格式\"></a><strong>XML标注格式</strong></h4><p> 对于目标检测来说，每一张图片对应一个xml格式的标注文件。所以你会猜到，就像gemfield准备的训练集有8万张照片一样，在存放xml文件的目录里，这里也将会有8万个xml文件。下面是其中一个xml文件的示例：<br> <figure class=\"highlight html\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"> &lt;?xml version=\"1.0\" encoding=\"utf-8\"?&gt;</span><br><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">annotation</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">folder</span>&gt;</span>VOC2007<span class=\"tag\">&lt;/<span class=\"name\">folder</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">filename</span>&gt;</span>test100.mp4_3380.jpeg<span class=\"tag\">&lt;/<span class=\"name\">filename</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">size</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">width</span>&gt;</span>1280<span class=\"tag\">&lt;/<span class=\"name\">width</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">height</span>&gt;</span>720<span class=\"tag\">&lt;/<span class=\"name\">height</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">depth</span>&gt;</span>3<span class=\"tag\">&lt;/<span class=\"name\">depth</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;/<span class=\"name\">size</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">object</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>gemfield<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">bndbox</span>&gt;</span></span><br><span class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">xmin</span>&gt;</span>549<span class=\"tag\">&lt;/<span class=\"name\">xmin</span>&gt;</span></span><br><span class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">xmax</span>&gt;</span>715<span class=\"tag\">&lt;/<span class=\"name\">xmax</span>&gt;</span></span><br><span class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">ymin</span>&gt;</span>257<span class=\"tag\">&lt;/<span class=\"name\">ymin</span>&gt;</span></span><br><span class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">ymax</span>&gt;</span>289<span class=\"tag\">&lt;/<span class=\"name\">ymax</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;/<span class=\"name\">bndbox</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">truncated</span>&gt;</span>0<span class=\"tag\">&lt;/<span class=\"name\">truncated</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">difficult</span>&gt;</span>0<span class=\"tag\">&lt;/<span class=\"name\">difficult</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;/<span class=\"name\">object</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">object</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>civilnet<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">bndbox</span>&gt;</span></span><br><span class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">xmin</span>&gt;</span>842<span class=\"tag\">&lt;/<span class=\"name\">xmin</span>&gt;</span></span><br><span class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">xmax</span>&gt;</span>1009<span class=\"tag\">&lt;/<span class=\"name\">xmax</span>&gt;</span></span><br><span class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">ymin</span>&gt;</span>138<span class=\"tag\">&lt;/<span class=\"name\">ymin</span>&gt;</span></span><br><span class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">ymax</span>&gt;</span>171<span class=\"tag\">&lt;/<span class=\"name\">ymax</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;/<span class=\"name\">bndbox</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">truncated</span>&gt;</span>0<span class=\"tag\">&lt;/<span class=\"name\">truncated</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">difficult</span>&gt;</span>0<span class=\"tag\">&lt;/<span class=\"name\">difficult</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;/<span class=\"name\">object</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">segmented</span>&gt;</span>0<span class=\"tag\">&lt;/<span class=\"name\">segmented</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">annotation</span>&gt;</span></span><br></pre></td></tr></table></figure></p>\n<p>在这个测试图片上，我们标注了2个object，一个是gemfield，另一个是civilnet。</p>\n<p>在这个xml例子中：</p>\n<ul>\n<li>bndbox是一个轴对齐的矩形，它框住的是目标在照片中的可见部分；</li>\n<li>truncated表明这个目标因为各种原因没有被框完整（被截断了），比如说一辆车有一部分在画面外；</li>\n<li>occluded是说一个目标的重要部分被遮挡了（不管是被背景的什么东西，还是被另一个待检测目标遮挡）；</li>\n<li>difficult表明这个待检测目标很难识别，有可能是虽然视觉上很清楚，但是没有上下文的话还是很难确认它属于哪个分类；标为difficult的目标在测试成绩的评估中一般会被忽略。</li>\n</ul>\n<p><strong>注意：在一个object中，name 标签要放在前面，否则的话，目标检测的一个重要工程实现SSD会出现解析数据集错误（另一个重要工程实现py-faster-rcnn则不会）。</strong></p>\n<h3 id=\"【07-06-2018】\"><a href=\"#【07-06-2018】\" class=\"headerlink\" title=\"【07/06/2018】\"></a>【07/06/2018】</h3><h4 id=\"Poster-conference\"><a href=\"#Poster-conference\" class=\"headerlink\" title=\"Poster conference\"></a><strong>Poster conference</strong></h4><p><img src=\"https://raw.githubusercontent.com/Trouble404/Object-Detection-System-Based-on-CNN-and-Capsule-Network/master/Poster/poster.png\" alt=\"iamge\"></p>\n<p>5 People in one group to present their object.<br>I present this object to my supervisor in this conference.</p>\n<h3 id=\"【11-06-2018】\"><a href=\"#【11-06-2018】\" class=\"headerlink\" title=\"【11/06/2018】\"></a>【11/06/2018】</h3><h4 id=\"R-CNN\"><a href=\"#R-CNN\" class=\"headerlink\" title=\"R-CNN\"></a><strong>R-CNN</strong></h4><p>Paper: <a href=\"https://arxiv.org/abs/1311.2524\" target=\"_blank\" rel=\"noopener\">Rich feature hierarchies for accurate object detection and semantic segmentation</a></p>\n<p>【<strong>论文主要特点</strong>】（相对传统方法的改进）</p>\n<ul>\n<li>速度： 经典的目标检测算法使用滑动窗法依次判断所有可能的区域。本文则(采用Selective Search方法)预先提取一系列较可能是物体的候选区域，之后仅在这些候选区域上(采用CNN)提取特征，进行判断。</li>\n<li>训练集： 经典的目标检测算法在区域中提取人工设定的特征。本文则采用深度网络进行特征提取。使用两个数据库： 一个较大的识别库   （ImageNet ILSVC 2012）：标定每张图片中物体的类别。一千万图像，1000类。 一个较小的检测库（PASCAL VOC 2007）：标定每张   图片中，物体的类别和位置，一万图像，20类。 本文使用识别库进行预训练得到CNN（有监督预训练），而后用检测库调优参数，最后在   检测库上评测。</li>\n</ul>\n<p>【<strong>流程</strong>】</p>\n<ol>\n<li>候选区域生成： 一张图像生成1K~2K个候选区域 （采用Selective Search 方法）</li>\n<li>特征提取： 对每个候选区域，使用深度卷积网络提取特征 （CNN） </li>\n<li>类别判断： 特征送入每一类的SVM 分类器，判别是否属于该类</li>\n<li>位置精修： 使用回归器精细修正候选框位置<center><img src=\"https://raw.githubusercontent.com/Trouble404/Object-Detection-System-Based-on-CNN-and-Capsule-Network/master/Research%20Review/LaTex/1.PNG\" alt=\"image\"></center>\n\n</li>\n</ol>\n<p>【<strong><a href=\"https://www.koen.me/research/pub/uijlings-ijcv2013-draft.pdf\" target=\"_blank\" rel=\"noopener\">Selective Search</a></strong>】</p>\n<ol>\n<li>使用一种过分割手段，将图像分割成小区域 (1k~2k 个)</li>\n<li>查看现有小区域，按照合并规则合并可能性最高的相邻两个区域。重复直到整张图像合并成一个区域位置</li>\n<li>输出所有曾经存在过的区域，所谓候选区域<br>其中合并规则如下： 优先合并以下四种区域：<ul>\n<li>颜色（颜色直方图）相近的</li>\n<li>纹理（梯度直方图）相近的</li>\n<li>合并后总面积小的： 保证合并操作的尺度较为均匀，避免一个大区域陆续“吃掉”其他小区域 （例：设有区域a-b-c-d-e-f-g-h。较好的合并方式是：ab-cd-ef-gh -&gt; abcd-efgh -&gt; abcdefgh。 不好的合并方法是：ab-c-d-e-f-g-h -&gt;abcd-e-f-g-h -&gt;abcdef-gh -&gt; abcdefgh）</li>\n<li>合并后，总面积在其BBOX中所占比例大的： 保证合并后形状规则。</li>\n</ul>\n</li>\n</ol>\n<h3 id=\"【12-06-2018】\"><a href=\"#【12-06-2018】\" class=\"headerlink\" title=\"【12/06/2018】\"></a>【12/06/2018】</h3><h4 id=\"SPP-CNN\"><a href=\"#SPP-CNN\" class=\"headerlink\" title=\"SPP-CNN\"></a><strong>SPP-CNN</strong></h4><p>Paper: <a href=\"https://arxiv.org/abs/1406.4729\" target=\"_blank\" rel=\"noopener\">Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition</a></p>\n<p>【<strong>论文主要特点</strong>】（相对传统方法的改进）</p>\n<p>RCNN使用CNN作为特征提取器，首次使得目标检测跨入深度学习的阶段。但是RCNN对于每一个区域候选都需要首先将图片放缩到固定的尺寸（224*224），然后为每个区域候选提取CNN特征。容易看出这里面存在的一些性能瓶颈：</p>\n<ul>\n<li>速度瓶颈：重复为每个region proposal提取特征是极其费时的，Selective Search对于每幅图片产生2K左右个region proposal，也就是意味着一幅图片需要经过2K次的完整的CNN计算得到最终的结果。</li>\n<li>性能瓶颈：对于所有的region proposal防缩到固定的尺寸会导致我们不期望看到的几何形变，而且由于速度瓶颈的存在，不可能采用多尺度或者是大量的数据增强去训练模型。</li>\n</ul>\n<p>【<strong>流程</strong>】</p>\n<ol>\n<li>首先通过selective search产生一系列的region proposal</li>\n<li>然后训练多尺寸识别网络用以提取区域特征，其中处理方法是每个尺寸的最短边大小在尺寸集合中：<br>$s \\in S = {480,576,688,864,1200}$<br>训练的时候通过上面提到的多尺寸训练方法，也就是在每个epoch中首先训练一个尺寸产生一个model，然后加载这个model并训练第二个尺寸，直到训练完所有的尺寸。空间金字塔池化使用的尺度为：1*1，2*2，3*3，6*6，一共是50个bins。</li>\n<li>在测试时，每个region proposal选择能使其包含的像素个数最接近224*224的尺寸，提取相 应特征。</li>\n<li>训练SVM，BoundingBox回归.<center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/SPP-NET.jpg\" alt=\"image\"></center>\n\n\n</li>\n</ol>\n<h3 id=\"【13-06-2018】\"><a href=\"#【13-06-2018】\" class=\"headerlink\" title=\"【13/06/2018】\"></a>【13/06/2018】</h3><h4 id=\"FAST-R-CNN\"><a href=\"#FAST-R-CNN\" class=\"headerlink\" title=\"FAST R-CNN\"></a><strong>FAST R-CNN</strong></h4><p>Paper: <a href=\"https://arxiv.org/abs/1504.08083\" target=\"_blank\" rel=\"noopener\">Fast R-CNN</a></p>\n<p>【<strong>论文主要特点</strong>】（相对传统方法的改进）</p>\n<ul>\n<li>测试时速度慢：RCNN一张图像内候选框之间大量重叠，提取特征操作冗余。本文将整张图像归一化后直接送入深度网络。在邻接时，才加入候选框信息，在末尾的少数几层处理每个候选框。</li>\n<li>训练时速度慢 ：原因同上。在训练时，本文先一张图像送入网络，紧接着送入从这幅图像上提取出的候选区域。这些候选区域的前几层特征不需要再重复计算。</li>\n<li>训练所需空间大: RCNN中独立的分类器和回归器需要大量特征作为训练样本。本文把类别判断和位置精调统一用深度网络实现，不再需要额外存储。</li>\n</ul>\n<p>【<strong>流程</strong>】</p>\n<ol>\n<li>网络首先用几个卷积层（conv）和最大池化层处理整个图像以产生conv特征图。</li>\n<li>然后，对于每个对象建议框（object proposals ），感兴趣区域（region of interest——RoI）池层从特征图提取固定长度的特征向量。</li>\n<li>每个特征向量被输送到分支成两个同级输出层的全连接（fc）层序列中：<br>其中一层进行分类，对 目标关于K个对象类（包括全部“背景background”类）产生softmax概率估计，即输出每一个RoI的概率分布；<br>另一层进行bbox regression，输出K个对象类中每一个类的四个实数值。每4个值编码K个类中的每个类的精确边界盒（bounding-box）位置，即输出每一个种类的的边界盒回归偏差。整个结构是使用多任务损失的端到端训练（trained end-to-end with a multi-task loss）。<center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/fast-rcnn.png\" alt=\"image\"></center>\n\n</li>\n</ol>\n<h3 id=\"【14-18-06-2018】\"><a href=\"#【14-18-06-2018】\" class=\"headerlink\" title=\"【14~18/06/2018】\"></a>【14~18/06/2018】</h3><h4 id=\"FASTER-R-CNN\"><a href=\"#FASTER-R-CNN\" class=\"headerlink\" title=\"FASTER R-CNN\"></a><strong>FASTER R-CNN</strong></h4><p>I want to use <strong>Faster R-cnn</strong> as the first method to implement object detection system.</p>\n<p>Paper: <a href=\"https://arxiv.org/abs/1506.01497\" target=\"_blank\" rel=\"noopener\">Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks</a></p>\n<p>在结构上，Faster RCNN已经将特征抽取(feature extraction)，proposal提取，bounding box regression(rect refine)，classification都整合在了一个网络中，使得综合性能有较大提高，在检测速度方面尤为明显。</p>\n<center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster-rcnn.jpg\" alt=\"image\"></center>\n\n<h4 id=\"流程\"><a href=\"#流程\" class=\"headerlink\" title=\"流程\"></a>流程</h4><ol>\n<li>Conv layers：作为一种CNN网络目标检测方法，Faster R-CNN首先使用一组基础的conv+relu+pooling层提取image的feature maps。该feature maps被共享用于后续RPN层和全连接层。</li>\n<li>Region Proposal Networks：RPN网络用于生成region proposals。该层通过softmax判断anchors属于foreground或者background，再利用bounding box regression修正anchors获得精确的proposals。</li>\n<li>Roi Pooling：该层收集输入的feature maps和proposals，综合这些信息后提取proposal feature maps，送入后续全连接层判定目标类别。</li>\n<li>Classification：利用proposal feature maps计算proposal的类别，同时再次bounding box regression获得检测框最终的精确位置。</li>\n</ol>\n<h4 id=\"解释\"><a href=\"#解释\" class=\"headerlink\" title=\"解释\"></a>解释</h4><p><strong>[1. Conv layers]</strong><br>   <center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_1.jpg\" alt=\"image\"></center><br>   Conv layers包含了conv，pooling，relu三种层。以python版本中的VGG16模型中的faster_rcnn_test.pt的网络结构为例，如图,    Conv layers部分共有13个conv层，13个relu层，4个pooling层。这里有一个非常容易被忽略但是又无比重要的信息，在Conv          layers中：</p>\n<ul>\n<li>所有的conv层都是： $kernel_size=3$ ， $pad=1$ ， $stride=1$ <br></li>\n<li><p>所有的pooling层都是： $kernel_size=2$ ， $pad=0$ ， $stride=2$</p>\n<p>为何重要？在Faster RCNN Conv layers中对所有的卷积都做了扩边处理（ $pad=1$ ，即填充一圈0），导致原图变为                $(M+2)\\times (N+2)$ 大小，再做3x3卷积后输出 $M\\times N$ 。正是这种设置，导致Conv layers中的conv层不改变输入和输出    矩阵大小。如下图：<br><center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_2.jpg\" alt=\"image\"></center><br>类似的是，Conv layers中的pooling层 $kernel_size=2$ ， $stride=2$ 。这样每个经过pooling层的 $M\\times N$ 矩阵，都会变为 $(M/2) \\times(N/2)$ 大小。综上所述，在整个Conv layers中，conv和relu层不改变输入输出大小，只有pooling层使输出长宽都变为输入的1/2。<br>那么，一个 $M\\times N$ 大小的矩阵经过Conv layers固定变为 $(M/16)\\times (N/16)$ ！这样Conv layers生成的featuure map中都可以和原图对应起来。</p>\n</li>\n</ul>\n<p><strong>[2. Region Proposal Networks(RPN)]</strong><br>   经典的检测方法生成检测框都非常耗时，如OpenCV adaboost使用滑动窗口+图像金字塔生成检测框；或如R-CNN使用SS(Selective      Search)方法生成检测框。而Faster RCNN则抛弃了传统的滑动窗口和SS方法，直接使用RPN生成检测框，这也是Faster R-CNN的巨大    优势，能极大提升检测框的生成速度。<br>   <center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_3.jpg\" alt=\"image\"></center><br>   上图展示了RPN网络的具体结构。可以看到RPN网络实际分为2条线，上面一条通过softmax分类anchors获得foreground和              background（检测目标是foreground），下面一条用于计算对于anchors的bounding box regression偏移量，以获得精确的          proposal。而最后的Proposal层则负责综合foreground anchors和bounding box regression偏移量获取proposals，同时剔除太    小和超出边界的proposals。其实整个网络到了Proposal Layer这里，就完成了相当于目标定位的功能。</p>\n<p>   <strong>2.1 多通道图像卷积基础知识介绍</strong></p>\n<ul>\n<li>对于单通道图像+单卷积核做卷积，之前展示了；</li>\n<li><p>对于多通道图像+多卷积核做卷积，计算方式如下：</p>\n<center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_4.jpg\" alt=\"image\"></center><br>输入有3个通道，同时有2个卷积核。对于每个卷积核，先在输入3个通道分别作卷积，再将3个通道结果加起来得到卷积输出。所以对     于某个卷积层，无论输入图像有多少个通道，输出图像通道数总是等于卷积核数量！<br>对多通道图像做 $1\\times1$ 卷积，其实就是将输入图像于每个通道乘以卷积系数后加在一起，即相当于把原图像中本来各个独立的     通道“联通”在了一起。<br><br><strong>2.2 Anchors</strong><br>提到RPN网络，就不能不说anchors。所谓anchors，实际上就是一组由rpn/generate_anchors.py生成的矩形。直接运行作者demo中    的generate_anchors.py可以得到以下输出：<br>[[ -84.  -40.   99.   55.]<br>[-176.  -88.  191.  103.]<br>[-360. -184.  375.  199.]<br>[ -56.  -56.   71.   71.]<br>[-120. -120.  135.  135.]<br>[-248. -248.  263.  263.]<br>[ -36.  -80.   51.   95.]<br>[ -80. -168.   95.  183.]<br>[-168. -344.  183.  359.]]<br><br>其中每行的4个值 $(x1,y1,x2,y2)$ 代表矩形左上和右下角点坐标。9个矩形共有3种形状，长宽比为大约为 $width:height = [1:1, 1:2, 2:1]$ 三种，如下图。实际上通过anchors就引入了检测中常用到的多尺度方法。<br><center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_5.jpg\" alt=\"image\"></center><br>注：关于上面的anchors size，其实是根据检测图像设置的。在python demo中，会把任意大小的输入图像reshape成 $800\\times600$。再回头来看anchors的大小，anchors中长宽 1:2 中最大为 $352\\times704$ ，长宽 2:1 中最大 $736\\times384$ ，基本是cover了 $800\\times600$ 的各个尺度和形状。<br>那么这9个anchors是做什么的呢？借用Faster RCNN论文中的原图，如下图，遍历Conv layers计算获得的feature maps，为每一个点都配备这9种anchors作为初始的检测框。这样做获得检测框很不准确，不用担心，后面还有2次bounding box regression可以修正检测框位置。<br><center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_6.jpg\" alt=\"image\"></center>\n\n<p>解释一下上面这张图的数字。</p>\n</li>\n</ul>\n<ul>\n<li>在原文中使用的是ZF model中，其Conv Layers中最后的conv5层num_output=256，对应生成256张特征图，所以相当于feature map每个点都是256-dimensions</li>\n<li>在conv5之后，做了rpn_conv/3x3卷积且num_output=256，相当于每个点又融合了周围3x3的空间信息（猜测这样做也许更鲁棒？反正我没测试），同时256-d不变（如图4和图7中的红框）</li>\n<li>假设在conv5 feature map中每个点上有k个anchor（默认k=9），而每个anhcor要分foreground和background，所以每个点由256d feature转化为cls=2k scores；而每个anchor都有[x, y, w, h]对应4个偏移量，所以reg=4k coordinates</li>\n<li><p>补充一点，全部anchors拿去训练太多了，训练程序会在合适的anchors中随机选取128个postive anchors+128个negative anchors进行训练（什么是合适的anchors下文5.1有解释）</p>\n<p> <strong>2.3 softmax判定foreground与background</strong><br> 一副MxN大小的矩阵送入Faster RCNN网络后，到RPN网络变为(M/16)x(N/16)，不妨设 W=M/16 ， H=N/16 。在进入reshape与softmax之前，先做了1x1卷积，如下图：<br> <center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_7.jpg\" alt=\"image\"></center><br> 该1x1卷积的caffe prototxt定义如下：<br> <center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_19.PNG\" alt=\"image\"></center><br>可以看到其num_output=18，也就是经过该卷积的输出图像为 $W\\times H \\times 18$ 大小（注意第二章开头提到的卷积计算方式）。这也就刚好对应了feature maps每一个点都有9个anchors，同时每个anchors又有可能是foreground和background，所有这些信息都保存 $W\\times H\\times (9\\cdot2)$ 大小的矩阵。为何这样做？后面接softmax分类获得foreground anchors，也就相当于初步提取了检测目标候选区域box（一般认为目标在foreground anchors中）。<br>综上所述，RPN网络中利用anchors和softmax初步提取出foreground anchors作为候选区域。</p>\n<p> <strong>2.4 bounding box regression原理</strong><br>如图所示绿色框为飞机的Ground Truth(GT)，红色为提取的foreground anchors，即便红色的框被分类器识别为飞机，但是由于红色的框定位不准，这张图相当于没有正确的检测出飞机。所以我们希望采用一种方法对红色的框进行微调，使得foreground anchors和GT更加接近。<br><center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_8.jpg\" alt=\"image\"></center><br>对于窗口一般使用四维向量 (x, y, w, h) 表示，分别表示窗口的中心点坐标和宽高。对于下图，红色的框A代表原始的Foreground Anchors，绿色的框G代表目标的GT，我们的目标是寻找一种关系，使得输入原始的anchor A经过映射得到一个跟真实窗口G更接近的回归窗口G’，即：</p>\n</li>\n<li>给定：$anchor A=(A_{x}, A_{y}, A_{w}, A_{h})$ 和 $GT=[G_{x}, G_{y}, G_{w}, G_{h}]$</li>\n<li><p>寻找一种变换F，使得：$F(A_{x}, A_{y}, A_{w}, A_{h})=(G_{x}^{‘}, G_{y}^{‘}, G_{w}^{‘}, G_{h}^{‘})$，其中 $(G_{x}^{‘}, G_{y}^{‘}, G_{w}^{‘}, G_{h}^{‘}) \\approx (G_{x}, G_{y}, G_{w}, G_{h})$<br><center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_9.jpg\" alt=\"image\"></center><br>那么经过何种变换F才能从图10中的anchor A变为G’呢？ 比较简单的思路就是:</p>\n</li>\n<li><p>先做平移</p>\n<center><br>$G^{‘}<em>{x} = A</em>{w} \\cdot d_{x}(A) + A_{x} $<br>$G^{‘}<em>{y} = A</em>{y} \\cdot d_{y}(A) + A_{y} $<br></center></li>\n<li>再做缩放<center><br>$G^{‘}<em>{w} = A</em>{w} \\cdot exp(d_{w}(A)) $<br>$G^{‘}<em>{h} = A</em>{h} \\cdot exp(d_{h}(A)) $<br></center>\n\n</li>\n</ul>\n<p>观察上面4个公式发现，需要学习的是 $d_{x}(A),d_{y}(A),d_{w}(A),d_{h}(A)$ 这四个变换。当输入的anchor A与GT相差较小时，可以认为这种变换是一种线性变换， 那么就可以用线性回归来建模对窗口进行微调（注意，只有当anchors A和GT比较接近时，才能使用线性回归模型，否则就是复杂的非线性问题了）。</p>\n<p>接下来的问题就是如何通过线性回归获得 $d_{x}(A),d_{y}(A),d_{w}(A),d_{h}(A)$ 了。线性回归就是给定输入的特征向量X, 学习一组参数W, 使得经过线性回归后的值跟真实值Y非常接近，即$Y=WX$。对于该问题，输入X是cnn feature map，定义为Φ；同时还有训练传入A与GT之间的变换量，即$(t_{x}, t_{y}, t_{w}, t_{h})$。输出是$d_{x}(A),d_{y}(A),d_{w}(A),d_{h}(A)$四个变换。那么目标函数可以表示为：</p>\n<center><br>$d_{<em>}(A) = w^{T}_{</em>} \\cdot \\phi(A)$<br></center>\n\n<p>其中Φ(A)是对应anchor的feature map组成的特征向量，w是需要学习的参数，d(A)是得到的预测值（*表示 x，y，w，h，也就是每一个变换对应一个上述目标函数）。为了让预测值$(t_{x}, t_{y}, t_{w}, t_{h})$与真实值差距最小，设计损失函数：</p>\n<center><br>$Loss = \\sum^{N}<em>{i}(t^{i}</em>{<em>} - \\hat{w}^{T}_{</em>} \\cdot \\phi(A^{i}))^{2}$<br></center><br>函数优化目标为：<br><center><br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_20.jpg\" alt=\"image\"><br></center><br>需要说明，只有在GT与需要回归框位置比较接近时，才可近似认为上述线性变换成立。<br>说完原理，对应于Faster RCNN原文，foreground anchor与ground truth之间的平移量 $(t_x, t_y)$ 与尺度因子 $(t_w, t_h)$ 如下：<br><center><br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_21.jpg\" alt=\"image\"><br></center><br>对于训练bouding box regression网络回归分支，输入是cnn feature Φ，监督信号是Anchor与GT的差距 $(t_x, t_y, t_w, t_h)$，即训练目标是：输入Φ的情况下使网络输出与监督信号尽可能接近。<br>那么当bouding box regression工作时，再输入Φ时，回归网络分支的输出就是每个Anchor的平移量和变换尺度 $(t_x, t_y, t_w, t_h)$，显然即可用来修正Anchor位置了。<br><br>   <strong>2.5 对proposals进行bounding box regression</strong><br>在了解bounding box regression后，再回头来看RPN网络第二条线路，如下图。<br><center><br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_10.jpg\" alt=\"image\"><br></center><br>其 $num_output=36$ ，即经过该卷积输出图像为 $W\\times H\\times 36$ ，在caffe blob存储为 [1, 36, H, W] ，这里相当于feature maps每个点都有9个anchors，每个anchors又都有4个用于回归的$d_{x}(A),d_{y}(A),d_{w}(A),d_{h}(A)$变换量。<br><br>   <strong>2.6 Proposal Layer</strong><br>Proposal Layer负责综合所有 $d_{x}(A),d_{y}(A),d_{w}(A),d_{h}(A)$ 变换量和foreground anchors，计算出精准的proposal，送入后续RoI Pooling Layer。<br>首先解释im_info。对于一副任意大小PxQ图像，传入Faster RCNN前首先reshape到固定 $M\\times N$ ，im_info=[M, N, scale_factor]则保存了此次缩放的所有信息。然后经过Conv Layers，经过4次pooling变为 $W\\times H=(M/16)\\times(N/16)$ 大小，其中feature_stride=16则保存了该信息，用于计算anchor偏移量。<br><center><br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_11.jpg\" alt=\"image\"><br></center>\n\n<p>Proposal Layer forward（caffe layer的前传函数）按照以下顺序依次处理：</p>\n<ol>\n<li>生成anchors，利用$[d_{x}(A),d_{y}(A),d_{w}(A),d_{h}(A)]$对所有的anchors做bbox regression回归（这里的anchors生成和训练时完全一致）</li>\n<li>按照输入的foreground softmax scores由大到小排序anchors，提取前pre_nms_topN(e.g. 6000)个anchors，即提取修正位置后的foreground anchors。</li>\n<li>限定超出图像边界的foreground anchors为图像边界（防止后续roi pooling时proposal超出图像边界）</li>\n<li>剔除非常小（width&lt;threshold or height&lt;threshold）的foreground anchors</li>\n<li>进行nonmaximum suppression</li>\n<li>再次按照nms后的foreground softmax scores由大到小排序fg anchors，提取前post_nms_topN(e.g. 300)结果作为proposal输出。</li>\n</ol>\n<p>之后输出 proposal=[x1, y1, x2, y2] ，注意，由于在第三步中将anchors映射回原图判断是否超出边界，所以这里输出的proposal是对应 $M\\times N$ 输入图像尺度的，这点在后续网络中有用。另外我认为，严格意义上的检测应该到此就结束了，后续部分应该属于识别了~   </p>\n<p><strong>RPN</strong>网络结构就介绍到这里，总结起来就是：<br><strong>生成anchors -&gt; softmax分类器提取fg anchors -&gt; bbox reg回归fg anchors -&gt; Proposal Layer生成proposals</strong></p>\n<h3 id=\"【19-06-2018】\"><a href=\"#【19-06-2018】\" class=\"headerlink\" title=\"【19/06/2018】\"></a>【19/06/2018】</h3><h4 id=\"处理-XML-文档\"><a href=\"#处理-XML-文档\" class=\"headerlink\" title=\"处理 XML 文档\"></a>处理 XML 文档</h4><p>使用 xml.etree.ElementTree 这个包去解析XML文件， 并且整理成为list形式<br>【流程】</p>\n<ul>\n<li>读取XML文件</li>\n<li>区分训练集测试集根据竞赛要求</li>\n<li>解析XML文档收录到PYTHON词典中<br><center><br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_23.PNG\" alt=\"image\"><br></center><br>Github 的 jupyter notebook <a href=\"https://github.com/Trouble404/Object-Detection-System-Based-on-CNN-and-Capsule-Network/blob/master/Faster-RCNN/parser_voc2012_xml_and_plotting.ipynb\" target=\"_blank\" rel=\"noopener\">地址</a> </li>\n</ul>\n<p>训练集根据竞赛的 trainval.txt 文件给的图片作为训练集<br>其余的作为训练集</p>\n<p>解析后， 总共有 17125 张图片，<br>其中 11540 张作为训练集</p>\n<p>图片中的20个类的统计情况：</p>\n<center><br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_24.PNG\" alt=\"image\"><br></center> \n\n\n<h3 id=\"【20-06-2018】\"><a href=\"#【20-06-2018】\" class=\"headerlink\" title=\"【20/06/2018】\"></a>【20/06/2018】</h3><h4 id=\"根据信息画出BBOXES\"><a href=\"#根据信息画出BBOXES\" class=\"headerlink\" title=\"根据信息画出BBOXES\"></a>根据信息画出BBOXES</h4><p>安装 cv2 这个包<br>  <figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">pip install opencv-python</span><br></pre></td></tr></table></figure><br>注意： OpenCV-python 中颜色格式 是BGR 而不是 RGB</p>\n<p>在VOC2012数据集里面，总共有20类， 根据不同的种类用不同的颜色和唯一的编码画BBOXES。</p>\n<table>\n<thead>\n<tr>\n<th style=\"text-align:left\">class</th>\n<th style=\"text-align:left\">class_mapping</th>\n<th style=\"text-align:left\">BGR of bbox</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:left\">Person</td>\n<td style=\"text-align:left\">0</td>\n<td style=\"text-align:left\">(0, 0, 255)</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">Aeroplane</td>\n<td style=\"text-align:left\">1</td>\n<td style=\"text-align:left\">(0, 0, 255)</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">Tvmonitor</td>\n<td style=\"text-align:left\">2</td>\n<td style=\"text-align:left\">(0, 128, 0)</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">Train</td>\n<td style=\"text-align:left\">3</td>\n<td style=\"text-align:left\">(128, 128, 128)</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">Boat</td>\n<td style=\"text-align:left\">4</td>\n<td style=\"text-align:left\">(0, 165, 255)</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">Dog</td>\n<td style=\"text-align:left\">5</td>\n<td style=\"text-align:left\">(0, 255, 255)</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">Chair</td>\n<td style=\"text-align:left\">6</td>\n<td style=\"text-align:left\">(80, 127, 255)</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">Bird</td>\n<td style=\"text-align:left\">7</td>\n<td style=\"text-align:left\">(208, 224, 64)</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">Bicycle</td>\n<td style=\"text-align:left\">8</td>\n<td style=\"text-align:left\">(235, 206, 135)</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">Bottle</td>\n<td style=\"text-align:left\">9</td>\n<td style=\"text-align:left\">(128, 0, 0)</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">Sheep</td>\n<td style=\"text-align:left\">10</td>\n<td style=\"text-align:left\">(140, 180, 210)</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">Diningtable</td>\n<td style=\"text-align:left\">11</td>\n<td style=\"text-align:left\">(0, 255, 0)</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">Horse</td>\n<td style=\"text-align:left\">12</td>\n<td style=\"text-align:left\">(133, 21, 199)</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">Motorbike</td>\n<td style=\"text-align:left\">13</td>\n<td style=\"text-align:left\">(47, 107, 85)</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">Sofa</td>\n<td style=\"text-align:left\">14</td>\n<td style=\"text-align:left\">(19, 69, 139)</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">Cow</td>\n<td style=\"text-align:left\">15</td>\n<td style=\"text-align:left\">(222, 196, 176)</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">Car</td>\n<td style=\"text-align:left\">16</td>\n<td style=\"text-align:left\">(0, 0, 0)</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">Cat</td>\n<td style=\"text-align:left\">17</td>\n<td style=\"text-align:left\">(225, 105, 65)</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">Bus</td>\n<td style=\"text-align:left\">18</td>\n<td style=\"text-align:left\">(255, 255, 255)</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">Pottedplant</td>\n<td style=\"text-align:left\">19</td>\n<td style=\"text-align:left\">(205, 250, 255)</td>\n</tr>\n</tbody>\n</table>\n<p>我写了一个show_image_with_bbox函数去画出带BBOXES的图根据处理XML文件得到的list:</p>\n<center><br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_22.PNG\" alt=\"image\"><br></center><br>Github 的 jupyter notebook <a href=\"https://github.com/Trouble404/Object-Detection-System-Based-on-CNN-and-Capsule-Network/blob/master/Faster-RCNN/parser_voc2012_xml_and_plotting.ipynb\" target=\"_blank\" rel=\"noopener\">地址</a><br><br>EXAMPLE:<br><center><br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/img_with_bboex_1.PNG\" alt=\"image\"><br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/img_with_bboex_2.PNG\" alt=\"image\"><br></center>  \n\n<h3 id=\"【21-06-2018】\"><a href=\"#【21-06-2018】\" class=\"headerlink\" title=\"【21/06/2018】\"></a>【21/06/2018】</h3><h4 id=\"config-setting\"><a href=\"#config-setting\" class=\"headerlink\" title=\"config setting\"></a>config setting</h4><p>set config class:<br>                 for image enhancement:</p>\n<center><br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/21_1.PNG\" alt=\"image\"><br></center>  \n\n<h4 id=\"image-enhancement\"><a href=\"#image-enhancement\" class=\"headerlink\" title=\"image enhancement\"></a>image enhancement</h4><center><br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/21_2.PNG\" alt=\"image\"><br></center><br>According to the config of three peremeters, users could augment image with 3 different ways or using them all.<br>For horizontal and vertical flips, 1/3 probability to triggle<br>With 0,90,180,270 rotation,<br>This function could increase the number of datasets.<br><br>image flips and rotation are realized by opencv and replace of height and width<br>New cordinates of bboxes are calculated acccording to different change of image<br><br>detailed in Github, jupyter notebook: <a href=\"https://github.com/Trouble404/Object-Detection-System-Based-on-CNN-and-Capsule-Network/blob/master/Faster-RCNN/config_set_and_image_enhance.ipynb\" target=\"_blank\" rel=\"noopener\">address</a><br><br>Orignal image:<br><center><br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/21_3.PNG\" alt=\"image\"><br></center><br>horizontal flip:<br><center><br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/21_4.PNG\" alt=\"image\"><br></center><br>Vertical filp:<br><center><br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/21_5.PNG\" alt=\"image\"><br></center><br>Random rotation:<br><center><br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/21_6.PNG\" alt=\"image\"><br></center><br>Horizontal and then vertical flips:<br><center><br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/21_7.PNG\" alt=\"image\"><br></center>  \n\n<h3 id=\"【22-06-2018】\"><a href=\"#【22-06-2018】\" class=\"headerlink\" title=\"【22/06/2018】\"></a>【22/06/2018】</h3><h4 id=\"Image-rezise\"><a href=\"#Image-rezise\" class=\"headerlink\" title=\"Image rezise\"></a>Image rezise</h4><p>This function is to rezise input image to a uniform size with same shortest side</p>\n<center><br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/22_2.PNG\" alt=\"image\"><br></center> \n\n<p>According to set the value of shortest side, convergent-divergent or augmented another side proportion</p>\n<p>Test:<br>Left image is resized image, in this case, the orignal image amplified.</p>\n<center><br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/22_1.png\" alt=\"image\"><br></center> \n\n<h4 id=\"Class-Balance\"><a href=\"#Class-Balance\" class=\"headerlink\" title=\"Class Balance\"></a>Class Balance</h4><p>When training the model, if we sent image with no repeating classes, it may help to improve the performance of model. Therefore, this function is to make sure no repeating classes in two closed input image.</p>\n<center><br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/22_3.PNG\" alt=\"image\"><br></center> \n\n<p>Test:</p>\n<p><center><br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/22_4.PNG\" alt=\"image\"><br></center><br>Random output 4 iamge with is function, it could find no repeating classes in two closed image. However, it may reduce the number of trainning image because skip some images.</p>\n<h3 id=\"【25-26-06-2018】\"><a href=\"#【25-26-06-2018】\" class=\"headerlink\" title=\"【25~26/06/2018】\"></a>【25~26/06/2018】</h3><h4 id=\"Region-Proposal-Networks-RPN\"><a href=\"#Region-Proposal-Networks-RPN\" class=\"headerlink\" title=\"Region Proposal Networks(RPN)\"></a>Region Proposal Networks(RPN)</h4><p><center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_3.jpg\" alt=\"image\"></center><br>可以看到RPN网络实际分为2条线，上面一条通过softmax分类anchors获得foreground和background（检测目标是foreground），下面一条用于计算对于anchors的bounding box regression偏移量，以获得精确的proposal。而最后的Proposal层则负责综合foreground anchors和bounding box regression偏移量获取proposals，同时剔除太小和超出边界的proposals。其实整个网络到了Proposal Layer这里，就完成了相当于目标定位的功能。</p>\n<h4 id=\"Anchors\"><a href=\"#Anchors\" class=\"headerlink\" title=\"Anchors\"></a>Anchors</h4><p>对每一个点生成的矩形</p>\n<p><center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_5.jpg\" alt=\"image\"></center><br>其中每行的4个值 (x1,y1,x2,y2) 代表矩形左上和右下角点坐标。9个矩形共有3种形状，长宽比为大约为 width:height = [1:1, 1:2, 2:1]</p>\n<p><center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_6.jpg\" alt=\"image\"></center><br>通过遍历Conv layers计算获得的feature maps，为每一个点都配备这9种anchors作为初始的检测框。这样做获得检测框很不准确，不用担心，后面还有2次bounding box regression可以修正检测框位置.</p>\n<h4 id=\"Code\"><a href=\"#Code\" class=\"headerlink\" title=\"Code\"></a>Code</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"string\">\"\"\" intersection of two bboxes</span></span><br><span class=\"line\"><span class=\"string\">@param ai: left top x,y and right bottom x,y coordinates of bbox 1</span></span><br><span class=\"line\"><span class=\"string\">@param bi: left top x,y and right bottom x,y coordinates of bbox 2</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">@return: area_union: whether contain target classes</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">intersection</span><span class=\"params\">(ai, bi)</span>:</span></span><br></pre></td></tr></table></figure>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"string\">\"\"\" union of two bboxes</span></span><br><span class=\"line\"><span class=\"string\">@param au: left top x,y and right bottom x,y coordinates of bbox 1</span></span><br><span class=\"line\"><span class=\"string\">@param bu: left top x,y and right bottom x,y coordinates of bbox 2</span></span><br><span class=\"line\"><span class=\"string\">@param area_intersection: intersection area</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">@return: area_union: whether contain target classes</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">union</span><span class=\"params\">(au, bu, area_intersection)</span>:</span></span><br></pre></td></tr></table></figure>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"string\">\"\"\" calculate ratio of intersection and union</span></span><br><span class=\"line\"><span class=\"string\">@param a: left top x,y and right bottom x,y coordinates of bbox 1</span></span><br><span class=\"line\"><span class=\"string\">@param b: left top x,y and right bottom x,y coordinates of bbox 2</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">@return: ratio of intersection and union of two bboxes</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">iou</span><span class=\"params\">(a, b)</span>:</span></span><br></pre></td></tr></table></figure>\n<p><strong>IOU is used to bounding box regression</strong></p>\n<hr>\n<p><strong> rpn calculation</strong></p>\n<ol>\n<li>Traversal all pre-anchors to calculate IOU with GT bboxes</li>\n<li>Set number and proprty of pre-anchors</li>\n<li>return specity number of result(Anchors)</li>\n</ol>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"string\">\"\"\" </span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">@param C: configuration</span></span><br><span class=\"line\"><span class=\"string\">@param img_data: parsered xml information</span></span><br><span class=\"line\"><span class=\"string\">@param width: orignal width of image</span></span><br><span class=\"line\"><span class=\"string\">@param hegiht: orignal height of image</span></span><br><span class=\"line\"><span class=\"string\">@param resized_width: resized width of image after image processing</span></span><br><span class=\"line\"><span class=\"string\">@param resized_heighth: resized height of image after image processing</span></span><br><span class=\"line\"><span class=\"string\">@param img_length_calc_function: Keras's image_dim_ordering function</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">@return: np.copy(y_rpn_cls): whether contain target classes</span></span><br><span class=\"line\"><span class=\"string\">@return: np.copy(y_rpn_regr): corrspoding return of gradient</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">\"\"\"</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">calc_rpn</span><span class=\"params\">(C, img_data, width, height, resized_width, resized_height, img_length_calc_function)</span>:</span></span><br></pre></td></tr></table></figure>\n<p>【注：其只会返回num_regions（这里设置为256）个有效的正负样本 】</p>\n<p>【流程】<br>Initialise paramters: see <a href=\"https://github.com/Trouble404/Object-Detection-System-Based-on-CNN-and-Capsule-Network/blob/master/Faster-RCNN/rpn_calculation.ipynb\" target=\"_blank\" rel=\"noopener\">jupyter notebook</a></p>\n<p>Calculate the size of map feature:<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">(output_width, output_height) = img_length_calc_function(resized_width, resized_height)</span><br></pre></td></tr></table></figure></p>\n<p><br><br>Get the GT box coordinates, and resize to account for image resizing<br>after rezised functon, the coordinates of bboxes need to re-calculation:<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">for</span> bbox_num, bbox <span class=\"keyword\">in</span> enumerate(img_data[<span class=\"string\">'bboxes'</span>]):</span><br><span class=\"line\">\tgta[bbox_num, <span class=\"number\">0</span>] = bbox[<span class=\"string\">'x1'</span>] * (resized_width / float(width))</span><br><span class=\"line\">\tgta[bbox_num, <span class=\"number\">1</span>] = bbox[<span class=\"string\">'x2'</span>] * (resized_width / float(width))</span><br><span class=\"line\">\tgta[bbox_num, <span class=\"number\">2</span>] = bbox[<span class=\"string\">'y1'</span>] * (resized_height / float(height))</span><br><span class=\"line\">\tgta[bbox_num, <span class=\"number\">3</span>] = bbox[<span class=\"string\">'y2'</span>] * (resized_height / float(height))</span><br></pre></td></tr></table></figure></p>\n<p>【注意gta的存储形式是（x1,x2,y1,y2）而不是（x1,y1,x2,y2）】<br><br><br>Traverse all possible group of sizes<br>anchor box scales [128, 256, 512]<br>anchor box ratios [1:1,1:2,2:1]<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">for</span> anchor_size_idx <span class=\"keyword\">in</span> range(len(anchor_sizes)):</span><br><span class=\"line\">\t<span class=\"keyword\">for</span> anchor_ratio_idx <span class=\"keyword\">in</span> range(len(anchor_ratios)):</span><br><span class=\"line\">\t\tanchor_x = anchor_sizes[anchor_size_idx] * anchor_ratios[anchor_ratio_idx][<span class=\"number\">0</span>]</span><br><span class=\"line\">\t\tanchor_y = anchor_sizes[anchor_size_idx] * anchor_ratios[anchor_ratio_idx][<span class=\"number\">1</span>]</span><br></pre></td></tr></table></figure></p>\n<p>Traver one bbox group, all pre boxes generated by anchors</p>\n<p>output_width，output_height：width and height of map feature<br>downscale：mapping ration, defualt 16<br>if to delete box out of iamge</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">for</span> ix <span class=\"keyword\">in</span> range(output_width):</span><br><span class=\"line\">\tx1_anc = downscale * (ix + <span class=\"number\">0.5</span>) - anchor_x / <span class=\"number\">2</span></span><br><span class=\"line\">\tx2_anc = downscale * (ix + <span class=\"number\">0.5</span>) + anchor_x / <span class=\"number\">2</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"keyword\">if</span> x1_anc &lt; <span class=\"number\">0</span> <span class=\"keyword\">or</span> x2_anc &gt; resized_width:</span><br><span class=\"line\">\t\t<span class=\"keyword\">continue</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"keyword\">for</span> jy <span class=\"keyword\">in</span> range(output_height):</span><br><span class=\"line\">\t\ty1_anc = downscale * (jy + <span class=\"number\">0.5</span>) - anchor_y / <span class=\"number\">2</span></span><br><span class=\"line\">\t\ty2_anc = downscale * (jy + <span class=\"number\">0.5</span>) + anchor_y / <span class=\"number\">2</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">if</span> y1_anc &lt; <span class=\"number\">0</span> <span class=\"keyword\">or</span> y2_anc &gt; resized_height:</span><br><span class=\"line\">\t\t\t<span class=\"keyword\">continue</span></span><br></pre></td></tr></table></figure>\n<p><br></p>\n<p>【注：现在我们确定了一个预选框组合有确定了中心点那就是唯一确定一个框了，接下来就是来确定这个宽的性质了：是否包含物体、如包含物体其回归梯度是多少】</p>\n<p>要确定以上两个性质，每一个框都需要遍历图中的所有bboxes 然后计算该预选框与bbox的交并比（IOU）<br>如果现在的交并比curr_iou大于该bbox最好的交并比或者大于给定的阈值则求下列参数，这些参数是后来要用的即回归梯度</p>\n<p>tx：两个框中心的宽的距离与预选框宽的比<br>ty:同tx<br>tw:bbox的宽与预选框宽的比<br>th:同理</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">if</span> curr_iou &gt; best_iou_for_bbox[bbox_num] <span class=\"keyword\">or</span> curr_iou &gt; C.rpn_max_overlap:</span><br><span class=\"line\">\tcx = (gta[bbox_num, <span class=\"number\">0</span>] + gta[bbox_num, <span class=\"number\">1</span>]) / <span class=\"number\">2.0</span></span><br><span class=\"line\">\tcy = (gta[bbox_num, <span class=\"number\">2</span>] + gta[bbox_num, <span class=\"number\">3</span>]) / <span class=\"number\">2.0</span></span><br><span class=\"line\">\tcxa = (x1_anc + x2_anc) / <span class=\"number\">2.0</span></span><br><span class=\"line\">\tcya = (y1_anc + y2_anc) / <span class=\"number\">2.0</span></span><br><span class=\"line\"></span><br><span class=\"line\">\ttx = (cx - cxa) / (x2_anc - x1_anc)</span><br><span class=\"line\">\tty = (cy - cya) / (y2_anc - y1_anc)</span><br><span class=\"line\">\ttw = np.log((gta[bbox_num, <span class=\"number\">1</span>] - gta[bbox_num, <span class=\"number\">0</span>]) / (x2_anc - x1_anc))</span><br><span class=\"line\">\tth = np.log((gta[bbox_num, <span class=\"number\">3</span>] - gta[bbox_num, <span class=\"number\">2</span>])) / (y2_anc - y1_anc)</span><br></pre></td></tr></table></figure>\n<p>对应于Faster RCNN原文，foreground anchor与ground truth之间的平移量 $(t_x, t_y)$ 如下：</p>\n<p><center><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/25_2.jpg\" alt=\"image\"></center><br>对于训练bouding box regression网络回归分支，输入是cnn feature Φ，监督信号是Anchor与GT的差距 $(t_x, t_y, t_w, t_h)$，即训练目标是：输入 Φ的情况下使网络输出与监督信号尽可能接近。<br>那么当bouding box regression工作时，再输入Φ时，回归网络分支的输出就是每个Anchor的平移量和变换尺度 $(t_x, t_y, t_w, t_h)$，显然即可用来修正Anchor位置了。</p>\n<p><br><br>如果相交的不是背景，那么进行一系列更新</p>\n<p>关于bbox的相关信息更新<br>预选框的相关更新：如果交并比大于阈值这是pos<br>best_iou_for_loc：其记录的是有最大交并比为多少和其对应的回归梯度<br>num_anchors_for_bbox[bbox_num]：记录的是bbox拥有的pos预选框的个数<br>如果小于最小阈值是neg，在这两个之间是neutral<br>需要注意的是：判断一个框为neg需要其与所有的bbox的交并比都小于最小的阈值</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">if</span> img_data[<span class=\"string\">'bboxes'</span>][bbox_num][<span class=\"string\">'class'</span>] != <span class=\"string\">'bg'</span>:</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"comment\"># all GT boxes should be mapped to an anchor box, so we keep track of which anchor box was best</span></span><br><span class=\"line\">\t<span class=\"keyword\">if</span> curr_iou &gt; best_iou_for_bbox[bbox_num]:</span><br><span class=\"line\">\t\tbest_anchor_for_bbox[bbox_num] = [jy, ix, anchor_ratio_idx, anchor_size_idx]</span><br><span class=\"line\">\t\tbest_iou_for_bbox[bbox_num] = curr_iou</span><br><span class=\"line\">\t\tbest_x_for_bbox[bbox_num, :] = [x1_anc, x2_anc, y1_anc, y2_anc]</span><br><span class=\"line\">\t\tbest_dx_for_bbox[bbox_num, :] = [tx, ty, tw, th]</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"keyword\">if</span> curr_iou &gt; C.rpn_max_overlap:</span><br><span class=\"line\">\t\tbbox_type = <span class=\"string\">'pos'</span></span><br><span class=\"line\">\t\tnum_anchors_for_bbox[bbox_num] += <span class=\"number\">1</span></span><br><span class=\"line\">\t\t<span class=\"keyword\">if</span> curr_iou &gt; best_iou_for_loc:</span><br><span class=\"line\">\t\t\tbest_iou_for_loc = curr_iou</span><br><span class=\"line\">\t\t\tbest_regr = (tx, ty, tw, th)</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"keyword\">if</span> C.rpn_min_overlap &lt; curr_iou &lt; C.rpn_max_overlap:</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">if</span> bbox_type != <span class=\"string\">'pos'</span>:</span><br><span class=\"line\">\t\t\tbbox_type = <span class=\"string\">'neutral'</span></span><br></pre></td></tr></table></figure>\n<p><br><br>当结束对所有的bbox的遍历时，来确定该预选宽的性质。</p>\n<p>y_is_box_valid：该预选框是否可用（nertual就是不可用的）<br>y_rpn_overlap：该预选框是否包含物体<br>y_rpn_regr:回归梯度<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">if</span> bbox_type == <span class=\"string\">'neg'</span>:</span><br><span class=\"line\">    y_is_box_valid[jy, ix, anchor_ratio_idx + n_anchratios * anchor_size_idx] = <span class=\"number\">1</span></span><br><span class=\"line\">    y_rpn_overlap[jy, ix, anchor_ratio_idx + n_anchratios * anchor_size_idx] = <span class=\"number\">0</span></span><br><span class=\"line\"><span class=\"keyword\">elif</span> bbox_type == <span class=\"string\">'neutral'</span>:</span><br><span class=\"line\">    y_is_box_valid[jy, ix, anchor_ratio_idx + n_anchratios * anchor_size_idx] = <span class=\"number\">0</span></span><br><span class=\"line\">    y_rpn_overlap[jy, ix, anchor_ratio_idx + n_anchratios * anchor_size_idx] = <span class=\"number\">0</span></span><br><span class=\"line\"><span class=\"keyword\">else</span>:</span><br><span class=\"line\">    y_is_box_valid[jy, ix, anchor_ratio_idx + n_anchratios * anchor_size_idx] = <span class=\"number\">1</span></span><br><span class=\"line\">    y_rpn_overlap[jy, ix, anchor_ratio_idx + n_anchratios * anchor_size_idx] = <span class=\"number\">1</span></span><br><span class=\"line\">    start = <span class=\"number\">4</span> * (anchor_ratio_idx + n_anchratios * anchor_size_idx)</span><br><span class=\"line\">    y_rpn_regr[jy, ix, start:start+<span class=\"number\">4</span>] = best_regr</span><br></pre></td></tr></table></figure></p>\n<p><br><br>如果有一个bbox没有pos的预选宽和其对应，这找一个与它交并比最高的anchor的设置为pos<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">for</span> idx <span class=\"keyword\">in</span> range(num_anchors_for_bbox.shape[<span class=\"number\">0</span>]):</span><br><span class=\"line\">\t<span class=\"keyword\">if</span> num_anchors_for_bbox[idx] == <span class=\"number\">0</span>:</span><br><span class=\"line\">\t\t<span class=\"comment\"># no box with an IOU greater than zero ...</span></span><br><span class=\"line\">\t\t<span class=\"keyword\">if</span> best_anchor_for_bbox[idx, <span class=\"number\">0</span>] == <span class=\"number\">-1</span>:</span><br><span class=\"line\">\t\t\t<span class=\"keyword\">continue</span></span><br><span class=\"line\">\t\ty_is_box_valid[best_anchor_for_bbox[idx,<span class=\"number\">0</span>], best_anchor_for_bbox[idx,<span class=\"number\">1</span>], best_anchor_for_bbox[idx,<span class=\"number\">2</span>] + n_anchratios *best_anchor_for_bbox[idx,<span class=\"number\">3</span>]] = <span class=\"number\">1</span></span><br><span class=\"line\">\t\ty_rpn_overlap[best_anchor_for_bbox[idx,<span class=\"number\">0</span>], best_anchor_for_bbox[idx,<span class=\"number\">1</span>], best_anchor_for_bbox[idx,<span class=\"number\">2</span>] + n_anchratios *best_anchor_for_bbox[idx,<span class=\"number\">3</span>]] = <span class=\"number\">1</span></span><br><span class=\"line\">\t\tstart = <span class=\"number\">4</span> * (best_anchor_for_bbox[idx,<span class=\"number\">2</span>] + n_anchratios * best_anchor_for_bbox[idx,<span class=\"number\">3</span>])</span><br><span class=\"line\">\t\ty_rpn_regr[best_anchor_for_bbox[idx,<span class=\"number\">0</span>], best_anchor_for_bbox[idx,<span class=\"number\">1</span>], start:start+<span class=\"number\">4</span>] = best_dx_for_bbox[idx, :]</span><br></pre></td></tr></table></figure></p>\n<p><br><br>将深度变到第一位，给向量增加一个维度, 在Tensorflow中， 第一纬度是batch size, 此外， 变换向量位置匹配要求<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">y_rpn_overlap = np.transpose(y_rpn_overlap, (<span class=\"number\">2</span>, <span class=\"number\">0</span>, <span class=\"number\">1</span>))</span><br><span class=\"line\">y_rpn_overlap = np.expand_dims(y_rpn_overlap, axis=<span class=\"number\">0</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">y_is_box_valid = np.transpose(y_is_box_valid, (<span class=\"number\">2</span>, <span class=\"number\">0</span>, <span class=\"number\">1</span>))</span><br><span class=\"line\">y_is_box_valid = np.expand_dims(y_is_box_valid, axis=<span class=\"number\">0</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">y_rpn_regr = np.transpose(y_rpn_regr, (<span class=\"number\">2</span>, <span class=\"number\">0</span>, <span class=\"number\">1</span>))</span><br><span class=\"line\">y_rpn_regr = np.expand_dims(y_rpn_regr, axis=<span class=\"number\">0</span>)</span><br></pre></td></tr></table></figure></p>\n<p><br><br>从可用的预选框中选择num_regions<br>如果pos的个数大于num_regions / 2，则将多下来的地方置为不可用。如果小于pos不做处理<br>接下来将pos与neg总是超过num_regions个的neg预选框置为不可用<br>最后， 256个预选框，128个positive,128个negative 会生成 在一张图片里面<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">pos_locs = np.where(np(y_rpn_overlap[<span class=\"number\">0</span>, :, :, :] =.logical_and= <span class=\"number\">1</span>, y_is_box_valid[<span class=\"number\">0</span>, :, :, :] == <span class=\"number\">1</span>))</span><br><span class=\"line\">neg_locs = np.where(np.logical_and(y_rpn_overlap[<span class=\"number\">0</span>, :, :, :] == <span class=\"number\">0</span>, y_is_box_valid[<span class=\"number\">0</span>, :, :, :] == <span class=\"number\">1</span>))</span><br><span class=\"line\">num_regions = <span class=\"number\">256</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> len(pos_locs[<span class=\"number\">0</span>]) &gt; num_regions / <span class=\"number\">2</span>:</span><br><span class=\"line\">\tval_locs = random.sample(range(len(pos_locs[<span class=\"number\">0</span>])), len(pos_locs[<span class=\"number\">0</span>]) - num_regions / <span class=\"number\">2</span>)</span><br><span class=\"line\">\ty_is_box_valid[<span class=\"number\">0</span>, pos_locs[<span class=\"number\">0</span>][val_locs], pos_locs[<span class=\"number\">1</span>][val_locs], pos_locs[<span class=\"number\">2</span>][val_locs]] = <span class=\"number\">0</span></span><br><span class=\"line\">\tnum_pos = num_regions / <span class=\"number\">2</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> len(neg_locs[<span class=\"number\">0</span>]) + num_pos &gt; num_regions:</span><br><span class=\"line\">\tval_locs = random.sample(range(len(neg_locs[<span class=\"number\">0</span>])), len(neg_locs[<span class=\"number\">0</span>]) - num_pos)</span><br><span class=\"line\">\t y_is_box_valid[<span class=\"number\">0</span>, neg_locs[<span class=\"number\">0</span>][val_locs], neg_locs[<span class=\"number\">1</span>][val_locs], neg_locs[<span class=\"number\">2</span>][val_locs]] = <span class=\"number\">0</span></span><br></pre></td></tr></table></figure></p>\n<p><br></p>\n<h3 id=\"【27-06-2018】\"><a href=\"#【27-06-2018】\" class=\"headerlink\" title=\"【27/06/2018】\"></a>【27/06/2018】</h3><h4 id=\"project-brief\"><a href=\"#project-brief\" class=\"headerlink\" title=\"project brief\"></a>project brief</h4><p>Re organization of Project plan</p>\n<h4 id=\"Anchors-Iterative\"><a href=\"#Anchors-Iterative\" class=\"headerlink\" title=\"Anchors Iterative\"></a>Anchors Iterative</h4><p>Integration of privous work:<br>In each anchor: config file -&gt; rpn_stride = 16 means generate one anchor in 16 pixels<br><a href=\"https://github.com/Trouble404/Object-Detection-System-Based-on-CNN-and-Capsule-Network/blob/master/Faster-RCNN/anchor_rpn.ipynb\" target=\"_blank\" rel=\"noopener\">Jupyter Notebook address</a></p>\n<p>【流程】<br>Function description<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">@param all_img_data: Parsered xml file  </span></span><br><span class=\"line\"><span class=\"string\">@param class_count: Counting of the number of all classes objects</span></span><br><span class=\"line\"><span class=\"string\">@param C: Configuration class</span></span><br><span class=\"line\"><span class=\"string\">@param img_length_calc_function: resnet's get_img_output_length() function</span></span><br><span class=\"line\"><span class=\"string\">@param backend: Tensorflow in this project</span></span><br><span class=\"line\"><span class=\"string\">#param mode: train or val</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">yield np.copy(x_img), [np.copy(y_rpn_cls), np.copy(y_rpn_regr)], img_data_aug</span></span><br><span class=\"line\"><span class=\"string\">@return: np.copy(x_img): image's matrix data</span></span><br><span class=\"line\"><span class=\"string\">@return: [np.copy(y_rpn_cls), np.copy(y_rpn_regr)]: calculated rpn class and radient</span></span><br><span class=\"line\"><span class=\"string\">@return: img_data_aug: correspoding parsed xml information</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">\"\"\"</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">get_anchor_gt</span><span class=\"params\">(all_img_data, class_count, C, img_length_calc_function, backend, mode=<span class=\"string\">'train'</span>)</span>:</span></span><br></pre></td></tr></table></figure></p>\n<p><br><br><strong>Traverse all input image based on input xml information</strong></p>\n<ul>\n<li>Apply class balance function: <figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">C.balanced_classes = <span class=\"keyword\">True</span></span><br><span class=\"line\">sample_selector = image_processing.SampleSelector(class_count)</span><br><span class=\"line\"><span class=\"keyword\">if</span> C.balanced_classes <span class=\"keyword\">and</span> sample_selector.skip_sample_for_balanced_class(img_data):</span><br><span class=\"line\">    <span class=\"keyword\">continue</span></span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<p><br></p>\n<ul>\n<li>Apply image enhance<br>if input mode is train, apply image enhance to obtain augmented image xml and matrix, if mode is val, obtain image orignal xml and matrix<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">if</span> mode == <span class=\"string\">'train'</span>:</span><br><span class=\"line\">    img_data_aug, x_img = image_enhance.augment(img_data, C, augment=<span class=\"keyword\">True</span>)</span><br><span class=\"line\"><span class=\"keyword\">else</span>:</span><br><span class=\"line\">    img_data_aug, x_img = image_enhance.augment(img_data, C, augment=<span class=\"keyword\">False</span>)</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<p>verifacation width and hegiht in xml and matrix<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">(width, height) = (img_data_aug[<span class=\"string\">'width'</span>], img_data_aug[<span class=\"string\">'height'</span>])</span><br><span class=\"line\">(rows, cols, _) = x_img.shape</span><br><span class=\"line\"><span class=\"keyword\">assert</span> cols == width</span><br><span class=\"line\"><span class=\"keyword\">assert</span> rows == height</span><br></pre></td></tr></table></figure></p>\n<p><br></p>\n<ul>\n<li>Apply rezise function<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">(resized_width, resized_height) = image_processing.get_new_img_size(width, height, C.im_size)</span><br><span class=\"line\">x_img = cv2.resize(x_img, (resized_width, resized_height), interpolation=cv2.INTER_CUBIC)</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<p><br></p>\n<ul>\n<li>Apply rpn calculation<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">y_rpn_cls, y_rpn_regr = rpn_calculation.calc_rpn(C, img_data_aug, width, height, resized_width, resized_height, img_length_calc_function)</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<p><br></p>\n<ul>\n<li><p>Zero-center by mean pixel, and preprocess image format<br>BGR -&gt; RGB because when apply resnet, it need RGB but in cv2, it use BGR</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">x_img = x_img[:,:, (<span class=\"number\">2</span>, <span class=\"number\">1</span>, <span class=\"number\">0</span>)]</span><br></pre></td></tr></table></figure>\n<p> For using pre-trainning model, needs to mins mean channel in each dim</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">x_img = x_img.astype(np.float32)</span><br><span class=\"line\">x_img[:, :, <span class=\"number\">0</span>] -= C.img_channel_mean[<span class=\"number\">0</span>]</span><br><span class=\"line\">x_img[:, :, <span class=\"number\">1</span>] -= C.img_channel_mean[<span class=\"number\">1</span>]</span><br><span class=\"line\">x_img[:, :, <span class=\"number\">2</span>] -= C.img_channel_mean[<span class=\"number\">2</span>]</span><br><span class=\"line\">x_img /= C.img_scaling_factor <span class=\"comment\"># default to 1,so no change here</span></span><br></pre></td></tr></table></figure>\n<p> expand for batch size</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">x_img = np.expand_dims(x_img, axis=<span class=\"number\">0</span>)</span><br></pre></td></tr></table></figure>\n<p>for using pre-trainning model, need to sclaling the std to match pre trained model</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">y_rpn_regr[:, y_rpn_regr.shape[<span class=\"number\">1</span>]//<span class=\"number\">2</span>:, :, :] *= C.std_scaling <span class=\"comment\"># scaling is 4 here</span></span><br></pre></td></tr></table></figure>\n<p>in tensorflow, sort as batch size, width, height, deep</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">if</span> backend == <span class=\"string\">'tf'</span>:</span><br><span class=\"line\">    x_img = np.transpose(x_img, (<span class=\"number\">0</span>, <span class=\"number\">3</span>, <span class=\"number\">2</span>, <span class=\"number\">1</span>))</span><br><span class=\"line\">    y_rpn_cls = np.transpose(y_rpn_cls, (<span class=\"number\">0</span>, <span class=\"number\">3</span>, <span class=\"number\">2</span>, <span class=\"number\">1</span>))</span><br><span class=\"line\">\ty_rpn_regr = np.transpose(y_rpn_regr, (<span class=\"number\">0</span>, <span class=\"number\">3</span>, <span class=\"number\">2</span>, <span class=\"number\">1</span>))</span><br></pre></td></tr></table></figure>\n<p>generator to iteror, using next() to loop</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">yield</span> np.copy(x_img), [np.copy(y_rpn_cls), np.copy(y_rpn_regr)], img_data_aug</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<p><br><br>【执行】<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">data_gen_train = get_anchor_gt(train_imgs, classes_count, C, nn.get_img_output_length, K.image_dim_ordering(), mode=<span class=\"string\">'train'</span>)</span><br><span class=\"line\">data_gen_val = get_anchor_gt(val_imgs, classes_count, C, nn.get_img_output_length,K.image_dim_ordering(), mode=<span class=\"string\">'val'</span>)</span><br></pre></td></tr></table></figure></p>\n<p>Test:<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">img,rpn,img_aug = next(data_gen_train)</span><br></pre></td></tr></table></figure></p>\n<p><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/26_1.PNG\" alt=\"image\"></p>\n<h3 id=\"【28-06-2018】\"><a href=\"#【28-06-2018】\" class=\"headerlink\" title=\"【28/06/2018】\"></a>【28/06/2018】</h3><h4 id=\"Resnet50-structure\"><a href=\"#Resnet50-structure\" class=\"headerlink\" title=\"Resnet50 structure\"></a>Resnet50 structure</h4><p>论文链接: <a href=\"https://arxiv.org/abs/1512.03385\" target=\"_blank\" rel=\"noopener\">https://arxiv.org/abs/1512.03385</a></p>\n<p>首先，我们要问一个问题：<br><strong>Is learning better networks as easy as stacking more layers?</strong></p>\n<p>很显然不是，原因有二。<br>一，<strong>vanishing/exploding gradients</strong>；深度会带来恶名昭著的梯度弥散/爆炸，导致系统不能收敛。然而梯度弥散/爆炸在很大程度上被normalized initialization and intermediate normalization layers处理了。<br>二、<strong>degradation</strong>；当深度开始增加的时候，accuracy经常会达到饱和，然后开始下降，但这并不是由于过拟合引起的。可见figure1，56-layer的error大于20-layer的error。</p>\n<p>He kaiMing大神认为靠堆layers竟然会导致degradation，那肯定是我们堆的方式不对。因此他提出了一种基于残差块的identity mapping，通过学习残差的方式，而非直接去学习直接的映射关系。 </p>\n<p><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/01_2.jpg\" alt=\"image\"></p>\n<p>事实证明，靠堆积残差块能够带来很好效果提升。而不断堆积plain layer却会带来很高的训练误差<br>残差块的两个优点：<br>1) Our extremely deep residual nets are easy to optimize, but the counterpart “plain” nets (that simply stack layers) exhibit higher training error when the depth increases;<br>2) Our deep residual nets can easily enjoy accuracy gains from greatly increased depth, producing results substantially better than previous networks.</p>\n<h3 id=\"【29-06-2018】\"><a href=\"#【29-06-2018】\" class=\"headerlink\" title=\"【29/06/2018】\"></a>【29/06/2018】</h3><h4 id=\"Resnet50-image-structure\"><a href=\"#Resnet50-image-structure\" class=\"headerlink\" title=\"Resnet50 image structure\"></a>Resnet50 image structure</h4><p><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/1_01.png\" alt=\"iamge\"><br>ResNet有2个基本的block，一个是Identity Block，输入和输出的dimension是一样的，所以可以串联多个；另外一个基本block是Conv Block，输入和输出的dimension是不一样的，所以不能连续串联，它的作用本来就是为了改变feature vector的dimension</p>\n<p>因为CNN最后都是要把image一点点的convert成很小但是depth很深的feature map，一般的套路是用统一的比较小的kernel（比如VGG都是用3x3），但是随着网络深度的增加，output的channel也增大（学到的东西越来越复杂），所以有必要在进入Identity Block之前，用Conv Block转换一下维度，这样后面就可以连续接Identity Block.</p>\n<p>可以看下Conv Block是怎么改变输出维度的:<br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/01_3.png\" alt=\"image\"><br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/01_4.png\" alt=\"image\"><br>其实就是在shortcut path的地方加上一个conv2D layer（1x1 filter size），然后在main path改变dimension，并与shortcut path对应起来.</p>\n<h2 id=\"July\"><a href=\"#July\" class=\"headerlink\" title=\"July\"></a>July</h2><h3 id=\"【02-07-2018】\"><a href=\"#【02-07-2018】\" class=\"headerlink\" title=\"【02/07/2018】\"></a>【02/07/2018】</h3><h4 id=\"Construct-resnet-by-keras\"><a href=\"#Construct-resnet-by-keras\" class=\"headerlink\" title=\"Construct resnet by keras\"></a>Construct resnet by keras</h4><p>残差网络的关键步骤，跨层的合并需要保证x和F(x)的shape是完全一样的，否则它们加不起来。</p>\n<p>理解了这一点，我们开始用keras做实现，我们把输入输出大小相同的模块称为identity_block，而把输出比输入小的模块称为conv_block，首先，导入所需的模块：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> keras.models <span class=\"keyword\">import</span> Model</span><br><span class=\"line\"><span class=\"keyword\">from</span> keras.layers <span class=\"keyword\">import</span> Input,Dense,BatchNormalization,Conv2D,MaxPooling2D,AveragePooling2D,ZeroPadding2D</span><br><span class=\"line\"><span class=\"keyword\">from</span> keras.layers <span class=\"keyword\">import</span> add,Flatten</span><br><span class=\"line\"><span class=\"keyword\">from</span> keras.optimizers <span class=\"keyword\">import</span> SGD</span><br></pre></td></tr></table></figure>\n<p>我们先来编写identity_block，这是一个函数，接受一个张量为输入，并返回一个张量, 然后是conv层，是有shortcut的：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">Conv2d_BN</span><span class=\"params\">(x, nb_filter,kernel_size, strides=<span class=\"params\">(<span class=\"number\">1</span>,<span class=\"number\">1</span>)</span>, padding=<span class=\"string\">'same'</span>,name=None)</span>:</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> name <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"keyword\">None</span>:</span><br><span class=\"line\">        bn_name = name + <span class=\"string\">'_bn'</span></span><br><span class=\"line\">        conv_name = name + <span class=\"string\">'_conv'</span></span><br><span class=\"line\">    <span class=\"keyword\">else</span>:</span><br><span class=\"line\">        bn_name = <span class=\"keyword\">None</span></span><br><span class=\"line\">        conv_name = <span class=\"keyword\">None</span></span><br><span class=\"line\"> </span><br><span class=\"line\">    x = Conv2D(nb_filter,kernel_size,padding=padding,strides=strides,activation=<span class=\"string\">'relu'</span>,name=conv_name)(x)</span><br><span class=\"line\">    x = BatchNormalization(axis=<span class=\"number\">3</span>,name=bn_name)(x)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> x</span><br><span class=\"line\">    </span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">Conv_Block</span><span class=\"params\">(inpt,nb_filter,kernel_size,strides=<span class=\"params\">(<span class=\"number\">1</span>,<span class=\"number\">1</span>)</span>, with_conv_shortcut=False)</span>:</span></span><br><span class=\"line\">    x = Conv2d_BN(inpt,nb_filter=nb_filter[<span class=\"number\">0</span>],kernel_size=(<span class=\"number\">1</span>,<span class=\"number\">1</span>),strides=strides,padding=<span class=\"string\">'same'</span>)</span><br><span class=\"line\">    x = Conv2d_BN(x, nb_filter=nb_filter[<span class=\"number\">1</span>], kernel_size=(<span class=\"number\">3</span>,<span class=\"number\">3</span>), padding=<span class=\"string\">'same'</span>)</span><br><span class=\"line\">    x = Conv2d_BN(x, nb_filter=nb_filter[<span class=\"number\">2</span>], kernel_size=(<span class=\"number\">1</span>,<span class=\"number\">1</span>), padding=<span class=\"string\">'same'</span>)</span><br><span class=\"line\">    <span class=\"keyword\">if</span> with_conv_shortcut:</span><br><span class=\"line\">        shortcut = Conv2d_BN(inpt,nb_filter=nb_filter[<span class=\"number\">2</span>],strides=strides,kernel_size=kernel_size)</span><br><span class=\"line\">        x = add([x,shortcut])</span><br><span class=\"line\">        <span class=\"keyword\">return</span> x</span><br><span class=\"line\">    <span class=\"keyword\">else</span>:</span><br><span class=\"line\">        x = add([x,inpt])</span><br><span class=\"line\">        <span class=\"keyword\">return</span> x</span><br></pre></td></tr></table></figure></p>\n<p>剩下的事情就很简单了，数好identity_block和conv_block是如何交错的，照着网络搭就好了：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">inpt = Input(shape=(<span class=\"number\">224</span>,<span class=\"number\">224</span>,<span class=\"number\">3</span>))</span><br><span class=\"line\">x = ZeroPadding2D((<span class=\"number\">3</span>,<span class=\"number\">3</span>))(inpt)</span><br><span class=\"line\">x = Conv2d_BN(x,nb_filter=<span class=\"number\">64</span>,kernel_size=(<span class=\"number\">7</span>,<span class=\"number\">7</span>),strides=(<span class=\"number\">2</span>,<span class=\"number\">2</span>),padding=<span class=\"string\">'valid'</span>)</span><br><span class=\"line\">x = MaxPooling2D(pool_size=(<span class=\"number\">3</span>,<span class=\"number\">3</span>),strides=(<span class=\"number\">2</span>,<span class=\"number\">2</span>),padding=<span class=\"string\">'same'</span>)(x)</span><br><span class=\"line\"> </span><br><span class=\"line\">x = Conv_Block(x,nb_filter=[<span class=\"number\">64</span>,<span class=\"number\">64</span>,<span class=\"number\">256</span>],kernel_size=(<span class=\"number\">3</span>,<span class=\"number\">3</span>),strides=(<span class=\"number\">1</span>,<span class=\"number\">1</span>),with_conv_shortcut=<span class=\"keyword\">True</span>)</span><br><span class=\"line\">x = Conv_Block(x,nb_filter=[<span class=\"number\">64</span>,<span class=\"number\">64</span>,<span class=\"number\">256</span>],kernel_size=(<span class=\"number\">3</span>,<span class=\"number\">3</span>))</span><br><span class=\"line\">x = Conv_Block(x,nb_filter=[<span class=\"number\">64</span>,<span class=\"number\">64</span>,<span class=\"number\">256</span>],kernel_size=(<span class=\"number\">3</span>,<span class=\"number\">3</span>))</span><br><span class=\"line\"> </span><br><span class=\"line\">x = Conv_Block(x,nb_filter=[<span class=\"number\">128</span>,<span class=\"number\">128</span>,<span class=\"number\">512</span>],kernel_size=(<span class=\"number\">3</span>,<span class=\"number\">3</span>),strides=(<span class=\"number\">2</span>,<span class=\"number\">2</span>),with_conv_shortcut=<span class=\"keyword\">True</span>)</span><br><span class=\"line\">x = Conv_Block(x,nb_filter=[<span class=\"number\">128</span>,<span class=\"number\">128</span>,<span class=\"number\">512</span>],kernel_size=(<span class=\"number\">3</span>,<span class=\"number\">3</span>))</span><br><span class=\"line\">x = Conv_Block(x,nb_filter=[<span class=\"number\">128</span>,<span class=\"number\">128</span>,<span class=\"number\">512</span>],kernel_size=(<span class=\"number\">3</span>,<span class=\"number\">3</span>))</span><br><span class=\"line\">x = Conv_Block(x,nb_filter=[<span class=\"number\">128</span>,<span class=\"number\">128</span>,<span class=\"number\">512</span>],kernel_size=(<span class=\"number\">3</span>,<span class=\"number\">3</span>))</span><br><span class=\"line\"> </span><br><span class=\"line\">x = Conv_Block(x,nb_filter=[<span class=\"number\">256</span>,<span class=\"number\">256</span>,<span class=\"number\">1024</span>],kernel_size=(<span class=\"number\">3</span>,<span class=\"number\">3</span>),strides=(<span class=\"number\">2</span>,<span class=\"number\">2</span>),with_conv_shortcut=<span class=\"keyword\">True</span>)</span><br><span class=\"line\">x = Conv_Block(x,nb_filter=[<span class=\"number\">256</span>,<span class=\"number\">256</span>,<span class=\"number\">1024</span>],kernel_size=(<span class=\"number\">3</span>,<span class=\"number\">3</span>))</span><br><span class=\"line\">x = Conv_Block(x,nb_filter=[<span class=\"number\">256</span>,<span class=\"number\">256</span>,<span class=\"number\">1024</span>],kernel_size=(<span class=\"number\">3</span>,<span class=\"number\">3</span>))</span><br><span class=\"line\">x = Conv_Block(x,nb_filter=[<span class=\"number\">256</span>,<span class=\"number\">256</span>,<span class=\"number\">1024</span>],kernel_size=(<span class=\"number\">3</span>,<span class=\"number\">3</span>))</span><br><span class=\"line\">x = Conv_Block(x,nb_filter=[<span class=\"number\">256</span>,<span class=\"number\">256</span>,<span class=\"number\">1024</span>],kernel_size=(<span class=\"number\">3</span>,<span class=\"number\">3</span>))</span><br><span class=\"line\">x = Conv_Block(x,nb_filter=[<span class=\"number\">256</span>,<span class=\"number\">256</span>,<span class=\"number\">1024</span>],kernel_size=(<span class=\"number\">3</span>,<span class=\"number\">3</span>))</span><br><span class=\"line\"> </span><br><span class=\"line\">x = Conv_Block(x,nb_filter=[<span class=\"number\">512</span>,<span class=\"number\">512</span>,<span class=\"number\">2048</span>],kernel_size=(<span class=\"number\">3</span>,<span class=\"number\">3</span>),strides=(<span class=\"number\">2</span>,<span class=\"number\">2</span>),with_conv_shortcut=<span class=\"keyword\">True</span>)</span><br><span class=\"line\">x = Conv_Block(x,nb_filter=[<span class=\"number\">512</span>,<span class=\"number\">512</span>,<span class=\"number\">2048</span>],kernel_size=(<span class=\"number\">3</span>,<span class=\"number\">3</span>))</span><br><span class=\"line\">x = Conv_Block(x,nb_filter=[<span class=\"number\">512</span>,<span class=\"number\">512</span>,<span class=\"number\">2048</span>],kernel_size=(<span class=\"number\">3</span>,<span class=\"number\">3</span>))</span><br><span class=\"line\">x = AveragePooling2D(pool_size=(<span class=\"number\">7</span>,<span class=\"number\">7</span>))(x)</span><br><span class=\"line\">x = Flatten()(x)</span><br><span class=\"line\">x = Dense(<span class=\"number\">1000</span>,activation=<span class=\"string\">'softmax'</span>)(x)</span><br><span class=\"line\"></span><br><span class=\"line\">model = Model(inputs=inpt,outputs=x)</span><br><span class=\"line\">sgd = SGD(decay=<span class=\"number\">0.0001</span>,momentum=<span class=\"number\">0.9</span>)</span><br><span class=\"line\">model.compile(loss=<span class=\"string\">'categorical_crossentropy'</span>,optimizer=sgd,metrics=[<span class=\"string\">'accuracy'</span>])</span><br><span class=\"line\">model.summary()</span><br></pre></td></tr></table></figure></p>\n<p><a href=\"https://github.com/Trouble404/Object-Detection-System-Based-on-CNN-and-Capsule-Network/blob/master/Faster-RCNN/Resnet50/resnet50_keras.ipynb\" target=\"_blank\" rel=\"noopener\">jupyter notebook</a></p>\n<h3 id=\"【03-07-2018】\"><a href=\"#【03-07-2018】\" class=\"headerlink\" title=\"【03/07/2018】\"></a>【03/07/2018】</h3><h4 id=\"load-pre-trained-model-of-resnet50\"><a href=\"#load-pre-trained-model-of-resnet50\" class=\"headerlink\" title=\"load pre-trained model of resnet50\"></a>load pre-trained model of resnet50</h4><p>步骤如下：</p>\n<ul>\n<li>下载ResNet50不包含全连接层的模型参数到本地（resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5）；</li>\n<li>定义好ResNet50的网络结构；</li>\n<li>将预训练的模型参数加载到我们所定义的网络结构中；</li>\n<li>更改全连接层结构，便于对我们的分类任务进行处</li>\n<li>或者根据需要解冻最后几个block，然后以很低的学习率开始训练。我们只选择最后一个block进行训练，是因为训练样本很少，而ResNet50模型层数很多，全部训练肯定不能训练好，会过拟合。 其次fine-tune时由于是在一个已经训练好的模型上进行的，故权值更新应该是一个小范围的，以免破坏预训练好的特征。</li>\n</ul>\n<p><a href=\"https://github.com/fchollet/deep-learning-models/releases\" target=\"_blank\" rel=\"noopener\">下载地址</a></p>\n<p><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/01_7.JPG\" alt=\"image\"></p>\n<p>因为使用了预训练模型，参数名称需要和预训练模型一致：<br>identity层：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">identity_block</span><span class=\"params\">(X, f, filters, stage, block)</span>:</span></span><br><span class=\"line\">    <span class=\"comment\"># defining name basis</span></span><br><span class=\"line\">    conv_name_base = <span class=\"string\">'res'</span> + str(stage) + block + <span class=\"string\">'_branch'</span></span><br><span class=\"line\">    bn_name_base = <span class=\"string\">'bn'</span> + str(stage) + block + <span class=\"string\">'_branch'</span></span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\"># Retrieve Filters</span></span><br><span class=\"line\">    F1, F2, F3 = filters</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\"># Save the input value. You'll need this later to add back to the main path. </span></span><br><span class=\"line\">    X_shortcut = X</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\"># First component of main path</span></span><br><span class=\"line\">    X = Conv2D(filters = F1, kernel_size = (<span class=\"number\">1</span>, <span class=\"number\">1</span>), strides = (<span class=\"number\">1</span>,<span class=\"number\">1</span>), padding = <span class=\"string\">'valid'</span>, name = conv_name_base + <span class=\"string\">'2a'</span>)(X)</span><br><span class=\"line\">    X = BatchNormalization(axis = <span class=\"number\">3</span>, name = bn_name_base + <span class=\"string\">'2a'</span>)(X)</span><br><span class=\"line\">    X = Activation(<span class=\"string\">'relu'</span>)(X)</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\"># Second component of main path (≈3 lines)</span></span><br><span class=\"line\">    X = Conv2D(filters= F2, kernel_size=(f,f),strides=(<span class=\"number\">1</span>,<span class=\"number\">1</span>),padding=<span class=\"string\">'same'</span>,name=conv_name_base + <span class=\"string\">'2b'</span>)(X)</span><br><span class=\"line\">    X = BatchNormalization(axis=<span class=\"number\">3</span>,name=bn_name_base+<span class=\"string\">'2b'</span>)(X)</span><br><span class=\"line\">    X = Activation(<span class=\"string\">'relu'</span>)(X)</span><br><span class=\"line\"> </span><br><span class=\"line\">    <span class=\"comment\"># Third component of main path (≈2 lines)</span></span><br><span class=\"line\">    X = Conv2D(filters=F3,kernel_size=(<span class=\"number\">1</span>,<span class=\"number\">1</span>),strides=(<span class=\"number\">1</span>,<span class=\"number\">1</span>),padding=<span class=\"string\">'valid'</span>,name=conv_name_base+<span class=\"string\">'2c'</span>)(X)</span><br><span class=\"line\">    X = BatchNormalization(axis=<span class=\"number\">3</span>,name=bn_name_base+<span class=\"string\">'2c'</span>)(X)</span><br><span class=\"line\"> </span><br><span class=\"line\">    <span class=\"comment\"># Final step: Add shortcut value to main path, and pass it through a RELU activation (≈2 lines)</span></span><br><span class=\"line\">    X = Add()([X, X_shortcut])</span><br><span class=\"line\">    X = Activation(<span class=\"string\">'relu'</span>)(X)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> X</span><br></pre></td></tr></table></figure></p>\n<p>conv层：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">convolutional_block</span><span class=\"params\">(X, f, filters, stage, block, s = <span class=\"number\">2</span>)</span>:</span></span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\"># defining name basis</span></span><br><span class=\"line\">    conv_name_base = <span class=\"string\">'res'</span> + str(stage) + block + <span class=\"string\">'_branch'</span></span><br><span class=\"line\">    bn_name_base = <span class=\"string\">'bn'</span> + str(stage) + block + <span class=\"string\">'_branch'</span></span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\"># Retrieve Filters</span></span><br><span class=\"line\">    F1, F2, F3 = filters</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\"># Save the input value</span></span><br><span class=\"line\">    X_shortcut = X</span><br><span class=\"line\"> </span><br><span class=\"line\">    <span class=\"comment\">##### MAIN PATH #####</span></span><br><span class=\"line\">    <span class=\"comment\"># First component of main path </span></span><br><span class=\"line\">    X = Conv2D(F1, (<span class=\"number\">1</span>, <span class=\"number\">1</span>), strides = (s,s),padding=<span class=\"string\">'valid'</span>,name = conv_name_base + <span class=\"string\">'2a'</span>)(X)</span><br><span class=\"line\">    X = BatchNormalization(axis = <span class=\"number\">3</span>, name = bn_name_base + <span class=\"string\">'2a'</span>)(X)</span><br><span class=\"line\">    X = Activation(<span class=\"string\">'relu'</span>)(X)</span><br><span class=\"line\"> </span><br><span class=\"line\">    <span class=\"comment\"># Second component of main path (≈3 lines)</span></span><br><span class=\"line\">    X = Conv2D(F2,(f,f),strides=(<span class=\"number\">1</span>,<span class=\"number\">1</span>),padding=<span class=\"string\">'same'</span>,name=conv_name_base+<span class=\"string\">'2b'</span>)(X)</span><br><span class=\"line\">    X = BatchNormalization(axis=<span class=\"number\">3</span>,name=bn_name_base+<span class=\"string\">'2b'</span>)(X)</span><br><span class=\"line\">    X = Activation(<span class=\"string\">'relu'</span>)(X)</span><br><span class=\"line\"> </span><br><span class=\"line\">    <span class=\"comment\"># Third component of main path (≈2 lines)</span></span><br><span class=\"line\">    X = Conv2D(F3,(<span class=\"number\">1</span>,<span class=\"number\">1</span>),strides=(<span class=\"number\">1</span>,<span class=\"number\">1</span>),padding=<span class=\"string\">'valid'</span>,name=conv_name_base+<span class=\"string\">'2c'</span>)(X)</span><br><span class=\"line\">    X = BatchNormalization(axis=<span class=\"number\">3</span>,name=bn_name_base+<span class=\"string\">'2c'</span>)(X)</span><br><span class=\"line\"> </span><br><span class=\"line\">    <span class=\"comment\">##### SHORTCUT PATH #### (≈2 lines)</span></span><br><span class=\"line\">    X_shortcut = Conv2D(F3,(<span class=\"number\">1</span>,<span class=\"number\">1</span>),strides=(s,s),padding=<span class=\"string\">'valid'</span>,name=conv_name_base+<span class=\"string\">'1'</span>)(X_shortcut)</span><br><span class=\"line\">    X_shortcut = BatchNormalization(axis=<span class=\"number\">3</span>,name =bn_name_base+<span class=\"string\">'1'</span>)(X_shortcut)</span><br><span class=\"line\"> </span><br><span class=\"line\">    <span class=\"comment\"># Final step: Add shortcut value to main path, and pass it through a RELU activation (≈2 lines)</span></span><br><span class=\"line\">    X = Add()([X,X_shortcut])</span><br><span class=\"line\">    X = Activation(<span class=\"string\">'relu'</span>)(X)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> X</span><br></pre></td></tr></table></figure></p>\n<p>resnet50结构：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">ResNet50</span><span class=\"params\">(input_shape = <span class=\"params\">(<span class=\"number\">64</span>, <span class=\"number\">64</span>, <span class=\"number\">3</span>)</span>, classes = <span class=\"number\">30</span>)</span>:</span></span><br><span class=\"line\">    <span class=\"comment\"># Define the input as a tensor with shape input_shape</span></span><br><span class=\"line\">    X_input = Input(input_shape)</span><br><span class=\"line\"> </span><br><span class=\"line\">    <span class=\"comment\"># Zero-Padding</span></span><br><span class=\"line\">    X = ZeroPadding2D((<span class=\"number\">3</span>, <span class=\"number\">3</span>))(X_input)</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\"># Stage 1</span></span><br><span class=\"line\">    X = Conv2D(<span class=\"number\">64</span>, (<span class=\"number\">7</span>, <span class=\"number\">7</span>), strides = (<span class=\"number\">2</span>, <span class=\"number\">2</span>), name = <span class=\"string\">'conv1'</span>)(X)</span><br><span class=\"line\">    X = BatchNormalization(axis = <span class=\"number\">3</span>, name = <span class=\"string\">'bn_conv1'</span>)(X)</span><br><span class=\"line\">    X = Activation(<span class=\"string\">'relu'</span>)(X)</span><br><span class=\"line\">    X = MaxPooling2D((<span class=\"number\">3</span>, <span class=\"number\">3</span>), strides=(<span class=\"number\">2</span>, <span class=\"number\">2</span>))(X)</span><br><span class=\"line\"> </span><br><span class=\"line\">    <span class=\"comment\"># Stage 2</span></span><br><span class=\"line\">    X = convolutional_block(X, f = <span class=\"number\">3</span>, filters = [<span class=\"number\">64</span>, <span class=\"number\">64</span>, <span class=\"number\">256</span>], stage = <span class=\"number\">2</span>, block=<span class=\"string\">'a'</span>, s = <span class=\"number\">1</span>)</span><br><span class=\"line\">    X = identity_block(X, <span class=\"number\">3</span>, [<span class=\"number\">64</span>, <span class=\"number\">64</span>, <span class=\"number\">256</span>], stage=<span class=\"number\">2</span>, block=<span class=\"string\">'b'</span>)</span><br><span class=\"line\">    X = identity_block(X, <span class=\"number\">3</span>, [<span class=\"number\">64</span>, <span class=\"number\">64</span>, <span class=\"number\">256</span>], stage=<span class=\"number\">2</span>, block=<span class=\"string\">'c'</span>)</span><br><span class=\"line\"> </span><br><span class=\"line\">    <span class=\"comment\">### START CODE HERE ###</span></span><br><span class=\"line\"> </span><br><span class=\"line\">    <span class=\"comment\"># Stage 3 (≈4 lines)</span></span><br><span class=\"line\">    X = convolutional_block(X, f = <span class=\"number\">3</span>,filters= [<span class=\"number\">128</span>,<span class=\"number\">128</span>,<span class=\"number\">512</span>],stage=<span class=\"number\">3</span>,block=<span class=\"string\">'a'</span>,s=<span class=\"number\">2</span>)</span><br><span class=\"line\">    X = identity_block(X,<span class=\"number\">3</span>,[<span class=\"number\">128</span>,<span class=\"number\">128</span>,<span class=\"number\">512</span>],stage=<span class=\"number\">3</span>,block=<span class=\"string\">'b'</span>)</span><br><span class=\"line\">    X = identity_block(X,<span class=\"number\">3</span>,[<span class=\"number\">128</span>,<span class=\"number\">128</span>,<span class=\"number\">512</span>],stage=<span class=\"number\">3</span>,block=<span class=\"string\">'c'</span>)</span><br><span class=\"line\">    X = identity_block(X,<span class=\"number\">3</span>,[<span class=\"number\">128</span>,<span class=\"number\">128</span>,<span class=\"number\">512</span>],stage=<span class=\"number\">3</span>,block=<span class=\"string\">'d'</span>)</span><br><span class=\"line\"> </span><br><span class=\"line\">    <span class=\"comment\"># Stage 4 (≈6 lines)</span></span><br><span class=\"line\">    X = convolutional_block(X,f=<span class=\"number\">3</span>,filters=[<span class=\"number\">256</span>,<span class=\"number\">256</span>,<span class=\"number\">1024</span>],stage=<span class=\"number\">4</span>,block=<span class=\"string\">'a'</span>,s=<span class=\"number\">2</span>)</span><br><span class=\"line\">    X = identity_block(X,<span class=\"number\">3</span>,[<span class=\"number\">256</span>,<span class=\"number\">256</span>,<span class=\"number\">1024</span>],stage=<span class=\"number\">4</span>,block=<span class=\"string\">'b'</span>)</span><br><span class=\"line\">    X = identity_block(X,<span class=\"number\">3</span>,[<span class=\"number\">256</span>,<span class=\"number\">256</span>,<span class=\"number\">1024</span>],stage=<span class=\"number\">4</span>,block=<span class=\"string\">'c'</span>)</span><br><span class=\"line\">    X = identity_block(X,<span class=\"number\">3</span>,[<span class=\"number\">256</span>,<span class=\"number\">256</span>,<span class=\"number\">1024</span>],stage=<span class=\"number\">4</span>,block=<span class=\"string\">'d'</span>)</span><br><span class=\"line\">    X = identity_block(X,<span class=\"number\">3</span>,[<span class=\"number\">256</span>,<span class=\"number\">256</span>,<span class=\"number\">1024</span>],stage=<span class=\"number\">4</span>,block=<span class=\"string\">'e'</span>)</span><br><span class=\"line\">    X = identity_block(X,<span class=\"number\">3</span>,[<span class=\"number\">256</span>,<span class=\"number\">256</span>,<span class=\"number\">1024</span>],stage=<span class=\"number\">4</span>,block=<span class=\"string\">'f'</span>)</span><br><span class=\"line\"> </span><br><span class=\"line\">    <span class=\"comment\"># Stage 5 (≈3 lines)</span></span><br><span class=\"line\">    X = convolutional_block(X, f = <span class=\"number\">3</span>,filters= [<span class=\"number\">512</span>,<span class=\"number\">512</span>,<span class=\"number\">2048</span>],stage=<span class=\"number\">5</span>,block=<span class=\"string\">'a'</span>,s=<span class=\"number\">2</span>)</span><br><span class=\"line\">    X = identity_block(X,<span class=\"number\">3</span>,[<span class=\"number\">512</span>,<span class=\"number\">512</span>,<span class=\"number\">2048</span>],stage=<span class=\"number\">5</span>,block=<span class=\"string\">'b'</span>)</span><br><span class=\"line\">    X = identity_block(X,<span class=\"number\">3</span>,[<span class=\"number\">512</span>,<span class=\"number\">512</span>,<span class=\"number\">2048</span>],stage=<span class=\"number\">5</span>,block=<span class=\"string\">'c'</span>)</span><br><span class=\"line\"> </span><br><span class=\"line\">    <span class=\"comment\"># AVGPOOL (≈1 line). Use \"X = AveragePooling2D(...)(X)\"</span></span><br><span class=\"line\">    X = AveragePooling2D((<span class=\"number\">2</span>,<span class=\"number\">2</span>),strides=(<span class=\"number\">2</span>,<span class=\"number\">2</span>))(X)</span><br><span class=\"line\"> </span><br><span class=\"line\">    <span class=\"comment\"># output layer</span></span><br><span class=\"line\">    X = Flatten()(X)</span><br><span class=\"line\">    model = Model(inputs = X_input, outputs = X, name=<span class=\"string\">'ResNet50'</span>)</span><br><span class=\"line\"> </span><br><span class=\"line\">    <span class=\"keyword\">return</span> model</span><br></pre></td></tr></table></figure></p>\n<p>构建网络并且载入权重：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">base_model = ResNet50(input_shape=(<span class=\"number\">224</span>,<span class=\"number\">224</span>,<span class=\"number\">3</span>),classes=<span class=\"number\">30</span>) </span><br><span class=\"line\">base_model.load_weights(<span class=\"string\">'resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5'</span>)</span><br></pre></td></tr></table></figure></p>\n<p>无法载入<br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/01_8.JPG\" alt=\"image\"></p>\n<h3 id=\"【04-07-2018】\"><a href=\"#【04-07-2018】\" class=\"headerlink\" title=\"【04/07/2018】\"></a>【04/07/2018】</h3><h4 id=\"Loading-pre-trained-model\"><a href=\"#Loading-pre-trained-model\" class=\"headerlink\" title=\"Loading pre-trained model\"></a>Loading pre-trained model</h4><p>对于keras：如果新模型和旧模型结构一样，直接调用model.load_weights读取参数就行。如果新模型中的几层和之前模型一样，也通过model.load_weights(‘my_model_weights.h5’, by_name=True)来读取参数， 或者手动对每一层进行参数的赋值，比如x= Dense(100, weights=oldModel.layers[1].get_weights())(x)</p>\n<p>修改代码：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">try</span>:</span><br><span class=\"line\">    base_model.load_weights(<span class=\"string\">'resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5'</span>,by_name=<span class=\"keyword\">True</span>)</span><br><span class=\"line\">    print(<span class=\"string\">\"load successful\"</span>)</span><br><span class=\"line\"><span class=\"keyword\">except</span>:</span><br><span class=\"line\">    print(<span class=\"string\">\"load failed\"</span>)</span><br></pre></td></tr></table></figure></p>\n<p>载入成功：<a href=\"https://github.com/Trouble404/Object-Detection-System-Based-on-CNN-and-Capsule-Network/blob/master/Faster-RCNN/Resnet50/resnet50_pre_load.ipynb\" target=\"_blank\" rel=\"noopener\">jupyter notebook</a></p>\n<h3 id=\"【05-06-07-2018】\"><a href=\"#【05-06-07-2018】\" class=\"headerlink\" title=\"【05~06/07/2018】\"></a>【05~06/07/2018】</h3><h4 id=\"construct-faster-rcnn-net\"><a href=\"#construct-faster-rcnn-net\" class=\"headerlink\" title=\"construct faster rcnn net\"></a>construct faster rcnn net</h4><p><strong>RoiPoolingConv</strong><br>该函数的作用是对将每一个预选框框定的特征图大小规整到相同大小<br>什么是ROI呢？<br>ROI是Region of Interest的简写，指的是在“特征图上的框”；<br>1）在Fast RCNN中， RoI是指Selective Search完成后得到的“候选框”在特征图上的映射，如下图所示；<br>2）在Faster RCNN中，候选框是经过RPN产生的，然后再把各个“候选框”映射到特征图上，得到RoIs<br>创建一个类，这里不同的是它是要继承keras的Layer类<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">RoiPoolingConv</span><span class=\"params\">(Layer)</span>:</span></span><br></pre></td></tr></table></figure></p>\n<p><a href=\"http://keras-cn.readthedocs.io/en/latest/layers/writting_layer/\" target=\"_blank\" rel=\"noopener\">编写自己的层</a></p>\n<p>定义：<br>**<a href=\"https://www.cnblogs.com/xuyuanyuan123/p/6674645.html\" target=\"_blank\" rel=\"noopener\">kwargs</a>：表示的就是形参中按照关键字传值把多余的传值以字典的方式呈现<br><a href=\"https://link.zhihu.com/?target=http%3A//python.jobbole.com/86787/\" target=\"_blank\" rel=\"noopener\">super</a>:子类调用父类的初始化方法<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"string\">'''ROI pooling layer for 2D inputs.</span></span><br><span class=\"line\"><span class=\"string\">    # Arguments</span></span><br><span class=\"line\"><span class=\"string\">        pool_size: int</span></span><br><span class=\"line\"><span class=\"string\">            Size of pooling region to use. pool_size = 7 will result in a 7x7 region.</span></span><br><span class=\"line\"><span class=\"string\">        num_rois: number of regions of interest to be used</span></span><br><span class=\"line\"><span class=\"string\">    '''</span></span><br><span class=\"line\"><span class=\"comment\"># 第一个是规整后特征图大小 第二个是预选框个数</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self, pool_size, num_rois, **kwargs)</span>:</span></span><br><span class=\"line\"></span><br><span class=\"line\">        self.dim_ordering = K.image_dim_ordering()</span><br><span class=\"line\">        <span class=\"comment\"># print error when kernel not tensorflow or thoean</span></span><br><span class=\"line\">        <span class=\"keyword\">assert</span> self.dim_ordering <span class=\"keyword\">in</span> &#123;<span class=\"string\">'tf'</span>&#125;, <span class=\"string\">'dim_ordering must be in tf'</span></span><br><span class=\"line\"></span><br><span class=\"line\">        self.pool_size = pool_size</span><br><span class=\"line\">        self.num_rois = num_rois</span><br><span class=\"line\"></span><br><span class=\"line\">        super(RoiPoolingConv, self).__init__(**kwargs)</span><br></pre></td></tr></table></figure></p>\n<p>得到特征图的输出通道个数:<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">build</span><span class=\"params\">(self, input_shape)</span>:</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> self.dim_ordering == <span class=\"string\">'tf'</span>:</span><br><span class=\"line\">            self.nb_channels = input_shape[<span class=\"number\">0</span>][<span class=\"number\">3</span>]</span><br></pre></td></tr></table></figure></p>\n<p>定义输出特征图的形状：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">compute_output_shape</span><span class=\"params\">(self, input_shape)</span>:</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> self.dim_ordering == <span class=\"string\">'tf'</span>:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"keyword\">None</span>, self.num_rois, self.pool_size, self.pool_size, self.nb_channels</span><br></pre></td></tr></table></figure></p>\n<p>遍历提供的所有预选框,将预选宽里的特征图规整到指定大小, 并且加入到output:<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">call</span><span class=\"params\">(self, x, mask=None)</span>:</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">assert</span>(len(x) == <span class=\"number\">2</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">        img = x[<span class=\"number\">0</span>]</span><br><span class=\"line\">        rois = x[<span class=\"number\">1</span>]</span><br><span class=\"line\"></span><br><span class=\"line\">        input_shape = K.shape(img)</span><br><span class=\"line\"></span><br><span class=\"line\">        outputs = []</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">for</span> roi_idx <span class=\"keyword\">in</span> range(self.num_rois):</span><br><span class=\"line\"></span><br><span class=\"line\">            x = rois[<span class=\"number\">0</span>, roi_idx, <span class=\"number\">0</span>]</span><br><span class=\"line\">            y = rois[<span class=\"number\">0</span>, roi_idx, <span class=\"number\">1</span>]</span><br><span class=\"line\">            w = rois[<span class=\"number\">0</span>, roi_idx, <span class=\"number\">2</span>]</span><br><span class=\"line\">            h = rois[<span class=\"number\">0</span>, roi_idx, <span class=\"number\">3</span>]</span><br><span class=\"line\">            </span><br><span class=\"line\">            row_length = w / float(self.pool_size)</span><br><span class=\"line\">            col_length = h / float(self.pool_size)</span><br><span class=\"line\"></span><br><span class=\"line\">            num_pool_regions = self.pool_size</span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"keyword\">if</span> self.dim_ordering == <span class=\"string\">'tf'</span>:</span><br><span class=\"line\">                x = K.cast(x, <span class=\"string\">'int32'</span>)</span><br><span class=\"line\">                y = K.cast(y, <span class=\"string\">'int32'</span>)</span><br><span class=\"line\">                w = K.cast(w, <span class=\"string\">'int32'</span>)</span><br><span class=\"line\">                h = K.cast(h, <span class=\"string\">'int32'</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">                <span class=\"comment\"># resize porposal of feature map</span></span><br><span class=\"line\">                rs = tf.image.resize_images(img[:, y:y+h, x:x+w, :], (self.pool_size, self.pool_size))</span><br><span class=\"line\">                outputs.append(rs)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># 将outputs里面的变量按照第一个维度合在一起【shape:(?, 7, 7, 512)】</span></span><br><span class=\"line\">        final_output = K.concatenate(outputs, axis=<span class=\"number\">0</span>)</span><br><span class=\"line\">        final_output = K.reshape(final_output, (<span class=\"number\">1</span>, self.num_rois, self.pool_size, self.pool_size, self.nb_channels))</span><br><span class=\"line\">        <span class=\"comment\"># 将变量规整到相应的大小【shape:(1, 32, 7, 7, 512)】</span></span><br><span class=\"line\">        final_output = K.permute_dimensions(final_output, (<span class=\"number\">0</span>, <span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">4</span>))</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">return</span> final_output</span><br></pre></td></tr></table></figure></p>\n<p>输出是batch个vector，其中batch的值等于RoI的个数，vector的大小为channel * w * h；RoI Pooling的过程就是将一个个大小不同的box矩形框，都映射成大小固定（w * h）的矩形框.</p>\n<p><strong>TimeDistributed 包装器</strong><br>FastRcnn在做完ROIpooling后，需要将生产的所有的Roi全部送入分类和回归网络，Keras用的TimeDistributed函数：</p>\n<p>Relu激活函数本身就是逐元素计算激活值的，无论进来多少维的tensor都一样，所以不需要使用TimeDistributed。conv2D需要TimeDistributed，是因为一个ROI内的数据计算是互相依赖的，而不同ROI之间又是独立的。</p>\n<p>在最后Faster RCNN的结构中进行类别判断和bbox框的回归时，需要对设置的num_rois个感兴趣区域进行回归处理，由于每一个区域的处理是相对独立的，便等价于此时的时间步为num_rois，因此用TimeDistributed来wrap。</p>\n<p>改编之前的conv 和 identity层：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">conv_block_td</span><span class=\"params\">(input_tensor, kernel_size, filters, stage, block, input_shape, strides=<span class=\"params\">(<span class=\"number\">2</span>, <span class=\"number\">2</span>)</span>, trainable=True)</span>:</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># conv block time distributed</span></span><br><span class=\"line\"></span><br><span class=\"line\">    nb_filter1, nb_filter2, nb_filter3 = filters</span><br><span class=\"line\"></span><br><span class=\"line\">    bn_axis = <span class=\"number\">3</span></span><br><span class=\"line\"></span><br><span class=\"line\">    conv_name_base = <span class=\"string\">'res'</span> + str(stage) + block + <span class=\"string\">'_branch'</span></span><br><span class=\"line\">    bn_name_base = <span class=\"string\">'bn'</span> + str(stage) + block + <span class=\"string\">'_branch'</span></span><br><span class=\"line\"></span><br><span class=\"line\">    x = TimeDistributed(Convolution2D(nb_filter1, (<span class=\"number\">1</span>, <span class=\"number\">1</span>), strides=strides, trainable=trainable, kernel_initializer=<span class=\"string\">'normal'</span>), input_shape=input_shape, name=conv_name_base + <span class=\"string\">'2a'</span>)(input_tensor)</span><br><span class=\"line\">    x = TimeDistributed(BatchNormalization(axis=bn_axis), name=bn_name_base + <span class=\"string\">'2a'</span>)(x)</span><br><span class=\"line\">    x = Activation(<span class=\"string\">'relu'</span>)(x)</span><br><span class=\"line\"></span><br><span class=\"line\">    x = TimeDistributed(Convolution2D(nb_filter2, (kernel_size, kernel_size), padding=<span class=\"string\">'same'</span>, trainable=trainable, kernel_initializer=<span class=\"string\">'normal'</span>), name=conv_name_base + <span class=\"string\">'2b'</span>)(x)</span><br><span class=\"line\">    x = TimeDistributed(BatchNormalization(axis=bn_axis), name=bn_name_base + <span class=\"string\">'2b'</span>)(x)</span><br><span class=\"line\">    x = Activation(<span class=\"string\">'relu'</span>)(x)</span><br><span class=\"line\"></span><br><span class=\"line\">    x = TimeDistributed(Convolution2D(nb_filter3, (<span class=\"number\">1</span>, <span class=\"number\">1</span>), kernel_initializer=<span class=\"string\">'normal'</span>), name=conv_name_base + <span class=\"string\">'2c'</span>, trainable=trainable)(x)</span><br><span class=\"line\">    x = TimeDistributed(BatchNormalization(axis=bn_axis), name=bn_name_base + <span class=\"string\">'2c'</span>)(x)</span><br><span class=\"line\"></span><br><span class=\"line\">    shortcut = TimeDistributed(Convolution2D(nb_filter3, (<span class=\"number\">1</span>, <span class=\"number\">1</span>), strides=strides, trainable=trainable, kernel_initializer=<span class=\"string\">'normal'</span>), name=conv_name_base + <span class=\"string\">'1'</span>)(input_tensor)</span><br><span class=\"line\">    shortcut = TimeDistributed(BatchNormalization(axis=bn_axis), name=bn_name_base + <span class=\"string\">'1'</span>)(shortcut)</span><br><span class=\"line\"></span><br><span class=\"line\">    x = Add()([x, shortcut])</span><br><span class=\"line\">    x = Activation(<span class=\"string\">'relu'</span>)(x)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> x</span><br></pre></td></tr></table></figure></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">identity_block_td</span><span class=\"params\">(input_tensor, kernel_size, filters, stage, block, trainable=True)</span>:</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># identity block time distributed</span></span><br><span class=\"line\"></span><br><span class=\"line\">    nb_filter1, nb_filter2, nb_filter3 = filters</span><br><span class=\"line\"></span><br><span class=\"line\">    bn_axis = <span class=\"number\">3</span></span><br><span class=\"line\"></span><br><span class=\"line\">    conv_name_base = <span class=\"string\">'res'</span> + str(stage) + block + <span class=\"string\">'_branch'</span></span><br><span class=\"line\">    bn_name_base = <span class=\"string\">'bn'</span> + str(stage) + block + <span class=\"string\">'_branch'</span></span><br><span class=\"line\"></span><br><span class=\"line\">    x = TimeDistributed(Convolution2D(nb_filter1, (<span class=\"number\">1</span>, <span class=\"number\">1</span>), trainable=trainable, kernel_initializer=<span class=\"string\">'normal'</span>), name=conv_name_base + <span class=\"string\">'2a'</span>)(input_tensor)</span><br><span class=\"line\">    x = TimeDistributed(BatchNormalization(axis=bn_axis), name=bn_name_base + <span class=\"string\">'2a'</span>)(x)</span><br><span class=\"line\">    x = Activation(<span class=\"string\">'relu'</span>)(x)</span><br><span class=\"line\"></span><br><span class=\"line\">    x = TimeDistributed(Convolution2D(nb_filter2, (kernel_size, kernel_size), trainable=trainable, kernel_initializer=<span class=\"string\">'normal'</span>,padding=<span class=\"string\">'same'</span>), name=conv_name_base + <span class=\"string\">'2b'</span>)(x)</span><br><span class=\"line\">    x = TimeDistributed(BatchNormalization(axis=bn_axis), name=bn_name_base + <span class=\"string\">'2b'</span>)(x)</span><br><span class=\"line\">    x = Activation(<span class=\"string\">'relu'</span>)(x)</span><br><span class=\"line\"></span><br><span class=\"line\">    x = TimeDistributed(Convolution2D(nb_filter3, (<span class=\"number\">1</span>, <span class=\"number\">1</span>), trainable=trainable, kernel_initializer=<span class=\"string\">'normal'</span>), name=conv_name_base + <span class=\"string\">'2c'</span>)(x)</span><br><span class=\"line\">    x = TimeDistributed(BatchNormalization(axis=bn_axis), name=bn_name_base + <span class=\"string\">'2c'</span>)(x)</span><br><span class=\"line\"></span><br><span class=\"line\">    x = Add()([x, input_tensor])</span><br><span class=\"line\">    x = Activation(<span class=\"string\">'relu'</span>)(x)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">return</span> x</span><br></pre></td></tr></table></figure>\n<p>如果将时序信号看作是2D矩阵，则TimeDistributed包装后的Dense就是分别对矩阵的每一行进行全连接。</p>\n<p><strong>把resnet50最后一个stage拿出来做分类层：</strong><br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">classifier_layers</span><span class=\"params\">(x, input_shape, trainable=False)</span>:</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># Stage 5</span></span><br><span class=\"line\">    x = conv_block_td(x, <span class=\"number\">3</span>, [<span class=\"number\">512</span>, <span class=\"number\">512</span>, <span class=\"number\">2048</span>], stage=<span class=\"number\">5</span>, block=<span class=\"string\">'a'</span>, input_shape=input_shape, strides=(<span class=\"number\">2</span>, <span class=\"number\">2</span>), trainable=trainable)</span><br><span class=\"line\">    x = identity_block_td(x, <span class=\"number\">3</span>, [<span class=\"number\">512</span>, <span class=\"number\">512</span>, <span class=\"number\">2048</span>], stage=<span class=\"number\">5</span>, block=<span class=\"string\">'b'</span>, trainable=trainable)</span><br><span class=\"line\">    x = identity_block_td(x, <span class=\"number\">3</span>, [<span class=\"number\">512</span>, <span class=\"number\">512</span>, <span class=\"number\">2048</span>], stage=<span class=\"number\">5</span>, block=<span class=\"string\">'c'</span>, trainable=trainable)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># AVGPOOL</span></span><br><span class=\"line\">    x = TimeDistributed(AveragePooling2D((<span class=\"number\">7</span>, <span class=\"number\">7</span>)), name=<span class=\"string\">'avg_pool'</span>)(x)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">return</span> x</span><br></pre></td></tr></table></figure></p>\n<ul>\n<li>RoiPoolingConv：返回的shape为(1, 32, 7, 7, 512)含义是batch_size,预选框的个数，特征图宽，特征图高度，特征图深度</li>\n<li>TimeDistributed：输入至少为3D张量，下标为1的维度将被认为是时间维。即对以一个维度下的变量当作一个完整变量来看待本文是32。你要实现的目的就是对32个预选宽提出的32个图片做出判断。</li>\n<li>out_class的shape:(?, 32, 21); out_regr的shape:(?, 32, 80)<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">classifier</span><span class=\"params\">(base_layers, input_rois, num_rois, nb_classes = <span class=\"number\">21</span>, trainable=False)</span>:</span></span><br><span class=\"line\"></span><br><span class=\"line\">    pooling_regions = <span class=\"number\">14</span></span><br><span class=\"line\">    input_shape = (num_rois,<span class=\"number\">14</span>,<span class=\"number\">14</span>,<span class=\"number\">1024</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    out_roi_pool = RoiPoolingConv(pooling_regions, num_rois)([base_layers, input_rois])</span><br><span class=\"line\">    out = classifier_layers(out_roi_pool, input_shape=input_shape, trainable=<span class=\"keyword\">True</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    out = TimeDistributed(Flatten())(out)</span><br><span class=\"line\"></span><br><span class=\"line\">    out_class = TimeDistributed(Dense(nb_classes, activation=<span class=\"string\">'softmax'</span>, kernel_initializer=<span class=\"string\">'zero'</span>), name=<span class=\"string\">'dense_class_&#123;&#125;'</span>.format(nb_classes))(out)</span><br><span class=\"line\">    <span class=\"comment\"># note: no regression target for bg class</span></span><br><span class=\"line\">    out_regr = TimeDistributed(Dense(<span class=\"number\">4</span> * (nb_classes<span class=\"number\">-1</span>), activation=<span class=\"string\">'linear'</span>, kernel_initializer=<span class=\"string\">'zero'</span>), name=<span class=\"string\">'dense_regress_&#123;&#125;'</span>.format(nb_classes))(out)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> [out_class, out_regr]</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<p><strong>定义RPN网络：</strong></p>\n<ul>\n<li>x_class:每一个锚点属于前景还是背景【注：这里使用的是sigmoid激活函数所以其输出的通道数是num_anchors】</li>\n<li>x_regr：每一个锚点对应的回归梯度<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">rpn</span><span class=\"params\">(base_layers,num_anchors)</span>:</span></span><br><span class=\"line\"></span><br><span class=\"line\">    x = Convolution2D(<span class=\"number\">512</span>, (<span class=\"number\">3</span>, <span class=\"number\">3</span>), padding=<span class=\"string\">'same'</span>, activation=<span class=\"string\">'relu'</span>, kernel_initializer=<span class=\"string\">'normal'</span>, name=<span class=\"string\">'rpn_conv1'</span>)(base_layers)</span><br><span class=\"line\"></span><br><span class=\"line\">    x_class = Convolution2D(num_anchors, (<span class=\"number\">1</span>, <span class=\"number\">1</span>), activation=<span class=\"string\">'sigmoid'</span>, kernel_initializer=<span class=\"string\">'uniform'</span>, name=<span class=\"string\">'rpn_out_class'</span>)(x)</span><br><span class=\"line\">    x_regr = Convolution2D(num_anchors * <span class=\"number\">4</span>, (<span class=\"number\">1</span>, <span class=\"number\">1</span>), activation=<span class=\"string\">'linear'</span>, kernel_initializer=<span class=\"string\">'zero'</span>, name=<span class=\"string\">'rpn_out_regress'</span>)(x)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">return</span> [x_class, x_regr, base_layers]</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<p><strong>resnet前面部分作为公共层：</strong><br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">nn_base</span><span class=\"params\">(input_tensor=None, trainable=False)</span>:</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># Determine proper input shape</span></span><br><span class=\"line\"></span><br><span class=\"line\">    input_shape = (<span class=\"keyword\">None</span>, <span class=\"keyword\">None</span>, <span class=\"number\">3</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">if</span> input_tensor <span class=\"keyword\">is</span> <span class=\"keyword\">None</span>:</span><br><span class=\"line\">        img_input = Input(shape=input_shape)</span><br><span class=\"line\">    <span class=\"keyword\">else</span>:</span><br><span class=\"line\">        <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> K.is_keras_tensor(input_tensor):</span><br><span class=\"line\">            img_input = Input(tensor=input_tensor, shape=input_shape)</span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            img_input = input_tensor</span><br><span class=\"line\"></span><br><span class=\"line\">    bn_axis = <span class=\"number\">3</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># Zero-Padding</span></span><br><span class=\"line\">    x = ZeroPadding2D((<span class=\"number\">3</span>, <span class=\"number\">3</span>))(img_input)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># Stage 1</span></span><br><span class=\"line\">    x = Convolution2D(<span class=\"number\">64</span>, (<span class=\"number\">7</span>, <span class=\"number\">7</span>), strides=(<span class=\"number\">2</span>, <span class=\"number\">2</span>), name=<span class=\"string\">'conv1'</span>, trainable = trainable)(x)</span><br><span class=\"line\">    x = BatchNormalization(axis=bn_axis, name=<span class=\"string\">'bn_conv1'</span>)(x)</span><br><span class=\"line\">    x = Activation(<span class=\"string\">'relu'</span>)(x)</span><br><span class=\"line\">    x = MaxPooling2D((<span class=\"number\">3</span>, <span class=\"number\">3</span>), strides=(<span class=\"number\">2</span>, <span class=\"number\">2</span>))(x)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># Stage 2</span></span><br><span class=\"line\">    x = conv_block(x, <span class=\"number\">3</span>, [<span class=\"number\">64</span>, <span class=\"number\">64</span>, <span class=\"number\">256</span>], stage=<span class=\"number\">2</span>, block=<span class=\"string\">'a'</span>, strides=(<span class=\"number\">1</span>, <span class=\"number\">1</span>), trainable = trainable)</span><br><span class=\"line\">    x = identity_block(x, <span class=\"number\">3</span>, [<span class=\"number\">64</span>, <span class=\"number\">64</span>, <span class=\"number\">256</span>], stage=<span class=\"number\">2</span>, block=<span class=\"string\">'b'</span>, trainable = trainable)</span><br><span class=\"line\">    x = identity_block(x, <span class=\"number\">3</span>, [<span class=\"number\">64</span>, <span class=\"number\">64</span>, <span class=\"number\">256</span>], stage=<span class=\"number\">2</span>, block=<span class=\"string\">'c'</span>, trainable = trainable)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># Stage 3</span></span><br><span class=\"line\">    x = conv_block(x, <span class=\"number\">3</span>, [<span class=\"number\">128</span>, <span class=\"number\">128</span>, <span class=\"number\">512</span>], stage=<span class=\"number\">3</span>, block=<span class=\"string\">'a'</span>, trainable = trainable)</span><br><span class=\"line\">    x = identity_block(x, <span class=\"number\">3</span>, [<span class=\"number\">128</span>, <span class=\"number\">128</span>, <span class=\"number\">512</span>], stage=<span class=\"number\">3</span>, block=<span class=\"string\">'b'</span>, trainable = trainable)</span><br><span class=\"line\">    x = identity_block(x, <span class=\"number\">3</span>, [<span class=\"number\">128</span>, <span class=\"number\">128</span>, <span class=\"number\">512</span>], stage=<span class=\"number\">3</span>, block=<span class=\"string\">'c'</span>, trainable = trainable)</span><br><span class=\"line\">    x = identity_block(x, <span class=\"number\">3</span>, [<span class=\"number\">128</span>, <span class=\"number\">128</span>, <span class=\"number\">512</span>], stage=<span class=\"number\">3</span>, block=<span class=\"string\">'d'</span>, trainable = trainable)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># Stage 4</span></span><br><span class=\"line\">    x = conv_block(x, <span class=\"number\">3</span>, [<span class=\"number\">256</span>, <span class=\"number\">256</span>, <span class=\"number\">1024</span>], stage=<span class=\"number\">4</span>, block=<span class=\"string\">'a'</span>, trainable = trainable)</span><br><span class=\"line\">    x = identity_block(x, <span class=\"number\">3</span>, [<span class=\"number\">256</span>, <span class=\"number\">256</span>, <span class=\"number\">1024</span>], stage=<span class=\"number\">4</span>, block=<span class=\"string\">'b'</span>, trainable = trainable)</span><br><span class=\"line\">    x = identity_block(x, <span class=\"number\">3</span>, [<span class=\"number\">256</span>, <span class=\"number\">256</span>, <span class=\"number\">1024</span>], stage=<span class=\"number\">4</span>, block=<span class=\"string\">'c'</span>, trainable = trainable)</span><br><span class=\"line\">    x = identity_block(x, <span class=\"number\">3</span>, [<span class=\"number\">256</span>, <span class=\"number\">256</span>, <span class=\"number\">1024</span>], stage=<span class=\"number\">4</span>, block=<span class=\"string\">'d'</span>, trainable = trainable)</span><br><span class=\"line\">    x = identity_block(x, <span class=\"number\">3</span>, [<span class=\"number\">256</span>, <span class=\"number\">256</span>, <span class=\"number\">1024</span>], stage=<span class=\"number\">4</span>, block=<span class=\"string\">'e'</span>, trainable = trainable)</span><br><span class=\"line\">    x = identity_block(x, <span class=\"number\">3</span>, [<span class=\"number\">256</span>, <span class=\"number\">256</span>, <span class=\"number\">1024</span>], stage=<span class=\"number\">4</span>, block=<span class=\"string\">'f'</span>, trainable = trainable)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">return</span> x</span><br></pre></td></tr></table></figure></p>\n<p><strong>搭建网络：</strong><br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># define the base network (resnet here)</span></span><br><span class=\"line\">shared_layers = nn.nn_base(img_input, trainable=<span class=\"keyword\">True</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># define the RPN, built on the base layers</span></span><br><span class=\"line\"><span class=\"comment\"># 9 types of anchors</span></span><br><span class=\"line\">num_anchors = len(C.anchor_box_scales) * len(C.anchor_box_ratios)</span><br><span class=\"line\">rpn = nn.rpn(shared_layers, num_anchors)</span><br><span class=\"line\"></span><br><span class=\"line\">classifier = nn.classifier(shared_layers, roi_input, C.num_rois, nb_classes=len(classes_count), trainable=<span class=\"keyword\">True</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">model_rpn = Model(img_input, rpn[:<span class=\"number\">2</span>])</span><br><span class=\"line\">model_classifier = Model([img_input, roi_input], classifier)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># this is a model that holds both the RPN and the classifier, used to load/save weights for the models</span></span><br><span class=\"line\">model_all = Model([img_input, roi_input], rpn[:<span class=\"number\">2</span>] + classifier)</span><br></pre></td></tr></table></figure></p>\n<p><a href=\"https://github.com/Trouble404/Object-Detection-System-Based-on-CNN-and-Capsule-Network/blob/master/Faster-RCNN/Resnet50/workflow_model.ipynb\" target=\"_blank\" rel=\"noopener\">jupyter notebook</a></p>\n<h3 id=\"【09-07-2018】\"><a href=\"#【09-07-2018】\" class=\"headerlink\" title=\"【09/07/2018】\"></a>【09/07/2018】</h3><h4 id=\"Loss-define\"><a href=\"#Loss-define\" class=\"headerlink\" title=\"Loss define\"></a>Loss define</h4><p><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/6_1.JPG\" alt=\"image\"></p>\n<p>由于涉及到分类和回归，所以需要定义一个多任务损失函数(Multi-task Loss Function)，包括Softmax Classification Loss和Bounding Box Regression Loss，公式定义如下：</p>\n<p>$L({p_{i}},{t_{i}}) = \\frac{1}{N_{cls}}\\sum_{i}L_{cls}(p_{i},p_{i}^{\\ast}) + \\lambda\\frac{1}{N_{reg}}\\sum_{i}L_{reg}(t_{i},t_{i}^{\\ast})$</p>\n<p><strong>Softmax Classification：</strong><br>对于RPN网络的分类层(cls)，其向量维数为2k = 18，考虑整个特征图conv5-3，则输出大小为W×H×18，正好对应conv5-3上每个点有9个anchors，而每个anchor又有两个score(fg/bg)输出，对于单个anchor训练样本，其实是一个二分类问题。为了便于Softmax分类，需要对分类层执行reshape操作，这也是由底层数据结构决定的。<br>在上式中，$p_{i}$为样本分类的概率值，$p_{i}^{\\ast}$为样本的标定值(label)，anchor为正样本时$p_{i}^{\\ast}$为1，为负样本时$p_{i}^{\\ast}$为0，$L_{cls}$为两种类别的对数损失(log loss)。</p>\n<p><strong>Bounding Box Regression：</strong><br>RPN网络的回归层输出向量的维数为4k = 36，回归参数为每个样本的坐标$[x,y,w,h]$，分别为box的中心位置和宽高，考虑三组参数预测框(predicted box)坐标$[x,y,w,h]$，anchor坐标$[x_{a},y_{a},w_{a},h_{a}]$，ground truth坐标$[x^{\\ast},y^{\\ast},w^{\\ast},h^{\\ast}]$，分别计算预测框相对anchor中心位置的偏移量以及宽高的缩放量{$t$}，ground truth相对anchor的偏移量和缩放量{$t^{\\ast}$}</p>\n<p>$t_{x}=\\frac{(x-x_{a})}{w_{a}}$ , $t_{y}=\\frac{(y-y_{a})}{h_{a}}$ , $t_{w}=log(\\frac{w}{w_{a}})$ , $t_{h}=log(\\frac{h}{h_{a}})$ (1)<br>$t_{x}^{\\ast}=\\frac{(x^{\\ast}-x_{a})}{w_{a}}$ , $t_{y}^{\\ast}=\\frac{(y^{\\ast}-y_{a})}{h_{a}}$ , $t_{w}^{\\ast}=log(\\frac{w^{\\ast}}{w_{a}})$ , $t_{h}^{\\ast}=log(\\frac{h^{\\ast}}{h_{a}})$ (2)</p>\n<p>回归目标就是让{t}尽可能地接近${t^{\\ast}}$，所以回归真正预测输出的是${t}$，而训练样本的标定真值为${t^{\\ast}}$。得到预测输出${t}$后，通过上式(1)即可反推获取预测框的真实坐标。</p>\n<p>在损失函数中，回归损失采用Smooth L1函数:</p>\n<p>$$ Smooth_{L1}(x) =\\left{<br>\\begin{aligned}<br>0.5x^{2} \\ \\ |x| \\leqslant 1\\<br>|x| - 0.5 \\ \\ otherwise<br>\\end{aligned}<br>\\right.<br>$$</p>\n<p>$L_{reg} = Smooth_{L1}(t-t^{\\ast})$<br>Smooth L1损失函数曲线如下图所示，相比于L2损失函数，L1对离群点或异常值不敏感，可控制梯度的量级使训练更易收敛。<br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/7_1.JPG\" alt=\"iamge\"></p>\n<p>在损失函数中，$p_{i}^{\\ast}L_{reg}$这一项表示只有目标anchor$(p_{i}^{\\ast}=1)$才有回归损失，其他anchor不参与计算。这里需要注意的是，当样本bbox和ground truth比较接近时(IoU大于某一阈值)，可以认为上式的坐标变换是一种线性变换，因此可将样本用于训练线性回归模型，否则当bbox与ground truth离得较远时，就是非线性问题，用线性回归建模显然不合理，会导致模型不work。分类层(cls)和回归层(reg)的输出分别为{p}和{t}，两项损失函数分别由$N_{cls}$和$N_{reg}$以及一个平衡权重λ归一化。</p>\n<h3 id=\"【10-07-2018】\"><a href=\"#【10-07-2018】\" class=\"headerlink\" title=\"【10/07/2018】\"></a>【10/07/2018】</h3><h4 id=\"loss-code\"><a href=\"#loss-code\" class=\"headerlink\" title=\"loss code\"></a>loss code</h4><p>  generator to iteror, using next() to loop<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">yield</span> np.copy(x_img), [np.copy(y_rpn_cls), np.copy(y_rpn_regr)], img_data_aug</span><br></pre></td></tr></table></figure></p>\n<p>Rpn calculation:<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">img,rpn,img_aug = next(data_gen_train)</span><br></pre></td></tr></table></figure></p>\n<p> <img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/7_2.JPG\" alt=\"image\"></p>\n<p>连续两个def 是装饰器，<br>装饰器其实也就是一个函数，一个用来包装函数的函数，返回一个修改之后的函数对象。经常被用于有切面需求的场景，较为经典的有插入日志、<br>性能测试、事务处理等。装饰器是解决这类问题的绝佳设计，有了装饰器，我们就可以抽离出大量函数中与函数功能本身无关的雷同代码并继续重用。概括的讲，装<br>饰器的作用就是为已经存在的对象添加额外的功能。</p>\n<p>根据：$L$ 的 cls 部分<br>$L({p_{i}},{t_{i}}) = \\frac{1}{N_{cls}}\\sum_{i}L_{cls}(p_{i},p_{i}^{\\ast})$</p>\n<p>在上式中，$p_{i}$为样本分类的概率值，$p_{i}^{\\ast}$为样本的标定值(label)，anchor为正样本时$p_{i}^{\\ast}$为1，为负样本时$p_{i}^{\\ast}$为0，$L_{cls}$为两种类别的对数损失(log loss)。</p>\n<p>因此， 定义 rpn loss cls:<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">rpn_loss_cls</span><span class=\"params\">(num_anchors)</span>:</span></span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">rpn_loss_cls_fixed_num</span><span class=\"params\">(y_true, y_pred)</span>:</span></span><br><span class=\"line\">            <span class=\"comment\"># binary_crossentropy -&gt; logloss</span></span><br><span class=\"line\">            <span class=\"comment\"># epsilon to increase robustness</span></span><br><span class=\"line\">\t\t<span class=\"keyword\">return</span> lambda_rpn_class * K.sum(y_true[:, :, :, :num_anchors] * K.binary_crossentropy(y_pred[:, :, :, :], y_true[:, :, :, num_anchors:])) / K.sum(epsilon + y_true[:, :, :, :num_anchors])</span><br><span class=\"line\">\t<span class=\"keyword\">return</span> rpn_loss_cls_fixed_num</span><br></pre></td></tr></table></figure></p>\n<p>根据$L$ 的 reg 部分<br>$L({p_{i}},{t_{i}}) =  \\lambda\\frac{1}{N_{reg}}\\sum_{i}L_{reg}(t_{i},t_{i}^{\\ast})$<br>在损失函数中，回归损失采用Smooth L1函数:</p>\n<p>$$ Smooth_{L1}(x) =\\left{<br>\\begin{aligned}<br>0.5x^{2} \\ \\ |x| \\leqslant 1\\<br>|x| - 0.5 \\ \\ otherwise<br>\\end{aligned}<br>\\right.<br>$$<br>$L_{reg} = Smooth_{L1}(t-t^{\\ast})$</p>\n<p>因此， 定义 rpn loss reg:<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">rpn_loss_regr</span><span class=\"params\">(num_anchors)</span>:</span></span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">rpn_loss_regr_fixed_num</span><span class=\"params\">(y_true, y_pred)</span>:</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"comment\"># difference of ture value and predicted value</span></span><br><span class=\"line\">\t\tx = y_true[:, :, :, <span class=\"number\">4</span> * num_anchors:] - y_pred</span><br><span class=\"line\">\t\t<span class=\"comment\"># absulote value of difference</span></span><br><span class=\"line\">\t\tx_abs = K.abs(x)</span><br><span class=\"line\">\t\t<span class=\"comment\"># if absulote value less than 1, x_bool == 1, else x_bool = 0</span></span><br><span class=\"line\">\t\tx_bool = K.cast(K.less_equal(x_abs, <span class=\"number\">1.0</span>), tf.float32)</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">return</span> lambda_rpn_regr * K.sum(y_true[:, :, :, :<span class=\"number\">4</span> * num_anchors] * (x_bool * (<span class=\"number\">0.5</span> * x * x) + (<span class=\"number\">1</span> - x_bool) * (x_abs - <span class=\"number\">0.5</span>))) / K.sum(epsilon + y_true[:, :, :, :<span class=\"number\">4</span> * num_anchors])</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"keyword\">return</span> rpn_loss_regr_fixed_num</span><br></pre></td></tr></table></figure></p>\n<p>对class的loss来说用一样的方程，但是class_loss_cls是无差别求loss【这个可以用K.mean，是因为其是无差别的求loss】，不用管是否可用<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">class_loss_regr</span><span class=\"params\">(num_classes)</span>:</span></span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">class_loss_regr_fixed_num</span><span class=\"params\">(y_true, y_pred)</span>:</span></span><br><span class=\"line\">\t\tx = y_true[:, :, <span class=\"number\">4</span>*num_classes:] - y_pred</span><br><span class=\"line\">\t\tx_abs = K.abs(x)</span><br><span class=\"line\">\t\tx_bool = K.cast(K.less_equal(x_abs, <span class=\"number\">1.0</span>), <span class=\"string\">'float32'</span>)</span><br><span class=\"line\">\t\t<span class=\"keyword\">return</span> lambda_cls_regr * K.sum(y_true[:, :, :<span class=\"number\">4</span>*num_classes] * (x_bool * (<span class=\"number\">0.5</span> * x * x) + (<span class=\"number\">1</span> - x_bool) * (x_abs - <span class=\"number\">0.5</span>))) / K.sum(epsilon + y_true[:, :, :<span class=\"number\">4</span>*num_classes])</span><br><span class=\"line\">\t<span class=\"keyword\">return</span> class_loss_regr_fixed_num</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">class_loss_cls</span><span class=\"params\">(y_true, y_pred)</span>:</span></span><br><span class=\"line\">\t<span class=\"keyword\">return</span> lambda_cls_class * K.mean(categorical_crossentropy(y_true[<span class=\"number\">0</span>, :, :], y_pred[<span class=\"number\">0</span>, :, :]))</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"【11-07-2018】\"><a href=\"#【11-07-2018】\" class=\"headerlink\" title=\"【11/07/2018】\"></a>【11/07/2018】</h3><h4 id=\"Iridis\"><a href=\"#Iridis\" class=\"headerlink\" title=\"Iridis\"></a>Iridis</h4><h4 id=\"High-Performance-Computing-HPC\"><a href=\"#High-Performance-Computing-HPC\" class=\"headerlink\" title=\"High Performance Computing (HPC)\"></a>High Performance Computing (HPC)</h4><p><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/8_1.jpg\" alt=\"image\"><br><a href=\"https://www.southampton.ac.uk/isolutions/staff/high-performance-computing.page\" target=\"_blank\" rel=\"noopener\">Introduction</a></p>\n<p>Iridis 5 specifications</p>\n<ul>\n<li>#251 in hte world(Based on July 2018 TOPP500 list) with $R_{peak}\\sim1305.6\\ TFlops/s$</li>\n<li>464 2.0 GHz nodes with 40 cores per node, 192 GB memeory</li>\n<li>10 nodes with 4xGTX1080TI GPUs, 28 cores(hyper-threaded), 128 GB memeory</li>\n<li>10 nodes with 2xVolta Tesia GPUs, same as thandard compute</li>\n<li>2.2 PB disk with paraller file system (&gt;12GB\\s)</li>\n<li>£5M Project delivered by OCF/IBM</li>\n</ul>\n<p><a href=\"https://mobaxterm.mobatek.net/\" target=\"_blank\" rel=\"noopener\">MobaXterm</a></p>\n<h4 id=\"create-my-own-conda-envieroment\"><a href=\"#create-my-own-conda-envieroment\" class=\"headerlink\" title=\"create my own conda envieroment\"></a>create my own conda envieroment</h4><p>Fllowing instroduction before</p>\n<h4 id=\"Slurm-command\"><a href=\"#Slurm-command\" class=\"headerlink\" title=\"Slurm command\"></a>Slurm command</h4><table>\n<thead>\n<tr>\n<th>Command</th>\n<th>Definition</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>sbatch</td>\n<td>Submits job scripts into system for execution (queued)</td>\n</tr>\n<tr>\n<td>scancel</td>\n<td>Cancels a job</td>\n</tr>\n<tr>\n<td>scontrol</td>\n<td>Used to display Slurm state, several options only available to root</td>\n</tr>\n<tr>\n<td>sinfo</td>\n<td>Display state of partitions and nodes</td>\n</tr>\n<tr>\n<td>squeue</td>\n<td>Display state of jobs</td>\n</tr>\n<tr>\n<td>salloc</td>\n<td>Submit a job for execution, or initiate job in real time</td>\n</tr>\n</tbody>\n</table>\n<p><strong> Bash script</strong><br><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#!/bin/bash</span></span><br><span class=\"line\"><span class=\"comment\">#SBATCH -J faster_rcnn </span></span><br><span class=\"line\"><span class=\"comment\">#SBATCH -o train_7.out</span></span><br><span class=\"line\"><span class=\"comment\">#SBATCH --ntasks=28</span></span><br><span class=\"line\"><span class=\"comment\">#SBATCH --nodes=1</span></span><br><span class=\"line\"><span class=\"comment\">#SBATCH --ntasks-per-node=8</span></span><br><span class=\"line\"><span class=\"comment\">#SBATCH --time=00:05:00</span></span><br><span class=\"line\"><span class=\"comment\">#SBATCH --gres=gpu:1</span></span><br><span class=\"line\"><span class=\"comment\">#SBATCH -p lyceum</span></span><br><span class=\"line\"></span><br><span class=\"line\">module load conda</span><br><span class=\"line\">module load cuda</span><br><span class=\"line\"><span class=\"built_in\">source</span> activate project</span><br><span class=\"line\">python test_frcnn.py</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"【12-13-07-2018】\"><a href=\"#【12-13-07-2018】\" class=\"headerlink\" title=\"【12~13/07/2018】\"></a>【12~13/07/2018】</h3><h4 id=\"change-plan\"><a href=\"#change-plan\" class=\"headerlink\" title=\"change plan\"></a>change plan</h4><p>因为faster r-cnn的搭建过程比想象中复杂，在咨询老师的意见以后，决定砍掉capsule的测试，专心faster-rcnn并且找到一些fine turn的方法。</p>\n<p>1）基础特征提取网络<br>ResNet，IncRes V2，ResNeXt 都是显著超越 VGG 的特征网络，当然网络的改进带来的是计算量的增加。</p>\n<p>2）RPN<br>通过更准确地  RPN 方法，减少 Proposal 个数，提高准确度。</p>\n<p>3）改进分类回归层<br>分类回归层的改进，包括 通过多层来提取特征 和 判别。</p>\n<hr>\n<p>@改进1：ION<br>论文：Inside outside net: Detecting objects in context with skip pooling and recurrent neural networks<br>提出了两个方面的贡献：</p>\n<p>1）Inside Net<br>所谓 Inside 是指在 ROI 区域之内，通过连接不同 Scale 下的 Feature Map，实现多尺度特征融合。<br>这里采用的是 Skip-Pooling，从 conv3-4-5-context 分别提取特征，后面会讲到。<br>多尺度特征 能够提升对小目标的检测精度。</p>\n<p>2）Outside Net<br>所谓 Outside 是指 ROI 区域之外，也就是目标周围的 上下文（Contextual）信息。<br>作者通过添加了两个 RNN 层（修改后的 IRNN）实现上下文特征提取。<br>上下文信息 对于目标遮挡有比较好的适应。</p>\n<hr>\n<p>@改进2：多尺度之 HyperNet<br>论文：Hypernet: Towards accurate region proposal generation and joint object detection<br>基于 Region Proposal 的方法，通过多尺度的特征提取来提高对小目标的检测能力，来看网络框图：<br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/8_2.JPG\" alt=\"image\"><br>分为 三个主要特征 来介绍（对应上面网络拓扑图的 三个红色框）：</p>\n<p>1）Hyper Feature Extraction （特征提取）<br>多尺度特征提取是本文的核心点，作者的方法稍微有所不同，他是以中间的 Feature 尺度为参考，前面的层通过 Max Pooling 到对应大小，后面的层则是通过 反卷积（Deconv）进行放大。<br>多尺度 Feature ConCat 的时候，作者使用了 LRN进行归一化（类似于 ION 的 L2 Norm）。</p>\n<p>2）Region Proposal Generation（建议框生成）<br>作者设计了一个轻量级的 ConvNet，与 RPN 的区别不大（为写论文强创新)。<br>一个 ROI Pooling层，一个 Conv 层，还有一个 FC 层。每个 Position 通过 ROI Pooling 得到一个 13*13 的 bin，通过 Conv（3*3*4）层得到一个 13*13*4 的 Cube，再通过 FC 层得到一个 256d 的向量。<br>后面的 Score+ BBox_Reg 与 Faster并无区别，用于目标得分 和 Location OffSet。<br>考虑到建议框的 Overlap，作者用了 Greedy NMS 去重，文中将 IOU参考设为 0.7，每个 Image 保留 1k 个 Region，并选择其中 Top-200 做 Detetcion。<br>通过对比，要优于基于 Edge Box 重排序的 Deep Box，从多尺度上考虑比 Deep Proposal 效果更好。</p>\n<p>3）Object Detection（目标检测）<br>与 Fast RCNN基本一致，在原来的检测网络基础上做了两点改进：<br>a）在 FC 层之前添加了一个 卷积层（3<em>3</em>63），对特征有效降维；<br>b）将 DropOut 从 0.5 降到 0.25；<br>另外，与 Proposal一样采用了 NMS 进行 Box抑制，但由于之前已经做了，这一步的意义不大。</p>\n<hr>\n<p>@改进3：多尺度之 MSCNN<br>论文：A Unified Multi-scale Deep Convolutional Neural Network for Fast Object Detection<br>a）原图缩放，多个Scale的原图对应不同Scale的Feature；<br>该方法计算多次Scale，每个Scale提取一次Feature，计算量巨大。</p>\n<p>b）一幅输入图像对应多个分类器；<br>不需要重复提取特征图，但对分类器要求很高，一般很难得到理想的结果。</p>\n<p>c）原图缩放，少量Scale原图-&gt;少量特征图-&gt;多个Model模板；<br>相当于对 a）和 b）的 Trade-Off。</p>\n<p>d）原图缩放，少量Scale原图-&gt;少量特征图-&gt;特征图插值-&gt;1个Model；</p>\n<p>e）RCNN方法，Proposal直接给到CNN；<br>和 a）全图计算不同，只针对Patch计算。</p>\n<p>f）RPN方法，特征图是通过CNN卷积层得到；<br>和 b）类似，不过采用的是同尺度的不同模板，容易导致尺度不一致问题。</p>\n<p>g）上套路，提出我们自己的方法，多尺度特征图；<br>每个尺度特征图对应一个 输出模板，每个尺度cover一个目标尺寸范围。</p>\n<hr>\n<p>NMS和soft-nms算法<br>Repulsion loss：遮挡下的行人检测 加入overlapping 与不同的 loss<br>融合以上两个到faster rcnn中</p>\n<h3 id=\"【16-20-07-2018】\"><a href=\"#【16-20-07-2018】\" class=\"headerlink\" title=\"【16~20/07/2018】\"></a>【16~20/07/2018】</h3><p>旅行</p>\n<h3 id=\"【23-07-2018】\"><a href=\"#【23-07-2018】\" class=\"headerlink\" title=\"【23/07/2018】\"></a>【23/07/2018】</h3><h4 id=\"fix-boxes-location-by-regrident\"><a href=\"#fix-boxes-location-by-regrident\" class=\"headerlink\" title=\"fix boxes location by regrident\"></a>fix boxes location by regrident</h4><p>使用regr对anchor所确定的框进行修正</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"string\">\"\"\" fix boxes with grident</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">@param X: current cordinates of box</span></span><br><span class=\"line\"><span class=\"string\">@param T: coresspoding grident</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">@return: Fixed cordinates of box</span></span><br><span class=\"line\"><span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">apply_regr_np</span><span class=\"params\">(X, T)</span>:</span></span><br><span class=\"line\">\t<span class=\"keyword\">try</span>:</span><br><span class=\"line\">\t\tx = X[<span class=\"number\">0</span>, :, :]</span><br><span class=\"line\">\t\ty = X[<span class=\"number\">1</span>, :, :]</span><br><span class=\"line\">\t\tw = X[<span class=\"number\">2</span>, :, :]</span><br><span class=\"line\">\t\th = X[<span class=\"number\">3</span>, :, :]</span><br><span class=\"line\"></span><br><span class=\"line\">\t\ttx = T[<span class=\"number\">0</span>, :, :]</span><br><span class=\"line\">\t\tty = T[<span class=\"number\">1</span>, :, :]</span><br><span class=\"line\">\t\ttw = T[<span class=\"number\">2</span>, :, :]</span><br><span class=\"line\">\t\tth = T[<span class=\"number\">3</span>, :, :]</span><br></pre></td></tr></table></figure>\n<p>$t_{x}=\\frac{(x-x_{a})}{w_{a}}$ , $t_{y}=\\frac{(y-y_{a})}{h_{a}}$ , $t_{w}=log(\\frac{w}{w_{a}})$ , $t_{h}=log(\\frac{h}{h_{a}})$ (1)<br>$t_{x}^{\\ast}=\\frac{(x^{\\ast}-x_{a})}{w_{a}}$ , $t_{y}^{\\ast}=\\frac{(y^{\\ast}-y_{a})}{h_{a}}$ , $t_{w}^{\\ast}=log(\\frac{w^{\\ast}}{w_{a}})$ , $t_{h}^{\\ast}=log(\\frac{h^{\\ast}}{h_{a}})$ (2)</p>\n<p>回归目标就是让{t}尽可能地接近${t^{\\ast}}$，所以回归真正预测输出的是${t}$，而训练样本的标定真值为${t^{\\ast}}$。得到预测输出${t}$后，通过上式(1)即可反推获取预测框的真实坐标。</p>\n<p>过程：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">\t<span class=\"comment\"># centre cordinate</span></span><br><span class=\"line\">\tcx = x + w/<span class=\"number\">2.</span></span><br><span class=\"line\">\tcy = y + h/<span class=\"number\">2.</span></span><br><span class=\"line\">\t<span class=\"comment\"># fixed centre cordinate</span></span><br><span class=\"line\">\tcx1 = tx * w + cx</span><br><span class=\"line\">\tcy1 = ty * h + cy</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"comment\"># fixed wdith and height</span></span><br><span class=\"line\">\tw1 = np.exp(tw.astype(np.float64)) * w</span><br><span class=\"line\">\th1 = np.exp(th.astype(np.float64)) * h</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"comment\"># fixed left top corner's cordinate</span></span><br><span class=\"line\">\tx1 = cx1 - w1/<span class=\"number\">2.</span></span><br><span class=\"line\">\ty1 = cy1 - h1/<span class=\"number\">2.</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"comment\"># apporximate</span></span><br><span class=\"line\">\tx1 = np.round(x1)</span><br><span class=\"line\">\ty1 = np.round(y1)</span><br><span class=\"line\">\tw1 = np.round(w1)</span><br><span class=\"line\">\th1 = np.round(h1)</span><br><span class=\"line\">\t<span class=\"keyword\">return</span> np.stack([x1, y1, w1, h1])</span><br><span class=\"line\"><span class=\"keyword\">except</span> Exception <span class=\"keyword\">as</span> e:</span><br><span class=\"line\">\tprint(e)</span><br><span class=\"line\">\t<span class=\"keyword\">return</span> X</span><br></pre></td></tr></table></figure></p>\n<h4 id=\"NMS-no-max-suppression\"><a href=\"#NMS-no-max-suppression\" class=\"headerlink\" title=\"NMS no max suppression\"></a>NMS no max suppression</h4><p>该函数的作用是从所给定的所有预选框中选择指定个数最合理的边框。</p>\n<p>定义：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"string\">\"\"\" NMS , delete overlapping box</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">@param boxes: (n,4) box and coresspoding cordinates</span></span><br><span class=\"line\"><span class=\"string\">@param probs: (n,) box adn coresspding possibility</span></span><br><span class=\"line\"><span class=\"string\">@param overlap_thresh: treshold of delet box overlapping</span></span><br><span class=\"line\"><span class=\"string\">@param max_boxes: maximum keeping number of boxes</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">@return: boxes: boxes cordinates(x1,y1,x2,y2)</span></span><br><span class=\"line\"><span class=\"string\">@return: probs: coresspoding possibility</span></span><br><span class=\"line\"><span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">non_max_suppression_fast</span><span class=\"params\">(boxes, probs, overlap_thresh=<span class=\"number\">0.9</span>, max_boxes=<span class=\"number\">300</span>)</span>:</span></span><br></pre></td></tr></table></figure></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">if</span> len(boxes) == <span class=\"number\">0</span>:</span><br><span class=\"line\">   <span class=\"keyword\">return</span> []</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># grab the coordinates of the bounding boxes</span></span><br><span class=\"line\">x1 = boxes[:, <span class=\"number\">0</span>]</span><br><span class=\"line\">y1 = boxes[:, <span class=\"number\">1</span>]</span><br><span class=\"line\">x2 = boxes[:, <span class=\"number\">2</span>]</span><br><span class=\"line\">y2 = boxes[:, <span class=\"number\">3</span>]</span><br><span class=\"line\"></span><br><span class=\"line\">np.testing.assert_array_less(x1, x2)</span><br><span class=\"line\">np.testing.assert_array_less(y1, y2)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># if the bounding boxes integers, convert them to floats --</span></span><br><span class=\"line\"><span class=\"comment\"># this is important since we'll be doing a bunch of divisions</span></span><br><span class=\"line\"><span class=\"keyword\">if</span> boxes.dtype.kind == <span class=\"string\">\"i\"</span>:</span><br><span class=\"line\">\tboxes = boxes.astype(<span class=\"string\">\"float\"</span>)</span><br></pre></td></tr></table></figure>\n<p>对输入的数据进行确认<br>不能为空<br>左上角的坐标小于右下角<br>数据类型的转换<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># initialize the list of picked indexes\t</span></span><br><span class=\"line\">pick = []</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># calculate the areas</span></span><br><span class=\"line\">area = (x2 - x1) * (y2 - y1)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># sort the bounding boxes </span></span><br><span class=\"line\">idxs = np.argsort(probs)</span><br></pre></td></tr></table></figure></p>\n<p>pick（拾取）用来存放边框序号<br>计算框的面积<br>probs按照概率从小到大排序<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">while</span> len(idxs) &gt; <span class=\"number\">0</span>:</span><br><span class=\"line\"><span class=\"comment\"># grab the last index in the indexes list and add the</span></span><br><span class=\"line\"><span class=\"comment\"># index value to the list of picked indexes</span></span><br><span class=\"line\">last = len(idxs) - <span class=\"number\">1</span></span><br><span class=\"line\">i = idxs[last]</span><br><span class=\"line\">pick.append(i)</span><br></pre></td></tr></table></figure></p>\n<p>接下来就是按照概率从大到小取出框，且框的重合度不可以高于overlap_thresh。代码的思路是这样的：</p>\n<p>每一次取概率最大的框（即idxs最后一个）<br>删除掉剩下的框中重和度高于overlap_thresh的框<br>直到取满max_boxes为止<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># find the intersection</span></span><br><span class=\"line\"></span><br><span class=\"line\">xx1_int = np.maximum(x1[i], x1[idxs[:last]])</span><br><span class=\"line\">yy1_int = np.maximum(y1[i], y1[idxs[:last]])</span><br><span class=\"line\">xx2_int = np.minimum(x2[i], x2[idxs[:last]])</span><br><span class=\"line\">yy2_int = np.minimum(y2[i], y2[idxs[:last]])</span><br><span class=\"line\"></span><br><span class=\"line\">ww_int = np.maximum(<span class=\"number\">0</span>, xx2_int - xx1_int)</span><br><span class=\"line\">hh_int = np.maximum(<span class=\"number\">0</span>, yy2_int - yy1_int)</span><br><span class=\"line\"></span><br><span class=\"line\">area_int = ww_int * hh_int</span><br></pre></td></tr></table></figure></p>\n<p>取出idxs队列中最大概率框的序号，将其添加到pick中<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># find the union</span></span><br><span class=\"line\">area_union = area[i] + area[idxs[:last]] - area_int</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># compute the ratio of overlap</span></span><br><span class=\"line\">overlap = area_int/(area_union + <span class=\"number\">1e-6</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># delete all indexes from the index list that have</span></span><br><span class=\"line\">idxs = np.delete(idxs, np.concatenate(([last],np.where(overlap &gt; overlap_thresh)[<span class=\"number\">0</span>])))</span><br></pre></td></tr></table></figure></p>\n<p>计算取出来的框与剩下来的框区域的交集<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">if</span> len(pick) &gt;= max_boxes:</span><br><span class=\"line\">   <span class=\"keyword\">break</span></span><br></pre></td></tr></table></figure></p>\n<p>计算重叠率，然后删除掉重叠率较高的位置[np.concatest]，是因为最后一个位置你已经用过了，就得将其从队列中删掉<br>当取足max_boxes框，停止循环<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">boxes = boxes[pick].astype(<span class=\"string\">\"int\"</span>)</span><br><span class=\"line\">probs = probs[pick]</span><br><span class=\"line\"><span class=\"keyword\">return</span> boxes, probs</span><br></pre></td></tr></table></figure></p>\n<p>返回pick内存取的边框和对应的概率</p>\n<h3 id=\"【24-07-2018】\"><a href=\"#【24-07-2018】\" class=\"headerlink\" title=\"【24/07/2018】\"></a>【24/07/2018】</h3><h4 id=\"rpn-to-porposal-fixed\"><a href=\"#rpn-to-porposal-fixed\" class=\"headerlink\" title=\"rpn to porposal fixed\"></a>rpn to porposal fixed</h4><p>该函数的作用是将rpn网络的预测结果转化到一个个预选框<br>函数流程：<br>遍历anchor_size，在遍历anchor_ratio</p>\n<p>得到框的长宽在原图上的映射</p>\n<p>得到相应尺寸的框对应的回归梯度，将深度都放到第一个维度<br>注1：regr_layer[0, :, :, 4 * curr_layer:4 * curr_layer + 4]当某一个维度的取值为一个值时，那么新的变量就会减小一个维度<br>注2：curr_layer代表的是特定长度和比例的框所代表的编号</p>\n<p>得到anchor对应的（x,y,w,h）</p>\n<p>使用regr对anchor所确定的框进行修正</p>\n<p>对修正后的边框一些不合理的地方进行矫正。<br>如，边框回归后的左上角和右下角的点不能超过图片外，框的宽高不可以小于0<br>注：得到框的形式是（x1,y1,x2,y2）</p>\n<p>得到all_boxes形状是（n,4），和每一个框对应的概率all_probs形状是（n,）</p>\n<p>删除掉一些不合理的点，即右下角的点值要小于左上角的点值<br>注：np.where() 返回位置信息，这也是删除不符合要求点的一种方法<br>np.delete(all_boxes, idxs, 0)最后一个参数是在哪一个维度删除</p>\n<p>最后是根据要求选取指定个数的合理预选框。这一步是重要的，因为每一个点可以有9个预选框，而又拥有很多点，一张图片可能会有几万个预选框。而经过这一步预选迅速下降到几百个。<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"string\">\"\"\" rpn to porposal</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">@param rpn_layer: porposal's coresspoding possibiliy, whether item exciseted</span></span><br><span class=\"line\"><span class=\"string\">@param regr_layer: porposal's coresspoding regrident</span></span><br><span class=\"line\"><span class=\"string\">@param C: Configuration</span></span><br><span class=\"line\"><span class=\"string\">@param dim_ordering: Dimensional organization</span></span><br><span class=\"line\"><span class=\"string\">@param use_regr=True: wether use regurident to fix proposal</span></span><br><span class=\"line\"><span class=\"string\">@param max_boxes=300: max boxes after apply this function</span></span><br><span class=\"line\"><span class=\"string\">@param overlap_thresh=0.9: threshold of overlapping</span></span><br><span class=\"line\"><span class=\"string\">@param C: Configuration</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">@return: max_boxes proposal with format (x1,y1,x2,y2)</span></span><br><span class=\"line\"><span class=\"string\">\"\"\"</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">rpn_to_roi</span><span class=\"params\">(rpn_layer, regr_layer, C, dim_ordering, use_regr=True, max_boxes=<span class=\"number\">300</span>,overlap_thresh=<span class=\"number\">0.9</span>)</span>:</span></span><br><span class=\"line\">\t<span class=\"comment\"># std_scaling default 4</span></span><br><span class=\"line\">\tregr_layer = regr_layer / C.std_scaling</span><br><span class=\"line\"></span><br><span class=\"line\">\tanchor_sizes = C.anchor_box_scales</span><br><span class=\"line\">\tanchor_ratios = C.anchor_box_ratios</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"keyword\">assert</span> rpn_layer.shape[<span class=\"number\">0</span>] == <span class=\"number\">1</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"comment\"># obtain img's width and height's matrix</span></span><br><span class=\"line\">\t(rows, cols) = rpn_layer.shape[<span class=\"number\">1</span>:<span class=\"number\">3</span>]</span><br><span class=\"line\"></span><br><span class=\"line\">\tcurr_layer = <span class=\"number\">0</span></span><br><span class=\"line\"></span><br><span class=\"line\">\tA = np.zeros((<span class=\"number\">4</span>, rpn_layer.shape[<span class=\"number\">1</span>], rpn_layer.shape[<span class=\"number\">2</span>], rpn_layer.shape[<span class=\"number\">3</span>]))</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"comment\"># anchor size is [128, 256, 512]</span></span><br><span class=\"line\">\t<span class=\"keyword\">for</span> anchor_size <span class=\"keyword\">in</span> anchor_sizes:</span><br><span class=\"line\">\t\t<span class=\"comment\"># anchor ratio is [1,2,1]</span></span><br><span class=\"line\">\t\t<span class=\"keyword\">for</span> anchor_ratio <span class=\"keyword\">in</span> anchor_ratios:</span><br><span class=\"line\">\t\t\t<span class=\"comment\"># rpn_stride = 16</span></span><br><span class=\"line\">\t\t\t<span class=\"comment\"># obatin anchor's weidth and height on feature map</span></span><br><span class=\"line\">\t\t\tanchor_x = (anchor_size * anchor_ratio[<span class=\"number\">0</span>])/C.rpn_stride</span><br><span class=\"line\">\t\t\tanchor_y = (anchor_size * anchor_ratio[<span class=\"number\">1</span>])/C.rpn_stride</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t<span class=\"comment\"># obtain current regrident</span></span><br><span class=\"line\">\t\t\t<span class=\"comment\"># when one dimentional obtain a value, the new varirant will decrease one dimenttion</span></span><br><span class=\"line\">\t\t\tregr = regr_layer[<span class=\"number\">0</span>, :, :, <span class=\"number\">4</span> * curr_layer:<span class=\"number\">4</span> * curr_layer + <span class=\"number\">4</span>]</span><br><span class=\"line\">\t\t\t<span class=\"comment\"># put depth to first bacause tensorflow as backend</span></span><br><span class=\"line\">\t\t\tregr = np.transpose(regr, (<span class=\"number\">2</span>, <span class=\"number\">0</span>, <span class=\"number\">1</span>))</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t<span class=\"comment\"># The rows of the output array X are copies of the vector x; columns of the output array Y are copies of the vector y</span></span><br><span class=\"line\">\t\t\t<span class=\"comment\"># each cordinartes of matrix cls and rows</span></span><br><span class=\"line\">\t\t\tX, Y = np.meshgrid(np.arange(cols),np. arange(rows))</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t<span class=\"comment\"># obtain anchors's (x,y,w,h)</span></span><br><span class=\"line\">\t\t\tA[<span class=\"number\">0</span>, :, :, curr_layer] = X - anchor_x/<span class=\"number\">2</span></span><br><span class=\"line\">\t\t\tA[<span class=\"number\">1</span>, :, :, curr_layer] = Y - anchor_y/<span class=\"number\">2</span></span><br><span class=\"line\">\t\t\tA[<span class=\"number\">2</span>, :, :, curr_layer] = anchor_x</span><br><span class=\"line\">\t\t\tA[<span class=\"number\">3</span>, :, :, curr_layer] = anchor_y</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t<span class=\"comment\"># fix boxes with grident</span></span><br><span class=\"line\">\t\t\t<span class=\"keyword\">if</span> use_regr:</span><br><span class=\"line\">\t\t\t\t<span class=\"comment\"># fixed corinates of box</span></span><br><span class=\"line\">\t\t\t\tA[:, :, :, curr_layer] = apply_regr_np(A[:, :, :, curr_layer], regr)</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t<span class=\"comment\"># fix unreasonable cordinates</span></span><br><span class=\"line\">\t\t\t<span class=\"comment\"># np.maximum(1,[]) will set the value less than 1 in [] to 1</span></span><br><span class=\"line\">\t\t\t<span class=\"comment\"># box's width and height can't less than 0</span></span><br><span class=\"line\">\t\t\tA[<span class=\"number\">2</span>, :, :, curr_layer] = np.maximum(<span class=\"number\">1</span>, A[<span class=\"number\">2</span>, :, :, curr_layer])</span><br><span class=\"line\">\t\t\tA[<span class=\"number\">3</span>, :, :, curr_layer] = np.maximum(<span class=\"number\">1</span>, A[<span class=\"number\">3</span>, :, :, curr_layer])</span><br><span class=\"line\">\t\t\t<span class=\"comment\"># fixed right bottom cordinates</span></span><br><span class=\"line\">\t\t\tA[<span class=\"number\">2</span>, :, :, curr_layer] += A[<span class=\"number\">0</span>, :, :, curr_layer]</span><br><span class=\"line\">\t\t\tA[<span class=\"number\">3</span>, :, :, curr_layer] += A[<span class=\"number\">1</span>, :, :, curr_layer]</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t<span class=\"comment\"># left top corner cordinates can't out image</span></span><br><span class=\"line\">\t\t\tA[<span class=\"number\">0</span>, :, :, curr_layer] = np.maximum(<span class=\"number\">0</span>, A[<span class=\"number\">0</span>, :, :, curr_layer])</span><br><span class=\"line\">\t\t\tA[<span class=\"number\">1</span>, :, :, curr_layer] = np.maximum(<span class=\"number\">0</span>, A[<span class=\"number\">1</span>, :, :, curr_layer])</span><br><span class=\"line\">\t\t\t<span class=\"comment\"># right bottom corner cordinates can't out img</span></span><br><span class=\"line\">\t\t\tA[<span class=\"number\">2</span>, :, :, curr_layer] = np.minimum(cols<span class=\"number\">-1</span>, A[<span class=\"number\">2</span>, :, :, curr_layer])</span><br><span class=\"line\">\t\t\tA[<span class=\"number\">3</span>, :, :, curr_layer] = np.minimum(rows<span class=\"number\">-1</span>, A[<span class=\"number\">3</span>, :, :, curr_layer])</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t<span class=\"comment\"># next layer</span></span><br><span class=\"line\">\t\t\tcurr_layer += <span class=\"number\">1</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"comment\"># obtain (n,4) object and coresspoding cordinate</span></span><br><span class=\"line\">\tall_boxes = np.reshape(A.transpose((<span class=\"number\">0</span>, <span class=\"number\">3</span>, <span class=\"number\">1</span>,<span class=\"number\">2</span>)), (<span class=\"number\">4</span>, <span class=\"number\">-1</span>)).transpose((<span class=\"number\">1</span>, <span class=\"number\">0</span>))</span><br><span class=\"line\">\t<span class=\"comment\"># obtain(n,) object and creoespdoing possibility</span></span><br><span class=\"line\">\tall_probs = rpn_layer.transpose((<span class=\"number\">0</span>, <span class=\"number\">3</span>, <span class=\"number\">1</span>, <span class=\"number\">2</span>)).reshape((<span class=\"number\">-1</span>))</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"comment\"># cordinates of left top and right bottom of box</span></span><br><span class=\"line\">\tx1 = all_boxes[:, <span class=\"number\">0</span>]</span><br><span class=\"line\">\ty1 = all_boxes[:, <span class=\"number\">1</span>]</span><br><span class=\"line\">\tx2 = all_boxes[:, <span class=\"number\">2</span>]</span><br><span class=\"line\">\ty2 = all_boxes[:, <span class=\"number\">3</span>]</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"comment\"># find where right cordinate bigger than left cordinate</span></span><br><span class=\"line\">\tidxs = np.where((x1 - x2 &gt;= <span class=\"number\">0</span>) | (y1 - y2 &gt;= <span class=\"number\">0</span>))</span><br><span class=\"line\">\t<span class=\"comment\"># delete thoese point at 0 dimentional -&gt; all boxes</span></span><br><span class=\"line\">\tall_boxes = np.delete(all_boxes, idxs, <span class=\"number\">0</span>)</span><br><span class=\"line\">\tall_probs = np.delete(all_probs, idxs, <span class=\"number\">0</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"comment\"># apply NMS to reduce overlapping boxes</span></span><br><span class=\"line\">\tresult = non_max_suppression_fast(all_boxes, all_probs, overlap_thresh=overlap_thresh, max_boxes=max_boxes)[<span class=\"number\">0</span>]</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"keyword\">return</span> result</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"【25-07-2018】\"><a href=\"#【25-07-2018】\" class=\"headerlink\" title=\"【25/07/2018】\"></a>【25/07/2018】</h3><h4 id=\"generate-classifier’s-trainning-data\"><a href=\"#generate-classifier’s-trainning-data\" class=\"headerlink\" title=\"generate classifier’s trainning data\"></a>generate classifier’s trainning data</h4><p>该函数的作用是生成classifier网络训练的数据,需要注意的是它对提供的预选框还会做一次选择就是将容易判断的背景删除</p>\n<p>代码流程：<br>得到图片的基本信息，并将图片的最短边规整到相应的长度。并将bboxes的长度做相应的变化</p>\n<p>遍历所有的预选框R, 将每一个预选框与所有的bboxes求交并比，记录最大交并比。用来确定该预选框的类别。</p>\n<p>对最佳的交并比作不同的判断:<br>当最佳交并比小于最小的阈值时，放弃概框。因为，交并比太低就说明是很好判断的背景没必要训练。当大于最小阈值时，则保留相关的边框信息<br>当在最小和最大之间，就认为是背景。有必要进行训练。<br>大于最大阈值时认为是物体，计算其边框回归梯度</p>\n<p>得到该类别对应的数字<br>将该数字对应的地方置为1【one-hot】<br>将该类别加入到y_class_num<br>coords是用来存储边框回归梯度的，labels来决定是否要加入计算loss中<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">class_num = <span class=\"number\">2</span></span><br><span class=\"line\">class_label = <span class=\"number\">10</span> * [<span class=\"number\">0</span>]</span><br><span class=\"line\">print(class_label)</span><br><span class=\"line\">class_label[class_num] = <span class=\"number\">1</span></span><br><span class=\"line\">print(class_label)</span><br><span class=\"line\">输出：</span><br><span class=\"line\">[<span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>]</span><br><span class=\"line\">[<span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">1</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>]</span><br></pre></td></tr></table></figure></p>\n<p>如果不是背景的话，计算相应的回归梯度</p>\n<p>返回数据</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br><span class=\"line\">107</span><br><span class=\"line\">108</span><br><span class=\"line\">109</span><br><span class=\"line\">110</span><br><span class=\"line\">111</span><br><span class=\"line\">112</span><br><span class=\"line\">113</span><br><span class=\"line\">114</span><br><span class=\"line\">115</span><br><span class=\"line\">116</span><br><span class=\"line\">117</span><br><span class=\"line\">118</span><br><span class=\"line\">119</span><br><span class=\"line\">120</span><br><span class=\"line\">121</span><br><span class=\"line\">122</span><br><span class=\"line\">123</span><br><span class=\"line\">124</span><br><span class=\"line\">125</span><br><span class=\"line\">126</span><br><span class=\"line\">127</span><br><span class=\"line\">128</span><br><span class=\"line\">129</span><br><span class=\"line\">130</span><br><span class=\"line\">131</span><br><span class=\"line\">132</span><br><span class=\"line\">133</span><br><span class=\"line\">134</span><br><span class=\"line\">135</span><br><span class=\"line\">136</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"string\">\"\"\" generate classifier training data</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">@param R: porposal -&gt; boxes</span></span><br><span class=\"line\"><span class=\"string\">@param img_data: image data</span></span><br><span class=\"line\"><span class=\"string\">@param C: configuration</span></span><br><span class=\"line\"><span class=\"string\">@param class_mapping: classes and coresspoding numbers</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">@return: np.expand_dims(X, axis=0): boxes after filter</span></span><br><span class=\"line\"><span class=\"string\">@return: np.expand_dims(Y1, axis=0): boxes coresspoding class</span></span><br><span class=\"line\"><span class=\"string\">@return: np.expand_dims(Y2, axis=0): boxes coresspoding regurident</span></span><br><span class=\"line\"><span class=\"string\">@return IoUs: IOU</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">calc_iou</span><span class=\"params\">(R, img_data, C, class_mapping)</span>:</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"comment\"># obtain boxxes information from img data</span></span><br><span class=\"line\">\tbboxes = img_data[<span class=\"string\">'bboxes'</span>]</span><br><span class=\"line\">\t<span class=\"comment\"># obtain width and height of img</span></span><br><span class=\"line\">\t(width, height) = (img_data[<span class=\"string\">'width'</span>], img_data[<span class=\"string\">'height'</span>])</span><br><span class=\"line\">\t<span class=\"comment\"># get image dimensions for resizing</span></span><br><span class=\"line\">\t<span class=\"comment\"># Fix image's shortest edge to config setting: eg: 600</span></span><br><span class=\"line\">\t(resized_width, resized_height) = get_new_img_size(width, height, C.im_size)</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"comment\"># record parameters, bboxes cordinates on feature map</span></span><br><span class=\"line\">\tgta = np.zeros((len(bboxes), <span class=\"number\">4</span>))</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"comment\"># change bboxes's width and height because the img was rezised</span></span><br><span class=\"line\">\t<span class=\"keyword\">for</span> bbox_num, bbox <span class=\"keyword\">in</span> enumerate(bboxes):</span><br><span class=\"line\">\t\t<span class=\"comment\"># get the GT box coordinates, and resize to account for image resizing</span></span><br><span class=\"line\">\t\t<span class=\"comment\"># /C.rpn_stride mapping to feature map</span></span><br><span class=\"line\">\t\tgta[bbox_num, <span class=\"number\">0</span>] = int(round(bbox[<span class=\"string\">'x1'</span>] * (resized_width / float(width))/C.rpn_stride))</span><br><span class=\"line\">\t\tgta[bbox_num, <span class=\"number\">1</span>] = int(round(bbox[<span class=\"string\">'x2'</span>] * (resized_width / float(width))/C.rpn_stride))</span><br><span class=\"line\">\t\tgta[bbox_num, <span class=\"number\">2</span>] = int(round(bbox[<span class=\"string\">'y1'</span>] * (resized_height / float(height))/C.rpn_stride))</span><br><span class=\"line\">\t\tgta[bbox_num, <span class=\"number\">3</span>] = int(round(bbox[<span class=\"string\">'y2'</span>] * (resized_height / float(height))/C.rpn_stride))</span><br><span class=\"line\"></span><br><span class=\"line\">\tx_roi = []</span><br><span class=\"line\">\ty_class_num = []</span><br><span class=\"line\">\ty_class_regr_coords = []</span><br><span class=\"line\">\ty_class_regr_label = []</span><br><span class=\"line\">\tIoUs = [] <span class=\"comment\"># for debugging only</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"comment\"># for all given proposals -&gt; boxes</span></span><br><span class=\"line\">\t<span class=\"keyword\">for</span> ix <span class=\"keyword\">in</span> range(R.shape[<span class=\"number\">0</span>]):</span><br><span class=\"line\">\t\t<span class=\"comment\"># current boxes's cordinates</span></span><br><span class=\"line\">\t\t(x1, y1, x2, y2) = R[ix, :]</span><br><span class=\"line\">\t\tx1 = int(round(x1))</span><br><span class=\"line\">\t\ty1 = int(round(y1))</span><br><span class=\"line\">\t\tx2 = int(round(x2))</span><br><span class=\"line\">\t\ty2 = int(round(y2))</span><br><span class=\"line\"></span><br><span class=\"line\">\t\tbest_iou = <span class=\"number\">0.0</span></span><br><span class=\"line\">\t\tbest_bbox = <span class=\"number\">-1</span></span><br><span class=\"line\">\t\t<span class=\"comment\"># using current proposal to compare with given xml's boxes</span></span><br><span class=\"line\">\t\t<span class=\"keyword\">for</span> bbox_num <span class=\"keyword\">in</span> range(len(bboxes)):</span><br><span class=\"line\">\t\t\t<span class=\"comment\"># calculate current iou</span></span><br><span class=\"line\">\t\t\tcurr_iou = iou([gta[bbox_num, <span class=\"number\">0</span>], gta[bbox_num, <span class=\"number\">2</span>], gta[bbox_num, <span class=\"number\">1</span>], gta[bbox_num, <span class=\"number\">3</span>]], [x1, y1, x2, y2])</span><br><span class=\"line\">\t\t\t<span class=\"comment\"># update parameters</span></span><br><span class=\"line\">\t\t\t<span class=\"keyword\">if</span> curr_iou &gt; best_iou:</span><br><span class=\"line\">\t\t\t\tbest_iou = curr_iou</span><br><span class=\"line\">\t\t\t\tbest_bbox = bbox_num</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"comment\"># if iou to small, we don't put it in trainning because it should be backgroud</span></span><br><span class=\"line\">\t\t<span class=\"keyword\">if</span> best_iou &lt; C.classifier_min_overlap:</span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">continue</span></span><br><span class=\"line\">\t\t<span class=\"keyword\">else</span>:</span><br><span class=\"line\">\t\t\t<span class=\"comment\"># saveing left top cordinates, width and height</span></span><br><span class=\"line\">\t\t\tw = x2 - x1</span><br><span class=\"line\">\t\t\th = y2 - y1</span><br><span class=\"line\">\t\t\tx_roi.append([x1, y1, w, h])</span><br><span class=\"line\">\t\t\t<span class=\"comment\"># saving this bbox's iou</span></span><br><span class=\"line\">\t\t\tIoUs.append(best_iou)</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t<span class=\"comment\"># hard to classfier -&gt; set it to backgroud</span></span><br><span class=\"line\">\t\t\t<span class=\"keyword\">if</span> C.classifier_min_overlap &lt;= best_iou &lt; C.classifier_max_overlap:</span><br><span class=\"line\">\t\t\t\t<span class=\"comment\"># hard negative example</span></span><br><span class=\"line\">\t\t\t\tcls_name = <span class=\"string\">'bg'</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t<span class=\"comment\"># valid proposal</span></span><br><span class=\"line\">\t\t\t<span class=\"keyword\">elif</span> C.classifier_max_overlap &lt;= best_iou:</span><br><span class=\"line\">\t\t\t\t<span class=\"comment\"># coresspoding class name</span></span><br><span class=\"line\">\t\t\t\tcls_name = bboxes[best_bbox][<span class=\"string\">'class'</span>]</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t<span class=\"comment\"># calculate rpn graident with true cordinates given by xml file</span></span><br><span class=\"line\">\t\t\t\tcxg = (gta[best_bbox, <span class=\"number\">0</span>] + gta[best_bbox, <span class=\"number\">1</span>]) / <span class=\"number\">2.0</span></span><br><span class=\"line\">\t\t\t\tcyg = (gta[best_bbox, <span class=\"number\">2</span>] + gta[best_bbox, <span class=\"number\">3</span>]) / <span class=\"number\">2.0</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\tcx = x1 + w / <span class=\"number\">2.0</span></span><br><span class=\"line\">\t\t\t\tcy = y1 + h / <span class=\"number\">2.0</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\ttx = (cxg - cx) / float(w)</span><br><span class=\"line\">\t\t\t\tty = (cyg - cy) / float(h)</span><br><span class=\"line\">\t\t\t\ttw = np.log((gta[best_bbox, <span class=\"number\">1</span>] - gta[best_bbox, <span class=\"number\">0</span>]) / float(w))</span><br><span class=\"line\">\t\t\t\tth = np.log((gta[best_bbox, <span class=\"number\">3</span>] - gta[best_bbox, <span class=\"number\">2</span>]) / float(h))</span><br><span class=\"line\">\t\t\t<span class=\"keyword\">else</span>:</span><br><span class=\"line\">\t\t\t\tprint(<span class=\"string\">'roi = &#123;&#125;'</span>.format(best_iou))</span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">raise</span> RuntimeError</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"comment\"># class name's mapping number</span></span><br><span class=\"line\">\t\tclass_num = class_mapping[cls_name]</span><br><span class=\"line\">\t\t<span class=\"comment\"># list of calss label</span></span><br><span class=\"line\">\t\tclass_label = len(class_mapping) * [<span class=\"number\">0</span>]</span><br><span class=\"line\">\t\t<span class=\"comment\"># set class_num's coresspoding location to 1</span></span><br><span class=\"line\">\t\tclass_label[class_num] = <span class=\"number\">1</span></span><br><span class=\"line\">\t\t<span class=\"comment\"># privous is one-hot vector</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"comment\"># saving the one-hot vector</span></span><br><span class=\"line\">\t\ty_class_num.append(copy.deepcopy(class_label))</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"comment\"># coords used to saving calculated graident</span></span><br><span class=\"line\">\t\tcoords = [<span class=\"number\">0</span>] * <span class=\"number\">4</span> * (len(class_mapping) - <span class=\"number\">1</span>)</span><br><span class=\"line\">\t\t<span class=\"comment\"># labels used to decide whether adding to loss calculation</span></span><br><span class=\"line\">\t\tlabels = [<span class=\"number\">0</span>] * <span class=\"number\">4</span> * (len(class_mapping) - <span class=\"number\">1</span>)</span><br><span class=\"line\">\t\t<span class=\"keyword\">if</span> cls_name != <span class=\"string\">'bg'</span>:</span><br><span class=\"line\">\t\t\tlabel_pos = <span class=\"number\">4</span> * class_num</span><br><span class=\"line\">\t\t\tsx, sy, sw, sh = C.classifier_regr_std</span><br><span class=\"line\">\t\t\tcoords[label_pos:<span class=\"number\">4</span>+label_pos] = [sx*tx, sy*ty, sw*tw, sh*th]</span><br><span class=\"line\">\t\t\tlabels[label_pos:<span class=\"number\">4</span>+label_pos] = [<span class=\"number\">1</span>, <span class=\"number\">1</span>, <span class=\"number\">1</span>, <span class=\"number\">1</span>]</span><br><span class=\"line\">\t\t\ty_class_regr_coords.append(copy.deepcopy(coords))</span><br><span class=\"line\">\t\t\ty_class_regr_label.append(copy.deepcopy(labels))</span><br><span class=\"line\">\t\t<span class=\"keyword\">else</span>:</span><br><span class=\"line\">\t\t\ty_class_regr_coords.append(copy.deepcopy(coords))</span><br><span class=\"line\">\t\t\ty_class_regr_label.append(copy.deepcopy(labels))</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"comment\"># no bboxes</span></span><br><span class=\"line\">\t<span class=\"keyword\">if</span> len(x_roi) == <span class=\"number\">0</span>:</span><br><span class=\"line\">\t\t<span class=\"keyword\">return</span> <span class=\"keyword\">None</span>, <span class=\"keyword\">None</span>, <span class=\"keyword\">None</span>, <span class=\"keyword\">None</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"comment\"># matrix with [x1, y1, w, h]</span></span><br><span class=\"line\">\tX = np.array(x_roi)</span><br><span class=\"line\">\t<span class=\"comment\"># boxxes coresspoding class number</span></span><br><span class=\"line\">\tY1 = np.array(y_class_num)</span><br><span class=\"line\">\t<span class=\"comment\"># matrix of whether adding to calculation and coresspoding regrident</span></span><br><span class=\"line\">\tY2 = np.concatenate([np.array(y_class_regr_label),np.array(y_class_regr_coords)],axis=<span class=\"number\">1</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"comment\"># adding batch size dimention</span></span><br><span class=\"line\">\t<span class=\"keyword\">return</span> np.expand_dims(X, axis=<span class=\"number\">0</span>), np.expand_dims(Y1, axis=<span class=\"number\">0</span>), np.expand_dims(Y2, axis=<span class=\"number\">0</span>), IoUs</span><br></pre></td></tr></table></figure>\n<h3 id=\"【26-27-07-2018】\"><a href=\"#【26-27-07-2018】\" class=\"headerlink\" title=\"【26~27/07/2018】\"></a>【26~27/07/2018】</h3><h4 id=\"model-parameters\"><a href=\"#model-parameters\" class=\"headerlink\" title=\"model parameters\"></a>model parameters</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># rpn optimizer</span></span><br><span class=\"line\">optimizer = Adam(lr=<span class=\"number\">1e-5</span>)</span><br><span class=\"line\"><span class=\"comment\"># classifier optimizer</span></span><br><span class=\"line\">optimizer_classifier = Adam(lr=<span class=\"number\">1e-5</span>)</span><br><span class=\"line\"><span class=\"comment\"># defined loss apply, metrics used to print accury</span></span><br><span class=\"line\">model_rpn.compile(optimizer=optimizer, loss=[losses.rpn_loss_cls(num_anchors), losses.rpn_loss_regr(num_anchors)])</span><br><span class=\"line\">model_classifier.compile(optimizer=optimizer_classifier, loss=[losses.class_loss_cls, losses.class_loss_regr(len(classes_count)<span class=\"number\">-1</span>)], metrics=&#123;<span class=\"string\">'dense_class_&#123;&#125;'</span>.format(len(classes_count)): <span class=\"string\">'accuracy'</span>&#125;)</span><br><span class=\"line\"><span class=\"comment\"># for saving weight</span></span><br><span class=\"line\">model_all.compile(optimizer=<span class=\"string\">'sgd'</span>, loss=<span class=\"string\">'mae'</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># traing time of each epochs</span></span><br><span class=\"line\">epoch_length = <span class=\"number\">1000</span></span><br><span class=\"line\"><span class=\"comment\"># totoal epochs</span></span><br><span class=\"line\">num_epochs = <span class=\"number\">2000</span></span><br><span class=\"line\"><span class=\"comment\">#</span></span><br><span class=\"line\">iter_num = <span class=\"number\">0</span></span><br><span class=\"line\"><span class=\"comment\"># losses saving matrix</span></span><br><span class=\"line\">losses = np.zeros((epoch_length, <span class=\"number\">5</span>))</span><br><span class=\"line\">rpn_accuracy_rpn_monitor = []</span><br><span class=\"line\">rpn_accuracy_for_epoch = []</span><br><span class=\"line\">start_time = time.time()</span><br><span class=\"line\"><span class=\"comment\"># current total loss</span></span><br><span class=\"line\">best_loss = np.Inf</span><br><span class=\"line\"><span class=\"comment\"># sorted classing mapping</span></span><br><span class=\"line\">class_mapping_inv = &#123;v: k <span class=\"keyword\">for</span> k, v <span class=\"keyword\">in</span> class_mapping.items()&#125;</span><br></pre></td></tr></table></figure>\n<h4 id=\"Training-process\"><a href=\"#Training-process\" class=\"headerlink\" title=\"Training process\"></a>Training process</h4><p>函数流程：<br><strong>训练rpn网络并且进行预测：</strong><br>训练RPN网络,X是图片、Y是对应类别和回归梯度【注：并不是所有的点都参与训练，只有符合条件的点才参与训练】</p>\n<p><strong>根据rpn网络的预测结果得到classifier网络的训练数据:</strong><br>将预测结果转化为预选框<br>计算宽属于哪一类，回归梯度是多少<br>如果没有有效的预选框则结束本次循环<br>得到正负样本在的位置【Y1[0, :, -1]：0指定batch的位置，：指所有框，-1指最后一个维度即背景类】<br>neg_samples = neg_samples[0]：这样做的原因是将其变为一维的数组<br>下面这一步是选择C.num_rois个数的框，送入classifier网络进行训练。思路是：当C.num_rois大于1的时候正负样本尽量取到各一半，小于1的时候正负样本随机取一个。需要注意的是我们这是拿到的是正负样本在的位置而不是正负样本本身，这也是随机抽取的一般方法</p>\n<p><strong>训练classifier网络:</strong><br>打印Loss和accury<br>如果网络有两个不同的输出，那么第一个是和损失接下来是分损失【loss_class[3]：代表是准确率在定义网络的时候定义了】<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">classifer网络的loss输出：</span><br><span class=\"line\">[<span class=\"number\">1.4640709</span>, <span class=\"number\">1.0986123</span>, <span class=\"number\">0.36545864</span>, <span class=\"number\">0.15625</span>]</span><br></pre></td></tr></table></figure></p>\n<p>还有就是这些loss都是list数据类型，所以要把它倒腾到numpy数据中<br>当结束一轮的epoch时，只有当这轮epoch的loss小于最优的时候才会存储这轮的训练数据。并结束这轮epoch进入下一轮epoch.</p>\n<hr>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br><span class=\"line\">107</span><br><span class=\"line\">108</span><br><span class=\"line\">109</span><br><span class=\"line\">110</span><br><span class=\"line\">111</span><br><span class=\"line\">112</span><br><span class=\"line\">113</span><br><span class=\"line\">114</span><br><span class=\"line\">115</span><br><span class=\"line\">116</span><br><span class=\"line\">117</span><br><span class=\"line\">118</span><br><span class=\"line\">119</span><br><span class=\"line\">120</span><br><span class=\"line\">121</span><br><span class=\"line\">122</span><br><span class=\"line\">123</span><br><span class=\"line\">124</span><br><span class=\"line\">125</span><br><span class=\"line\">126</span><br><span class=\"line\">127</span><br><span class=\"line\">128</span><br><span class=\"line\">129</span><br><span class=\"line\">130</span><br><span class=\"line\">131</span><br><span class=\"line\">132</span><br><span class=\"line\">133</span><br><span class=\"line\">134</span><br><span class=\"line\">135</span><br><span class=\"line\">136</span><br><span class=\"line\">137</span><br><span class=\"line\">138</span><br><span class=\"line\">139</span><br><span class=\"line\">140</span><br><span class=\"line\">141</span><br><span class=\"line\">142</span><br><span class=\"line\">143</span><br><span class=\"line\">144</span><br><span class=\"line\">145</span><br><span class=\"line\">146</span><br><span class=\"line\">147</span><br><span class=\"line\">148</span><br><span class=\"line\">149</span><br><span class=\"line\">150</span><br><span class=\"line\">151</span><br><span class=\"line\">152</span><br><span class=\"line\">153</span><br><span class=\"line\">154</span><br><span class=\"line\">155</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># Training Process</span></span><br><span class=\"line\">print(<span class=\"string\">'Starting training'</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">for</span> epoch_num <span class=\"keyword\">in</span> range(num_epochs):</span><br><span class=\"line\">    <span class=\"comment\">#progbar is used to print % of processing</span></span><br><span class=\"line\">\tprogbar = generic_utils.Progbar(epoch_length)</span><br><span class=\"line\">    <span class=\"comment\"># print current process</span></span><br><span class=\"line\">\tprint(<span class=\"string\">'Epoch &#123;&#125;/&#123;&#125;'</span>.format(epoch_num + <span class=\"number\">1</span>, num_epochs))</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"keyword\">while</span> <span class=\"keyword\">True</span>:</span><br><span class=\"line\">\t\t<span class=\"keyword\">try</span>:</span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"comment\"># verbose True to print RPN situation, if can't generate boxes on positivate object, it will print error</span></span><br><span class=\"line\">\t\t\t<span class=\"keyword\">if</span> len(rpn_accuracy_rpn_monitor) == epoch_length <span class=\"keyword\">and</span> C.verbose:</span><br><span class=\"line\">                <span class=\"comment\"># postivate boxes / all boxes</span></span><br><span class=\"line\">\t\t\t\tmean_overlapping_bboxes = float(sum(rpn_accuracy_rpn_monitor))/len(rpn_accuracy_rpn_monitor)</span><br><span class=\"line\">\t\t\t\trpn_accuracy_rpn_monitor = []</span><br><span class=\"line\">\t\t\t\tprint(<span class=\"string\">'Average number of overlapping bounding boxes from RPN = &#123;&#125; for &#123;&#125; previous iterations'</span>.format(mean_overlapping_bboxes, epoch_length))</span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">if</span> mean_overlapping_bboxes == <span class=\"number\">0</span>:</span><br><span class=\"line\">\t\t\t\t\tprint(<span class=\"string\">'RPN is not producing bounding boxes that overlap the ground truth boxes. Check RPN settings or keep training.'</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"comment\"># obtain img, rpn information and img xml format</span></span><br><span class=\"line\">\t\t\tX, Y, img_data = next(data_gen_train)</span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"comment\"># train RPN net, X is img, Y is correspoding class type and graident</span></span><br><span class=\"line\">\t\t\tloss_rpn = model_rpn.train_on_batch(X, Y)</span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"comment\"># predict new Y from privious rpn model</span></span><br><span class=\"line\">\t\t\tP_rpn = model_rpn.predict_on_batch(X)</span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"comment\"># transform predicted rpn to cordinates of boxes</span></span><br><span class=\"line\">\t\t\tR = rpn_to_boxes.rpn_to_roi(P_rpn[<span class=\"number\">0</span>], P_rpn[<span class=\"number\">1</span>], C, K.image_dim_ordering(), use_regr=<span class=\"keyword\">True</span>, overlap_thresh=<span class=\"number\">0.7</span>, max_boxes=<span class=\"number\">300</span>)</span><br><span class=\"line\">\t\t\t<span class=\"comment\"># note: calc_iou converts from (x1,y1,x2,y2) to (x,y,w,h) format</span></span><br><span class=\"line\">            <span class=\"comment\"># X2: [x,y,w,h]</span></span><br><span class=\"line\">            <span class=\"comment\"># Y1: coresspoding class number -&gt; one hot vector</span></span><br><span class=\"line\">            <span class=\"comment\"># Y2: boxes coresspoding regrident</span></span><br><span class=\"line\">\t\t\tX2, Y1, Y2, IouS = rpn_to_classifier.calc_iou(R, img_data, C, class_mapping)</span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"comment\"># no box, stop this epoch</span></span><br><span class=\"line\">\t\t\t<span class=\"keyword\">if</span> X2 <span class=\"keyword\">is</span> <span class=\"keyword\">None</span>:</span><br><span class=\"line\">\t\t\t\trpn_accuracy_rpn_monitor.append(<span class=\"number\">0</span>)</span><br><span class=\"line\">\t\t\t\trpn_accuracy_for_epoch.append(<span class=\"number\">0</span>)</span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">continue</span></span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"comment\"># if last position of one-hot is 1 -&gt; is background</span></span><br><span class=\"line\">\t\t\tneg_samples = np.where(Y1[<span class=\"number\">0</span>, :, <span class=\"number\">-1</span>] == <span class=\"number\">1</span>)</span><br><span class=\"line\">            <span class=\"comment\"># else is postivate sample</span></span><br><span class=\"line\">\t\t\tpos_samples = np.where(Y1[<span class=\"number\">0</span>, :, <span class=\"number\">-1</span>] == <span class=\"number\">0</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"comment\"># obtain backgourd samples's coresspoding rows</span></span><br><span class=\"line\">\t\t\t<span class=\"keyword\">if</span> len(neg_samples) &gt; <span class=\"number\">0</span>:</span><br><span class=\"line\">\t\t\t\tneg_samples = neg_samples[<span class=\"number\">0</span>]</span><br><span class=\"line\">\t\t\t<span class=\"keyword\">else</span>:</span><br><span class=\"line\">\t\t\t\tneg_samples = []</span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"comment\"># obtain posivate samples's coresspoding rows</span></span><br><span class=\"line\">\t\t\t<span class=\"keyword\">if</span> len(pos_samples) &gt; <span class=\"number\">0</span>:</span><br><span class=\"line\">\t\t\t\tpos_samples = pos_samples[<span class=\"number\">0</span>]</span><br><span class=\"line\">\t\t\t<span class=\"keyword\">else</span>:</span><br><span class=\"line\">\t\t\t\tpos_samples = []</span><br><span class=\"line\">\t\t\t<span class=\"comment\"># saving posivate samples's number</span></span><br><span class=\"line\">\t\t\trpn_accuracy_rpn_monitor.append(len(pos_samples))</span><br><span class=\"line\">\t\t\trpn_accuracy_for_epoch.append((len(pos_samples)))</span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"comment\"># default 4 here</span></span><br><span class=\"line\">\t\t\t<span class=\"keyword\">if</span> C.num_rois &gt; <span class=\"number\">1</span>:</span><br><span class=\"line\">                <span class=\"comment\"># wehn postivate samples less than 2</span></span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">if</span> len(pos_samples) &lt; C.num_rois//<span class=\"number\">2</span>:</span><br><span class=\"line\">                    <span class=\"comment\"># chosse all samples</span></span><br><span class=\"line\">\t\t\t\t\tselected_pos_samples = pos_samples.tolist()</span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">else</span>:</span><br><span class=\"line\">                    <span class=\"comment\"># random choose 2 samples</span></span><br><span class=\"line\">\t\t\t\t\tselected_pos_samples = np.random.choice(pos_samples, C.num_rois//<span class=\"number\">2</span>, replace=<span class=\"keyword\">False</span>).tolist()</span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">try</span>:</span><br><span class=\"line\">                    <span class=\"comment\"># random choose num_rois - positave samples naegivate samples</span></span><br><span class=\"line\">\t\t\t\t\tselected_neg_samples = np.random.choice(neg_samples, C.num_rois - len(selected_pos_samples), replace=<span class=\"keyword\">False</span>).tolist()</span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">except</span>:</span><br><span class=\"line\">                    <span class=\"comment\"># if no enought neg samples, copy priouvs neg sample</span></span><br><span class=\"line\">\t\t\t\t\tselected_neg_samples = np.random.choice(neg_samples, C.num_rois - len(selected_pos_samples), replace=<span class=\"keyword\">True</span>).tolist()</span><br><span class=\"line\"></span><br><span class=\"line\">                <span class=\"comment\"># samples picked to classifier network</span></span><br><span class=\"line\">\t\t\t\tsel_samples = selected_pos_samples + selected_neg_samples</span><br><span class=\"line\">\t\t\t<span class=\"keyword\">else</span>:</span><br><span class=\"line\">\t\t\t\t<span class=\"comment\"># in the extreme case where num_rois = 1, we pick a random pos or neg sample</span></span><br><span class=\"line\">\t\t\t\tselected_pos_samples = pos_samples.tolist()</span><br><span class=\"line\">\t\t\t\tselected_neg_samples = neg_samples.tolist()</span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">if</span> np.random.randint(<span class=\"number\">0</span>, <span class=\"number\">2</span>):</span><br><span class=\"line\">\t\t\t\t\tsel_samples = random.choice(neg_samples)</span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">else</span>:</span><br><span class=\"line\">\t\t\t\t\tsel_samples = random.choice(pos_samples)</span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"comment\"># train classifier, img data, selceted samples' cordinates, mapping number of selected samples, coresspoding regreident</span></span><br><span class=\"line\">\t\t\tloss_class = model_classifier.train_on_batch([X, X2[:, sel_samples, :]], [Y1[:, sel_samples, :], Y2[:, sel_samples, :]])</span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"comment\"># in Keras, if loss part bigger than 1, it will return sum of part losses, each loss and accury</span></span><br><span class=\"line\">            <span class=\"comment\"># put each losses and accury into losses</span></span><br><span class=\"line\">\t\t\tlosses[iter_num, <span class=\"number\">0</span>] = loss_rpn[<span class=\"number\">1</span>]</span><br><span class=\"line\">\t\t\tlosses[iter_num, <span class=\"number\">1</span>] = loss_rpn[<span class=\"number\">2</span>]</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\tlosses[iter_num, <span class=\"number\">2</span>] = loss_class[<span class=\"number\">1</span>]</span><br><span class=\"line\">\t\t\tlosses[iter_num, <span class=\"number\">3</span>] = loss_class[<span class=\"number\">2</span>]</span><br><span class=\"line\">\t\t\tlosses[iter_num, <span class=\"number\">4</span>] = loss_class[<span class=\"number\">3</span>]</span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"comment\"># next iter</span></span><br><span class=\"line\">\t\t\titer_num += <span class=\"number\">1</span></span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"comment\"># display and update current mean value of losses</span></span><br><span class=\"line\">\t\t\tprogbar.update(iter_num, [(<span class=\"string\">'rpn_cls'</span>, np.mean(losses[:iter_num, <span class=\"number\">0</span>])), (<span class=\"string\">'rpn_regr'</span>, np.mean(losses[:iter_num, <span class=\"number\">1</span>])),</span><br><span class=\"line\">\t\t\t\t\t\t\t\t\t  (<span class=\"string\">'detector_cls'</span>, np.mean(losses[:iter_num, <span class=\"number\">2</span>])), (<span class=\"string\">'detector_regr'</span>, np.mean(losses[:iter_num, <span class=\"number\">3</span>]))])</span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"comment\"># reach epoch_length</span></span><br><span class=\"line\">\t\t\t<span class=\"keyword\">if</span> iter_num == epoch_length:</span><br><span class=\"line\">\t\t\t\tloss_rpn_cls = np.mean(losses[:, <span class=\"number\">0</span>])</span><br><span class=\"line\">\t\t\t\tloss_rpn_regr = np.mean(losses[:, <span class=\"number\">1</span>])</span><br><span class=\"line\">\t\t\t\tloss_class_cls = np.mean(losses[:, <span class=\"number\">2</span>])</span><br><span class=\"line\">\t\t\t\tloss_class_regr = np.mean(losses[:, <span class=\"number\">3</span>])</span><br><span class=\"line\">\t\t\t\tclass_acc = np.mean(losses[:, <span class=\"number\">4</span>])</span><br><span class=\"line\"></span><br><span class=\"line\">                <span class=\"comment\"># negativate samples / all samples</span></span><br><span class=\"line\">\t\t\t\tmean_overlapping_bboxes = float(sum(rpn_accuracy_for_epoch)) / len(rpn_accuracy_for_epoch)</span><br><span class=\"line\">                <span class=\"comment\"># reset</span></span><br><span class=\"line\">\t\t\t\trpn_accuracy_for_epoch = []</span><br><span class=\"line\"></span><br><span class=\"line\">                <span class=\"comment\"># print trainning loss and accrury</span></span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">if</span> C.verbose:</span><br><span class=\"line\">\t\t\t\t\tprint(<span class=\"string\">'Mean number of bounding boxes from RPN overlapping ground truth boxes: &#123;&#125;'</span>.format(mean_overlapping_bboxes))</span><br><span class=\"line\">\t\t\t\t\tprint(<span class=\"string\">'Classifier accuracy for bounding boxes from RPN: &#123;&#125;'</span>.format(class_acc))</span><br><span class=\"line\">\t\t\t\t\tprint(<span class=\"string\">'Loss RPN classifier: &#123;&#125;'</span>.format(loss_rpn_cls))</span><br><span class=\"line\">\t\t\t\t\tprint(<span class=\"string\">'Loss RPN regression: &#123;&#125;'</span>.format(loss_rpn_regr))</span><br><span class=\"line\">\t\t\t\t\tprint(<span class=\"string\">'Loss Detector classifier: &#123;&#125;'</span>.format(loss_class_cls))</span><br><span class=\"line\">\t\t\t\t\tprint(<span class=\"string\">'Loss Detector regression: &#123;&#125;'</span>.format(loss_class_regr))</span><br><span class=\"line\">                    <span class=\"comment\"># trainng time of one epoch</span></span><br><span class=\"line\">\t\t\t\t\tprint(<span class=\"string\">'Elapsed time: &#123;&#125;'</span>.format(time.time() - start_time))</span><br><span class=\"line\">                    </span><br><span class=\"line\">                <span class=\"comment\"># total loss</span></span><br><span class=\"line\">\t\t\t\tcurr_loss = loss_rpn_cls + loss_rpn_regr + loss_class_cls + loss_class_regr</span><br><span class=\"line\">                <span class=\"comment\"># reset</span></span><br><span class=\"line\">\t\t\t\titer_num = <span class=\"number\">0</span></span><br><span class=\"line\">                <span class=\"comment\"># reset time</span></span><br><span class=\"line\">\t\t\t\tstart_time = time.time()</span><br><span class=\"line\"></span><br><span class=\"line\">                <span class=\"comment\"># if obtain smaller total loss, save weight of current model</span></span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">if</span> curr_loss &lt; best_loss:</span><br><span class=\"line\">\t\t\t\t\t<span class=\"keyword\">if</span> C.verbose:</span><br><span class=\"line\">\t\t\t\t\t\tprint(<span class=\"string\">'Total loss decreased from &#123;&#125; to &#123;&#125;, saving weights'</span>.format(best_loss,curr_loss))</span><br><span class=\"line\">\t\t\t\t\tbest_loss = curr_loss</span><br><span class=\"line\">\t\t\t\t\tmodel_all.save_weights(C.model_path)</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">break</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">except</span> Exception <span class=\"keyword\">as</span> e:</span><br><span class=\"line\">\t\t\tprint(<span class=\"string\">'Exception: &#123;&#125;'</span>.format(e))</span><br><span class=\"line\">\t\t\t<span class=\"keyword\">continue</span></span><br><span class=\"line\"></span><br><span class=\"line\">print(<span class=\"string\">'Training complete, exiting.'</span>)</span><br></pre></td></tr></table></figure>\n<p><a href=\"https://github.com/Trouble404/Object-Detection-System-Based-on-CNN-and-Capsule-Network/blob/master/Faster-RCNN/workflow_train.ipynb\" target=\"_blank\" rel=\"noopener\">jupyter notebook</a></p>\n<h3 id=\"【30-07-2018】\"><a href=\"#【30-07-2018】\" class=\"headerlink\" title=\"【30/07/2018】\"></a>【30/07/2018】</h3><h4 id=\"Running-at-GPU-enviorment\"><a href=\"#Running-at-GPU-enviorment\" class=\"headerlink\" title=\"Running at GPU enviorment\"></a>Running at GPU enviorment</h4><p><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/9_1.JPG\" alt=\"image\"><br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/9_2.JPG\" alt=\"image\"><br>Meet error in GPU version tensorflow<br>No enough memory.</p>\n<p>Try to Running at Irius:</p>\n<p>Setting 3 differnet configration:<br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/9_3.JPG\" alt=\"image\"><br>at Prjoect1 file:<br>set epoch_length to number of training img<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">epoch_length = <span class=\"number\">11540</span></span><br><span class=\"line\">num_epochs = <span class=\"number\">100</span></span><br></pre></td></tr></table></figure></p>\n<p>Apply img enhance function<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">C.use_horizontal_flips = <span class=\"keyword\">True</span></span><br><span class=\"line\">C.use_vertical_flips = <span class=\"keyword\">True</span></span><br><span class=\"line\">C.rot_90 = <span class=\"keyword\">True</span></span><br></pre></td></tr></table></figure></p>\n<hr>\n<p>at Prjoect file:<br>set epoch_length to 1000, increase epoch<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">epoch_length = <span class=\"number\">1000</span></span><br><span class=\"line\">num_epochs = <span class=\"number\">2000</span></span><br></pre></td></tr></table></figure></p>\n<p>Apply img enhance and class balance function<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">C.use_horizontal_flips = <span class=\"keyword\">True</span></span><br><span class=\"line\">C.use_vertical_flips = <span class=\"keyword\">True</span></span><br><span class=\"line\">C.rot_90 = <span class=\"keyword\">True</span></span><br><span class=\"line\">C.balanced_classes = <span class=\"keyword\">True</span></span><br></pre></td></tr></table></figure></p>\n<hr>\n<p>at Prjoect3 file:<br>set epoch_length to 1000, increase epoch<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">epoch_length = <span class=\"number\">1000</span></span><br><span class=\"line\">num_epochs = <span class=\"number\">2000</span></span><br></pre></td></tr></table></figure></p>\n<p>Apply img enhance<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">C.use_horizontal_flips = <span class=\"keyword\">True</span></span><br><span class=\"line\">C.use_vertical_flips = <span class=\"keyword\">True</span></span><br><span class=\"line\">C.rot_90 = <span class=\"keyword\">True</span></span><br></pre></td></tr></table></figure></p>\n<hr>\n<h4 id=\"check-irdius-work\"><a href=\"#check-irdius-work\" class=\"headerlink\" title=\"check irdius work\"></a>check irdius work</h4><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">myqueue</span><br></pre></td></tr></table></figure>\n<p><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/9_4.JPG\" alt=\"image\"></p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ssh pink59</span><br><span class=\"line\">nvidia-smi</span><br></pre></td></tr></table></figure>\n<p><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/9_5.JPG\" alt=\"image\"></p>\n<h3 id=\"【31-07-2018】\"><a href=\"#【31-07-2018】\" class=\"headerlink\" title=\"【31/07/2018】\"></a>【31/07/2018】</h3><h4 id=\"obtain-trained-model-and-log-file\"><a href=\"#obtain-trained-model-and-log-file\" class=\"headerlink\" title=\"obtain trained model and log file\"></a>obtain trained model and log file</h4><p>因为 Iriuds 的GPU使用时长限制最高为24小时，因此，需要在下一次开始前载入上一次训练的模型。<br>每次训练的粗略结果更新在LogBook最前面.</p>\n<h4 id=\"plot-rpn-and-classfier-loss\"><a href=\"#plot-rpn-and-classfier-loss\" class=\"headerlink\" title=\"plot rpn and classfier loss\"></a>plot rpn and classfier loss</h4><p>获取日志中每个小epoch的rpn_cls, rpn_regr, detc_cls, detc_regr<br>遍历日志，用正则匹配出相应的数值添加到List中：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">obtain_each_batch</span><span class=\"params\">(filename)</span>:</span></span><br><span class=\"line\">    n = <span class=\"number\">0</span></span><br><span class=\"line\">    rpn_cls = []</span><br><span class=\"line\">    rpn_regr = []</span><br><span class=\"line\">    detector_cls = []</span><br><span class=\"line\">    detector_regr = []</span><br><span class=\"line\">    f = open(filename,<span class=\"string\">'r'</span>,buffering=<span class=\"number\">-1</span>)</span><br><span class=\"line\">    lines = f.readlines()</span><br><span class=\"line\">    <span class=\"keyword\">for</span> line <span class=\"keyword\">in</span> lines:</span><br><span class=\"line\">        n = n + <span class=\"number\">1</span></span><br><span class=\"line\">        match = re.match(<span class=\"string\">r'.* - rpn_cls: (.*) - rpn_regr: .*'</span>, line, re.M|re.I)</span><br><span class=\"line\">        <span class=\"keyword\">if</span> match <span class=\"keyword\">is</span> <span class=\"keyword\">None</span>:</span><br><span class=\"line\">            <span class=\"keyword\">continue</span></span><br><span class=\"line\">        <span class=\"keyword\">else</span>: </span><br><span class=\"line\">            rpn_cls.append(float(match.group(<span class=\"number\">1</span>)))</span><br><span class=\"line\"></span><br><span class=\"line\">        match = re.match(<span class=\"string\">r'.* - rpn_regr: (.*) - detector_cls: .*'</span>, line, re.M|re.I)</span><br><span class=\"line\">        <span class=\"keyword\">if</span> match <span class=\"keyword\">is</span> <span class=\"keyword\">None</span>:</span><br><span class=\"line\">            <span class=\"keyword\">continue</span></span><br><span class=\"line\">        <span class=\"keyword\">else</span>: </span><br><span class=\"line\">            rpn_regr.append(float(match.group(<span class=\"number\">1</span>)))            </span><br><span class=\"line\">            </span><br><span class=\"line\">        match = re.match(<span class=\"string\">r'.* - detector_cls: (.*) - detector_regr: .*'</span>, line, re.M|re.I)</span><br><span class=\"line\">        <span class=\"keyword\">if</span> match <span class=\"keyword\">is</span> <span class=\"keyword\">None</span>:</span><br><span class=\"line\">            <span class=\"keyword\">continue</span></span><br><span class=\"line\">        <span class=\"keyword\">else</span>: </span><br><span class=\"line\">            detector_cls.append(float(match.group(<span class=\"number\">1</span>))) </span><br><span class=\"line\">            </span><br><span class=\"line\">        match = re.match(<span class=\"string\">r'.* - detector_regr: (.*)\\n'</span>, line, re.M|re.I)</span><br><span class=\"line\">        <span class=\"keyword\">if</span> match <span class=\"keyword\">is</span> <span class=\"keyword\">None</span>:</span><br><span class=\"line\">            <span class=\"keyword\">continue</span></span><br><span class=\"line\">        <span class=\"keyword\">else</span>: </span><br><span class=\"line\">            det_regr = match.group(<span class=\"number\">1</span>)[<span class=\"number\">0</span>:<span class=\"number\">6</span>]</span><br><span class=\"line\">            detector_regr.append(float(det_regr))</span><br><span class=\"line\"></span><br><span class=\"line\">    f.close()</span><br><span class=\"line\">    print(n)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> rpn_cls, rpn_regr, detector_cls, detector_regr</span><br></pre></td></tr></table></figure></p>\n<p>每个epoch都会计算accury, loss of rpn cls, loss of rpn regr, loss of detc cls, loss of detc regr<br>遍历日志找到相应的数值添加到list中：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">obtain_batch</span><span class=\"params\">(filename)</span>:</span></span><br><span class=\"line\">    n = <span class=\"number\">0</span></span><br><span class=\"line\">    accuracy = []</span><br><span class=\"line\">    loss_rpn_cls = []</span><br><span class=\"line\">    loss_rpn_regr = []</span><br><span class=\"line\">    loss_detc_cls = []</span><br><span class=\"line\">    loss_detc_regr = []</span><br><span class=\"line\">    f = open(filename,<span class=\"string\">'r'</span>,buffering=<span class=\"number\">-1</span>)</span><br><span class=\"line\">    lines = f.readlines()</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"keyword\">for</span> line <span class=\"keyword\">in</span> lines:</span><br><span class=\"line\">        n = n + <span class=\"number\">1</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> <span class=\"string\">'Classifier accuracy for bounding boxes from RPN'</span> <span class=\"keyword\">in</span> line:</span><br><span class=\"line\">            result = re.findall(<span class=\"string\">r\"\\d+\\.?\\d*\"</span>,line)</span><br><span class=\"line\">            accuracy.append(float(result[<span class=\"number\">0</span>]))</span><br><span class=\"line\">            </span><br><span class=\"line\">        <span class=\"keyword\">if</span> <span class=\"string\">'Loss RPN classifier'</span> <span class=\"keyword\">in</span> line:</span><br><span class=\"line\">            result = re.findall(<span class=\"string\">r\"\\d+\\.?\\d*\"</span>,line)</span><br><span class=\"line\">            loss_rpn_cls.append(float(result[<span class=\"number\">0</span>]))       </span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">if</span> <span class=\"string\">'Loss RPN regression'</span> <span class=\"keyword\">in</span> line:</span><br><span class=\"line\">            result = re.findall(<span class=\"string\">r\"\\d+\\.?\\d*\"</span>,line)</span><br><span class=\"line\">            loss_rpn_regr.append(float(result[<span class=\"number\">0</span>]))</span><br><span class=\"line\">            </span><br><span class=\"line\">        <span class=\"keyword\">if</span> <span class=\"string\">'Loss Detector classifier'</span> <span class=\"keyword\">in</span> line:</span><br><span class=\"line\">            result = re.findall(<span class=\"string\">r\"\\d+\\.?\\d*\"</span>,line)</span><br><span class=\"line\">            loss_detc_cls.append(float(result[<span class=\"number\">0</span>]))</span><br><span class=\"line\">            </span><br><span class=\"line\">        <span class=\"keyword\">if</span> <span class=\"string\">'Loss Detector regression'</span> <span class=\"keyword\">in</span> line:</span><br><span class=\"line\">            result = re.findall(<span class=\"string\">r\"\\d+\\.?\\d*\"</span>,line)</span><br><span class=\"line\">            loss_detc_regr.append(float(result[<span class=\"number\">0</span>])) </span><br><span class=\"line\">            </span><br><span class=\"line\">    f.close()</span><br><span class=\"line\">    print(n)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> accuracy, loss_rpn_cls, loss_rpn_regr, loss_detc_cls, loss_detc_regr</span><br></pre></td></tr></table></figure></p>\n<h4 id=\"plot-epoch-loss-and-accury\"><a href=\"#plot-epoch-loss-and-accury\" class=\"headerlink\" title=\"plot epoch loss and accury\"></a>plot epoch loss and accury</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">filename = <span class=\"string\">r'F:\\desktop\\新建文件夹\\1000-no_balance\\train1.out'</span></span><br><span class=\"line\">aa,bb,cc,dd,ee = obtain_batch(filename)</span><br><span class=\"line\">x_cor = np.arange(<span class=\"number\">0</span>,len(aa),<span class=\"number\">1</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">plt.plot(x_cor,aa, c=<span class=\"string\">'b'</span>, label = <span class=\"string\">\"Accuracy\"</span>)</span><br><span class=\"line\">plt.plot(x_cor,bb, c=<span class=\"string\">'c'</span>, label = <span class=\"string\">\"Loss RPN classifier\"</span>)</span><br><span class=\"line\">plt.plot(x_cor,cc, c=<span class=\"string\">'g'</span>, label = <span class=\"string\">\"Loss RPN regression\"</span>)</span><br><span class=\"line\">plt.plot(x_cor,dd, c=<span class=\"string\">'k'</span>, label = <span class=\"string\">\"Loss Detector classifier\"</span>)</span><br><span class=\"line\">plt.plot(x_cor,ee, c=<span class=\"string\">'m'</span>, label = <span class=\"string\">\"Loss Detector regression\"</span>)</span><br><span class=\"line\">plt.ylabel(<span class=\"string\">\"Value of Accuracy and Loss\"</span>) </span><br><span class=\"line\">plt.xlabel(<span class=\"string\">\"Number of Epoch\"</span>)</span><br><span class=\"line\">plt.title(<span class=\"string\">'Loss and Accuracy for Totoal Epochs'</span>)  </span><br><span class=\"line\">plt.legend()</span><br><span class=\"line\">plt.ylim(<span class=\"number\">0</span>,<span class=\"number\">2</span>)</span><br><span class=\"line\"><span class=\"comment\">#plt.xlim(0,11540)</span></span><br><span class=\"line\">plt.savefig(<span class=\"string\">\"pic1.PNG\"</span>, dpi = <span class=\"number\">600</span>)</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n<p><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/pic1.PNG\" alt=\"image\"></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">filename = <span class=\"string\">r'F:\\desktop\\新建文件夹\\1000-no_balance\\train1.out'</span></span><br><span class=\"line\">a,b,c,d = obtain_each_batch(filename)</span><br><span class=\"line\">x_cor = np.arange(<span class=\"number\">0</span>,len(a),<span class=\"number\">1</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">plt.plot(x_cor,a, c=<span class=\"string\">'b'</span>, label = <span class=\"string\">\"rpn_cls\"</span>)</span><br><span class=\"line\">plt.plot(x_cor,b, c=<span class=\"string\">'c'</span>, label = <span class=\"string\">\"rpn_regr\"</span>)</span><br><span class=\"line\">plt.plot(x_cor,c, c=<span class=\"string\">'g'</span>, label = <span class=\"string\">\"detector_cls\"</span>)</span><br><span class=\"line\">plt.plot(x_cor,d, c=<span class=\"string\">'k'</span>, label = <span class=\"string\">\"detector_regr\"</span>)</span><br><span class=\"line\">plt.ylabel(<span class=\"string\">\"Value of Loss\"</span>) </span><br><span class=\"line\">plt.xlabel(<span class=\"string\">\"Epoch Length\"</span>)</span><br><span class=\"line\">plt.title(<span class=\"string\">'Loss for Lenght of Epoch'</span>)  </span><br><span class=\"line\">plt.legend()</span><br><span class=\"line\"><span class=\"comment\">#plt.ylim(0,2)</span></span><br><span class=\"line\">plt.xlim(<span class=\"number\">80787</span>,<span class=\"number\">92327</span>)</span><br><span class=\"line\">plt.savefig(<span class=\"string\">\"pic2.PNG\"</span>, dpi = <span class=\"number\">600</span>)</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n<p><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/pic2.PNG\" alt=\"image\"></p>\n<p><a href=\"https://github.com/Trouble404/Object-Detection-System-Based-on-CNN-and-Capsule-Network/blob/master/Plot/logbook_plot.ipynb\" target=\"_blank\" rel=\"noopener\">Jupyter notebook</a></p>\n<h2 id=\"August\"><a href=\"#August\" class=\"headerlink\" title=\"August\"></a>August</h2><h3 id=\"【01-02-08-2018】\"><a href=\"#【01-02-08-2018】\" class=\"headerlink\" title=\"【01~02/08/2018】\"></a>【01~02/08/2018】</h3><h4 id=\"test-network\"><a href=\"#test-network\" class=\"headerlink\" title=\"test network\"></a>test network</h4><p>首先是搭建网络，用于train部分相同的设置搭建<br>不过在这里图像增强就设置为关闭了</p>\n<p><strong>构建rpn输出</strong><br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">shared_layers = nn.nn_base(img_input, trainable=<span class=\"keyword\">True</span>)</span><br><span class=\"line\">num_anchors = len(C.anchor_box_scales)*len(C.anchor_box_ratios)</span><br><span class=\"line\">rpn_layers = nn.rpn(shared_layers,num_anchors)</span><br></pre></td></tr></table></figure></p>\n<p><strong>构建classifier输出</strong>，参数分别是：特征层输出，预选框，探测框的数目，多少个类，是否可训练<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">classifier = nn.classifier(feature_map_input, roi_input, C.num_rois,nb_classes=len(class_mapping), trainable=<span class=\"keyword\">True</span>)</span><br></pre></td></tr></table></figure></p>\n<p><strong>载入训练好的权重：</strong><br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">C.model_path = <span class=\"string\">'gpu_resnet50_weights.h5'</span></span><br><span class=\"line\"><span class=\"keyword\">try</span>:</span><br><span class=\"line\">    print(<span class=\"string\">'Loading weights from &#123;&#125;'</span>.format(C.model_path))</span><br><span class=\"line\">    model_rpn.load_weights(C.model_path, by_name=<span class=\"keyword\">True</span>)</span><br><span class=\"line\">    model_classifier.load_weights(C.model_path, by_name=<span class=\"keyword\">True</span>)</span><br><span class=\"line\"><span class=\"keyword\">except</span>:</span><br><span class=\"line\">    print(<span class=\"string\">'can not load'</span>)</span><br></pre></td></tr></table></figure></p>\n<p><strong>读取需要检测的图片：</strong><br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/test/test4.jpg\" alt=\"image\"><br>将图片规整到制定的大小</p>\n<ol>\n<li><p>将图片缩放到规定的大小<br> 首先从配置文件夹中得到最小边的大小<br> 得到图片的高度和宽度<br> 根据高度和宽度谁大谁小，确定规整后图片的高宽<br> 将图片缩放到指定的大小，用的是立方插值。返回的缩放后的图片img和相应的缩放的比例。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">format_img_size</span><span class=\"params\">(img, C)</span>:</span></span><br><span class=\"line\">    (height,width,_) = img.shape</span><br><span class=\"line\">    <span class=\"keyword\">if</span> width &lt;= height:</span><br><span class=\"line\">        ratio = C.im_size/width</span><br><span class=\"line\">    <span class=\"keyword\">else</span>:</span><br><span class=\"line\">        ratio = C.im_size/height</span><br><span class=\"line\">    new_width, new_height = image_processing.get_new_img_size(width,height, C.im_size)</span><br><span class=\"line\">    img = cv2.resize(img, (new_width, new_height), interpolation=cv2.INTER_CUBIC)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> img, ratio</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>对图片每一个通道的像素值做规整<br>  将图片的BGR变成RGB，因为网上训练好的RESNET图片都是以此训练的<br> 将图片数据类型转换为np.float32，并减去每一个通道的均值，理由同上<br> 图片的像素值除一个缩放因子，此处为1<br> 将图片的深度变到第一个位置<br> 给图片增加一个维度</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">format_img_channels</span><span class=\"params\">(img, C)</span>:</span></span><br><span class=\"line\">\t<span class=\"string\">\"\"\" formats the image channels based on config \"\"\"</span></span><br><span class=\"line\">\timg = img[:, :, (<span class=\"number\">2</span>, <span class=\"number\">1</span>, <span class=\"number\">0</span>)]</span><br><span class=\"line\">\timg = img.astype(np.float32)</span><br><span class=\"line\">\timg[:, :, <span class=\"number\">0</span>] -= C.img_channel_mean[<span class=\"number\">0</span>]</span><br><span class=\"line\">\timg[:, :, <span class=\"number\">1</span>] -= C.img_channel_mean[<span class=\"number\">1</span>]</span><br><span class=\"line\">\timg[:, :, <span class=\"number\">2</span>] -= C.img_channel_mean[<span class=\"number\">2</span>]</span><br><span class=\"line\">\timg /= C.img_scaling_factor</span><br><span class=\"line\">\timg = np.transpose(img, (<span class=\"number\">2</span>, <span class=\"number\">0</span>, <span class=\"number\">1</span>))</span><br><span class=\"line\">\timg = np.expand_dims(img, axis=<span class=\"number\">0</span>)</span><br><span class=\"line\">\t<span class=\"keyword\">return</span> img</span><br></pre></td></tr></table></figure>\n</li>\n</ol>\n<p>如果用的是tensorflow内核，需要将图片的深度变换到最后一位。</p>\n<p><strong>进行区域预测</strong><br>Y1:anchor包含物体的概率<br>Y2:每一个anchor对应的回归梯度<br>F:卷积后的特征图，接下来会有用</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[Y1, Y2, F] = model_rpn.predict(X)</span><br></pre></td></tr></table></figure>\n<p>获得rpn预测的结果以及对应的回归梯度，这一步就是对图片上隔16个像素的每个anchor进行rpn计算<br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/pic_anchors.png\" alt=\"image\"><br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/25_1.jpg\" alt=\"image\"></p>\n<p><strong>根据rpn预测的结果，得到预选框:</strong><br>这里会返回300个预选框以及它们对应的坐标(x1,y1,x2,y2)<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># transform predicted rpn to cordinates of boxes</span></span><br><span class=\"line\">R = rpn_to_boxes.rpn_to_roi(Y1, Y2, C, K.image_dim_ordering(), use_regr=<span class=\"keyword\">True</span>, overlap_thresh=<span class=\"number\">0.7</span>)</span><br></pre></td></tr></table></figure></p>\n<p><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/pic_rois.png\" alt=\"image\"></p>\n<p>将预选框的坐标由(x1,y1,x2,y2) 改到 (x,y,w,h)<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">R[:, <span class=\"number\">2</span>] -= R[:, <span class=\"number\">0</span>]</span><br><span class=\"line\">R[:, <span class=\"number\">3</span>] -= R[:, <span class=\"number\">1</span>]</span><br></pre></td></tr></table></figure></p>\n<p><strong>遍历所有的预选框</strong><br>需要注意的是每一次遍历预选框的个数为C.num_rois<br>每一次遍历32个预选框，那么总共需要300/32, 10批次<br>取出32个预选框，并增加一个维度【注：当不满一个32，其自动只取到最后一个】<br>当预选框被取空的时候，停止循环<br>当最后一次去不足32个预选框时，补第一个框使其达到32个。<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># divided 32 bboxes as one group</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> jk <span class=\"keyword\">in</span> range(R.shape[<span class=\"number\">0</span>]//C.num_rois + <span class=\"number\">1</span>):</span><br><span class=\"line\">    <span class=\"comment\"># pick num_rios(32) bboxes one time, only pick to last bboxes in last group</span></span><br><span class=\"line\">    ROIs = np.expand_dims(R[C.num_rois*jk:C.num_rois*(jk+<span class=\"number\">1</span>), :], axis=<span class=\"number\">0</span>)</span><br><span class=\"line\">    <span class=\"comment\">#print(ROIs.shape)</span></span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\"># no proposals, out iter</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> ROIs.shape[<span class=\"number\">1</span>] == <span class=\"number\">0</span>:</span><br><span class=\"line\">        <span class=\"keyword\">break</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># when last time can't obtain num_rios(32) bboxes, adding bboxes with 0 to fill to 32 bboxes</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> jk == R.shape[<span class=\"number\">0</span>]//C.num_rois:</span><br><span class=\"line\">        <span class=\"comment\">#pad R</span></span><br><span class=\"line\">        curr_shape = ROIs.shape</span><br><span class=\"line\">        target_shape = (curr_shape[<span class=\"number\">0</span>],C.num_rois,curr_shape[<span class=\"number\">2</span>])</span><br><span class=\"line\">        ROIs_padded = np.zeros(target_shape).astype(ROIs.dtype)</span><br><span class=\"line\">        ROIs_padded[:, :curr_shape[<span class=\"number\">1</span>], :] = ROIs</span><br><span class=\"line\">        ROIs_padded[<span class=\"number\">0</span>, curr_shape[<span class=\"number\">1</span>]:, :] = ROIs[<span class=\"number\">0</span>, <span class=\"number\">0</span>, :]</span><br><span class=\"line\">        <span class=\"comment\"># 10 group with 320 bboxes</span></span><br><span class=\"line\">        ROIs = ROIs_padded</span><br></pre></td></tr></table></figure></p>\n<p>这样就可以送入分类网络了</p>\n<p><strong>进行类别预测和边框回归</strong></p>\n<p>预测<br>P_cls：该边框属于某一类别的概率<br>P_regr：每一个类别对应的边框回归梯度<br>F:rpn网络得到的卷积后的特征图<br>ROIS:处理得到的区域预选框<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[P_cls, P_regr] = model_classifier_only.predict([F, ROIs])</span><br></pre></td></tr></table></figure></p>\n<p>遍历每一个预选宽<br>如果该预选框的最大概率小于设定的阈值（即预测的肯定程度大于一定的值，我们才认为这次的类别的概率预测是有效的，或者最大的概率出现在背景上，则认为这个预选框是无效的，进行下一次预测。<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">for</span> ii <span class=\"keyword\">in</span> range(P_cls.shape[<span class=\"number\">1</span>]):</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># if smaller than setting threshold, we think this bbox invalid</span></span><br><span class=\"line\">    <span class=\"comment\"># and if this bbox's class is background, we don't need to care about it</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> np.max(P_cls[<span class=\"number\">0</span>, ii, :]) &lt; bbox_threshold <span class=\"keyword\">or</span> np.argmax(P_cls[<span class=\"number\">0</span>, ii, :]) == (P_cls.shape[<span class=\"number\">2</span>] - <span class=\"number\">1</span>):</span><br><span class=\"line\">        <span class=\"keyword\">continue</span></span><br></pre></td></tr></table></figure></p>\n<p>不属于上面的两种情况，取最大的概率处为此边框的类别得到其名称。<br>创建两个list，用于存放不同类别对应的边框和概率<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># obatain max possibility's class name by class mapping</span></span><br><span class=\"line\">cls_name = class_mapping[np.argmax(P_cls[<span class=\"number\">0</span>, ii, :])]</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># saving bboxes and probs</span></span><br><span class=\"line\"><span class=\"keyword\">if</span> cls_name <span class=\"keyword\">not</span> <span class=\"keyword\">in</span> bboxes:</span><br><span class=\"line\">    bboxes[cls_name] = []</span><br><span class=\"line\">    probs[cls_name] = []</span><br></pre></td></tr></table></figure></p>\n<p>得到该预选框的信息<br>得到类别对应的编号<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># obtain current cordinates of proposal</span></span><br><span class=\"line\">(x, y, w, h) = ROIs[<span class=\"number\">0</span>, ii, :]</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># obtain the position with max possibility</span></span><br><span class=\"line\">cls_num = np.argmax(P_cls[<span class=\"number\">0</span>, ii, :])</span><br></pre></td></tr></table></figure></p>\n<p>这样符合条件的预选框以及对应的分类类别和概率就可以画在图片上了<br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/pic_cls1.png\" alt=\"iamge\"></p>\n<p>根据类别编号得到该类的边框回归梯度<br>对回归梯度进行规整化<br>对预测的边框进行修正<br>向相应的类里面添加信息【乘 C.rpn_stride，边框的预测都是在特征图上进行的要将其映射到规整后的原图上】<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">try</span>:</span><br><span class=\"line\">    <span class=\"comment\"># obtain privous position's bbox's regrient</span></span><br><span class=\"line\">    (tx, ty, tw, th) = P_regr[<span class=\"number\">0</span>, ii, <span class=\"number\">4</span>*cls_num:<span class=\"number\">4</span>*(cls_num+<span class=\"number\">1</span>)]</span><br><span class=\"line\">    <span class=\"comment\"># waiting test</span></span><br><span class=\"line\">    tx /= C.classifier_regr_std[<span class=\"number\">0</span>]</span><br><span class=\"line\">    ty /= C.classifier_regr_std[<span class=\"number\">1</span>]</span><br><span class=\"line\">    tw /= C.classifier_regr_std[<span class=\"number\">2</span>]</span><br><span class=\"line\">    th /= C.classifier_regr_std[<span class=\"number\">3</span>]</span><br><span class=\"line\">    <span class=\"comment\"># fix box with regreient</span></span><br><span class=\"line\">    x, y, w, h = rpn_to_boxes.apply_regr(x, y, w, h, tx, ty, tw, th)</span><br><span class=\"line\"><span class=\"keyword\">except</span>:</span><br><span class=\"line\">    <span class=\"keyword\">pass</span></span><br><span class=\"line\"><span class=\"comment\"># cordinates of current's box on real img</span></span><br><span class=\"line\">bboxes[cls_name].append([C.rpn_stride*x, C.rpn_stride*y, C.rpn_stride*(x+w), C.rpn_stride*(y+h)])</span><br><span class=\"line\"><span class=\"comment\"># coresspoding posbility</span></span><br><span class=\"line\">probs[cls_name].append(np.max(P_cls[<span class=\"number\">0</span>, ii, :]))</span><br></pre></td></tr></table></figure></p>\n<p>这样修正过的框可以画在图上：<br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/pic_cls2.png\" alt=\"image\"></p>\n<p>遍历bboxes里的类，取出某一类的bbox，合并一些重合度较高的选框<br>No Max Supression<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># for all classes in current boxes</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> key <span class=\"keyword\">in</span> bboxes:</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># bboxes's cordinates</span></span><br><span class=\"line\">    bbox = np.array(bboxes[key])</span><br><span class=\"line\">    <span class=\"comment\"># apply NMX to merge some  overlapping boxes</span></span><br><span class=\"line\">    new_boxes, new_probs = rpn_to_boxes.non_max_suppression_fast(bbox, np.array(probs[key]), overlap_thresh=<span class=\"number\">0.5</span>)</span><br></pre></td></tr></table></figure></p>\n<p>最终的图：<br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/pic_cls3.png\" alt=\"image\"></p>\n<p><a href=\"https://github.com/Trouble404/Object-Detection-System-Based-on-CNN-and-Capsule-Network/blob/master/Faster-RCNN/test.ipynb\" target=\"_blank\" rel=\"noopener\">Jupyter notebook</a></p>\n<h4 id=\"result\"><a href=\"#result\" class=\"headerlink\" title=\"result\"></a>result</h4><p>Small img, only 8k</p>\n<div align=\"center\"><br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/test/test1.jpg\" height=\"200px\"><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/test/test1.png\" height=\"200px\"><br></div>\n\n<hr>\n<p>Overlapping img</p>\n<div align=\"center\"><br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/test/test2.jpg\" height=\"200px\"><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/test/test2.png\" height=\"200px\"><br></div>\n\n<hr>\n<p>Crowed People</p>\n<div align=\"center\"><br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/test/test3.jpg\" height=\"260px\"><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/test/test3.png\" height=\"260px\"><br></div>\n\n<hr>\n<p>cow and people</p>\n<div align=\"center\"><br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/test/test4.jpg\" height=\"260px\"><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/test/test4.png\" height=\"260px\"><br></div>\n\n<hr>\n<p>car and plane</p>\n<div align=\"center\"><br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/test/test5.jpg\" height=\"220px\"><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/test/test5.png\" height=\"220px\"><br></div>\n\n<hr>\n<p>Street img</p>\n<div align=\"center\"><br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/test/test6.jpg\" height=\"270px\"><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/test/test6.png\" height=\"270px\"><br></div>\n\n<hr>\n<p>Lots Dogs</p>\n<div align=\"center\"><br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/test/test7.jpg\" height=\"250px\"><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/test/test7.png\" height=\"250px\"><br></div>\n\n<hr>\n<p>Overlapping car and people</p>\n<div align=\"center\"><br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/test/test8.jpg\" height=\"290px\"><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/test/test8.png\" height=\"290px\"><br></div>\n\n<p>直观看的话效果还不错，但是一些重叠的物体框会出现反复，或者取不到。而且分类有一点过拟合。</p>\n<h3 id=\"【03-08-2018】\"><a href=\"#【03-08-2018】\" class=\"headerlink\" title=\"【03/08/2018】\"></a>【03/08/2018】</h3><h4 id=\"evaluation\"><a href=\"#evaluation\" class=\"headerlink\" title=\"evaluation\"></a>evaluation</h4><p><strong>mAP</strong><br>mAP是目标算法中衡量算法的精确度的指标，涉及两个概念：查准率Precision、查全率Recall。对于object detection任务，每一个object都可以计算出其Precision和Recall，多次计算/试验，每个类都 可以得到一条P-R曲线，曲线下的面积就是AP的值，这个mean的意思是对每个类的AP再求平均，得到的就是mAP的值，mAP的大小一定在[0,1]区间。 </p>\n<p><strong>AP</strong>:Precision对Recall积分，可通过改变正负样本阈值求得矩形面积，进而求积分得到，也可以通过sklearn.metrics.average_precision_score函数直接得到。 </p>\n<p>传入预测值和真实值和resize比例，得到可以传入sklearn.metrics.average_precision_score函数的值，即：真实值和预测概率</p>\n<hr>\n<p>首先搭建rpn和分类器网络，按照之前的train部分来就可以了<br>这里注意分类网络的输入换成测试图片的feature map<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">num_features = <span class=\"number\">1024</span></span><br><span class=\"line\"></span><br><span class=\"line\">input_shape_img = (<span class=\"keyword\">None</span>, <span class=\"keyword\">None</span>, <span class=\"number\">3</span>)</span><br><span class=\"line\">input_shape_features = (<span class=\"keyword\">None</span>, <span class=\"keyword\">None</span>, num_features)</span><br><span class=\"line\"></span><br><span class=\"line\">img_input = Input(shape=input_shape_img)</span><br><span class=\"line\">roi_input = Input(shape=(C.num_rois, <span class=\"number\">4</span>))</span><br><span class=\"line\">feature_map_input = Input(shape=input_shape_features)</span><br><span class=\"line\"></span><br><span class=\"line\">classifier = nn.classifier(feature_map_input, roi_input, C.num_rois, nb_classes=len(class_mapping), trainable=<span class=\"keyword\">True</span>)</span><br></pre></td></tr></table></figure></p>\n<p>然后载入需要测试的模型权重</p>\n<p>按照VOC的数据集标注，把测试集分出来：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">train_imgs = []</span><br><span class=\"line\">test_imgs = []</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">for</span> each <span class=\"keyword\">in</span> all_imgs:</span><br><span class=\"line\">\t<span class=\"keyword\">if</span> each[<span class=\"string\">'imageset'</span>] == <span class=\"string\">'trainval'</span>:</span><br><span class=\"line\">\t\ttrain_imgs.append(each)</span><br><span class=\"line\">\t<span class=\"keyword\">if</span> each[<span class=\"string\">'imageset'</span>] == <span class=\"string\">'test'</span>:</span><br><span class=\"line\">\t\ttest_imgs.append(each)</span><br></pre></td></tr></table></figure></p>\n<p>按照之前的预测方法，求出图片的预测框坐标以及对应的分类名字，然后把这些信息放入对应的字典里面，与xml解析的文件一样的格式：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">for</span> jk <span class=\"keyword\">in</span> range(new_boxes.shape[<span class=\"number\">0</span>]):</span><br><span class=\"line\">    (x1, y1, x2, y2) = new_boxes[jk, :]</span><br><span class=\"line\">    det = &#123;<span class=\"string\">'x1'</span>: x1, <span class=\"string\">'x2'</span>: x2, <span class=\"string\">'y1'</span>: y1, <span class=\"string\">'y2'</span>: y2, <span class=\"string\">'class'</span>: key, <span class=\"string\">'prob'</span>: new_probs[jk]&#125;</span><br><span class=\"line\">    all_dets.append(det)</span><br></pre></td></tr></table></figure></p>\n<p><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/014_1.JPG\" alt=\"image\"></p>\n<p>然后读取标注的框的真实数值：<br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/014_2.JPG\" alt=\"image\"></p>\n<p>遍历真实信息里面的每一个狂，将bbox_matched这个属性标注为FALSE，之后如果预测框和标注框对应上的话，这个属性就会被设置为True<br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/14_3.JPG\" alt=\"image\"></p>\n<p>获取预测框里面的分类对应概率，并且按照概率从大到小得到idx位置：<br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/014_3.JPG\" alt=\"image\"></p>\n<p>按照概率大小，对每一个对应的预测框，对比每一个标注的框，如果预测的类与当前标注框的类相同并且没有被匹配过，计算两个框的iou，如果大于0.5的话就表明预测框匹配当前标注框，保存预测概率以及对应的是否匹配：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># process each bbox with hightest prob</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> box_idx <span class=\"keyword\">in</span> box_idx_sorted_by_prob:</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\"># obtain current box's cordinates, class and prob</span></span><br><span class=\"line\">    pred_box = pred[box_idx]</span><br><span class=\"line\">    pred_class = pred_box[<span class=\"string\">'class'</span>]</span><br><span class=\"line\">    pred_x1 = pred_box[<span class=\"string\">'x1'</span>]</span><br><span class=\"line\">    pred_x2 = pred_box[<span class=\"string\">'x2'</span>]</span><br><span class=\"line\">    pred_y1 = pred_box[<span class=\"string\">'y1'</span>]</span><br><span class=\"line\">    pred_y2 = pred_box[<span class=\"string\">'y2'</span>]</span><br><span class=\"line\">    pred_prob = pred_box[<span class=\"string\">'prob'</span>]</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\"># if not in P list, save current class infomration to it</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> pred_class <span class=\"keyword\">not</span> <span class=\"keyword\">in</span> P:</span><br><span class=\"line\">        P[pred_class] = []</span><br><span class=\"line\">        T[pred_class] = []</span><br><span class=\"line\">        <span class=\"comment\"># put porb to P</span></span><br><span class=\"line\">    P[pred_class].append(pred_prob)</span><br><span class=\"line\">    <span class=\"comment\"># used to check whether find current object</span></span><br><span class=\"line\">    found_match = <span class=\"keyword\">False</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># compare each real bbox</span></span><br><span class=\"line\">    <span class=\"comment\"># obtain real box's cordinates, class and prob</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> gt_box <span class=\"keyword\">in</span> gt:</span><br><span class=\"line\">        gt_class = gt_box[<span class=\"string\">'class'</span>]</span><br><span class=\"line\">        <span class=\"comment\"># bacause the image is rezied, so calculate the real cordinates</span></span><br><span class=\"line\">        gt_x1 = gt_box[<span class=\"string\">'x1'</span>]/fx</span><br><span class=\"line\">        gt_x2 = gt_box[<span class=\"string\">'x2'</span>]/fx</span><br><span class=\"line\">        gt_y1 = gt_box[<span class=\"string\">'y1'</span>]/fy</span><br><span class=\"line\">        gt_y2 = gt_box[<span class=\"string\">'y2'</span>]/fy</span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"comment\"># obtain box_matched - all false at beginning</span></span><br><span class=\"line\">        gt_seen = gt_box[<span class=\"string\">'bbox_matched'</span>]</span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"comment\"># ture class != predicted class</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> gt_class != pred_class:</span><br><span class=\"line\">            <span class=\"keyword\">continue</span></span><br><span class=\"line\">        <span class=\"comment\"># already matched</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> gt_seen:</span><br><span class=\"line\">            <span class=\"keyword\">continue</span></span><br><span class=\"line\">        <span class=\"comment\"># calculate iou of predicted bbox and real bbox </span></span><br><span class=\"line\">        iou = rpn_calculation.iou((pred_x1, pred_y1, pred_x2, pred_y2), (gt_x1, gt_y1, gt_x2, gt_y2))</span><br><span class=\"line\">        <span class=\"comment\"># if iou &gt; 0.5, we will set this prediction correct</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> iou &gt;= <span class=\"number\">0.5</span>:</span><br><span class=\"line\">            found_match = <span class=\"keyword\">True</span></span><br><span class=\"line\">            gt_box[<span class=\"string\">'bbox_matched'</span>] = <span class=\"keyword\">True</span></span><br><span class=\"line\">            <span class=\"keyword\">break</span></span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            <span class=\"keyword\">continue</span></span><br><span class=\"line\">    <span class=\"comment\"># 1 means this position's bbox correct match with orignal image</span></span><br><span class=\"line\">    T[pred_class].append(int(found_match))</span><br></pre></td></tr></table></figure>\n<p><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/014_4.JPG\" alt=\"image\"></p>\n<p>遍历每一个标注框，如果没有被匹配到并且diffcult属性不是true的话，说明这个框漏检了，在之前保存的概率以及对应是否有概率里面加入物体1以及对应概率0<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># adding missing object compared to orignal image</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> gt_box <span class=\"keyword\">in</span> gt:</span><br><span class=\"line\">    <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> gt_box[<span class=\"string\">'bbox_matched'</span>] <span class=\"keyword\">and</span> <span class=\"keyword\">not</span> gt_box[<span class=\"string\">'difficult'</span>]:</span><br><span class=\"line\">        <span class=\"keyword\">if</span> gt_box[<span class=\"string\">'class'</span>] <span class=\"keyword\">not</span> <span class=\"keyword\">in</span> P:</span><br><span class=\"line\">            P[gt_box[<span class=\"string\">'class'</span>]] = []</span><br><span class=\"line\">            T[gt_box[<span class=\"string\">'class'</span>]] = []</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># T = 1 means there are object, P = 0 means we did't detected that</span></span><br><span class=\"line\">        T[gt_box[<span class=\"string\">'class'</span>]].append(<span class=\"number\">1</span>)</span><br><span class=\"line\">        P[gt_box[<span class=\"string\">'class'</span>]].append(<span class=\"number\">0</span>)</span><br></pre></td></tr></table></figure></p>\n<p><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/014_5.JPG\" alt=\"image\"></p>\n<p>把当前信息存入到总的一个词典里面，就可以使用average_precision_score这个sklearn里面的函数计算ap了。与此同时，保存得到的结果并且显示总的map：<br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/014_6.JPG\" alt=\"image\"></p>\n<p><a href=\"https://github.com/Trouble404/Object-Detection-System-Based-on-CNN-and-Capsule-Network/blob/master/Faster-RCNN/map.ipynb\" target=\"_blank\" rel=\"noopener\">jupyter notebook</a></p>\n<h3 id=\"【06-10-08-2018】\"><a href=\"#【06-10-08-2018】\" class=\"headerlink\" title=\"【06~10/08/2018】\"></a>【06~10/08/2018】</h3><h4 id=\"adjust\"><a href=\"#adjust\" class=\"headerlink\" title=\"adjust\"></a>adjust</h4><p>将计算ap的函数包装好：<br><a href=\"https://github.com/Trouble404/Object-Detection-System-Based-on-CNN-and-Capsule-Network/blob/master/Faster-RCNN/map_all.ipynb\" target=\"_blank\" rel=\"noopener\">jupyter notebook</a></p>\n<h5 id=\"Project1-all-9-models\"><a href=\"#Project1-all-9-models\" class=\"headerlink\" title=\"Project1 all: 9 models:\"></a>Project1 all: 9 models:</h5><p>ALL_2 NO THRESHOLD OTHER WITH THRESHOLD MOST 0.8</p>\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">Classes</th>\n<th style=\"text-align:center\">ALL_1</th>\n<th style=\"text-align:center\">ALL_2</th>\n<th style=\"text-align:center\">ALL_3</th>\n<th style=\"text-align:center\">ALL_4</th>\n<th style=\"text-align:center\">ALL_5</th>\n<th style=\"text-align:center\">ALL_6</th>\n<th style=\"text-align:center\">ALL_7</th>\n<th style=\"text-align:center\">ALL_8</th>\n<th style=\"text-align:center\">ALL_9</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">motorbike</td>\n<td style=\"text-align:center\">0.2305</td>\n<td style=\"text-align:center\">0.2412</td>\n<td style=\"text-align:center\">0.2132</td>\n<td style=\"text-align:center\">0.2220</td>\n<td style=\"text-align:center\">0.2889</td>\n<td style=\"text-align:center\">0.2528</td>\n<td style=\"text-align:center\">0.2204</td>\n<td style=\"text-align:center\">0.2644</td>\n<td style=\"text-align:center\">0.2336</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">person</td>\n<td style=\"text-align:center\">0.6489</td>\n<td style=\"text-align:center\">0.6735</td>\n<td style=\"text-align:center\">0.7107</td>\n<td style=\"text-align:center\">0.6652</td>\n<td style=\"text-align:center\">0.7120</td>\n<td style=\"text-align:center\">0.7041</td>\n<td style=\"text-align:center\">0.7238</td>\n<td style=\"text-align:center\">0.7003</td>\n<td style=\"text-align:center\">0.7201</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">car</td>\n<td style=\"text-align:center\">0.1697</td>\n<td style=\"text-align:center\">0.1563</td>\n<td style=\"text-align:center\">0.2032</td>\n<td style=\"text-align:center\">0.2105</td>\n<td style=\"text-align:center\">0.2308</td>\n<td style=\"text-align:center\">0.2221</td>\n<td style=\"text-align:center\">0.2436</td>\n<td style=\"text-align:center\">0.2053</td>\n<td style=\"text-align:center\">0.2080</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">aeroplane</td>\n<td style=\"text-align:center\">0.7968</td>\n<td style=\"text-align:center\">0.7062</td>\n<td style=\"text-align:center\">0.7941</td>\n<td style=\"text-align:center\">0.6412</td>\n<td style=\"text-align:center\">0.7871</td>\n<td style=\"text-align:center\">0.7331</td>\n<td style=\"text-align:center\">0.7648</td>\n<td style=\"text-align:center\">0.7659</td>\n<td style=\"text-align:center\">0.6902</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">bottle</td>\n<td style=\"text-align:center\">0.2213</td>\n<td style=\"text-align:center\">0.2428</td>\n<td style=\"text-align:center\">0.2261</td>\n<td style=\"text-align:center\">0.1943</td>\n<td style=\"text-align:center\">0.2570</td>\n<td style=\"text-align:center\">0.2437</td>\n<td style=\"text-align:center\">0.2899</td>\n<td style=\"text-align:center\">0.1442</td>\n<td style=\"text-align:center\">0.2265</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">sheep</td>\n<td style=\"text-align:center\">0.6162</td>\n<td style=\"text-align:center\">0.5702</td>\n<td style=\"text-align:center\">0.6876</td>\n<td style=\"text-align:center\">0.6295</td>\n<td style=\"text-align:center\">0.6364</td>\n<td style=\"text-align:center\">0.5710</td>\n<td style=\"text-align:center\">0.6536</td>\n<td style=\"text-align:center\">0.6349</td>\n<td style=\"text-align:center\">0.6455</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">tvmonitor</td>\n<td style=\"text-align:center\">0.1582</td>\n<td style=\"text-align:center\">0.1601</td>\n<td style=\"text-align:center\">0.2231</td>\n<td style=\"text-align:center\">0.1748</td>\n<td style=\"text-align:center\">0.1551</td>\n<td style=\"text-align:center\">0.1603</td>\n<td style=\"text-align:center\">0.1317</td>\n<td style=\"text-align:center\">0.1584</td>\n<td style=\"text-align:center\">0.1678</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">boat</td>\n<td style=\"text-align:center\">0.3842</td>\n<td style=\"text-align:center\">0.2621</td>\n<td style=\"text-align:center\">0.2261</td>\n<td style=\"text-align:center\">0.1943</td>\n<td style=\"text-align:center\">0.3499</td>\n<td style=\"text-align:center\">0.2437</td>\n<td style=\"text-align:center\">0.2057</td>\n<td style=\"text-align:center\">0.2748</td>\n<td style=\"text-align:center\">0.3509</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">chair</td>\n<td style=\"text-align:center\">0.2811</td>\n<td style=\"text-align:center\">0.0563</td>\n<td style=\"text-align:center\">0.0891</td>\n<td style=\"text-align:center\">0.0621</td>\n<td style=\"text-align:center\">0.1353</td>\n<td style=\"text-align:center\">0.0865</td>\n<td style=\"text-align:center\">0.0907</td>\n<td style=\"text-align:center\">0.0854</td>\n<td style=\"text-align:center\">0.1282</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">bicycle</td>\n<td style=\"text-align:center\">0.1464</td>\n<td style=\"text-align:center\">0.1224</td>\n<td style=\"text-align:center\">0.1346</td>\n<td style=\"text-align:center\">0.1781</td>\n<td style=\"text-align:center\">0.1406</td>\n<td style=\"text-align:center\">0.1448</td>\n<td style=\"text-align:center\">0.1810</td>\n<td style=\"text-align:center\">0.1071</td>\n<td style=\"text-align:center\">0.1673</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">cat</td>\n<td style=\"text-align:center\">0.8901</td>\n<td style=\"text-align:center\">0.8565</td>\n<td style=\"text-align:center\">0.9103</td>\n<td style=\"text-align:center\">0.8417</td>\n<td style=\"text-align:center\">0.8289</td>\n<td style=\"text-align:center\">0.8274</td>\n<td style=\"text-align:center\">0.7572</td>\n<td style=\"text-align:center\">0.9143</td>\n<td style=\"text-align:center\">0.8118</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">pottedplant</td>\n<td style=\"text-align:center\">0.2075</td>\n<td style=\"text-align:center\">0.0926</td>\n<td style=\"text-align:center\">0.1790</td>\n<td style=\"text-align:center\">0.0532</td>\n<td style=\"text-align:center\">0.1517</td>\n<td style=\"text-align:center\">0.1150</td>\n<td style=\"text-align:center\">0.1080</td>\n<td style=\"text-align:center\">0.1022</td>\n<td style=\"text-align:center\">0.0939</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">horse</td>\n<td style=\"text-align:center\">0.1185</td>\n<td style=\"text-align:center\">0.0588</td>\n<td style=\"text-align:center\">0.0726</td>\n<td style=\"text-align:center\">0.0489</td>\n<td style=\"text-align:center\">0.0696</td>\n<td style=\"text-align:center\">0.0695</td>\n<td style=\"text-align:center\">0.0637</td>\n<td style=\"text-align:center\">0.0651</td>\n<td style=\"text-align:center\">0.0640</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">sofa</td>\n<td style=\"text-align:center\">0.2797</td>\n<td style=\"text-align:center\">0.2309</td>\n<td style=\"text-align:center\">0.2852</td>\n<td style=\"text-align:center\">0.2966</td>\n<td style=\"text-align:center\">0.3855</td>\n<td style=\"text-align:center\">0.4817</td>\n<td style=\"text-align:center\">0.3659</td>\n<td style=\"text-align:center\">0.3132</td>\n<td style=\"text-align:center\">0.3090</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">dog</td>\n<td style=\"text-align:center\">0.5359</td>\n<td style=\"text-align:center\">0.5077</td>\n<td style=\"text-align:center\">0.5578</td>\n<td style=\"text-align:center\">0.4413</td>\n<td style=\"text-align:center\">0.4832</td>\n<td style=\"text-align:center\">0.5793</td>\n<td style=\"text-align:center\">0.5687</td>\n<td style=\"text-align:center\">0.4910</td>\n<td style=\"text-align:center\">0.4598</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">cow</td>\n<td style=\"text-align:center\">0.7582</td>\n<td style=\"text-align:center\">0.6229</td>\n<td style=\"text-align:center\">0.7295</td>\n<td style=\"text-align:center\">0.5420</td>\n<td style=\"text-align:center\">0.5379</td>\n<td style=\"text-align:center\">0.5312</td>\n<td style=\"text-align:center\">0.5147</td>\n<td style=\"text-align:center\">0.5706</td>\n<td style=\"text-align:center\">0.6503</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">diningtable</td>\n<td style=\"text-align:center\">0.3979</td>\n<td style=\"text-align:center\">0.2734</td>\n<td style=\"text-align:center\">0.3739</td>\n<td style=\"text-align:center\">0.2963</td>\n<td style=\"text-align:center\">0.4715</td>\n<td style=\"text-align:center\">0.4987</td>\n<td style=\"text-align:center\">0.3895</td>\n<td style=\"text-align:center\">0.4983</td>\n<td style=\"text-align:center\">0.4666</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">bus</td>\n<td style=\"text-align:center\">0.6203</td>\n<td style=\"text-align:center\">0.5572</td>\n<td style=\"text-align:center\">0.6468</td>\n<td style=\"text-align:center\">0.6032</td>\n<td style=\"text-align:center\">0.6320</td>\n<td style=\"text-align:center\">0.6096</td>\n<td style=\"text-align:center\">0.7169</td>\n<td style=\"text-align:center\">0.5938</td>\n<td style=\"text-align:center\">0.5485</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">bird</td>\n<td style=\"text-align:center\">0.6164</td>\n<td style=\"text-align:center\">0.6662</td>\n<td style=\"text-align:center\">0.5692</td>\n<td style=\"text-align:center\">0.5751</td>\n<td style=\"text-align:center\">0.5407</td>\n<td style=\"text-align:center\">0.4125</td>\n<td style=\"text-align:center\">0.4925</td>\n<td style=\"text-align:center\">0.4347</td>\n<td style=\"text-align:center\">0.5208</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">train</td>\n<td style=\"text-align:center\">0.8655</td>\n<td style=\"text-align:center\">0.6916</td>\n<td style=\"text-align:center\">0.7141</td>\n<td style=\"text-align:center\">0.7166</td>\n<td style=\"text-align:center\">0.7643</td>\n<td style=\"text-align:center\">0.8107</td>\n<td style=\"text-align:center\">0.7100</td>\n<td style=\"text-align:center\">0.7194</td>\n<td style=\"text-align:center\">0.6263</td>\n</tr>\n<tr>\n<td style=\"text-align:center\"><strong>mAP</strong></td>\n<td style=\"text-align:center\"><strong>0.4472</strong></td>\n<td style=\"text-align:center\"><strong>0.3874</strong></td>\n<td style=\"text-align:center\"><strong>0.4341</strong></td>\n<td style=\"text-align:center\"><strong>0.3859</strong></td>\n<td style=\"text-align:center\"><strong>0.4279</strong></td>\n<td style=\"text-align:center\"><strong>0.4141</strong></td>\n<td style=\"text-align:center\"><strong>0.4096</strong></td>\n<td style=\"text-align:center\"><strong>0.4022</strong></td>\n<td style=\"text-align:center\"><strong>0.4045</strong></td>\n</tr>\n</tbody>\n</table>\n<hr>\n<h5 id=\"Project1-epoch-lenght-1000-epoch-1041-7-models\"><a href=\"#Project1-epoch-lenght-1000-epoch-1041-7-models\" class=\"headerlink\" title=\"Project1 epoch_lenght=1000, epoch:1041 : 7 models:\"></a>Project1 epoch_lenght=1000, epoch:1041 : 7 models:</h5><p>ALL WITH THRESHOLD MOST 0.51</p>\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">Classes</th>\n<th style=\"text-align:center\">ALL_1</th>\n<th style=\"text-align:center\">ALL_2</th>\n<th style=\"text-align:center\">ALL_3</th>\n<th style=\"text-align:center\">ALL_4</th>\n<th style=\"text-align:center\">ALL_5</th>\n<th style=\"text-align:center\">ALL_6</th>\n<th style=\"text-align:center\">ALL_7</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">motorbike</td>\n<td style=\"text-align:center\">0.2433</td>\n<td style=\"text-align:center\">0.2128</td>\n<td style=\"text-align:center\">0.2232</td>\n<td style=\"text-align:center\">0.2262</td>\n<td style=\"text-align:center\">0.2286</td>\n<td style=\"text-align:center\">0.2393</td>\n<td style=\"text-align:center\">0.2279</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">person</td>\n<td style=\"text-align:center\">0.6560</td>\n<td style=\"text-align:center\">0.6537</td>\n<td style=\"text-align:center\">0.6742</td>\n<td style=\"text-align:center\">0.6952</td>\n<td style=\"text-align:center\">0.6852</td>\n<td style=\"text-align:center\">0.6719</td>\n<td style=\"text-align:center\">0.6636</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">car</td>\n<td style=\"text-align:center\">0.1562</td>\n<td style=\"text-align:center\">0.1905</td>\n<td style=\"text-align:center\">0.1479</td>\n<td style=\"text-align:center\">0.2024</td>\n<td style=\"text-align:center\">0.2010</td>\n<td style=\"text-align:center\">0.1379</td>\n<td style=\"text-align:center\">0.1583</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">aeroplane</td>\n<td style=\"text-align:center\">0.7359</td>\n<td style=\"text-align:center\">0.6837</td>\n<td style=\"text-align:center\">0.6729</td>\n<td style=\"text-align:center\">0.6687</td>\n<td style=\"text-align:center\">0.6957</td>\n<td style=\"text-align:center\">0.7339</td>\n<td style=\"text-align:center\">0.6391</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">bottle</td>\n<td style=\"text-align:center\">0.1913</td>\n<td style=\"text-align:center\">0.1937</td>\n<td style=\"text-align:center\">0.2635</td>\n<td style=\"text-align:center\">0.1843</td>\n<td style=\"text-align:center\">0.2570</td>\n<td style=\"text-align:center\">0.1632</td>\n<td style=\"text-align:center\">0.1863</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">sheep</td>\n<td style=\"text-align:center\">0.5429</td>\n<td style=\"text-align:center\">0.5579</td>\n<td style=\"text-align:center\">0.6219</td>\n<td style=\"text-align:center\">0.5355</td>\n<td style=\"text-align:center\">0.5881</td>\n<td style=\"text-align:center\">0.5441</td>\n<td style=\"text-align:center\">0.5824</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">tvmonitor</td>\n<td style=\"text-align:center\">0.1295</td>\n<td style=\"text-align:center\">0.1601</td>\n<td style=\"text-align:center\">0.1368</td>\n<td style=\"text-align:center\">0.1407</td>\n<td style=\"text-align:center\">0.1147</td>\n<td style=\"text-align:center\">0.1349</td>\n<td style=\"text-align:center\">0.1154</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">boat</td>\n<td style=\"text-align:center\">0.1913</td>\n<td style=\"text-align:center\">0.2880</td>\n<td style=\"text-align:center\">0.2635</td>\n<td style=\"text-align:center\">0.3433</td>\n<td style=\"text-align:center\">0.3335</td>\n<td style=\"text-align:center\">0.3422</td>\n<td style=\"text-align:center\">0.3069</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">chair</td>\n<td style=\"text-align:center\">0.0587</td>\n<td style=\"text-align:center\">0.0657</td>\n<td style=\"text-align:center\">0.0342</td>\n<td style=\"text-align:center\">0.0680</td>\n<td style=\"text-align:center\">0.0695</td>\n<td style=\"text-align:center\">0.0752</td>\n<td style=\"text-align:center\">0.0760</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">bicycle</td>\n<td style=\"text-align:center\">0.1013</td>\n<td style=\"text-align:center\">0.1485</td>\n<td style=\"text-align:center\">0.1225</td>\n<td style=\"text-align:center\">0.1871</td>\n<td style=\"text-align:center\">0.1685</td>\n<td style=\"text-align:center\">0.1037</td>\n<td style=\"text-align:center\">0.1490</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">cat</td>\n<td style=\"text-align:center\">0.8737</td>\n<td style=\"text-align:center\">0.8557</td>\n<td style=\"text-align:center\">0.8007</td>\n<td style=\"text-align:center\">0.7982</td>\n<td style=\"text-align:center\">0.8045</td>\n<td style=\"text-align:center\">0.8067</td>\n<td style=\"text-align:center\">0.7732</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">pottedplant</td>\n<td style=\"text-align:center\">0.0694</td>\n<td style=\"text-align:center\">0.1059</td>\n<td style=\"text-align:center\">0.0748</td>\n<td style=\"text-align:center\">0.0878</td>\n<td style=\"text-align:center\">0.0893</td>\n<td style=\"text-align:center\">0.0690</td>\n<td style=\"text-align:center\">0.0865</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">horse</td>\n<td style=\"text-align:center\">0.0556</td>\n<td style=\"text-align:center\">0.0561</td>\n<td style=\"text-align:center\">0.0581</td>\n<td style=\"text-align:center\">0.0770</td>\n<td style=\"text-align:center\">0.0575</td>\n<td style=\"text-align:center\">0.0539</td>\n<td style=\"text-align:center\">0.0522</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">sofa</td>\n<td style=\"text-align:center\">0.2177</td>\n<td style=\"text-align:center\">0.2917</td>\n<td style=\"text-align:center\">0.1699</td>\n<td style=\"text-align:center\">0.1940</td>\n<td style=\"text-align:center\">0.3177</td>\n<td style=\"text-align:center\">0.1863</td>\n<td style=\"text-align:center\">0.1857</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">dog</td>\n<td style=\"text-align:center\">0.6269</td>\n<td style=\"text-align:center\">0.4989</td>\n<td style=\"text-align:center\">0.5015</td>\n<td style=\"text-align:center\">0.5333</td>\n<td style=\"text-align:center\">0.4914</td>\n<td style=\"text-align:center\">0.5572</td>\n<td style=\"text-align:center\">0.4747</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">cow</td>\n<td style=\"text-align:center\">0.5216</td>\n<td style=\"text-align:center\">0.6229</td>\n<td style=\"text-align:center\">0.5283</td>\n<td style=\"text-align:center\">0.6426</td>\n<td style=\"text-align:center\">0.4358</td>\n<td style=\"text-align:center\">0.4227</td>\n<td style=\"text-align:center\">0.4589</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">diningtable</td>\n<td style=\"text-align:center\">0.3076</td>\n<td style=\"text-align:center\">0.3889</td>\n<td style=\"text-align:center\">0.3283</td>\n<td style=\"text-align:center\">0.2404</td>\n<td style=\"text-align:center\">0.4219</td>\n<td style=\"text-align:center\">0.4153</td>\n<td style=\"text-align:center\">0.2627</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">bus</td>\n<td style=\"text-align:center\">0.5865</td>\n<td style=\"text-align:center\">0.5222</td>\n<td style=\"text-align:center\">0.6312</td>\n<td style=\"text-align:center\">0.5853</td>\n<td style=\"text-align:center\">0.5042</td>\n<td style=\"text-align:center\">0.4882</td>\n<td style=\"text-align:center\">0.5576</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">bird</td>\n<td style=\"text-align:center\">0.5339</td>\n<td style=\"text-align:center\">0.5039</td>\n<td style=\"text-align:center\">0.5150</td>\n<td style=\"text-align:center\">0.5152</td>\n<td style=\"text-align:center\">0.5838</td>\n<td style=\"text-align:center\">0.3890</td>\n<td style=\"text-align:center\">0.4680</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">train</td>\n<td style=\"text-align:center\">0.4994</td>\n<td style=\"text-align:center\">0.6541</td>\n<td style=\"text-align:center\">0.6702</td>\n<td style=\"text-align:center\">0.6920</td>\n<td style=\"text-align:center\">0.5959</td>\n<td style=\"text-align:center\">0.5893</td>\n<td style=\"text-align:center\">0.6861</td>\n</tr>\n<tr>\n<td style=\"text-align:center\"><strong>mAP</strong></td>\n<td style=\"text-align:center\"><strong>0.3699</strong></td>\n<td style=\"text-align:center\"><strong>0.3765</strong></td>\n<td style=\"text-align:center\"><strong>0.3748</strong></td>\n<td style=\"text-align:center\"><strong>0.3814</strong></td>\n<td style=\"text-align:center\"><strong>0.3786</strong></td>\n<td style=\"text-align:center\"><strong>0.3562</strong></td>\n<td style=\"text-align:center\"><strong>0.3555</strong></td>\n</tr>\n</tbody>\n</table>\n<p>在测试集上的结果不是很好，不同class的ap差距较大，可能是由于训练时候不平均或者训练集太小的原因</p>\n<p><strong>尝试加入VOC2007的数据进训练集当中，观察结果。</strong><br>解析VOC2007的过程中遇到了OpenCV读取不了图片的BUG。<br>（已修复）<br>VOC2012的数据莫名没有了，因为之前测试过的原因，一直以为是VOC2007的数据解析有问题，大概是Irius的文件上限时间到了自动清除了数据。</p>\n<p>20个类当中的AP差距过大，其实数据集是不平衡的，有的类只有大概1000个样本，但是人这个样本就有2W多，而且之前的训练过程中每次图片都是在训练集中随机选的，所以尝试修改了流程，当所有训练集中的数据都被读取训练过以后再打乱训练集，与此同时配合class_balance的功能使用。</p>\n<p>实际上使用的时候class balance效果不是很好，后面没有开启。</p>\n<p>用了一个较大的学习率尝试训练没有载入imagenet预训练权重的版本。</p>\n<p>交叉法</p>\n<h3 id=\"【13-08-2018】\"><a href=\"#【13-08-2018】\" class=\"headerlink\" title=\"【13/08/2018】\"></a>【13/08/2018】</h3><h4 id=\"soft-NMS\"><a href=\"#soft-NMS\" class=\"headerlink\" title=\"soft-NMS\"></a>soft-NMS</h4><p>传统的非最大抑制算法首先在被检测图片中产生一系列的检测框B以及对应的分数S。当选中最大分数的检测框M，它被从集合B中移出并放入最终检测结果集合D。于此同时，集合B中任何与检测框M的重叠部分大于重叠阈值Nt的检测框也将随之移除。非最大抑制算法中的最大问题就是它将相邻检测框的分数均强制归零。在这种情况下，如果一个真实物体在重叠区域出现，则将导致对该物体的检测失败并降低了算法的平均检测率（average precision, AP）。</p>\n<p>换一种思路，如果我们只是通过一个基于与M重叠程度相关的函数来降低相邻检测框的分数而非彻底剔除。虽然分数被降低，但相邻的检测框仍在物体检测的序列中。图二中的实例可以说明这个问题。<br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/018_1.JPG\" alt=\"image\"><br>针对NMS存在的这个问题，我们提出了一种新的Soft-NMS算法（图三），它只需改动一行代码即可有效改进传统贪心NMS算法。在该算法中，我们基于重叠部分的大小为相邻检测框设置一个衰减函数而非彻底将其分数置为零。<strong>简单来讲，如果一个检测框与M有大部分重叠，它会有很低的分数；而如果检测框与M只有小部分重叠，那么它的原有检测分数不会受太大影响</strong>。在标准数据集PASCAL VOC 和 MS-COCO等标准数据集上，Soft-NMS对现有物体检测算法在多个重叠物体检测的平均准确率有显著的提升。同时，Soft-NMS不需要额外的训练且易于实现，因此，它很容易被集成到当前的物体检测流程中。</p>\n<p>伪代码：<br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/018_2.JPG\" alt=\"image\"></p>\n<p>公式：<br>NMS<br>$$ s_{i}=\\left{<br>\\begin{aligned}<br>s_{i}, \\ \\ \\ \\ iou(M,b_{i}) &lt;  N_{t} \\<br>0, \\ \\ \\ \\ iou(M,b_{i}) \\geq  N_{t}<br>\\end{aligned}<br>\\right.<br>$$</p>\n<p>SOFT NMS<br>$$ s_{i}=\\left{<br>\\begin{aligned}<br>s_{i}, \\ \\ \\ \\ iou(M,b_{i}) &lt;  N_{t} \\<br>1-iou(M,b_{i}), \\ \\ \\ \\ iou(M,b_{i}) \\geq  N_{t}<br>\\end{aligned}<br>\\right.<br>$$</p>\n<p>当相邻检测框与M的重叠度超过重叠阈值Nt后，检测框的检测分数呈线性衰减。在这种情况下，与M相邻很近的检测框衰减程度很大，而远离M的检测框并不受影响。</p>\n<p>但是，上述分数重置函数并不是一个连续函数，在重叠程度超过重叠阈值Nt时，该分数重置函数产生突变，从而可能导致检测结果序列产生大的变动，因此我们更希望找到一个连续的分数重置函数。它对没有重叠的检测框的原有检测分数不产生衰减，同时对高度重叠的检测框产生大的衰减。综合考虑这些因素，我们进一步对soft-NMS中的分数重置函数进行了改进：</p>\n<p>Gaussian penalty:<br><img src=\"https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/018_3.JPG\" alt=\"image\"></p>\n<p>根据这个伪代码以及公式，实现代码：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br><span class=\"line\">107</span><br><span class=\"line\">108</span><br><span class=\"line\">109</span><br><span class=\"line\">110</span><br><span class=\"line\">111</span><br><span class=\"line\">112</span><br><span class=\"line\">113</span><br><span class=\"line\">114</span><br><span class=\"line\">115</span><br><span class=\"line\">116</span><br><span class=\"line\">117</span><br><span class=\"line\">118</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"string\">\"\"\" NMS , delete overlapping box</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">@param boxes: (n,4) box and coresspoding cordinates</span></span><br><span class=\"line\"><span class=\"string\">@param probs: (n,) box adn coresspding possibility</span></span><br><span class=\"line\"><span class=\"string\">@param overlap_thresh: treshold of delet box overlapping</span></span><br><span class=\"line\"><span class=\"string\">@param max_boxes: maximum keeping number of boxes</span></span><br><span class=\"line\"><span class=\"string\">@param method: 1 for linear soft NMS, 2 for gaussian soft NMS</span></span><br><span class=\"line\"><span class=\"string\">@param sigma: parameter of gaussian soft NMS</span></span><br><span class=\"line\"><span class=\"string\">prob_thresh: threshold of probs after soft NMS</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">@return: boxes: boxes cordinates(x1,y1,x2,y2)</span></span><br><span class=\"line\"><span class=\"string\">@return: probs: coresspoding possibility</span></span><br><span class=\"line\"><span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">soft_nms</span><span class=\"params\">(boxes, probs, overlap_thresh=<span class=\"number\">0.9</span>, max_boxes=<span class=\"number\">300</span>, method = <span class=\"number\">1</span>, sigma=<span class=\"number\">0.5</span>, prob_thresh=<span class=\"number\">0.49</span>)</span>:</span></span><br><span class=\"line\">    <span class=\"comment\"># number of input boxes</span></span><br><span class=\"line\">    N = boxes.shape[<span class=\"number\">0</span>]</span><br><span class=\"line\">    <span class=\"comment\"># if the bounding boxes integers, convert them to floats --</span></span><br><span class=\"line\">    <span class=\"comment\"># this is important since we'll be doing a bunch of divisions</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> boxes.dtype.kind == <span class=\"string\">\"i\"</span>:</span><br><span class=\"line\">        boxes = boxes.astype(<span class=\"string\">\"float\"</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># iterate all boxes</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(N):</span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"comment\"># obtain current boxes' cordinates and probs</span></span><br><span class=\"line\">        maxscore = probs[i]</span><br><span class=\"line\">        maxpos = i</span><br><span class=\"line\"></span><br><span class=\"line\">        tx1 = boxes[i,<span class=\"number\">0</span>]</span><br><span class=\"line\">        ty1 = boxes[i,<span class=\"number\">1</span>]</span><br><span class=\"line\">        tx2 = boxes[i,<span class=\"number\">2</span>]</span><br><span class=\"line\">        ty2 = boxes[i,<span class=\"number\">3</span>]</span><br><span class=\"line\">        ts = probs[i]</span><br><span class=\"line\"></span><br><span class=\"line\">        pos = i + <span class=\"number\">1</span></span><br><span class=\"line\">        <span class=\"comment\"># get max box</span></span><br><span class=\"line\">        <span class=\"keyword\">while</span> pos &lt; N:</span><br><span class=\"line\">            <span class=\"keyword\">if</span> maxscore &lt; probs[pos]:</span><br><span class=\"line\">                maxscore = probs[pos]</span><br><span class=\"line\">                maxpos = pos</span><br><span class=\"line\">            pos = pos + <span class=\"number\">1</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># add max box as a detection </span></span><br><span class=\"line\">        boxes[i,<span class=\"number\">0</span>] = boxes[maxpos,<span class=\"number\">0</span>]</span><br><span class=\"line\">        boxes[i,<span class=\"number\">1</span>] = boxes[maxpos,<span class=\"number\">1</span>]</span><br><span class=\"line\">        boxes[i,<span class=\"number\">2</span>] = boxes[maxpos,<span class=\"number\">2</span>]</span><br><span class=\"line\">        boxes[i,<span class=\"number\">3</span>] = boxes[maxpos,<span class=\"number\">3</span>]</span><br><span class=\"line\">        probs[i] = probs[maxpos]</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># swap ith box with position of max box</span></span><br><span class=\"line\">        boxes[maxpos,<span class=\"number\">0</span>] = tx1</span><br><span class=\"line\">        boxes[maxpos,<span class=\"number\">1</span>] = ty1</span><br><span class=\"line\">        boxes[maxpos,<span class=\"number\">2</span>] = tx2</span><br><span class=\"line\">        boxes[maxpos,<span class=\"number\">3</span>] = ty2</span><br><span class=\"line\">        probs[maxpos] = ts</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># cordinates of max box</span></span><br><span class=\"line\">        tx1 = boxes[i,<span class=\"number\">0</span>]</span><br><span class=\"line\">        ty1 = boxes[i,<span class=\"number\">1</span>]</span><br><span class=\"line\">        tx2 = boxes[i,<span class=\"number\">2</span>]</span><br><span class=\"line\">        ty2 = boxes[i,<span class=\"number\">3</span>]</span><br><span class=\"line\">        ts = probs[i]</span><br><span class=\"line\"></span><br><span class=\"line\">        pos = i + <span class=\"number\">1</span></span><br><span class=\"line\">        <span class=\"comment\"># NMS iterations, note that N changes if detection boxes fall below threshold</span></span><br><span class=\"line\">        <span class=\"keyword\">while</span> pos &lt; N:</span><br><span class=\"line\">            x1 = boxes[pos, <span class=\"number\">0</span>]</span><br><span class=\"line\">            y1 = boxes[pos, <span class=\"number\">1</span>]</span><br><span class=\"line\">            x2 = boxes[pos, <span class=\"number\">2</span>]</span><br><span class=\"line\">            y2 = boxes[pos, <span class=\"number\">3</span>]</span><br><span class=\"line\">            s = probs[pos]</span><br><span class=\"line\">            </span><br><span class=\"line\">            <span class=\"comment\"># calculate the areas, +1 for robatness</span></span><br><span class=\"line\">            area = (x2 - x1 + <span class=\"number\">1</span>) * (y2 - y1 + <span class=\"number\">1</span>)</span><br><span class=\"line\">            iw = (min(tx2, x2) - max(tx1, x1) + <span class=\"number\">1</span>)</span><br><span class=\"line\">            <span class=\"comment\"># # confirm left top cordinates less than top right</span></span><br><span class=\"line\">            <span class=\"keyword\">if</span> iw &gt; <span class=\"number\">0</span>:</span><br><span class=\"line\">                ih = (min(ty2, y2) - max(ty1, y1) + <span class=\"number\">1</span>)</span><br><span class=\"line\">                <span class=\"comment\"># confirm left top cordinates less than top right</span></span><br><span class=\"line\">                <span class=\"keyword\">if</span> ih &gt; <span class=\"number\">0</span>:</span><br><span class=\"line\">                    <span class=\"comment\"># find the union</span></span><br><span class=\"line\">                    ua = float((tx2 - tx1 + <span class=\"number\">1</span>) * (ty2 - ty1 + <span class=\"number\">1</span>) + area - iw * ih)</span><br><span class=\"line\">                    <span class=\"comment\">#iou between max box and detection box</span></span><br><span class=\"line\">                    ov = iw * ih / ua</span><br><span class=\"line\"></span><br><span class=\"line\">                    <span class=\"keyword\">if</span> method == <span class=\"number\">1</span>: <span class=\"comment\"># linear</span></span><br><span class=\"line\">                        <span class=\"keyword\">if</span> ov &gt; overlap_thresh: </span><br><span class=\"line\">                            weight = <span class=\"number\">1</span> - ov</span><br><span class=\"line\">                        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">                            weight = <span class=\"number\">1</span></span><br><span class=\"line\">                    <span class=\"keyword\">elif</span> method == <span class=\"number\">2</span>: <span class=\"comment\"># gaussian</span></span><br><span class=\"line\">                        weight = np.exp(-(ov * ov)/sigma)</span><br><span class=\"line\">                    <span class=\"keyword\">else</span>: <span class=\"comment\"># original NMS</span></span><br><span class=\"line\">                        <span class=\"keyword\">if</span> ov &gt; overlap_thresh: </span><br><span class=\"line\">                            weight = <span class=\"number\">0</span></span><br><span class=\"line\">                        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">                            weight = <span class=\"number\">1</span></span><br><span class=\"line\"></span><br><span class=\"line\">                    <span class=\"comment\"># obtain adjusted probs</span></span><br><span class=\"line\">                    probs[pos] = weight*probs[pos]</span><br><span class=\"line\"></span><br><span class=\"line\">   </span><br><span class=\"line\">                    <span class=\"comment\"># if box score falls below threshold, discard the box by swapping with last box</span></span><br><span class=\"line\">                    <span class=\"comment\"># update N</span></span><br><span class=\"line\">                    <span class=\"keyword\">if</span> probs[pos] &lt; prob_thresh:</span><br><span class=\"line\">                        boxes[pos,<span class=\"number\">0</span>] = boxes[N<span class=\"number\">-1</span>, <span class=\"number\">0</span>]</span><br><span class=\"line\">                        boxes[pos,<span class=\"number\">1</span>] = boxes[N<span class=\"number\">-1</span>, <span class=\"number\">1</span>]</span><br><span class=\"line\">                        boxes[pos,<span class=\"number\">2</span>] = boxes[N<span class=\"number\">-1</span>, <span class=\"number\">2</span>]</span><br><span class=\"line\">                        boxes[pos,<span class=\"number\">3</span>] = boxes[N<span class=\"number\">-1</span>, <span class=\"number\">3</span>]</span><br><span class=\"line\">                        probs[pos] = probs[N<span class=\"number\">-1</span>]</span><br><span class=\"line\">                        N = N - <span class=\"number\">1</span></span><br><span class=\"line\">                        pos = pos - <span class=\"number\">1</span></span><br><span class=\"line\"></span><br><span class=\"line\">            pos = pos + <span class=\"number\">1</span></span><br><span class=\"line\">    <span class=\"comment\"># keep is the idx of current keeping objects, front ith objectes</span></span><br><span class=\"line\">    keep = [i <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(N)]</span><br><span class=\"line\">    <span class=\"keyword\">return</span> boxes[keep], probs[keep]</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"【14-08-2018】\"><a href=\"#【14-08-2018】\" class=\"headerlink\" title=\"【14/08/2018】\"></a>【14/08/2018】</h3><h4 id=\"OVERLAPPING-OBJECT-DETECTION\"><a href=\"#OVERLAPPING-OBJECT-DETECTION\" class=\"headerlink\" title=\"OVERLAPPING OBJECT DETECTION\"></a>OVERLAPPING OBJECT DETECTION</h4><h3 id=\"【15-08-2018】\"><a href=\"#【15-08-2018】\" class=\"headerlink\" title=\"【15/08/2018】\"></a>【15/08/2018】</h3><h3 id=\"【16-08-2018】\"><a href=\"#【16-08-2018】\" class=\"headerlink\" title=\"【16/08/2018】\"></a>【16/08/2018】</h3><h3 id=\"【17-08-2018】\"><a href=\"#【17-08-2018】\" class=\"headerlink\" title=\"【17/08/2018】\"></a>【17/08/2018】</h3><h2 id=\"September\"><a href=\"#September\" class=\"headerlink\" title=\"September\"></a>September</h2>"}],"PostAsset":[],"PostCategory":[{"post_id":"ck9jwdnfx0001jlrcgips8q0o","category_id":"ck9jwdnh6000cjlrcwlvmu5ot","_id":"ck9jwdnht000wjlrcle22eby6"},{"post_id":"ck9jwdng10003jlrc7j1i0314","category_id":"ck9jwdnha000ejlrc7w8nwbju","_id":"ck9jwdnhu000yjlrc3igx1rdt"},{"post_id":"ck9jwdng30005jlrcozi1jzkz","category_id":"ck9jwdnha000gjlrcmtigkx3m","_id":"ck9jwdnhu0010jlrc07epd8im"},{"post_id":"ck9jwdng40006jlrcowjfpwux","category_id":"ck9jwdnhb000ijlrcrmjbet7n","_id":"ck9jwdnhu0012jlrcqcsp73m4"},{"post_id":"ck9jwdng50007jlrcdxl2eea8","category_id":"ck9jwdnhb000ijlrcrmjbet7n","_id":"ck9jwdnhu0014jlrcc39pz5hc"},{"post_id":"ck9jwdng50008jlrcbfxepafd","category_id":"ck9jwdnhd000mjlrcleeolcba","_id":"ck9jwdnhu0016jlrc5up5xlbz"},{"post_id":"ck9jwdng60009jlrclgm4jkxk","category_id":"ck9jwdnhd000mjlrcleeolcba","_id":"ck9jwdnhv0018jlrcuk2prea4"},{"post_id":"ck9jwdng7000ajlrckzg7vut2","category_id":"ck9jwdnhd000mjlrcleeolcba","_id":"ck9jwdnhv001ajlrcw2qfd7gj"},{"post_id":"ck9jwdng9000bjlrcae01whra","category_id":"ck9jwdnhd000mjlrcleeolcba","_id":"ck9jwdnhv001cjlrc8pwuopr6"},{"post_id":"ck9jwdnmb001ejlrcuz1ek2ga","category_id":"ck9jwdnmm001fjlrcfjvwudkl","_id":"ck9jwdnn0001ijlrckl3ya8dm"}],"PostTag":[{"post_id":"ck9jwdnfx0001jlrcgips8q0o","tag_id":"ck9jwdnh9000djlrcg0y6kj4x","_id":"ck9jwdnht000vjlrcz7j76a0i"},{"post_id":"ck9jwdng10003jlrc7j1i0314","tag_id":"ck9jwdnha000fjlrcl9ysio6f","_id":"ck9jwdnhu000xjlrcgy1toe22"},{"post_id":"ck9jwdng30005jlrcozi1jzkz","tag_id":"ck9jwdnhb000hjlrcf921d18d","_id":"ck9jwdnhu000zjlrct76cqnj3"},{"post_id":"ck9jwdng40006jlrcowjfpwux","tag_id":"ck9jwdnhb000jjlrcaepavhro","_id":"ck9jwdnhu0011jlrco914igle"},{"post_id":"ck9jwdng50007jlrcdxl2eea8","tag_id":"ck9jwdnhc000ljlrcz97ycyuo","_id":"ck9jwdnhu0013jlrcqkt2554y"},{"post_id":"ck9jwdng50007jlrcdxl2eea8","tag_id":"ck9jwdnhe000njlrcuov4rg19","_id":"ck9jwdnhu0015jlrcds6pc41m"},{"post_id":"ck9jwdng50008jlrcbfxepafd","tag_id":"ck9jwdnhe000pjlrc6067xyo6","_id":"ck9jwdnhv0017jlrcpuhe1ehm"},{"post_id":"ck9jwdng60009jlrclgm4jkxk","tag_id":"ck9jwdnhe000pjlrc6067xyo6","_id":"ck9jwdnhv0019jlrclfknsnv7"},{"post_id":"ck9jwdng7000ajlrckzg7vut2","tag_id":"ck9jwdnhe000pjlrc6067xyo6","_id":"ck9jwdnhv001bjlrcrxwsij7o"},{"post_id":"ck9jwdng9000bjlrcae01whra","tag_id":"ck9jwdnhe000pjlrc6067xyo6","_id":"ck9jwdnhv001djlrcei4c0ku6"},{"post_id":"ck9jwdnmb001ejlrcuz1ek2ga","tag_id":"ck9jwdnhe000pjlrc6067xyo6","_id":"ck9jwdnn0001hjlrcalhwjwbk"},{"post_id":"ck9jwdnmb001ejlrcuz1ek2ga","tag_id":"ck9jwdnmm001gjlrcodx80cmr","_id":"ck9jwdnn1001jjlrcpxrnaqgx"}],"Tag":[{"name":"友人A","_id":"ck9jwdnh9000djlrcg0y6kj4x"},{"name":"data analysis","_id":"ck9jwdnha000fjlrcl9ysio6f"},{"name":"Linux","_id":"ck9jwdnhb000hjlrcf921d18d"},{"name":"MXnet","_id":"ck9jwdnhb000jjlrcaepavhro"},{"name":"Machine Learning","_id":"ck9jwdnhc000ljlrcz97ycyuo"},{"name":"NBA","_id":"ck9jwdnhe000njlrcuov4rg19"},{"name":"Deep Learning","_id":"ck9jwdnhe000pjlrc6067xyo6"},{"name":"Object Detection","_id":"ck9jwdnmm001gjlrcodx80cmr"}]}}