<!DOCTYPE html>



  


<html class="theme-next gemini use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.2" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="Deep Learning,Object Detection," />








  <link rel="shortcut icon" type="image/x-icon" href="/images/favicon.ico?v=5.1.2" />






<meta name="description" content="Gantt chart  Check list  1) preparation  1.1) Familiarization with develop tools  1.1.1) Keras  1.1.2) Pythrch  1.2) Presentation  1.2.1) Poster conference  2) Create image database  2.1) Confirmation">
<meta name="keywords" content="Deep Learning,Object Detection">
<meta property="og:type" content="article">
<meta property="og:title" content="(Logbook) -- Object Detection System Based on CNN and Capsule Network">
<meta property="og:url" content="http://yoursite.com/2018/05/25/LogBook/index.html">
<meta property="og:site_name" content="KAMISAMA&#39;S SPACE">
<meta property="og:description" content="Gantt chart  Check list  1) preparation  1.1) Familiarization with develop tools  1.1.1) Keras  1.1.2) Pythrch  1.2) Presentation  1.2.1) Poster conference  2) Create image database  2.1) Confirmation">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="https://raw.githubusercontent.com/Trouble404/Object-Detection-System-Based-on-CNN-and-Capsule-Network/master/Project%20Plan/gantt%20chart%20of%20project.PNG">
<meta property="og:image" content="https://raw.githubusercontent.com/Trouble404/Object-Detection-System-Based-on-CNN-and-Capsule-Network/master/Learned%20skills/jupyter%20_setting.PNG">
<meta property="og:image" content="https://raw.githubusercontent.com/Microsoft/MMdnn/master/docs/supported.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/Trouble404/Object-Detection-System-Based-on-CNN-and-Capsule-Network/master/Poster/poster.png">
<meta property="og:image" content="https://raw.githubusercontent.com/Trouble404/Object-Detection-System-Based-on-CNN-and-Capsule-Network/master/Research%20Review/LaTex/1.PNG">
<meta property="og:image" content="https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/SPP-NET.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/fast-rcnn.png">
<meta property="og:image" content="https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster-rcnn.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_1.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_2.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_3.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_4.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_5.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_6.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_7.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_19.PNG">
<meta property="og:image" content="https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_8.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_9.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_20.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_21.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_10.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_11.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_23.PNG">
<meta property="og:image" content="https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_24.PNG">
<meta property="og:image" content="https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_22.PNG">
<meta property="og:image" content="https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/img_with_bboex_1.PNG">
<meta property="og:image" content="https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/img_with_bboex_2.PNG">
<meta property="og:image" content="https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/21_1.PNG">
<meta property="og:image" content="https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/21_2.PNG">
<meta property="og:image" content="https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/21_3.PNG">
<meta property="og:image" content="https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/21_4.PNG">
<meta property="og:image" content="https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/21_5.PNG">
<meta property="og:image" content="https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/21_6.PNG">
<meta property="og:image" content="https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/21_7.PNG">
<meta property="og:image" content="https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/22_2.PNG">
<meta property="og:image" content="https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/22_1.png">
<meta property="og:image" content="https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/22_3.PNG">
<meta property="og:image" content="https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/22_4.PNG">
<meta property="og:image" content="https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_3.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_5.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_6.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/25_2.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/26_1.PNG">
<meta property="og:image" content="https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/01_2.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/1_01.png">
<meta property="og:image" content="https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/01_3.png">
<meta property="og:image" content="https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/01_4.png">
<meta property="og:image" content="https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/01_7.JPG">
<meta property="og:image" content="https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/01_8.JPG">
<meta property="og:updated_time" content="2018-08-01T21:28:44.534Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="(Logbook) -- Object Detection System Based on CNN and Capsule Network">
<meta name="twitter:description" content="Gantt chart  Check list  1) preparation  1.1) Familiarization with develop tools  1.1.1) Keras  1.1.2) Pythrch  1.2) Presentation  1.2.1) Poster conference  2) Create image database  2.1) Confirmation">
<meta name="twitter:image" content="https://raw.githubusercontent.com/Trouble404/Object-Detection-System-Based-on-CNN-and-Capsule-Network/master/Project%20Plan/gantt%20chart%20of%20project.PNG">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    sidebar: {"position":"left","display":"post","offset":12,"offset_float":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2018/05/25/LogBook/"/>





  <title>(Logbook) -- Object Detection System Based on CNN and Capsule Network | KAMISAMA'S SPACE</title>
  





  <script type="text/javascript">
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?14798f1d1a9de82a174c7d52dd582793";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>










</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">KAMISAMA'S SPACE</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off"
             placeholder="搜索..." spellcheck="false"
             type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/05/25/LogBook/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Junming Zhang">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="KAMISAMA'S SPACE">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">(Logbook) -- Object Detection System Based on CNN and Capsule Network</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-05-25T00:00:00+01:00">
                2018-05-25
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Msc-Project/" itemprop="url" rel="index">
                    <span itemprop="name">Msc Project</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          
             <span id="/2018/05/25/LogBook/" class="leancloud_visitors" data-flag-title="(Logbook) -- Object Detection System Based on CNN and Capsule Network">
               <span class="post-meta-divider">|</span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               
                 <span class="post-meta-item-text">阅读次数 </span>
               
                 <span class="leancloud-visitors-count"></span>
             </span>
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计</span>
                
                <span title="字数统计">
                  12,300
                </span>
              

              

              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h2 id="Gantt-chart"><a href="#Gantt-chart" class="headerlink" title="Gantt chart"></a>Gantt chart</h2><p><img src="https://raw.githubusercontent.com/Trouble404/Object-Detection-System-Based-on-CNN-and-Capsule-Network/master/Project%20Plan/gantt%20chart%20of%20project.PNG" alt="image"></p>
<hr>
<h2 id="Check-list"><a href="#Check-list" class="headerlink" title="Check list"></a>Check list</h2><ul>
<li style="list-style: none"><input type="checkbox" checked> <strong>1) preparation</strong></li>
<li style="list-style: none"><input type="checkbox" checked> <strong><em>1.1) Familiarization with develop tools</em></strong></li>
<li style="list-style: none"><input type="checkbox" checked> 1.1.1) Keras</li>
<li style="list-style: none"><input type="checkbox" checked> 1.1.2) Pythrch</li>
<li style="list-style: none"><input type="checkbox" checked> <strong><em>1.2) Presentation</em></strong></li>
<li style="list-style: none"><input type="checkbox" checked> 1.2.1) Poster conference</li>
<li style="list-style: none"><input type="checkbox" checked> <strong>2) Create image database</strong></li>
<li style="list-style: none"><input type="checkbox" checked> 2.1) Confirmation of detected objects</li>
<li style="list-style: none"><input type="checkbox" checked> 2.2) Collect and generate the dataset</li>
<li style="list-style: none"><input type="checkbox" checked> <strong>3) Familiarization with CNN based object detection methods</strong></li>
<li style="list-style: none"><input type="checkbox" checked> 3.1) R-CNN</li>
<li style="list-style: none"><input type="checkbox" checked> 3.2) SPP-net</li>
<li style="list-style: none"><input type="checkbox" checked> 3.3) Fast R-CNN</li>
<li style="list-style: none"><input type="checkbox" checked> 3.4) Faster R-CNN</li>
<li style="list-style: none"><input type="checkbox"> <strong>4) Implement object detection system based on one chosen CNN method</strong></li>
<li style="list-style: none"><input type="checkbox" checked> 4.1) Pre-processing of images</li>
<li style="list-style: none"><input type="checkbox"> 4.2) Extracting features</li>
<li style="list-style: none"><input type="checkbox"> 4.3) Mode architecture</li>
<li style="list-style: none"><input type="checkbox"> 4.4) Train model and optimization</li>
<li style="list-style: none"><input type="checkbox"> 4.5) Models ensemble</li>
<li style="list-style: none"><input type="checkbox"> <strong>5) Implement object detection system based on Capsule network</strong></li>
<li style="list-style: none"><input type="checkbox"> 5.1) Investigate and familiarisation with the principle of Capsule network.</li>
<li style="list-style: none"><input type="checkbox"> 5.2) Building and testing mode</li>
<li style="list-style: none"><input type="checkbox"> <strong>6) Analysis work</strong></li>
<li style="list-style: none"><input type="checkbox"> 6.1) Evaluation of detection result of two methods.</li>
<li style="list-style: none"><input type="checkbox"> 6.2) Analyse the difference between two methods.</li>
<li style="list-style: none"><input type="checkbox"> <strong>7) Paperwork and bench inspection</strong></li>
<li style="list-style: none"><input type="checkbox"> 7.1) Logbook</li>
<li style="list-style: none"><input type="checkbox"> 7.2) Write the thesis</li>
<li style="list-style: none"><input type="checkbox"> 7.3) Project video</li>
<li style="list-style: none"><input type="checkbox"> 7.4) Speech and ppt of bench inspection</li>
<li style="list-style: none"><input type="checkbox"> <strong>8) Documents</strong></li>
<li style="list-style: none"><input type="checkbox"> 8.1) Project Brief</li>
</ul>
<hr>
<h2 id="May"><a href="#May" class="headerlink" title="May"></a>May</h2><h3 id="【28-05-2018】"><a href="#【28-05-2018】" class="headerlink" title="【28/05/2018】"></a>【28/05/2018】</h3><p>Keras is a high-level neural networks API, written in Python and capable of running on top of <a href="https://github.com/tensorflow/tensorflow" target="_blank" rel="noopener">TensorFlow</a>, CNTK, or Theano.</p>
<ul>
<li><p><strong><a href="https://keras.io/" target="_blank" rel="noopener">Keras document</a></strong></p>
</li>
<li><p><strong><a href="https://keras-cn.readthedocs.io/en/latest/#keraspython" target="_blank" rel="noopener">Keras 文档</a></strong></p>
</li>
</ul>
<hr>
<h4 id="Installation"><a href="#Installation" class="headerlink" title="Installation"></a>Installation</h4><ul>
<li><p><strong>TensorFlow</strong><br><a href="https://www.visualstudio.com/zh-hans/vs/older-downloads/" target="_blank" rel="noopener">Microsoft Visual Studio 2015</a><br><a href="http://docs.nvidia.com/cuda/cuda-installation-guide-microsoft-windows/" target="_blank" rel="noopener">CUDA 9.0</a><br><a href="https://developer.nvidia.com/cudnn" target="_blank" rel="noopener">cuDNN7</a><br><a href="https://www.anaconda.com/download/" target="_blank" rel="noopener">Anaconda</a></p>
<ul>
<li>Step 1: Install VS2015</li>
<li>Step 2: Install CUDA 9.0 并添加环境变量</li>
<li>Step 3: Install cuDNN7 解压后把cudnn目录下的bin目录加到PATH环境变量里</li>
<li>Step 4: Install Anaconda 把安装路径添加到PATH里去, 在这里我用了 <strong>Python 3.5</strong></li>
<li>Step 5: 使用Anaconda的命令行 新建一个虚拟环境,激活并且关联到jupyterbook<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">conda create  --name tensorflow python=3.5</span><br><span class="line">activate tensorflow</span><br><span class="line">conda install nb_conda</span><br></pre></td></tr></table></figure></li>
<li>Step 6: Install GPU version TensorFlow.<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install -i https://pypi.tuna.tsinghua.edu.cn/simple --ignore-installed --upgrade tensorflow-gpu </span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p><strong>Keras</strong></p>
<ul>
<li>Step 1: 启动之前的 虚拟环境， 并且安装Keras GPU 版本<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">activate tensorflow</span><br><span class="line">pip install keras -U --pre</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
</ul>
<h4 id="在硕士学习过程中，使用Keras的项目"><a href="#在硕士学习过程中，使用Keras的项目" class="headerlink" title="在硕士学习过程中，使用Keras的项目**"></a>在硕士学习过程中，使用Keras的项目**</h4><ul>
<li><strong><a href="https://github.com/Trouble404/NBA-with-Machine-Learning" target="_blank" rel="noopener">NBA with Machine Learning</a></strong></li>
<li><strong><a href="https://github.com/Trouble404/kaggle-Job-Salary-Prediction" target="_blank" rel="noopener">Kaggle- Job salary prediction</a></strong></li>
</ul>
<h4 id="TensorFlow-CPU-切换"><a href="#TensorFlow-CPU-切换" class="headerlink" title="TensorFlow CPU 切换"></a>TensorFlow CPU 切换</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf  </span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> keras.backend.tensorflow_backend <span class="keyword">as</span> KTF  </span><br><span class="line"></span><br><span class="line">os.environ[<span class="string">"CUDA_VISIBLE_DEVICES"</span>] = <span class="string">"0"</span>  <span class="comment">#设置需要使用的GPU的编号</span></span><br><span class="line">config = tf.ConfigProto()</span><br><span class="line">config.gpu_options.per_process_gpu_memory_fraction = <span class="number">0.4</span> <span class="comment">#设置使用GPU容量占GPU总容量的比例</span></span><br><span class="line">sess = tf.Session(config=config)</span><br><span class="line">KTF.set_session(sess)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.device(<span class="string">'/cpu:0'</span>):</span><br></pre></td></tr></table></figure>
<p>这样可以在GPU版本的虚拟环境里面使用CPU计算</p>
<h4 id="Jupyter-Notebook-工作目录设置"><a href="#Jupyter-Notebook-工作目录设置" class="headerlink" title="Jupyter Notebook 工作目录设置"></a>Jupyter Notebook 工作目录设置</h4><p>启动命令行，切换至预设的工作目录， 运行：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">jupyter notebook --generate-config</span><br></pre></td></tr></table></figure></p>
<p><img src="https://raw.githubusercontent.com/Trouble404/Object-Detection-System-Based-on-CNN-and-Capsule-Network/master/Learned%20skills/jupyter%20_setting.PNG" alt="image"></p>
<h2 id="June"><a href="#June" class="headerlink" title="June"></a>June</h2><h3 id="【01-06-2018】"><a href="#【01-06-2018】" class="headerlink" title="【01/06/2018】"></a>【01/06/2018】</h3><p><strong><a href="https://pytorch.org/about/" target="_blank" rel="noopener">PyTorch</a></strong> is a python package that provides two high-level features:</p>
<ul>
<li>Tensor computation (like numpy) with strong GPU acceleration</li>
<li>Deep Neural Networks built on a tape-based autodiff system</li>
</ul>
<table>
<thead>
<tr>
<th style="text-align:left">Package</th>
<th style="text-align:left">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">torch</td>
<td style="text-align:left">a Tensor library like NumPy, with strong GPU support</td>
</tr>
<tr>
<td style="text-align:left">torch.autograd</td>
<td style="text-align:left">a tape based automatic differentiation library that supports all differentiable Tensor operations in torch</td>
</tr>
<tr>
<td style="text-align:left">torch.nn</td>
<td style="text-align:left">a neural networks library deeply integrated with autograd designed for maximum flexibility</td>
</tr>
<tr>
<td style="text-align:left">torch.optim</td>
<td style="text-align:left">an optimization package to be used with torch.nn with standard optimization methods such as SGD, RMSProp, LBFGS, Adam etc.</td>
</tr>
<tr>
<td style="text-align:left">torch.multiprocessing</td>
<td style="text-align:left">python multiprocessing, but with magical memory sharing of torch Tensors across processes. Useful for data loading and hogwild training.</td>
</tr>
<tr>
<td style="text-align:left">torch.utils</td>
<td style="text-align:left">DataLoader, Trainer and other utility functions for convenience</td>
</tr>
<tr>
<td style="text-align:left">torch.legacy(.nn/.optim)</td>
<td style="text-align:left">legacy code that has been ported over from torch for backward compatibility reasons</td>
</tr>
</tbody>
</table>
<hr>
<h4 id="Installation-1"><a href="#Installation-1" class="headerlink" title="Installation"></a>Installation</h4><ul>
<li>Step 1: 使用Anaconda的命令行 新建一个虚拟环境,激活并且关联到jupyterbook<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">conda create  --name pytorch python=3.5</span><br><span class="line">activate pytorch</span><br><span class="line">conda install nb_conda</span><br></pre></td></tr></table></figure></li>
<li>Step 2: Install GPU version PyTorch.<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">conda install pytorch cuda90 -c pytorch </span><br><span class="line">pip install torchvision</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h4 id="Understanding-of-PyTorch"><a href="#Understanding-of-PyTorch" class="headerlink" title="Understanding of PyTorch"></a>Understanding of PyTorch</h4><ul>
<li><p><strong>Tensors</strong><br>Tensors和numpy中的ndarrays较为相似, 与此同时Tensor也能够使用GPU来加速运算</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">x = torch.Tensor(<span class="number">5</span>, <span class="number">3</span>)  <span class="comment"># 构造一个未初始化的5*3的矩阵</span></span><br><span class="line">x = torch.rand(<span class="number">5</span>, <span class="number">3</span>)  <span class="comment"># 构造一个随机初始化的矩阵</span></span><br><span class="line">x <span class="comment"># 此处在notebook中输出x的值来查看具体的x内容</span></span><br><span class="line">x.size()</span><br><span class="line"></span><br><span class="line"><span class="comment">#<span class="doctag">NOTE:</span> torch.Size 事实上是一个tuple, 所以其支持相关的操作*</span></span><br><span class="line">y = torch.rand(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#此处 将两个同形矩阵相加有两种语法结构</span></span><br><span class="line">x + y <span class="comment"># 语法一</span></span><br><span class="line">torch.add(x, y) <span class="comment"># 语法二</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 另外输出tensor也有两种写法</span></span><br><span class="line">result = torch.Tensor(<span class="number">5</span>, <span class="number">3</span>) <span class="comment"># 语法一</span></span><br><span class="line">torch.add(x, y, out=result) <span class="comment"># 语法二</span></span><br><span class="line">y.add_(x) <span class="comment"># 将y与x相加</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 特别注明：任何可以改变tensor内容的操作都会在方法名后加一个下划线'_'</span></span><br><span class="line"><span class="comment"># 例如：x.copy_(y), x.t_(), 这俩都会改变x的值。</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#另外python中的切片操作也是资次的。</span></span><br><span class="line">x[:,<span class="number">1</span>] <span class="comment">#这一操作会输出x矩阵的第二列的所有值</span></span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>Numpy桥</strong><br>将Torch的Tensor和numpy的array相互转换简，注意Torch的Tensor和numpy的array会共享他们的存储空间，修改一个会导致另外的一个也被修改。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 此处演示tensor和numpy数据结构的相互转换</span></span><br><span class="line">a = torch.ones(<span class="number">5</span>)</span><br><span class="line">b = a.numpy()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 此处演示当修改numpy数组之后,与之相关联的tensor也会相应的被修改</span></span><br><span class="line">a.add_(<span class="number">1</span>)</span><br><span class="line">print(a)</span><br><span class="line">print(b)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将numpy的Array转换为torch的Tensor</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">a = np.ones(<span class="number">5</span>)</span><br><span class="line">b = torch.from_numpy(a)</span><br><span class="line">np.add(a, <span class="number">1</span>, out=a)</span><br><span class="line">print(a)</span><br><span class="line">print(b)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 另外除了CharTensor之外，所有的tensor都可以在CPU运算和GPU预算之间相互转换</span></span><br><span class="line"><span class="comment"># 使用CUDA函数来将Tensor移动到GPU上</span></span><br><span class="line"><span class="comment"># 当CUDA可用时会进行GPU的运算</span></span><br><span class="line"><span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">    x = x.cuda()</span><br><span class="line">    y = y.cuda()</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>使用PyTorch设计一个CIFAR10数据集的分类模型</strong><br><strong><a href="https://github.com/Trouble404/Object-Detection-System-Based-on-CNN-and-Capsule-Network/blob/master/Learned%20skills/pytorch.ipynb" target="_blank" rel="noopener">code</a></strong></p>
</li>
<li><p><strong>MMdnn</strong><br>MMdnn is a set of tools to help users inter-operate among different deep learning frameworks. E.g. model conversion and visualization. Convert models between Caffe, Keras, MXNet, Tensorflow, CNTK, PyTorch Onnx and CoreML.</p>
<p><img src="https://raw.githubusercontent.com/Microsoft/MMdnn/master/docs/supported.jpg" alt="iamge"></p>
<p>MMdnn主要有以下特征：</p>
<ul>
<li>模型文件转换器，不同的框架间转换DNN模型</li>
<li>模型代码片段生成器，生成适合不同框架的代码</li>
<li>模型可视化，DNN网络结构和框架参数可视化</li>
<li>模型兼容性测试（正在进行中）</li>
</ul>
<p><strong><a href="https://github.com/Microsoft/MMdnn" target="_blank" rel="noopener">Github</a></strong></p>
</li>
</ul>
<h3 id="【04-06-2018】"><a href="#【04-06-2018】" class="headerlink" title="【04/06/2018】"></a>【04/06/2018】</h3><h4 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset:"></a><strong>Dataset:</strong></h4><p> <strong><a href="http://host.robots.ox.ac.uk/pascal/VOC/voc2012/index.html" target="_blank" rel="noopener">VOC 2012 Dataset</a></strong></p>
<h4 id="Introduce"><a href="#Introduce" class="headerlink" title="Introduce:"></a><strong>Introduce:</strong></h4><p> <strong>Visual Object Classes Challenge 2012 (VOC2012)</strong><br><a href="http://host.robots.ox.ac.uk/pascal/VOC/" target="_blank" rel="noopener">PASCAL</a>‘s full name is Pattern Analysis, Statistical Modelling and Computational Learning.<br>VOC’s full name is <strong>Visual OBject Classes</strong>.<br>The first competition was held in 2005 and terminated in 2012. I will use the last updated dataset which is <a href="http://host.robots.ox.ac.uk/pascal/VOC/voc2012/index.html" target="_blank" rel="noopener">VOC2012</a> dataset.</p>
<p>The main aim of this competition is object detection, there are 20 classes objects in the dataset:</p>
<ul>
<li>person</li>
<li>bird, cat, cow, dog, horse, sheep</li>
<li>aeroplane, bicycle, boat, bus, car, motorbike, train</li>
<li>bottle, chair, dining table, potted plant, sofa, tv/monitor</li>
</ul>
<h4 id="Detection-Task"><a href="#Detection-Task" class="headerlink" title="Detection Task"></a><strong>Detection Task</strong></h4><p>Referenced:<br><strong>The PASCAL Visual Object Classes Challenge 2012 (VOC2012) Development Kit</strong><br><strong>Mark Everingham - John Winn</strong><br><a href="http://host.robots.ox.ac.uk/pascal/VOC/voc2012/htmldoc/index.html" target="_blank" rel="noopener">http://host.robots.ox.ac.uk/pascal/VOC/voc2012/htmldoc/index.html</a></p>
<p><strong>Task:</strong><br>For each of the twenty classes predict the bounding boxes of each object of that class in a test image (if any). Each bounding box should be output with an associated real-valued confidence of the detection so that a precision/recall curve can be drawn. Participants may choose to tackle all, or any subset of object classes, for example ‘cars only’ or ‘motorbikes and cars’.</p>
<p><strong>Competitions</strong>:<br>Two competitions are defined according to the choice of training data:</p>
<ul>
<li>taken from the $VOC_{trainval}$ data provided.</li>
<li>from any source excluding the $VOC_{test}$ data provided.</li>
</ul>
<p><strong>Submission of Results</strong>:<br>A separate text file of results should be generated for each competition and each class e.g. `car’. Each line should be a detection output by the detector in the following format:<br>    <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;image identifier&gt; &lt;confidence&gt; &lt;left&gt; &lt;top&gt; &lt;right&gt; &lt;bottom&gt;</span><br></pre></td></tr></table></figure></p>
<p>where (left,top)-(right,bottom) defines the bounding box of the detected object. The top-left pixel in the image has coordinates $(1,1)$. Greater confidence values signify greater confidence that the detection is correct. An example file excerpt is shown below. Note that for the image 2009_000032, multiple objects are detected:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">comp3_det_test_car.txt:</span><br><span class="line">    ...</span><br><span class="line">    2009_000026 0.949297 172.000000 233.000000 191.000000 248.000000</span><br><span class="line">    2009_000032 0.013737 1.000000 147.000000 114.000000 242.000000</span><br><span class="line">    2009_000032 0.013737 1.000000 134.000000 94.000000 168.000000</span><br><span class="line">    2009_000035 0.063948 455.000000 229.000000 491.000000 243.000000</span><br><span class="line">    ...</span><br></pre></td></tr></table></figure></p>
<p><strong>Evaluation</strong>:<br>The detection task will be judged by the precision/recall curve. The principal quantitative measure used will be the average precision (AP). Detections are considered true or false positives based on the area of overlap with ground truth bounding boxes. To be considered a correct detection, the area of overlap $a_o$ between the predicted bounding box $B_p$ and ground truth bounding box $B_{gt}$ must exceed $50\%$ by the formula: </p>
<center> $a_o = \frac{area(B_p \cap B_{gt})}{area(B_p \cup B_{gt})}$ </center>

<h4 id="XML标注格式"><a href="#XML标注格式" class="headerlink" title="XML标注格式"></a><strong>XML标注格式</strong></h4><p> 对于目标检测来说，每一张图片对应一个xml格式的标注文件。所以你会猜到，就像gemfield准备的训练集有8万张照片一样，在存放xml文件的目录里，这里也将会有8万个xml文件。下面是其中一个xml文件的示例：<br> <figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"> &lt;?xml version="1.0" encoding="utf-8"?&gt;</span><br><span class="line"><span class="tag">&lt;<span class="name">annotation</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">folder</span>&gt;</span>VOC2007<span class="tag">&lt;/<span class="name">folder</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">filename</span>&gt;</span>test100.mp4_3380.jpeg<span class="tag">&lt;/<span class="name">filename</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">size</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">width</span>&gt;</span>1280<span class="tag">&lt;/<span class="name">width</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">height</span>&gt;</span>720<span class="tag">&lt;/<span class="name">height</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">depth</span>&gt;</span>3<span class="tag">&lt;/<span class="name">depth</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">size</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">object</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>gemfield<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">bndbox</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">xmin</span>&gt;</span>549<span class="tag">&lt;/<span class="name">xmin</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">xmax</span>&gt;</span>715<span class="tag">&lt;/<span class="name">xmax</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">ymin</span>&gt;</span>257<span class="tag">&lt;/<span class="name">ymin</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">ymax</span>&gt;</span>289<span class="tag">&lt;/<span class="name">ymax</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">bndbox</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">truncated</span>&gt;</span>0<span class="tag">&lt;/<span class="name">truncated</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">difficult</span>&gt;</span>0<span class="tag">&lt;/<span class="name">difficult</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">object</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">object</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>civilnet<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">bndbox</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">xmin</span>&gt;</span>842<span class="tag">&lt;/<span class="name">xmin</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">xmax</span>&gt;</span>1009<span class="tag">&lt;/<span class="name">xmax</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">ymin</span>&gt;</span>138<span class="tag">&lt;/<span class="name">ymin</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">ymax</span>&gt;</span>171<span class="tag">&lt;/<span class="name">ymax</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">bndbox</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">truncated</span>&gt;</span>0<span class="tag">&lt;/<span class="name">truncated</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">difficult</span>&gt;</span>0<span class="tag">&lt;/<span class="name">difficult</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">object</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">segmented</span>&gt;</span>0<span class="tag">&lt;/<span class="name">segmented</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">annotation</span>&gt;</span></span><br></pre></td></tr></table></figure></p>
<p>在这个测试图片上，我们标注了2个object，一个是gemfield，另一个是civilnet。</p>
<p>在这个xml例子中：</p>
<ul>
<li>bndbox是一个轴对齐的矩形，它框住的是目标在照片中的可见部分；</li>
<li>truncated表明这个目标因为各种原因没有被框完整（被截断了），比如说一辆车有一部分在画面外；</li>
<li>occluded是说一个目标的重要部分被遮挡了（不管是被背景的什么东西，还是被另一个待检测目标遮挡）；</li>
<li>difficult表明这个待检测目标很难识别，有可能是虽然视觉上很清楚，但是没有上下文的话还是很难确认它属于哪个分类；标为difficult的目标在测试成绩的评估中一般会被忽略。</li>
</ul>
<p><strong>注意：在一个object中，name 标签要放在前面，否则的话，目标检测的一个重要工程实现SSD会出现解析数据集错误（另一个重要工程实现py-faster-rcnn则不会）。</strong></p>
<h3 id="【07-06-2018】"><a href="#【07-06-2018】" class="headerlink" title="【07/06/2018】"></a>【07/06/2018】</h3><h4 id="Poster-conference"><a href="#Poster-conference" class="headerlink" title="Poster conference"></a><strong>Poster conference</strong></h4><p><img src="https://raw.githubusercontent.com/Trouble404/Object-Detection-System-Based-on-CNN-and-Capsule-Network/master/Poster/poster.png" alt="iamge"></p>
<p>5 People in one group to present their object.<br>I present this object to my supervisor in this conference.</p>
<h3 id="【11-06-2018】"><a href="#【11-06-2018】" class="headerlink" title="【11/06/2018】"></a>【11/06/2018】</h3><h4 id="R-CNN"><a href="#R-CNN" class="headerlink" title="R-CNN"></a><strong>R-CNN</strong></h4><p>Paper: <a href="https://arxiv.org/abs/1311.2524" target="_blank" rel="noopener">Rich feature hierarchies for accurate object detection and semantic segmentation</a></p>
<p>【<strong>论文主要特点</strong>】（相对传统方法的改进）</p>
<ul>
<li>速度： 经典的目标检测算法使用滑动窗法依次判断所有可能的区域。本文则(采用Selective Search方法)预先提取一系列较可能是物体的候选区域，之后仅在这些候选区域上(采用CNN)提取特征，进行判断。</li>
<li>训练集： 经典的目标检测算法在区域中提取人工设定的特征。本文则采用深度网络进行特征提取。使用两个数据库： 一个较大的识别库   （ImageNet ILSVC 2012）：标定每张图片中物体的类别。一千万图像，1000类。 一个较小的检测库（PASCAL VOC 2007）：标定每张   图片中，物体的类别和位置，一万图像，20类。 本文使用识别库进行预训练得到CNN（有监督预训练），而后用检测库调优参数，最后在   检测库上评测。</li>
</ul>
<p>【<strong>流程</strong>】</p>
<ol>
<li>候选区域生成： 一张图像生成1K~2K个候选区域 （采用Selective Search 方法）</li>
<li>特征提取： 对每个候选区域，使用深度卷积网络提取特征 （CNN） </li>
<li>类别判断： 特征送入每一类的SVM 分类器，判别是否属于该类</li>
<li>位置精修： 使用回归器精细修正候选框位置<center><img src="https://raw.githubusercontent.com/Trouble404/Object-Detection-System-Based-on-CNN-and-Capsule-Network/master/Research%20Review/LaTex/1.PNG" alt="image"></center>

</li>
</ol>
<p>【<strong><a href="https://www.koen.me/research/pub/uijlings-ijcv2013-draft.pdf" target="_blank" rel="noopener">Selective Search</a></strong>】</p>
<ol>
<li>使用一种过分割手段，将图像分割成小区域 (1k~2k 个)</li>
<li>查看现有小区域，按照合并规则合并可能性最高的相邻两个区域。重复直到整张图像合并成一个区域位置</li>
<li>输出所有曾经存在过的区域，所谓候选区域<br>其中合并规则如下： 优先合并以下四种区域：<ul>
<li>颜色（颜色直方图）相近的</li>
<li>纹理（梯度直方图）相近的</li>
<li>合并后总面积小的： 保证合并操作的尺度较为均匀，避免一个大区域陆续“吃掉”其他小区域 （例：设有区域a-b-c-d-e-f-g-h。较好的合并方式是：ab-cd-ef-gh -&gt; abcd-efgh -&gt; abcdefgh。 不好的合并方法是：ab-c-d-e-f-g-h -&gt;abcd-e-f-g-h -&gt;abcdef-gh -&gt; abcdefgh）</li>
<li>合并后，总面积在其BBOX中所占比例大的： 保证合并后形状规则。</li>
</ul>
</li>
</ol>
<h3 id="【12-06-2018】"><a href="#【12-06-2018】" class="headerlink" title="【12/06/2018】"></a>【12/06/2018】</h3><h4 id="SPP-CNN"><a href="#SPP-CNN" class="headerlink" title="SPP-CNN"></a><strong>SPP-CNN</strong></h4><p>Paper: <a href="https://arxiv.org/abs/1406.4729" target="_blank" rel="noopener">Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition</a></p>
<p>【<strong>论文主要特点</strong>】（相对传统方法的改进）</p>
<p>RCNN使用CNN作为特征提取器，首次使得目标检测跨入深度学习的阶段。但是RCNN对于每一个区域候选都需要首先将图片放缩到固定的尺寸（224*224），然后为每个区域候选提取CNN特征。容易看出这里面存在的一些性能瓶颈：</p>
<ul>
<li>速度瓶颈：重复为每个region proposal提取特征是极其费时的，Selective Search对于每幅图片产生2K左右个region proposal，也就是意味着一幅图片需要经过2K次的完整的CNN计算得到最终的结果。</li>
<li>性能瓶颈：对于所有的region proposal防缩到固定的尺寸会导致我们不期望看到的几何形变，而且由于速度瓶颈的存在，不可能采用多尺度或者是大量的数据增强去训练模型。</li>
</ul>
<p>【<strong>流程</strong>】</p>
<ol>
<li>首先通过selective search产生一系列的region proposal</li>
<li>然后训练多尺寸识别网络用以提取区域特征，其中处理方法是每个尺寸的最短边大小在尺寸集合中：<br>$s \in S = {480,576,688,864,1200}$<br>训练的时候通过上面提到的多尺寸训练方法，也就是在每个epoch中首先训练一个尺寸产生一个model，然后加载这个model并训练第二个尺寸，直到训练完所有的尺寸。空间金字塔池化使用的尺度为：1*1，2*2，3*3，6*6，一共是50个bins。</li>
<li>在测试时，每个region proposal选择能使其包含的像素个数最接近224*224的尺寸，提取相 应特征。</li>
<li>训练SVM，BoundingBox回归.<center><img src="https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/SPP-NET.jpg" alt="image"></center>


</li>
</ol>
<h3 id="【13-06-2018】"><a href="#【13-06-2018】" class="headerlink" title="【13/06/2018】"></a>【13/06/2018】</h3><h4 id="FAST-R-CNN"><a href="#FAST-R-CNN" class="headerlink" title="FAST R-CNN"></a><strong>FAST R-CNN</strong></h4><p>Paper: <a href="https://arxiv.org/abs/1504.08083" target="_blank" rel="noopener">Fast R-CNN</a></p>
<p>【<strong>论文主要特点</strong>】（相对传统方法的改进）</p>
<ul>
<li>测试时速度慢：RCNN一张图像内候选框之间大量重叠，提取特征操作冗余。本文将整张图像归一化后直接送入深度网络。在邻接时，才加入候选框信息，在末尾的少数几层处理每个候选框。</li>
<li>训练时速度慢 ：原因同上。在训练时，本文先一张图像送入网络，紧接着送入从这幅图像上提取出的候选区域。这些候选区域的前几层特征不需要再重复计算。</li>
<li>训练所需空间大: RCNN中独立的分类器和回归器需要大量特征作为训练样本。本文把类别判断和位置精调统一用深度网络实现，不再需要额外存储。</li>
</ul>
<p>【<strong>流程</strong>】</p>
<ol>
<li>网络首先用几个卷积层（conv）和最大池化层处理整个图像以产生conv特征图。</li>
<li>然后，对于每个对象建议框（object proposals ），感兴趣区域（region of interest——RoI）池层从特征图提取固定长度的特征向量。</li>
<li>每个特征向量被输送到分支成两个同级输出层的全连接（fc）层序列中：<br>其中一层进行分类，对 目标关于K个对象类（包括全部“背景background”类）产生softmax概率估计，即输出每一个RoI的概率分布；<br>另一层进行bbox regression，输出K个对象类中每一个类的四个实数值。每4个值编码K个类中的每个类的精确边界盒（bounding-box）位置，即输出每一个种类的的边界盒回归偏差。整个结构是使用多任务损失的端到端训练（trained end-to-end with a multi-task loss）。<center><img src="https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/fast-rcnn.png" alt="image"></center>

</li>
</ol>
<h3 id="【14-18-06-2018】"><a href="#【14-18-06-2018】" class="headerlink" title="【14~18/06/2018】"></a>【14~18/06/2018】</h3><h4 id="FASTER-R-CNN"><a href="#FASTER-R-CNN" class="headerlink" title="FASTER R-CNN"></a><strong>FASTER R-CNN</strong></h4><p>I want to use <strong>Faster R-cnn</strong> as the first method to implement object detection system.</p>
<p>Paper: <a href="https://arxiv.org/abs/1506.01497" target="_blank" rel="noopener">Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks</a></p>
<p>在结构上，Faster RCNN已经将特征抽取(feature extraction)，proposal提取，bounding box regression(rect refine)，classification都整合在了一个网络中，使得综合性能有较大提高，在检测速度方面尤为明显。</p>
<center><img src="https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster-rcnn.jpg" alt="image"></center>

<h4 id="流程"><a href="#流程" class="headerlink" title="流程"></a>流程</h4><ol>
<li>Conv layers：作为一种CNN网络目标检测方法，Faster R-CNN首先使用一组基础的conv+relu+pooling层提取image的feature maps。该feature maps被共享用于后续RPN层和全连接层。</li>
<li>Region Proposal Networks：RPN网络用于生成region proposals。该层通过softmax判断anchors属于foreground或者background，再利用bounding box regression修正anchors获得精确的proposals。</li>
<li>Roi Pooling：该层收集输入的feature maps和proposals，综合这些信息后提取proposal feature maps，送入后续全连接层判定目标类别。</li>
<li>Classification：利用proposal feature maps计算proposal的类别，同时再次bounding box regression获得检测框最终的精确位置。</li>
</ol>
<h4 id="解释"><a href="#解释" class="headerlink" title="解释"></a>解释</h4><p><strong>[1. Conv layers]</strong><br>   <center><img src="https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_1.jpg" alt="image"></center><br>   Conv layers包含了conv，pooling，relu三种层。以python版本中的VGG16模型中的faster_rcnn_test.pt的网络结构为例，如图,    Conv layers部分共有13个conv层，13个relu层，4个pooling层。这里有一个非常容易被忽略但是又无比重要的信息，在Conv          layers中：</p>
<ul>
<li>所有的conv层都是： $kernel_size=3$ ， $pad=1$ ， $stride=1$ <br></li>
<li><p>所有的pooling层都是： $kernel_size=2$ ， $pad=0$ ， $stride=2$</p>
<p>为何重要？在Faster RCNN Conv layers中对所有的卷积都做了扩边处理（ $pad=1$ ，即填充一圈0），导致原图变为                $(M+2)\times (N+2)$ 大小，再做3x3卷积后输出 $M\times N$ 。正是这种设置，导致Conv layers中的conv层不改变输入和输出    矩阵大小。如下图：<br><center><img src="https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_2.jpg" alt="image"></center><br>类似的是，Conv layers中的pooling层 $kernel_size=2$ ， $stride=2$ 。这样每个经过pooling层的 $M\times N$ 矩阵，都会变为 $(M/2) \times(N/2)$ 大小。综上所述，在整个Conv layers中，conv和relu层不改变输入输出大小，只有pooling层使输出长宽都变为输入的1/2。<br>那么，一个 $M\times N$ 大小的矩阵经过Conv layers固定变为 $(M/16)\times (N/16)$ ！这样Conv layers生成的featuure map中都可以和原图对应起来。</p>
</li>
</ul>
<p><strong>[2. Region Proposal Networks(RPN)]</strong><br>   经典的检测方法生成检测框都非常耗时，如OpenCV adaboost使用滑动窗口+图像金字塔生成检测框；或如R-CNN使用SS(Selective      Search)方法生成检测框。而Faster RCNN则抛弃了传统的滑动窗口和SS方法，直接使用RPN生成检测框，这也是Faster R-CNN的巨大    优势，能极大提升检测框的生成速度。<br>   <center><img src="https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_3.jpg" alt="image"></center><br>   上图展示了RPN网络的具体结构。可以看到RPN网络实际分为2条线，上面一条通过softmax分类anchors获得foreground和              background（检测目标是foreground），下面一条用于计算对于anchors的bounding box regression偏移量，以获得精确的          proposal。而最后的Proposal层则负责综合foreground anchors和bounding box regression偏移量获取proposals，同时剔除太    小和超出边界的proposals。其实整个网络到了Proposal Layer这里，就完成了相当于目标定位的功能。</p>
<p>   <strong>2.1 多通道图像卷积基础知识介绍</strong></p>
<ul>
<li>对于单通道图像+单卷积核做卷积，之前展示了；</li>
<li><p>对于多通道图像+多卷积核做卷积，计算方式如下：</p>
<center><img src="https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_4.jpg" alt="image"></center><br>输入有3个通道，同时有2个卷积核。对于每个卷积核，先在输入3个通道分别作卷积，再将3个通道结果加起来得到卷积输出。所以对     于某个卷积层，无论输入图像有多少个通道，输出图像通道数总是等于卷积核数量！<br>对多通道图像做 $1\times1$ 卷积，其实就是将输入图像于每个通道乘以卷积系数后加在一起，即相当于把原图像中本来各个独立的     通道“联通”在了一起。<br><br><strong>2.2 Anchors</strong><br>提到RPN网络，就不能不说anchors。所谓anchors，实际上就是一组由rpn/generate_anchors.py生成的矩形。直接运行作者demo中    的generate_anchors.py可以得到以下输出：<br>[[ -84.  -40.   99.   55.]<br>[-176.  -88.  191.  103.]<br>[-360. -184.  375.  199.]<br>[ -56.  -56.   71.   71.]<br>[-120. -120.  135.  135.]<br>[-248. -248.  263.  263.]<br>[ -36.  -80.   51.   95.]<br>[ -80. -168.   95.  183.]<br>[-168. -344.  183.  359.]]<br><br>其中每行的4个值 $(x1,y1,x2,y2)$ 代表矩形左上和右下角点坐标。9个矩形共有3种形状，长宽比为大约为 $width:height = [1:1, 1:2, 2:1]$ 三种，如下图。实际上通过anchors就引入了检测中常用到的多尺度方法。<br><center><img src="https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_5.jpg" alt="image"></center><br>注：关于上面的anchors size，其实是根据检测图像设置的。在python demo中，会把任意大小的输入图像reshape成 $800\times600$。再回头来看anchors的大小，anchors中长宽 1:2 中最大为 $352\times704$ ，长宽 2:1 中最大 $736\times384$ ，基本是cover了 $800\times600$ 的各个尺度和形状。<br>那么这9个anchors是做什么的呢？借用Faster RCNN论文中的原图，如下图，遍历Conv layers计算获得的feature maps，为每一个点都配备这9种anchors作为初始的检测框。这样做获得检测框很不准确，不用担心，后面还有2次bounding box regression可以修正检测框位置。<br><center><img src="https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_6.jpg" alt="image"></center>

<p>解释一下上面这张图的数字。</p>
</li>
</ul>
<ul>
<li>在原文中使用的是ZF model中，其Conv Layers中最后的conv5层num_output=256，对应生成256张特征图，所以相当于feature map每个点都是256-dimensions</li>
<li>在conv5之后，做了rpn_conv/3x3卷积且num_output=256，相当于每个点又融合了周围3x3的空间信息（猜测这样做也许更鲁棒？反正我没测试），同时256-d不变（如图4和图7中的红框）</li>
<li>假设在conv5 feature map中每个点上有k个anchor（默认k=9），而每个anhcor要分foreground和background，所以每个点由256d feature转化为cls=2k scores；而每个anchor都有[x, y, w, h]对应4个偏移量，所以reg=4k coordinates</li>
<li><p>补充一点，全部anchors拿去训练太多了，训练程序会在合适的anchors中随机选取128个postive anchors+128个negative anchors进行训练（什么是合适的anchors下文5.1有解释）</p>
<p> <strong>2.3 softmax判定foreground与background</strong><br> 一副MxN大小的矩阵送入Faster RCNN网络后，到RPN网络变为(M/16)x(N/16)，不妨设 W=M/16 ， H=N/16 。在进入reshape与softmax之前，先做了1x1卷积，如下图：<br> <center><img src="https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_7.jpg" alt="image"></center><br> 该1x1卷积的caffe prototxt定义如下：<br> <center><img src="https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_19.PNG" alt="image"></center><br>可以看到其num_output=18，也就是经过该卷积的输出图像为 $W\times H \times 18$ 大小（注意第二章开头提到的卷积计算方式）。这也就刚好对应了feature maps每一个点都有9个anchors，同时每个anchors又有可能是foreground和background，所有这些信息都保存 $W\times H\times (9\cdot2)$ 大小的矩阵。为何这样做？后面接softmax分类获得foreground anchors，也就相当于初步提取了检测目标候选区域box（一般认为目标在foreground anchors中）。<br>综上所述，RPN网络中利用anchors和softmax初步提取出foreground anchors作为候选区域。</p>
<p> <strong>2.4 bounding box regression原理</strong><br>如图所示绿色框为飞机的Ground Truth(GT)，红色为提取的foreground anchors，即便红色的框被分类器识别为飞机，但是由于红色的框定位不准，这张图相当于没有正确的检测出飞机。所以我们希望采用一种方法对红色的框进行微调，使得foreground anchors和GT更加接近。<br><center><img src="https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_8.jpg" alt="image"></center><br>对于窗口一般使用四维向量 (x, y, w, h) 表示，分别表示窗口的中心点坐标和宽高。对于下图，红色的框A代表原始的Foreground Anchors，绿色的框G代表目标的GT，我们的目标是寻找一种关系，使得输入原始的anchor A经过映射得到一个跟真实窗口G更接近的回归窗口G’，即：</p>
</li>
<li>给定：$anchor A=(A_{x}, A_{y}, A_{w}, A_{h})$ 和 $GT=[G_{x}, G_{y}, G_{w}, G_{h}]$</li>
<li><p>寻找一种变换F，使得：$F(A_{x}, A_{y}, A_{w}, A_{h})=(G_{x}^{‘}, G_{y}^{‘}, G_{w}^{‘}, G_{h}^{‘})$，其中 $(G_{x}^{‘}, G_{y}^{‘}, G_{w}^{‘}, G_{h}^{‘}) \approx (G_{x}, G_{y}, G_{w}, G_{h})$<br><center><img src="https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_9.jpg" alt="image"></center><br>那么经过何种变换F才能从图10中的anchor A变为G’呢？ 比较简单的思路就是:</p>
</li>
<li><p>先做平移</p>
<center><br>$G^{‘}<em>{x} = A</em>{w} \cdot d_{x}(A) + A_{x} $<br>$G^{‘}<em>{y} = A</em>{y} \cdot d_{y}(A) + A_{y} $<br></center></li>
<li>再做缩放<center><br>$G^{‘}<em>{w} = A</em>{w} \cdot exp(d_{w}(A)) $<br>$G^{‘}<em>{h} = A</em>{h} \cdot exp(d_{h}(A)) $<br></center>

</li>
</ul>
<p>观察上面4个公式发现，需要学习的是 $d_{x}(A),d_{y}(A),d_{w}(A),d_{h}(A)$ 这四个变换。当输入的anchor A与GT相差较小时，可以认为这种变换是一种线性变换， 那么就可以用线性回归来建模对窗口进行微调（注意，只有当anchors A和GT比较接近时，才能使用线性回归模型，否则就是复杂的非线性问题了）。</p>
<p>接下来的问题就是如何通过线性回归获得 $d_{x}(A),d_{y}(A),d_{w}(A),d_{h}(A)$ 了。线性回归就是给定输入的特征向量X, 学习一组参数W, 使得经过线性回归后的值跟真实值Y非常接近，即$Y=WX$。对于该问题，输入X是cnn feature map，定义为Φ；同时还有训练传入A与GT之间的变换量，即$(t_{x}, t_{y}, t_{w}, t_{h})$。输出是$d_{x}(A),d_{y}(A),d_{w}(A),d_{h}(A)$四个变换。那么目标函数可以表示为：</p>
<center><br>$d_{<em>}(A) = w^{T}_{</em>} \cdot \phi(A)$<br></center>

<p>其中Φ(A)是对应anchor的feature map组成的特征向量，w是需要学习的参数，d(A)是得到的预测值（*表示 x，y，w，h，也就是每一个变换对应一个上述目标函数）。为了让预测值$(t_{x}, t_{y}, t_{w}, t_{h})$与真实值差距最小，设计损失函数：</p>
<center><br>$Loss = \sum^{N}<em>{i}(t^{i}</em>{<em>} - \hat{w}^{T}_{</em>} \cdot \phi(A^{i}))^{2}$<br></center><br>函数优化目标为：<br><center><br><img src="https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_20.jpg" alt="image"><br></center><br>需要说明，只有在GT与需要回归框位置比较接近时，才可近似认为上述线性变换成立。<br>说完原理，对应于Faster RCNN原文，foreground anchor与ground truth之间的平移量 $(t_x, t_y)$ 与尺度因子 $(t_w, t_h)$ 如下：<br><center><br><img src="https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_21.jpg" alt="image"><br></center><br>对于训练bouding box regression网络回归分支，输入是cnn feature Φ，监督信号是Anchor与GT的差距 $(t_x, t_y, t_w, t_h)$，即训练目标是：输入Φ的情况下使网络输出与监督信号尽可能接近。<br>那么当bouding box regression工作时，再输入Φ时，回归网络分支的输出就是每个Anchor的平移量和变换尺度 $(t_x, t_y, t_w, t_h)$，显然即可用来修正Anchor位置了。<br><br>   <strong>2.5 对proposals进行bounding box regression</strong><br>在了解bounding box regression后，再回头来看RPN网络第二条线路，如下图。<br><center><br><img src="https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_10.jpg" alt="image"><br></center><br>其 $num_output=36$ ，即经过该卷积输出图像为 $W\times H\times 36$ ，在caffe blob存储为 [1, 36, H, W] ，这里相当于feature maps每个点都有9个anchors，每个anchors又都有4个用于回归的$d_{x}(A),d_{y}(A),d_{w}(A),d_{h}(A)$变换量。<br><br>   <strong>2.6 Proposal Layer</strong><br>Proposal Layer负责综合所有 $d_{x}(A),d_{y}(A),d_{w}(A),d_{h}(A)$ 变换量和foreground anchors，计算出精准的proposal，送入后续RoI Pooling Layer。<br>首先解释im_info。对于一副任意大小PxQ图像，传入Faster RCNN前首先reshape到固定 $M\times N$ ，im_info=[M, N, scale_factor]则保存了此次缩放的所有信息。然后经过Conv Layers，经过4次pooling变为 $W\times H=(M/16)\times(N/16)$ 大小，其中feature_stride=16则保存了该信息，用于计算anchor偏移量。<br><center><br><img src="https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_11.jpg" alt="image"><br></center>

<p>Proposal Layer forward（caffe layer的前传函数）按照以下顺序依次处理：</p>
<ol>
<li>生成anchors，利用$[d_{x}(A),d_{y}(A),d_{w}(A),d_{h}(A)]$对所有的anchors做bbox regression回归（这里的anchors生成和训练时完全一致）</li>
<li>按照输入的foreground softmax scores由大到小排序anchors，提取前pre_nms_topN(e.g. 6000)个anchors，即提取修正位置后的foreground anchors。</li>
<li>限定超出图像边界的foreground anchors为图像边界（防止后续roi pooling时proposal超出图像边界）</li>
<li>剔除非常小（width&lt;threshold or height&lt;threshold）的foreground anchors</li>
<li>进行nonmaximum suppression</li>
<li>再次按照nms后的foreground softmax scores由大到小排序fg anchors，提取前post_nms_topN(e.g. 300)结果作为proposal输出。</li>
</ol>
<p>之后输出 proposal=[x1, y1, x2, y2] ，注意，由于在第三步中将anchors映射回原图判断是否超出边界，所以这里输出的proposal是对应 $M\times N$ 输入图像尺度的，这点在后续网络中有用。另外我认为，严格意义上的检测应该到此就结束了，后续部分应该属于识别了~   </p>
<p><strong>RPN</strong>网络结构就介绍到这里，总结起来就是：<br><strong>生成anchors -&gt; softmax分类器提取fg anchors -&gt; bbox reg回归fg anchors -&gt; Proposal Layer生成proposals</strong></p>
<h3 id="【19-06-2018】"><a href="#【19-06-2018】" class="headerlink" title="【19/06/2018】"></a>【19/06/2018】</h3><h4 id="处理-XML-文档"><a href="#处理-XML-文档" class="headerlink" title="处理 XML 文档"></a>处理 XML 文档</h4><p>使用 xml.etree.ElementTree 这个包去解析XML文件， 并且整理成为list形式<br>【流程】</p>
<ul>
<li>读取XML文件</li>
<li>区分训练集测试集根据竞赛要求</li>
<li>解析XML文档收录到PYTHON词典中<br><center><br><img src="https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_23.PNG" alt="image"><br></center><br>Github 的 jupyter notebook <a href="https://github.com/Trouble404/Object-Detection-System-Based-on-CNN-and-Capsule-Network/blob/master/Faster-RCNN/parser_voc2012_xml_and_plotting.ipynb" target="_blank" rel="noopener">地址</a> </li>
</ul>
<p>训练集根据竞赛的 trainval.txt 文件给的图片作为训练集<br>其余的作为训练集</p>
<p>解析后， 总共有 17125 张图片，<br>其中 11540 张作为训练集</p>
<p>图片中的20个类的统计情况：</p>
<center><br><img src="https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_24.PNG" alt="image"><br></center> 


<h3 id="【20-06-2018】"><a href="#【20-06-2018】" class="headerlink" title="【20/06/2018】"></a>【20/06/2018】</h3><h4 id="根据信息画出BBOXES"><a href="#根据信息画出BBOXES" class="headerlink" title="根据信息画出BBOXES"></a>根据信息画出BBOXES</h4><p>安装 cv2 这个包<br>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install opencv-python</span><br></pre></td></tr></table></figure><br>注意： OpenCV-python 中颜色格式 是BGR 而不是 RGB</p>
<p>在VOC2012数据集里面，总共有20类， 根据不同的种类用不同的颜色和唯一的编码画BBOXES。</p>
<table>
<thead>
<tr>
<th style="text-align:left">class</th>
<th style="text-align:left">class_mapping</th>
<th style="text-align:left">BGR of bbox</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">Person</td>
<td style="text-align:left">0</td>
<td style="text-align:left">(0, 0, 255)</td>
</tr>
<tr>
<td style="text-align:left">Aeroplane</td>
<td style="text-align:left">1</td>
<td style="text-align:left">(0, 0, 255)</td>
</tr>
<tr>
<td style="text-align:left">Tvmonitor</td>
<td style="text-align:left">2</td>
<td style="text-align:left">(0, 128, 0)</td>
</tr>
<tr>
<td style="text-align:left">Train</td>
<td style="text-align:left">3</td>
<td style="text-align:left">(128, 128, 128)</td>
</tr>
<tr>
<td style="text-align:left">Boat</td>
<td style="text-align:left">4</td>
<td style="text-align:left">(0, 165, 255)</td>
</tr>
<tr>
<td style="text-align:left">Dog</td>
<td style="text-align:left">5</td>
<td style="text-align:left">(0, 255, 255)</td>
</tr>
<tr>
<td style="text-align:left">Chair</td>
<td style="text-align:left">6</td>
<td style="text-align:left">(80, 127, 255)</td>
</tr>
<tr>
<td style="text-align:left">Bird</td>
<td style="text-align:left">7</td>
<td style="text-align:left">(208, 224, 64)</td>
</tr>
<tr>
<td style="text-align:left">Bicycle</td>
<td style="text-align:left">8</td>
<td style="text-align:left">(235, 206, 135)</td>
</tr>
<tr>
<td style="text-align:left">Bottle</td>
<td style="text-align:left">9</td>
<td style="text-align:left">(128, 0, 0)</td>
</tr>
<tr>
<td style="text-align:left">Sheep</td>
<td style="text-align:left">10</td>
<td style="text-align:left">(140, 180, 210)</td>
</tr>
<tr>
<td style="text-align:left">Diningtable</td>
<td style="text-align:left">11</td>
<td style="text-align:left">(0, 255, 0)</td>
</tr>
<tr>
<td style="text-align:left">Horse</td>
<td style="text-align:left">12</td>
<td style="text-align:left">(133, 21, 199)</td>
</tr>
<tr>
<td style="text-align:left">Motorbike</td>
<td style="text-align:left">13</td>
<td style="text-align:left">(47, 107, 85)</td>
</tr>
<tr>
<td style="text-align:left">Sofa</td>
<td style="text-align:left">14</td>
<td style="text-align:left">(19, 69, 139)</td>
</tr>
<tr>
<td style="text-align:left">Cow</td>
<td style="text-align:left">15</td>
<td style="text-align:left">(222, 196, 176)</td>
</tr>
<tr>
<td style="text-align:left">Car</td>
<td style="text-align:left">16</td>
<td style="text-align:left">(0, 0, 0)</td>
</tr>
<tr>
<td style="text-align:left">Cat</td>
<td style="text-align:left">17</td>
<td style="text-align:left">(225, 105, 65)</td>
</tr>
<tr>
<td style="text-align:left">Bus</td>
<td style="text-align:left">18</td>
<td style="text-align:left">(255, 255, 255)</td>
</tr>
<tr>
<td style="text-align:left">Pottedplant</td>
<td style="text-align:left">19</td>
<td style="text-align:left">(205, 250, 255)</td>
</tr>
</tbody>
</table>
<p>我写了一个show_image_with_bbox函数去画出带BBOXES的图根据处理XML文件得到的list:</p>
<center><br><img src="https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_22.PNG" alt="image"><br></center><br>Github 的 jupyter notebook <a href="https://github.com/Trouble404/Object-Detection-System-Based-on-CNN-and-Capsule-Network/blob/master/Faster-RCNN/parser_voc2012_xml_and_plotting.ipynb" target="_blank" rel="noopener">地址</a><br><br>EXAMPLE:<br><center><br><img src="https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/img_with_bboex_1.PNG" alt="image"><br><img src="https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/img_with_bboex_2.PNG" alt="image"><br></center>  

<h3 id="【21-06-2018】"><a href="#【21-06-2018】" class="headerlink" title="【21/06/2018】"></a>【21/06/2018】</h3><h4 id="config-setting"><a href="#config-setting" class="headerlink" title="config setting"></a>config setting</h4><p>set config class:<br>                 for image enhancement:</p>
<center><br><img src="https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/21_1.PNG" alt="image"><br></center>  

<h4 id="image-enhancement"><a href="#image-enhancement" class="headerlink" title="image enhancement"></a>image enhancement</h4><center><br><img src="https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/21_2.PNG" alt="image"><br></center><br>According to the config of three peremeters, users could augment image with 3 different ways or using them all.<br>For horizontal and vertical flips, 1/3 probability to triggle<br>With 0,90,180,270 rotation,<br>This function could increase the number of datasets.<br><br>image flips and rotation are realized by opencv and replace of height and width<br>New cordinates of bboxes are calculated acccording to different change of image<br><br>detailed in Github, jupyter notebook: <a href="https://github.com/Trouble404/Object-Detection-System-Based-on-CNN-and-Capsule-Network/blob/master/Faster-RCNN/config_set_and_image_enhance.ipynb" target="_blank" rel="noopener">address</a><br><br>Orignal image:<br><center><br><img src="https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/21_3.PNG" alt="image"><br></center><br>horizontal flip:<br><center><br><img src="https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/21_4.PNG" alt="image"><br></center><br>Vertical filp:<br><center><br><img src="https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/21_5.PNG" alt="image"><br></center><br>Random rotation:<br><center><br><img src="https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/21_6.PNG" alt="image"><br></center><br>Horizontal and then vertical flips:<br><center><br><img src="https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/21_7.PNG" alt="image"><br></center>  

<h3 id="【22-06-2018】"><a href="#【22-06-2018】" class="headerlink" title="【22/06/2018】"></a>【22/06/2018】</h3><h4 id="Image-rezise"><a href="#Image-rezise" class="headerlink" title="Image rezise"></a>Image rezise</h4><p>This function is to rezise input image to a uniform size with same shortest side</p>
<center><br><img src="https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/22_2.PNG" alt="image"><br></center> 

<p>According to set the value of shortest side, convergent-divergent or augmented another side proportion</p>
<p>Test:<br>Left image is resized image, in this case, the orignal image amplified.</p>
<center><br><img src="https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/22_1.png" alt="image"><br></center> 

<h4 id="Class-Balance"><a href="#Class-Balance" class="headerlink" title="Class Balance"></a>Class Balance</h4><p>When training the model, if we sent image with no repeating classes, it may help to improve the performance of model. Therefore, this function is to make sure no repeating classes in two closed input image.</p>
<center><br><img src="https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/22_3.PNG" alt="image"><br></center> 

<p>Test:</p>
<p><center><br><img src="https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/22_4.PNG" alt="image"><br></center><br>Random output 4 iamge with is function, it could find no repeating classes in two closed image. However, it may reduce the number of trainning image because skip some images.</p>
<h3 id="【25-26-06-2018】"><a href="#【25-26-06-2018】" class="headerlink" title="【25~26/06/2018】"></a>【25~26/06/2018】</h3><h4 id="Region-Proposal-Networks-RPN"><a href="#Region-Proposal-Networks-RPN" class="headerlink" title="Region Proposal Networks(RPN)"></a>Region Proposal Networks(RPN)</h4><p><center><img src="https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_3.jpg" alt="image"></center><br>可以看到RPN网络实际分为2条线，上面一条通过softmax分类anchors获得foreground和background（检测目标是foreground），下面一条用于计算对于anchors的bounding box regression偏移量，以获得精确的proposal。而最后的Proposal层则负责综合foreground anchors和bounding box regression偏移量获取proposals，同时剔除太小和超出边界的proposals。其实整个网络到了Proposal Layer这里，就完成了相当于目标定位的功能。</p>
<h4 id="Anchors"><a href="#Anchors" class="headerlink" title="Anchors"></a>Anchors</h4><p>对每一个点生成的矩形</p>
<p><center><img src="https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_5.jpg" alt="image"></center><br>其中每行的4个值 (x1,y1,x2,y2) 代表矩形左上和右下角点坐标。9个矩形共有3种形状，长宽比为大约为 width:height = [1:1, 1:2, 2:1]</p>
<p><center><img src="https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/faster_6.jpg" alt="image"></center><br>通过遍历Conv layers计算获得的feature maps，为每一个点都配备这9种anchors作为初始的检测框。这样做获得检测框很不准确，不用担心，后面还有2次bounding box regression可以修正检测框位置.</p>
<h4 id="Code"><a href="#Code" class="headerlink" title="Code"></a>Code</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">""" intersection of two bboxes</span></span><br><span class="line"><span class="string">@param ai: left top x,y and right bottom x,y coordinates of bbox 1</span></span><br><span class="line"><span class="string">@param bi: left top x,y and right bottom x,y coordinates of bbox 2</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">@return: area_union: whether contain target classes</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">intersection</span><span class="params">(ai, bi)</span>:</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">""" union of two bboxes</span></span><br><span class="line"><span class="string">@param au: left top x,y and right bottom x,y coordinates of bbox 1</span></span><br><span class="line"><span class="string">@param bu: left top x,y and right bottom x,y coordinates of bbox 2</span></span><br><span class="line"><span class="string">@param area_intersection: intersection area</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">@return: area_union: whether contain target classes</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">union</span><span class="params">(au, bu, area_intersection)</span>:</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">""" calculate ratio of intersection and union</span></span><br><span class="line"><span class="string">@param a: left top x,y and right bottom x,y coordinates of bbox 1</span></span><br><span class="line"><span class="string">@param b: left top x,y and right bottom x,y coordinates of bbox 2</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">@return: ratio of intersection and union of two bboxes</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">iou</span><span class="params">(a, b)</span>:</span></span><br></pre></td></tr></table></figure>
<p><strong>IOU is used to bounding box regression</strong></p>
<hr>
<p><strong> rpn calculation</strong></p>
<ol>
<li>Traversal all pre-anchors to calculate IOU with GT bboxes</li>
<li>Set number and proprty of pre-anchors</li>
<li>return specity number of result(Anchors)</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">""" </span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">@param C: configuration</span></span><br><span class="line"><span class="string">@param img_data: parsered xml information</span></span><br><span class="line"><span class="string">@param width: orignal width of image</span></span><br><span class="line"><span class="string">@param hegiht: orignal height of image</span></span><br><span class="line"><span class="string">@param resized_width: resized width of image after image processing</span></span><br><span class="line"><span class="string">@param resized_heighth: resized height of image after image processing</span></span><br><span class="line"><span class="string">@param img_length_calc_function: Keras's image_dim_ordering function</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">@return: np.copy(y_rpn_cls): whether contain target classes</span></span><br><span class="line"><span class="string">@return: np.copy(y_rpn_regr): corrspoding return of gradient</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calc_rpn</span><span class="params">(C, img_data, width, height, resized_width, resized_height, img_length_calc_function)</span>:</span></span><br></pre></td></tr></table></figure>
<p>【注：其只会返回num_regions（这里设置为256）个有效的正负样本 】</p>
<p>【流程】<br>Initialise paramters: see <a href="https://github.com/Trouble404/Object-Detection-System-Based-on-CNN-and-Capsule-Network/blob/master/Faster-RCNN/rpn_calculation.ipynb" target="_blank" rel="noopener">jupyter notebook</a></p>
<p>Calculate the size of map feature:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(output_width, output_height) = img_length_calc_function(resized_width, resized_height)</span><br></pre></td></tr></table></figure></p>
<p><br><br>Get the GT box coordinates, and resize to account for image resizing<br>after rezised functon, the coordinates of bboxes need to re-calculation:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> bbox_num, bbox <span class="keyword">in</span> enumerate(img_data[<span class="string">'bboxes'</span>]):</span><br><span class="line">	gta[bbox_num, <span class="number">0</span>] = bbox[<span class="string">'x1'</span>] * (resized_width / float(width))</span><br><span class="line">	gta[bbox_num, <span class="number">1</span>] = bbox[<span class="string">'x2'</span>] * (resized_width / float(width))</span><br><span class="line">	gta[bbox_num, <span class="number">2</span>] = bbox[<span class="string">'y1'</span>] * (resized_height / float(height))</span><br><span class="line">	gta[bbox_num, <span class="number">3</span>] = bbox[<span class="string">'y2'</span>] * (resized_height / float(height))</span><br></pre></td></tr></table></figure></p>
<p>【注意gta的存储形式是（x1,x2,y1,y2）而不是（x1,y1,x2,y2）】<br><br><br>Traverse all possible group of sizes<br>anchor box scales [128, 256, 512]<br>anchor box ratios [1:1,1:2,2:1]<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> anchor_size_idx <span class="keyword">in</span> range(len(anchor_sizes)):</span><br><span class="line">	<span class="keyword">for</span> anchor_ratio_idx <span class="keyword">in</span> range(len(anchor_ratios)):</span><br><span class="line">		anchor_x = anchor_sizes[anchor_size_idx] * anchor_ratios[anchor_ratio_idx][<span class="number">0</span>]</span><br><span class="line">		anchor_y = anchor_sizes[anchor_size_idx] * anchor_ratios[anchor_ratio_idx][<span class="number">1</span>]</span><br></pre></td></tr></table></figure></p>
<p>Traver one bbox group, all pre boxes generated by anchors</p>
<p>output_width，output_height：width and height of map feature<br>downscale：mapping ration, defualt 16<br>if to delete box out of iamge</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> ix <span class="keyword">in</span> range(output_width):</span><br><span class="line">	x1_anc = downscale * (ix + <span class="number">0.5</span>) - anchor_x / <span class="number">2</span></span><br><span class="line">	x2_anc = downscale * (ix + <span class="number">0.5</span>) + anchor_x / <span class="number">2</span></span><br><span class="line"></span><br><span class="line">	<span class="keyword">if</span> x1_anc &lt; <span class="number">0</span> <span class="keyword">or</span> x2_anc &gt; resized_width:</span><br><span class="line">		<span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line">	<span class="keyword">for</span> jy <span class="keyword">in</span> range(output_height):</span><br><span class="line">		y1_anc = downscale * (jy + <span class="number">0.5</span>) - anchor_y / <span class="number">2</span></span><br><span class="line">		y2_anc = downscale * (jy + <span class="number">0.5</span>) + anchor_y / <span class="number">2</span></span><br><span class="line"></span><br><span class="line">		<span class="keyword">if</span> y1_anc &lt; <span class="number">0</span> <span class="keyword">or</span> y2_anc &gt; resized_height:</span><br><span class="line">			<span class="keyword">continue</span></span><br></pre></td></tr></table></figure>
<p><br></p>
<p>【注：现在我们确定了一个预选框组合有确定了中心点那就是唯一确定一个框了，接下来就是来确定这个宽的性质了：是否包含物体、如包含物体其回归梯度是多少】</p>
<p>要确定以上两个性质，每一个框都需要遍历图中的所有bboxes 然后计算该预选框与bbox的交并比（IOU）<br>如果现在的交并比curr_iou大于该bbox最好的交并比或者大于给定的阈值则求下列参数，这些参数是后来要用的即回归梯度</p>
<p>tx：两个框中心的宽的距离与预选框宽的比<br>ty:同tx<br>tw:bbox的宽与预选框宽的比<br>th:同理</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> curr_iou &gt; best_iou_for_bbox[bbox_num] <span class="keyword">or</span> curr_iou &gt; C.rpn_max_overlap:</span><br><span class="line">	cx = (gta[bbox_num, <span class="number">0</span>] + gta[bbox_num, <span class="number">1</span>]) / <span class="number">2.0</span></span><br><span class="line">	cy = (gta[bbox_num, <span class="number">2</span>] + gta[bbox_num, <span class="number">3</span>]) / <span class="number">2.0</span></span><br><span class="line">	cxa = (x1_anc + x2_anc) / <span class="number">2.0</span></span><br><span class="line">	cya = (y1_anc + y2_anc) / <span class="number">2.0</span></span><br><span class="line"></span><br><span class="line">	tx = (cx - cxa) / (x2_anc - x1_anc)</span><br><span class="line">	ty = (cy - cya) / (y2_anc - y1_anc)</span><br><span class="line">	tw = np.log((gta[bbox_num, <span class="number">1</span>] - gta[bbox_num, <span class="number">0</span>]) / (x2_anc - x1_anc))</span><br><span class="line">	th = np.log((gta[bbox_num, <span class="number">3</span>] - gta[bbox_num, <span class="number">2</span>])) / (y2_anc - y1_anc)</span><br></pre></td></tr></table></figure>
<p>对应于Faster RCNN原文，foreground anchor与ground truth之间的平移量 $(t_x, t_y)$ 如下：</p>
<p><center><img src="https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/25_2.jpg" alt="image"></center><br>对于训练bouding box regression网络回归分支，输入是cnn feature Φ，监督信号是Anchor与GT的差距 $(t_x, t_y, t_w, t_h)$，即训练目标是：输入 Φ的情况下使网络输出与监督信号尽可能接近。<br>那么当bouding box regression工作时，再输入Φ时，回归网络分支的输出就是每个Anchor的平移量和变换尺度 $(t_x, t_y, t_w, t_h)$，显然即可用来修正Anchor位置了。</p>
<p><br><br>如果相交的不是背景，那么进行一系列更新</p>
<p>关于bbox的相关信息更新<br>预选框的相关更新：如果交并比大于阈值这是pos<br>best_iou_for_loc：其记录的是有最大交并比为多少和其对应的回归梯度<br>num_anchors_for_bbox[bbox_num]：记录的是bbox拥有的pos预选框的个数<br>如果小于最小阈值是neg，在这两个之间是neutral<br>需要注意的是：判断一个框为neg需要其与所有的bbox的交并比都小于最小的阈值</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> img_data[<span class="string">'bboxes'</span>][bbox_num][<span class="string">'class'</span>] != <span class="string">'bg'</span>:</span><br><span class="line"></span><br><span class="line">	<span class="comment"># all GT boxes should be mapped to an anchor box, so we keep track of which anchor box was best</span></span><br><span class="line">	<span class="keyword">if</span> curr_iou &gt; best_iou_for_bbox[bbox_num]:</span><br><span class="line">		best_anchor_for_bbox[bbox_num] = [jy, ix, anchor_ratio_idx, anchor_size_idx]</span><br><span class="line">		best_iou_for_bbox[bbox_num] = curr_iou</span><br><span class="line">		best_x_for_bbox[bbox_num, :] = [x1_anc, x2_anc, y1_anc, y2_anc]</span><br><span class="line">		best_dx_for_bbox[bbox_num, :] = [tx, ty, tw, th]</span><br><span class="line"></span><br><span class="line">	<span class="keyword">if</span> curr_iou &gt; C.rpn_max_overlap:</span><br><span class="line">		bbox_type = <span class="string">'pos'</span></span><br><span class="line">		num_anchors_for_bbox[bbox_num] += <span class="number">1</span></span><br><span class="line">		<span class="keyword">if</span> curr_iou &gt; best_iou_for_loc:</span><br><span class="line">			best_iou_for_loc = curr_iou</span><br><span class="line">			best_regr = (tx, ty, tw, th)</span><br><span class="line"></span><br><span class="line">	<span class="keyword">if</span> C.rpn_min_overlap &lt; curr_iou &lt; C.rpn_max_overlap:</span><br><span class="line"></span><br><span class="line">		<span class="keyword">if</span> bbox_type != <span class="string">'pos'</span>:</span><br><span class="line">			bbox_type = <span class="string">'neutral'</span></span><br></pre></td></tr></table></figure>
<p><br><br>当结束对所有的bbox的遍历时，来确定该预选宽的性质。</p>
<p>y_is_box_valid：该预选框是否可用（nertual就是不可用的）<br>y_rpn_overlap：该预选框是否包含物体<br>y_rpn_regr:回归梯度<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> bbox_type == <span class="string">'neg'</span>:</span><br><span class="line">    y_is_box_valid[jy, ix, anchor_ratio_idx + n_anchratios * anchor_size_idx] = <span class="number">1</span></span><br><span class="line">    y_rpn_overlap[jy, ix, anchor_ratio_idx + n_anchratios * anchor_size_idx] = <span class="number">0</span></span><br><span class="line"><span class="keyword">elif</span> bbox_type == <span class="string">'neutral'</span>:</span><br><span class="line">    y_is_box_valid[jy, ix, anchor_ratio_idx + n_anchratios * anchor_size_idx] = <span class="number">0</span></span><br><span class="line">    y_rpn_overlap[jy, ix, anchor_ratio_idx + n_anchratios * anchor_size_idx] = <span class="number">0</span></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    y_is_box_valid[jy, ix, anchor_ratio_idx + n_anchratios * anchor_size_idx] = <span class="number">1</span></span><br><span class="line">    y_rpn_overlap[jy, ix, anchor_ratio_idx + n_anchratios * anchor_size_idx] = <span class="number">1</span></span><br><span class="line">    start = <span class="number">4</span> * (anchor_ratio_idx + n_anchratios * anchor_size_idx)</span><br><span class="line">    y_rpn_regr[jy, ix, start:start+<span class="number">4</span>] = best_regr</span><br></pre></td></tr></table></figure></p>
<p><br><br>如果有一个bbox没有pos的预选宽和其对应，这找一个与它交并比最高的anchor的设置为pos<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> idx <span class="keyword">in</span> range(num_anchors_for_bbox.shape[<span class="number">0</span>]):</span><br><span class="line">	<span class="keyword">if</span> num_anchors_for_bbox[idx] == <span class="number">0</span>:</span><br><span class="line">		<span class="comment"># no box with an IOU greater than zero ...</span></span><br><span class="line">		<span class="keyword">if</span> best_anchor_for_bbox[idx, <span class="number">0</span>] == <span class="number">-1</span>:</span><br><span class="line">			<span class="keyword">continue</span></span><br><span class="line">		y_is_box_valid[best_anchor_for_bbox[idx,<span class="number">0</span>], best_anchor_for_bbox[idx,<span class="number">1</span>], best_anchor_for_bbox[idx,<span class="number">2</span>] + n_anchratios *best_anchor_for_bbox[idx,<span class="number">3</span>]] = <span class="number">1</span></span><br><span class="line">		y_rpn_overlap[best_anchor_for_bbox[idx,<span class="number">0</span>], best_anchor_for_bbox[idx,<span class="number">1</span>], best_anchor_for_bbox[idx,<span class="number">2</span>] + n_anchratios *best_anchor_for_bbox[idx,<span class="number">3</span>]] = <span class="number">1</span></span><br><span class="line">		start = <span class="number">4</span> * (best_anchor_for_bbox[idx,<span class="number">2</span>] + n_anchratios * best_anchor_for_bbox[idx,<span class="number">3</span>])</span><br><span class="line">		y_rpn_regr[best_anchor_for_bbox[idx,<span class="number">0</span>], best_anchor_for_bbox[idx,<span class="number">1</span>], start:start+<span class="number">4</span>] = best_dx_for_bbox[idx, :]</span><br></pre></td></tr></table></figure></p>
<p><br><br>将深度变到第一位，给向量增加一个维度, 在Tensorflow中， 第一纬度是batch size, 此外， 变换向量位置匹配要求<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">y_rpn_overlap = np.transpose(y_rpn_overlap, (<span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>))</span><br><span class="line">y_rpn_overlap = np.expand_dims(y_rpn_overlap, axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">y_is_box_valid = np.transpose(y_is_box_valid, (<span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>))</span><br><span class="line">y_is_box_valid = np.expand_dims(y_is_box_valid, axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">y_rpn_regr = np.transpose(y_rpn_regr, (<span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>))</span><br><span class="line">y_rpn_regr = np.expand_dims(y_rpn_regr, axis=<span class="number">0</span>)</span><br></pre></td></tr></table></figure></p>
<p><br><br>从可用的预选框中选择num_regions<br>如果pos的个数大于num_regions / 2，则将多下来的地方置为不可用。如果小于pos不做处理<br>接下来将pos与neg总是超过num_regions个的neg预选框置为不可用<br>最后， 256个预选框，128个positive,128个negative 会生成 在一张图片里面<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">pos_locs = np.where(np(y_rpn_overlap[<span class="number">0</span>, :, :, :] =.logical_and= <span class="number">1</span>, y_is_box_valid[<span class="number">0</span>, :, :, :] == <span class="number">1</span>))</span><br><span class="line">neg_locs = np.where(np.logical_and(y_rpn_overlap[<span class="number">0</span>, :, :, :] == <span class="number">0</span>, y_is_box_valid[<span class="number">0</span>, :, :, :] == <span class="number">1</span>))</span><br><span class="line">num_regions = <span class="number">256</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> len(pos_locs[<span class="number">0</span>]) &gt; num_regions / <span class="number">2</span>:</span><br><span class="line">	val_locs = random.sample(range(len(pos_locs[<span class="number">0</span>])), len(pos_locs[<span class="number">0</span>]) - num_regions / <span class="number">2</span>)</span><br><span class="line">	y_is_box_valid[<span class="number">0</span>, pos_locs[<span class="number">0</span>][val_locs], pos_locs[<span class="number">1</span>][val_locs], pos_locs[<span class="number">2</span>][val_locs]] = <span class="number">0</span></span><br><span class="line">	num_pos = num_regions / <span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> len(neg_locs[<span class="number">0</span>]) + num_pos &gt; num_regions:</span><br><span class="line">	val_locs = random.sample(range(len(neg_locs[<span class="number">0</span>])), len(neg_locs[<span class="number">0</span>]) - num_pos)</span><br><span class="line">	 y_is_box_valid[<span class="number">0</span>, neg_locs[<span class="number">0</span>][val_locs], neg_locs[<span class="number">1</span>][val_locs], neg_locs[<span class="number">2</span>][val_locs]] = <span class="number">0</span></span><br></pre></td></tr></table></figure></p>
<p><br></p>
<h3 id="【27-06-2018】"><a href="#【27-06-2018】" class="headerlink" title="【27/06/2018】"></a>【27/06/2018】</h3><h4 id="project-brief"><a href="#project-brief" class="headerlink" title="project brief"></a>project brief</h4><p>Re organization of Project plan</p>
<h4 id="Anchors-Iterative"><a href="#Anchors-Iterative" class="headerlink" title="Anchors Iterative"></a>Anchors Iterative</h4><p>Integration of privous work:<br>In each anchor: config file -&gt; rpn_stride = 16 means generate one anchor in 16 pixels<br><a href="https://github.com/Trouble404/Object-Detection-System-Based-on-CNN-and-Capsule-Network/blob/master/Faster-RCNN/anchor_rpn.ipynb" target="_blank" rel="noopener">Jupyter Notebook address</a></p>
<p>【流程】<br>Function description<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">@param all_img_data: Parsered xml file  </span></span><br><span class="line"><span class="string">@param class_count: Counting of the number of all classes objects</span></span><br><span class="line"><span class="string">@param C: Configuration class</span></span><br><span class="line"><span class="string">@param img_length_calc_function: resnet's get_img_output_length() function</span></span><br><span class="line"><span class="string">@param backend: Tensorflow in this project</span></span><br><span class="line"><span class="string">#param mode: train or val</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">yield np.copy(x_img), [np.copy(y_rpn_cls), np.copy(y_rpn_regr)], img_data_aug</span></span><br><span class="line"><span class="string">@return: np.copy(x_img): image's matrix data</span></span><br><span class="line"><span class="string">@return: [np.copy(y_rpn_cls), np.copy(y_rpn_regr)]: calculated rpn class and radient</span></span><br><span class="line"><span class="string">@return: img_data_aug: correspoding parsed xml information</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_anchor_gt</span><span class="params">(all_img_data, class_count, C, img_length_calc_function, backend, mode=<span class="string">'train'</span>)</span>:</span></span><br></pre></td></tr></table></figure></p>
<p><br><br><strong>Traverse all input image based on input xml information</strong></p>
<ul>
<li>Apply class balance function: <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">C.balanced_classes = <span class="keyword">True</span></span><br><span class="line">sample_selector = image_processing.SampleSelector(class_count)</span><br><span class="line"><span class="keyword">if</span> C.balanced_classes <span class="keyword">and</span> sample_selector.skip_sample_for_balanced_class(img_data):</span><br><span class="line">    <span class="keyword">continue</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
<p><br></p>
<ul>
<li>Apply image enhance<br>if input mode is train, apply image enhance to obtain augmented image xml and matrix, if mode is val, obtain image orignal xml and matrix<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> mode == <span class="string">'train'</span>:</span><br><span class="line">    img_data_aug, x_img = image_enhance.augment(img_data, C, augment=<span class="keyword">True</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    img_data_aug, x_img = image_enhance.augment(img_data, C, augment=<span class="keyword">False</span>)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>verifacation width and hegiht in xml and matrix<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">(width, height) = (img_data_aug[<span class="string">'width'</span>], img_data_aug[<span class="string">'height'</span>])</span><br><span class="line">(rows, cols, _) = x_img.shape</span><br><span class="line"><span class="keyword">assert</span> cols == width</span><br><span class="line"><span class="keyword">assert</span> rows == height</span><br></pre></td></tr></table></figure></p>
<p><br></p>
<ul>
<li>Apply rezise function<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">(resized_width, resized_height) = image_processing.get_new_img_size(width, height, C.im_size)</span><br><span class="line">x_img = cv2.resize(x_img, (resized_width, resized_height), interpolation=cv2.INTER_CUBIC)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p><br></p>
<ul>
<li>Apply rpn calculation<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">y_rpn_cls, y_rpn_regr = rpn_calculation.calc_rpn(C, img_data_aug, width, height, resized_width, resized_height, img_length_calc_function)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p><br></p>
<ul>
<li><p>Zero-center by mean pixel, and preprocess image format<br>BGR -&gt; RGB because when apply resnet, it need RGB but in cv2, it use BGR</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x_img = x_img[:,:, (<span class="number">2</span>, <span class="number">1</span>, <span class="number">0</span>)]</span><br></pre></td></tr></table></figure>
<p> For using pre-trainning model, needs to mins mean channel in each dim</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x_img = x_img.astype(np.float32)</span><br><span class="line">x_img[:, :, <span class="number">0</span>] -= C.img_channel_mean[<span class="number">0</span>]</span><br><span class="line">x_img[:, :, <span class="number">1</span>] -= C.img_channel_mean[<span class="number">1</span>]</span><br><span class="line">x_img[:, :, <span class="number">2</span>] -= C.img_channel_mean[<span class="number">2</span>]</span><br><span class="line">x_img /= C.img_scaling_factor <span class="comment"># default to 1,so no change here</span></span><br></pre></td></tr></table></figure>
<p> expand for batch size</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x_img = np.expand_dims(x_img, axis=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<p>for using pre-trainning model, need to sclaling the std to match pre trained model</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">y_rpn_regr[:, y_rpn_regr.shape[<span class="number">1</span>]//<span class="number">2</span>:, :, :] *= C.std_scaling <span class="comment"># scaling is 4 here</span></span><br></pre></td></tr></table></figure>
<p>in tensorflow, sort as batch size, width, height, deep</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> backend == <span class="string">'tf'</span>:</span><br><span class="line">    x_img = np.transpose(x_img, (<span class="number">0</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">1</span>))</span><br><span class="line">    y_rpn_cls = np.transpose(y_rpn_cls, (<span class="number">0</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">1</span>))</span><br><span class="line">	y_rpn_regr = np.transpose(y_rpn_regr, (<span class="number">0</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">1</span>))</span><br></pre></td></tr></table></figure>
<p>generator to iteror, using next() to loop</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">yield</span> np.copy(x_img), [np.copy(y_rpn_cls), np.copy(y_rpn_regr)], img_data_aug</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p><br><br>【执行】<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">data_gen_train = get_anchor_gt(train_imgs, classes_count, C, nn.get_img_output_length, K.image_dim_ordering(), mode=<span class="string">'train'</span>)</span><br><span class="line">data_gen_val = get_anchor_gt(val_imgs, classes_count, C, nn.get_img_output_length,K.image_dim_ordering(), mode=<span class="string">'val'</span>)</span><br></pre></td></tr></table></figure></p>
<p>Test:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">img,rpn,img_aug = next(data_gen_train)</span><br></pre></td></tr></table></figure></p>
<p><img src="https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/26_1.PNG" alt="image"></p>
<h3 id="【28-06-2018】"><a href="#【28-06-2018】" class="headerlink" title="【28/06/2018】"></a>【28/06/2018】</h3><h4 id="Resnet50-structure"><a href="#Resnet50-structure" class="headerlink" title="Resnet50 structure"></a>Resnet50 structure</h4><p>论文链接: <a href="https://arxiv.org/abs/1512.03385" target="_blank" rel="noopener">https://arxiv.org/abs/1512.03385</a></p>
<p>首先，我们要问一个问题：<br><strong>Is learning better networks as easy as stacking more layers?</strong></p>
<p>很显然不是，原因有二。<br>一，<strong>vanishing/exploding gradients</strong>；深度会带来恶名昭著的梯度弥散/爆炸，导致系统不能收敛。然而梯度弥散/爆炸在很大程度上被normalized initialization and intermediate normalization layers处理了。<br>二、<strong>degradation</strong>；当深度开始增加的时候，accuracy经常会达到饱和，然后开始下降，但这并不是由于过拟合引起的。可见figure1，56-layer的error大于20-layer的error。</p>
<p>He kaiMing大神认为靠堆layers竟然会导致degradation，那肯定是我们堆的方式不对。因此他提出了一种基于残差块的identity mapping，通过学习残差的方式，而非直接去学习直接的映射关系。 </p>
<p><img src="https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/01_2.jpg" alt="image"></p>
<p>事实证明，靠堆积残差块能够带来很好效果提升。而不断堆积plain layer却会带来很高的训练误差<br>残差块的两个优点：<br>1) Our extremely deep residual nets are easy to optimize, but the counterpart “plain” nets (that simply stack layers) exhibit higher training error when the depth increases;<br>2) Our deep residual nets can easily enjoy accuracy gains from greatly increased depth, producing results substantially better than previous networks.</p>
<h3 id="【29-06-2018】"><a href="#【29-06-2018】" class="headerlink" title="【29/06/2018】"></a>【29/06/2018】</h3><h4 id="Resnet50-image-structure"><a href="#Resnet50-image-structure" class="headerlink" title="Resnet50 image structure"></a>Resnet50 image structure</h4><p><img src="https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/1_01.png" alt="iamge"><br>ResNet有2个基本的block，一个是Identity Block，输入和输出的dimension是一样的，所以可以串联多个；另外一个基本block是Conv Block，输入和输出的dimension是不一样的，所以不能连续串联，它的作用本来就是为了改变feature vector的dimension</p>
<p>因为CNN最后都是要把image一点点的convert成很小但是depth很深的feature map，一般的套路是用统一的比较小的kernel（比如VGG都是用3x3），但是随着网络深度的增加，output的channel也增大（学到的东西越来越复杂），所以有必要在进入Identity Block之前，用Conv Block转换一下维度，这样后面就可以连续接Identity Block.</p>
<p>可以看下Conv Block是怎么改变输出维度的:<br><img src="https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/01_3.png" alt="image"><br><img src="https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/01_4.png" alt="image"><br>其实就是在shortcut path的地方加上一个conv2D layer（1x1 filter size），然后在main path改变dimension，并与shortcut path对应起来.</p>
<h2 id="July"><a href="#July" class="headerlink" title="July"></a>July</h2><h3 id="【02-07-2018】"><a href="#【02-07-2018】" class="headerlink" title="【02/07/2018】"></a>【02/07/2018】</h3><h4 id="Construct-resnet-by-keras"><a href="#Construct-resnet-by-keras" class="headerlink" title="Construct resnet by keras"></a>Construct resnet by keras</h4><p>残差网络的关键步骤，跨层的合并需要保证x和F(x)的shape是完全一样的，否则它们加不起来。</p>
<p>理解了这一点，我们开始用keras做实现，我们把输入输出大小相同的模块称为identity_block，而把输出比输入小的模块称为conv_block，首先，导入所需的模块：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Model</span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Input,Dense,BatchNormalization,Conv2D,MaxPooling2D,AveragePooling2D,ZeroPadding2D</span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> add,Flatten</span><br><span class="line"><span class="keyword">from</span> keras.optimizers <span class="keyword">import</span> SGD</span><br></pre></td></tr></table></figure>
<p>我们先来编写identity_block，这是一个函数，接受一个张量为输入，并返回一个张量, 然后是conv层，是有shortcut的：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Conv2d_BN</span><span class="params">(x, nb_filter,kernel_size, strides=<span class="params">(<span class="number">1</span>,<span class="number">1</span>)</span>, padding=<span class="string">'same'</span>,name=None)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> name <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">        bn_name = name + <span class="string">'_bn'</span></span><br><span class="line">        conv_name = name + <span class="string">'_conv'</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        bn_name = <span class="keyword">None</span></span><br><span class="line">        conv_name = <span class="keyword">None</span></span><br><span class="line"> </span><br><span class="line">    x = Conv2D(nb_filter,kernel_size,padding=padding,strides=strides,activation=<span class="string">'relu'</span>,name=conv_name)(x)</span><br><span class="line">    x = BatchNormalization(axis=<span class="number">3</span>,name=bn_name)(x)</span><br><span class="line">    <span class="keyword">return</span> x</span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Conv_Block</span><span class="params">(inpt,nb_filter,kernel_size,strides=<span class="params">(<span class="number">1</span>,<span class="number">1</span>)</span>, with_conv_shortcut=False)</span>:</span></span><br><span class="line">    x = Conv2d_BN(inpt,nb_filter=nb_filter[<span class="number">0</span>],kernel_size=(<span class="number">1</span>,<span class="number">1</span>),strides=strides,padding=<span class="string">'same'</span>)</span><br><span class="line">    x = Conv2d_BN(x, nb_filter=nb_filter[<span class="number">1</span>], kernel_size=(<span class="number">3</span>,<span class="number">3</span>), padding=<span class="string">'same'</span>)</span><br><span class="line">    x = Conv2d_BN(x, nb_filter=nb_filter[<span class="number">2</span>], kernel_size=(<span class="number">1</span>,<span class="number">1</span>), padding=<span class="string">'same'</span>)</span><br><span class="line">    <span class="keyword">if</span> with_conv_shortcut:</span><br><span class="line">        shortcut = Conv2d_BN(inpt,nb_filter=nb_filter[<span class="number">2</span>],strides=strides,kernel_size=kernel_size)</span><br><span class="line">        x = add([x,shortcut])</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        x = add([x,inpt])</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure></p>
<p>剩下的事情就很简单了，数好identity_block和conv_block是如何交错的，照着网络搭就好了：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">inpt = Input(shape=(<span class="number">224</span>,<span class="number">224</span>,<span class="number">3</span>))</span><br><span class="line">x = ZeroPadding2D((<span class="number">3</span>,<span class="number">3</span>))(inpt)</span><br><span class="line">x = Conv2d_BN(x,nb_filter=<span class="number">64</span>,kernel_size=(<span class="number">7</span>,<span class="number">7</span>),strides=(<span class="number">2</span>,<span class="number">2</span>),padding=<span class="string">'valid'</span>)</span><br><span class="line">x = MaxPooling2D(pool_size=(<span class="number">3</span>,<span class="number">3</span>),strides=(<span class="number">2</span>,<span class="number">2</span>),padding=<span class="string">'same'</span>)(x)</span><br><span class="line"> </span><br><span class="line">x = Conv_Block(x,nb_filter=[<span class="number">64</span>,<span class="number">64</span>,<span class="number">256</span>],kernel_size=(<span class="number">3</span>,<span class="number">3</span>),strides=(<span class="number">1</span>,<span class="number">1</span>),with_conv_shortcut=<span class="keyword">True</span>)</span><br><span class="line">x = Conv_Block(x,nb_filter=[<span class="number">64</span>,<span class="number">64</span>,<span class="number">256</span>],kernel_size=(<span class="number">3</span>,<span class="number">3</span>))</span><br><span class="line">x = Conv_Block(x,nb_filter=[<span class="number">64</span>,<span class="number">64</span>,<span class="number">256</span>],kernel_size=(<span class="number">3</span>,<span class="number">3</span>))</span><br><span class="line"> </span><br><span class="line">x = Conv_Block(x,nb_filter=[<span class="number">128</span>,<span class="number">128</span>,<span class="number">512</span>],kernel_size=(<span class="number">3</span>,<span class="number">3</span>),strides=(<span class="number">2</span>,<span class="number">2</span>),with_conv_shortcut=<span class="keyword">True</span>)</span><br><span class="line">x = Conv_Block(x,nb_filter=[<span class="number">128</span>,<span class="number">128</span>,<span class="number">512</span>],kernel_size=(<span class="number">3</span>,<span class="number">3</span>))</span><br><span class="line">x = Conv_Block(x,nb_filter=[<span class="number">128</span>,<span class="number">128</span>,<span class="number">512</span>],kernel_size=(<span class="number">3</span>,<span class="number">3</span>))</span><br><span class="line">x = Conv_Block(x,nb_filter=[<span class="number">128</span>,<span class="number">128</span>,<span class="number">512</span>],kernel_size=(<span class="number">3</span>,<span class="number">3</span>))</span><br><span class="line"> </span><br><span class="line">x = Conv_Block(x,nb_filter=[<span class="number">256</span>,<span class="number">256</span>,<span class="number">1024</span>],kernel_size=(<span class="number">3</span>,<span class="number">3</span>),strides=(<span class="number">2</span>,<span class="number">2</span>),with_conv_shortcut=<span class="keyword">True</span>)</span><br><span class="line">x = Conv_Block(x,nb_filter=[<span class="number">256</span>,<span class="number">256</span>,<span class="number">1024</span>],kernel_size=(<span class="number">3</span>,<span class="number">3</span>))</span><br><span class="line">x = Conv_Block(x,nb_filter=[<span class="number">256</span>,<span class="number">256</span>,<span class="number">1024</span>],kernel_size=(<span class="number">3</span>,<span class="number">3</span>))</span><br><span class="line">x = Conv_Block(x,nb_filter=[<span class="number">256</span>,<span class="number">256</span>,<span class="number">1024</span>],kernel_size=(<span class="number">3</span>,<span class="number">3</span>))</span><br><span class="line">x = Conv_Block(x,nb_filter=[<span class="number">256</span>,<span class="number">256</span>,<span class="number">1024</span>],kernel_size=(<span class="number">3</span>,<span class="number">3</span>))</span><br><span class="line">x = Conv_Block(x,nb_filter=[<span class="number">256</span>,<span class="number">256</span>,<span class="number">1024</span>],kernel_size=(<span class="number">3</span>,<span class="number">3</span>))</span><br><span class="line"> </span><br><span class="line">x = Conv_Block(x,nb_filter=[<span class="number">512</span>,<span class="number">512</span>,<span class="number">2048</span>],kernel_size=(<span class="number">3</span>,<span class="number">3</span>),strides=(<span class="number">2</span>,<span class="number">2</span>),with_conv_shortcut=<span class="keyword">True</span>)</span><br><span class="line">x = Conv_Block(x,nb_filter=[<span class="number">512</span>,<span class="number">512</span>,<span class="number">2048</span>],kernel_size=(<span class="number">3</span>,<span class="number">3</span>))</span><br><span class="line">x = Conv_Block(x,nb_filter=[<span class="number">512</span>,<span class="number">512</span>,<span class="number">2048</span>],kernel_size=(<span class="number">3</span>,<span class="number">3</span>))</span><br><span class="line">x = AveragePooling2D(pool_size=(<span class="number">7</span>,<span class="number">7</span>))(x)</span><br><span class="line">x = Flatten()(x)</span><br><span class="line">x = Dense(<span class="number">1000</span>,activation=<span class="string">'softmax'</span>)(x)</span><br><span class="line"></span><br><span class="line">model = Model(inputs=inpt,outputs=x)</span><br><span class="line">sgd = SGD(decay=<span class="number">0.0001</span>,momentum=<span class="number">0.9</span>)</span><br><span class="line">model.compile(loss=<span class="string">'categorical_crossentropy'</span>,optimizer=sgd,metrics=[<span class="string">'accuracy'</span>])</span><br><span class="line">model.summary()</span><br></pre></td></tr></table></figure></p>
<p><a href="https://github.com/Trouble404/Object-Detection-System-Based-on-CNN-and-Capsule-Network/blob/master/Faster-RCNN/Resnet50/resnet50_keras.ipynb" target="_blank" rel="noopener">jupyter notebook</a></p>
<h3 id="【03-07-2018】"><a href="#【03-07-2018】" class="headerlink" title="【03/07/2018】"></a>【03/07/2018】</h3><h4 id="load-pre-trained-model-of-resnet50"><a href="#load-pre-trained-model-of-resnet50" class="headerlink" title="load pre-trained model of resnet50"></a>load pre-trained model of resnet50</h4><p>步骤如下：</p>
<ul>
<li>下载ResNet50不包含全连接层的模型参数到本地（resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5）；</li>
<li>定义好ResNet50的网络结构；</li>
<li>将预训练的模型参数加载到我们所定义的网络结构中；</li>
<li>更改全连接层结构，便于对我们的分类任务进行处</li>
<li>或者根据需要解冻最后几个block，然后以很低的学习率开始训练。我们只选择最后一个block进行训练，是因为训练样本很少，而ResNet50模型层数很多，全部训练肯定不能训练好，会过拟合。 其次fine-tune时由于是在一个已经训练好的模型上进行的，故权值更新应该是一个小范围的，以免破坏预训练好的特征。</li>
</ul>
<p><a href="https://github.com/fchollet/deep-learning-models/releases" target="_blank" rel="noopener">下载地址</a></p>
<p><img src="https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/01_7.JPG" alt="image"></p>
<p>因为使用了预训练模型，参数名称需要和预训练模型一致：<br>identity层：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">identity_block</span><span class="params">(X, f, filters, stage, block)</span>:</span></span><br><span class="line">    <span class="comment"># defining name basis</span></span><br><span class="line">    conv_name_base = <span class="string">'res'</span> + str(stage) + block + <span class="string">'_branch'</span></span><br><span class="line">    bn_name_base = <span class="string">'bn'</span> + str(stage) + block + <span class="string">'_branch'</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Retrieve Filters</span></span><br><span class="line">    F1, F2, F3 = filters</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Save the input value. You'll need this later to add back to the main path. </span></span><br><span class="line">    X_shortcut = X</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># First component of main path</span></span><br><span class="line">    X = Conv2D(filters = F1, kernel_size = (<span class="number">1</span>, <span class="number">1</span>), strides = (<span class="number">1</span>,<span class="number">1</span>), padding = <span class="string">'valid'</span>, name = conv_name_base + <span class="string">'2a'</span>)(X)</span><br><span class="line">    X = BatchNormalization(axis = <span class="number">3</span>, name = bn_name_base + <span class="string">'2a'</span>)(X)</span><br><span class="line">    X = Activation(<span class="string">'relu'</span>)(X)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Second component of main path (≈3 lines)</span></span><br><span class="line">    X = Conv2D(filters= F2, kernel_size=(f,f),strides=(<span class="number">1</span>,<span class="number">1</span>),padding=<span class="string">'same'</span>,name=conv_name_base + <span class="string">'2b'</span>)(X)</span><br><span class="line">    X = BatchNormalization(axis=<span class="number">3</span>,name=bn_name_base+<span class="string">'2b'</span>)(X)</span><br><span class="line">    X = Activation(<span class="string">'relu'</span>)(X)</span><br><span class="line"> </span><br><span class="line">    <span class="comment"># Third component of main path (≈2 lines)</span></span><br><span class="line">    X = Conv2D(filters=F3,kernel_size=(<span class="number">1</span>,<span class="number">1</span>),strides=(<span class="number">1</span>,<span class="number">1</span>),padding=<span class="string">'valid'</span>,name=conv_name_base+<span class="string">'2c'</span>)(X)</span><br><span class="line">    X = BatchNormalization(axis=<span class="number">3</span>,name=bn_name_base+<span class="string">'2c'</span>)(X)</span><br><span class="line"> </span><br><span class="line">    <span class="comment"># Final step: Add shortcut value to main path, and pass it through a RELU activation (≈2 lines)</span></span><br><span class="line">    X = Add()([X, X_shortcut])</span><br><span class="line">    X = Activation(<span class="string">'relu'</span>)(X)</span><br><span class="line">    <span class="keyword">return</span> X</span><br></pre></td></tr></table></figure></p>
<p>conv层：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">convolutional_block</span><span class="params">(X, f, filters, stage, block, s = <span class="number">2</span>)</span>:</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># defining name basis</span></span><br><span class="line">    conv_name_base = <span class="string">'res'</span> + str(stage) + block + <span class="string">'_branch'</span></span><br><span class="line">    bn_name_base = <span class="string">'bn'</span> + str(stage) + block + <span class="string">'_branch'</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Retrieve Filters</span></span><br><span class="line">    F1, F2, F3 = filters</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Save the input value</span></span><br><span class="line">    X_shortcut = X</span><br><span class="line"> </span><br><span class="line">    <span class="comment">##### MAIN PATH #####</span></span><br><span class="line">    <span class="comment"># First component of main path </span></span><br><span class="line">    X = Conv2D(F1, (<span class="number">1</span>, <span class="number">1</span>), strides = (s,s),padding=<span class="string">'valid'</span>,name = conv_name_base + <span class="string">'2a'</span>)(X)</span><br><span class="line">    X = BatchNormalization(axis = <span class="number">3</span>, name = bn_name_base + <span class="string">'2a'</span>)(X)</span><br><span class="line">    X = Activation(<span class="string">'relu'</span>)(X)</span><br><span class="line"> </span><br><span class="line">    <span class="comment"># Second component of main path (≈3 lines)</span></span><br><span class="line">    X = Conv2D(F2,(f,f),strides=(<span class="number">1</span>,<span class="number">1</span>),padding=<span class="string">'same'</span>,name=conv_name_base+<span class="string">'2b'</span>)(X)</span><br><span class="line">    X = BatchNormalization(axis=<span class="number">3</span>,name=bn_name_base+<span class="string">'2b'</span>)(X)</span><br><span class="line">    X = Activation(<span class="string">'relu'</span>)(X)</span><br><span class="line"> </span><br><span class="line">    <span class="comment"># Third component of main path (≈2 lines)</span></span><br><span class="line">    X = Conv2D(F3,(<span class="number">1</span>,<span class="number">1</span>),strides=(<span class="number">1</span>,<span class="number">1</span>),padding=<span class="string">'valid'</span>,name=conv_name_base+<span class="string">'2c'</span>)(X)</span><br><span class="line">    X = BatchNormalization(axis=<span class="number">3</span>,name=bn_name_base+<span class="string">'2c'</span>)(X)</span><br><span class="line"> </span><br><span class="line">    <span class="comment">##### SHORTCUT PATH #### (≈2 lines)</span></span><br><span class="line">    X_shortcut = Conv2D(F3,(<span class="number">1</span>,<span class="number">1</span>),strides=(s,s),padding=<span class="string">'valid'</span>,name=conv_name_base+<span class="string">'1'</span>)(X_shortcut)</span><br><span class="line">    X_shortcut = BatchNormalization(axis=<span class="number">3</span>,name =bn_name_base+<span class="string">'1'</span>)(X_shortcut)</span><br><span class="line"> </span><br><span class="line">    <span class="comment"># Final step: Add shortcut value to main path, and pass it through a RELU activation (≈2 lines)</span></span><br><span class="line">    X = Add()([X,X_shortcut])</span><br><span class="line">    X = Activation(<span class="string">'relu'</span>)(X)</span><br><span class="line">    <span class="keyword">return</span> X</span><br></pre></td></tr></table></figure></p>
<p>resnet50结构：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ResNet50</span><span class="params">(input_shape = <span class="params">(<span class="number">64</span>, <span class="number">64</span>, <span class="number">3</span>)</span>, classes = <span class="number">30</span>)</span>:</span></span><br><span class="line">    <span class="comment"># Define the input as a tensor with shape input_shape</span></span><br><span class="line">    X_input = Input(input_shape)</span><br><span class="line"> </span><br><span class="line">    <span class="comment"># Zero-Padding</span></span><br><span class="line">    X = ZeroPadding2D((<span class="number">3</span>, <span class="number">3</span>))(X_input)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Stage 1</span></span><br><span class="line">    X = Conv2D(<span class="number">64</span>, (<span class="number">7</span>, <span class="number">7</span>), strides = (<span class="number">2</span>, <span class="number">2</span>), name = <span class="string">'conv1'</span>)(X)</span><br><span class="line">    X = BatchNormalization(axis = <span class="number">3</span>, name = <span class="string">'bn_conv1'</span>)(X)</span><br><span class="line">    X = Activation(<span class="string">'relu'</span>)(X)</span><br><span class="line">    X = MaxPooling2D((<span class="number">3</span>, <span class="number">3</span>), strides=(<span class="number">2</span>, <span class="number">2</span>))(X)</span><br><span class="line"> </span><br><span class="line">    <span class="comment"># Stage 2</span></span><br><span class="line">    X = convolutional_block(X, f = <span class="number">3</span>, filters = [<span class="number">64</span>, <span class="number">64</span>, <span class="number">256</span>], stage = <span class="number">2</span>, block=<span class="string">'a'</span>, s = <span class="number">1</span>)</span><br><span class="line">    X = identity_block(X, <span class="number">3</span>, [<span class="number">64</span>, <span class="number">64</span>, <span class="number">256</span>], stage=<span class="number">2</span>, block=<span class="string">'b'</span>)</span><br><span class="line">    X = identity_block(X, <span class="number">3</span>, [<span class="number">64</span>, <span class="number">64</span>, <span class="number">256</span>], stage=<span class="number">2</span>, block=<span class="string">'c'</span>)</span><br><span class="line"> </span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line"> </span><br><span class="line">    <span class="comment"># Stage 3 (≈4 lines)</span></span><br><span class="line">    X = convolutional_block(X, f = <span class="number">3</span>,filters= [<span class="number">128</span>,<span class="number">128</span>,<span class="number">512</span>],stage=<span class="number">3</span>,block=<span class="string">'a'</span>,s=<span class="number">2</span>)</span><br><span class="line">    X = identity_block(X,<span class="number">3</span>,[<span class="number">128</span>,<span class="number">128</span>,<span class="number">512</span>],stage=<span class="number">3</span>,block=<span class="string">'b'</span>)</span><br><span class="line">    X = identity_block(X,<span class="number">3</span>,[<span class="number">128</span>,<span class="number">128</span>,<span class="number">512</span>],stage=<span class="number">3</span>,block=<span class="string">'c'</span>)</span><br><span class="line">    X = identity_block(X,<span class="number">3</span>,[<span class="number">128</span>,<span class="number">128</span>,<span class="number">512</span>],stage=<span class="number">3</span>,block=<span class="string">'d'</span>)</span><br><span class="line"> </span><br><span class="line">    <span class="comment"># Stage 4 (≈6 lines)</span></span><br><span class="line">    X = convolutional_block(X,f=<span class="number">3</span>,filters=[<span class="number">256</span>,<span class="number">256</span>,<span class="number">1024</span>],stage=<span class="number">4</span>,block=<span class="string">'a'</span>,s=<span class="number">2</span>)</span><br><span class="line">    X = identity_block(X,<span class="number">3</span>,[<span class="number">256</span>,<span class="number">256</span>,<span class="number">1024</span>],stage=<span class="number">4</span>,block=<span class="string">'b'</span>)</span><br><span class="line">    X = identity_block(X,<span class="number">3</span>,[<span class="number">256</span>,<span class="number">256</span>,<span class="number">1024</span>],stage=<span class="number">4</span>,block=<span class="string">'c'</span>)</span><br><span class="line">    X = identity_block(X,<span class="number">3</span>,[<span class="number">256</span>,<span class="number">256</span>,<span class="number">1024</span>],stage=<span class="number">4</span>,block=<span class="string">'d'</span>)</span><br><span class="line">    X = identity_block(X,<span class="number">3</span>,[<span class="number">256</span>,<span class="number">256</span>,<span class="number">1024</span>],stage=<span class="number">4</span>,block=<span class="string">'e'</span>)</span><br><span class="line">    X = identity_block(X,<span class="number">3</span>,[<span class="number">256</span>,<span class="number">256</span>,<span class="number">1024</span>],stage=<span class="number">4</span>,block=<span class="string">'f'</span>)</span><br><span class="line"> </span><br><span class="line">    <span class="comment"># Stage 5 (≈3 lines)</span></span><br><span class="line">    X = convolutional_block(X, f = <span class="number">3</span>,filters= [<span class="number">512</span>,<span class="number">512</span>,<span class="number">2048</span>],stage=<span class="number">5</span>,block=<span class="string">'a'</span>,s=<span class="number">2</span>)</span><br><span class="line">    X = identity_block(X,<span class="number">3</span>,[<span class="number">512</span>,<span class="number">512</span>,<span class="number">2048</span>],stage=<span class="number">5</span>,block=<span class="string">'b'</span>)</span><br><span class="line">    X = identity_block(X,<span class="number">3</span>,[<span class="number">512</span>,<span class="number">512</span>,<span class="number">2048</span>],stage=<span class="number">5</span>,block=<span class="string">'c'</span>)</span><br><span class="line"> </span><br><span class="line">    <span class="comment"># AVGPOOL (≈1 line). Use "X = AveragePooling2D(...)(X)"</span></span><br><span class="line">    X = AveragePooling2D((<span class="number">2</span>,<span class="number">2</span>),strides=(<span class="number">2</span>,<span class="number">2</span>))(X)</span><br><span class="line"> </span><br><span class="line">    <span class="comment"># output layer</span></span><br><span class="line">    X = Flatten()(X)</span><br><span class="line">    model = Model(inputs = X_input, outputs = X, name=<span class="string">'ResNet50'</span>)</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure></p>
<p>构建网络并且载入权重：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">base_model = ResNet50(input_shape=(<span class="number">224</span>,<span class="number">224</span>,<span class="number">3</span>),classes=<span class="number">30</span>) </span><br><span class="line">base_model.load_weights(<span class="string">'resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5'</span>)</span><br></pre></td></tr></table></figure></p>
<p>无法载入<br><img src="https://raw.githubusercontent.com/Trouble404/Blog_Pics/master/FYP/01_8.JPG" alt="image"></p>
<h3 id="【04-07-2018】"><a href="#【04-07-2018】" class="headerlink" title="【04/07/2018】"></a>【04/07/2018】</h3><h4 id="Loading-pre-trained-model"><a href="#Loading-pre-trained-model" class="headerlink" title="Loading pre-trained model"></a>Loading pre-trained model</h4><p>对于keras：如果新模型和旧模型结构一样，直接调用model.load_weights读取参数就行。如果新模型中的几层和之前模型一样，也通过model.load_weights(‘my_model_weights.h5’, by_name=True)来读取参数， 或者手动对每一层进行参数的赋值，比如x= Dense(100, weights=oldModel.layers[1].get_weights())(x)</p>
<p>修改代码：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    base_model.load_weights(<span class="string">'resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5'</span>,by_name=<span class="keyword">True</span>)</span><br><span class="line">    print(<span class="string">"load successful"</span>)</span><br><span class="line"><span class="keyword">except</span>:</span><br><span class="line">    print(<span class="string">"load failed"</span>)</span><br></pre></td></tr></table></figure></p>
<p>载入成功：<a href="https://github.com/Trouble404/Object-Detection-System-Based-on-CNN-and-Capsule-Network/blob/master/Faster-RCNN/Resnet50/resnet50_pre_load.ipynb" target="_blank" rel="noopener">jupyter notebook</a></p>
<h3 id="【05-06-07-2018】"><a href="#【05-06-07-2018】" class="headerlink" title="【05~06/07/2018】"></a>【05~06/07/2018】</h3><h4 id="construct-faster-rcnn-net"><a href="#construct-faster-rcnn-net" class="headerlink" title="construct faster rcnn net"></a>construct faster rcnn net</h4><h2 id="August"><a href="#August" class="headerlink" title="August"></a>August</h2><h2 id="September"><a href="#September" class="headerlink" title="September"></a>September</h2>
      
    </div>
    
    
    

    

    
      <div>
        <div style="padding: 10px 0; margin: 20px auto; width: 90%; text-align: center;">
  <div>兴趣使然</div>
  <button id="rewardButton" disable="enable" onclick="var qr = document.getElementById('QR'); if (qr.style.display === 'none') {qr.style.display='block';} else {qr.style.display='none'}">
    <span>Donate</span>
  </button>
  <div id="QR" style="display: none;">

    
      <div id="wechat" style="display: inline-block">
        <img id="wechat_qr" src="/images/wechat-reward-image.jpg" alt="Junming Zhang WeChat Pay"/>
        <p>WeChat Pay</p>
      </div>
    

    

    

  </div>
</div>

      </div>
    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/Deep-Learning/" rel="tag"># Deep Learning</a>
          
            <a href="/tags/Object-Detection/" rel="tag"># Object Detection</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2018/04/30/Apri/" rel="next" title="四月は君の嘘">
                <i class="fa fa-chevron-left"></i> 四月は君の嘘
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
        
  <div class="bdsharebuttonbox">
    <a href="#" class="bds_tsina" data-cmd="tsina" title="分享到新浪微博"></a>
    <a href="#" class="bds_weixin" data-cmd="weixin" title="分享到微信"></a>
    <a href="#" class="bds_sqq" data-cmd="sqq" title="分享到QQ好友"></a>
    <a href="#" class="bds_fbook" data-cmd="fbook" title="分享到Facebook"></a>
    <a href="#" class="bds_twi" data-cmd="twi" title="分享到Twitter"></a>
    <a class="bds_count" data-cmd="count"></a>
  </div>
  <script>
    window._bd_share_config = {
      "common": {
        "bdText": "",
        "bdMini": "2",
        "bdMiniList": false,
        "bdPic": ""
      },
      "share": {
        "bdSize": "32",
        "bdStyle": "0"
      },

    }
  </script>

<script>
  with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='//bdimg.share.baidu.com/static/api/js/share.js?cdnversion='+~(-new Date()/36e5)];
</script>

      
    </div>
  </div>


          </div>
          


          
  <div class="comments" id="comments">
    
      <div id="lv-container" data-id="city" data-uid="MTAyMC8zNjA1NS8xMjU5MA=="></div>
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/images/avatar.gif"
               alt="Junming Zhang" />
          <p class="site-author-name" itemprop="name">Junming Zhang</p>
           
              <p class="site-description motion-element" itemprop="description">Someting for Noting</p>
          
        </div>
        <nav class="site-state motion-element">

          
            <div class="site-state-item site-state-posts">
              <a href="/archives/">
                <span class="site-state-item-count">5</span>
                <span class="site-state-item-name">日志</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-categories">
              <a href="/categories/index.html">
                <span class="site-state-item-count">4</span>
                <span class="site-state-item-name">分类</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-tags">
              <a href="/tags/index.html">
                <span class="site-state-item-count">7</span>
                <span class="site-state-item-name">标签</span>
              </a>
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="https://github.com/Trouble404" target="_blank" title="GitHub">
                  
                    <i class="fa fa-fw fa-github"></i>
                  
                    
                      GitHub
                    
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="https://weibo.com/2173518995/profile?topnav=1&wvr=6&is_all=1" target="_blank" title="Weibo">
                  
                    <i class="fa fa-fw fa-weibo"></i>
                  
                    
                      Weibo
                    
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="mailto:junming.zhang84@gmail.com" target="_blank" title="E-Mail">
                  
                    <i class="fa fa-fw fa-envelope"></i>
                  
                    
                      E-Mail
                    
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="https://www.instagram.com/lebron_kami/" target="_blank" title="Instagram">
                  
                    <i class="fa fa-fw fa-instagram"></i>
                  
                    
                      Instagram
                    
                </a>
              </span>
            
          
        </div>

        
        

        
        

        


      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#Gantt-chart"><span class="nav-number">1.</span> <span class="nav-text">Gantt chart</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Check-list"><span class="nav-number">2.</span> <span class="nav-text">Check list</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#May"><span class="nav-number">3.</span> <span class="nav-text">May</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#【28-05-2018】"><span class="nav-number">3.1.</span> <span class="nav-text">【28/05/2018】</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Installation"><span class="nav-number">3.1.1.</span> <span class="nav-text">Installation</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#在硕士学习过程中，使用Keras的项目"><span class="nav-number">3.1.2.</span> <span class="nav-text">在硕士学习过程中，使用Keras的项目**</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#TensorFlow-CPU-切换"><span class="nav-number">3.1.3.</span> <span class="nav-text">TensorFlow CPU 切换</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Jupyter-Notebook-工作目录设置"><span class="nav-number">3.1.4.</span> <span class="nav-text">Jupyter Notebook 工作目录设置</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#June"><span class="nav-number">4.</span> <span class="nav-text">June</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#【01-06-2018】"><span class="nav-number">4.1.</span> <span class="nav-text">【01/06/2018】</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Installation-1"><span class="nav-number">4.1.1.</span> <span class="nav-text">Installation</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Understanding-of-PyTorch"><span class="nav-number">4.1.2.</span> <span class="nav-text">Understanding of PyTorch</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#【04-06-2018】"><span class="nav-number">4.2.</span> <span class="nav-text">【04/06/2018】</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Dataset"><span class="nav-number">4.2.1.</span> <span class="nav-text">Dataset:</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Introduce"><span class="nav-number">4.2.2.</span> <span class="nav-text">Introduce:</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Detection-Task"><span class="nav-number">4.2.3.</span> <span class="nav-text">Detection Task</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#XML标注格式"><span class="nav-number">4.2.4.</span> <span class="nav-text">XML标注格式</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#【07-06-2018】"><span class="nav-number">4.3.</span> <span class="nav-text">【07/06/2018】</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Poster-conference"><span class="nav-number">4.3.1.</span> <span class="nav-text">Poster conference</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#【11-06-2018】"><span class="nav-number">4.4.</span> <span class="nav-text">【11/06/2018】</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#R-CNN"><span class="nav-number">4.4.1.</span> <span class="nav-text">R-CNN</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#【12-06-2018】"><span class="nav-number">4.5.</span> <span class="nav-text">【12/06/2018】</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#SPP-CNN"><span class="nav-number">4.5.1.</span> <span class="nav-text">SPP-CNN</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#【13-06-2018】"><span class="nav-number">4.6.</span> <span class="nav-text">【13/06/2018】</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#FAST-R-CNN"><span class="nav-number">4.6.1.</span> <span class="nav-text">FAST R-CNN</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#【14-18-06-2018】"><span class="nav-number">4.7.</span> <span class="nav-text">【14~18/06/2018】</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#FASTER-R-CNN"><span class="nav-number">4.7.1.</span> <span class="nav-text">FASTER R-CNN</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#流程"><span class="nav-number">4.7.2.</span> <span class="nav-text">流程</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#解释"><span class="nav-number">4.7.3.</span> <span class="nav-text">解释</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#【19-06-2018】"><span class="nav-number">4.8.</span> <span class="nav-text">【19/06/2018】</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#处理-XML-文档"><span class="nav-number">4.8.1.</span> <span class="nav-text">处理 XML 文档</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#【20-06-2018】"><span class="nav-number">4.9.</span> <span class="nav-text">【20/06/2018】</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#根据信息画出BBOXES"><span class="nav-number">4.9.1.</span> <span class="nav-text">根据信息画出BBOXES</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#【21-06-2018】"><span class="nav-number">4.10.</span> <span class="nav-text">【21/06/2018】</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#config-setting"><span class="nav-number">4.10.1.</span> <span class="nav-text">config setting</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#image-enhancement"><span class="nav-number">4.10.2.</span> <span class="nav-text">image enhancement</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#【22-06-2018】"><span class="nav-number">4.11.</span> <span class="nav-text">【22/06/2018】</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Image-rezise"><span class="nav-number">4.11.1.</span> <span class="nav-text">Image rezise</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Class-Balance"><span class="nav-number">4.11.2.</span> <span class="nav-text">Class Balance</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#【25-26-06-2018】"><span class="nav-number">4.12.</span> <span class="nav-text">【25~26/06/2018】</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Region-Proposal-Networks-RPN"><span class="nav-number">4.12.1.</span> <span class="nav-text">Region Proposal Networks(RPN)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Anchors"><span class="nav-number">4.12.2.</span> <span class="nav-text">Anchors</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Code"><span class="nav-number">4.12.3.</span> <span class="nav-text">Code</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#【27-06-2018】"><span class="nav-number">4.13.</span> <span class="nav-text">【27/06/2018】</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#project-brief"><span class="nav-number">4.13.1.</span> <span class="nav-text">project brief</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Anchors-Iterative"><span class="nav-number">4.13.2.</span> <span class="nav-text">Anchors Iterative</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#【28-06-2018】"><span class="nav-number">4.14.</span> <span class="nav-text">【28/06/2018】</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Resnet50-structure"><span class="nav-number">4.14.1.</span> <span class="nav-text">Resnet50 structure</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#【29-06-2018】"><span class="nav-number">4.15.</span> <span class="nav-text">【29/06/2018】</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Resnet50-image-structure"><span class="nav-number">4.15.1.</span> <span class="nav-text">Resnet50 image structure</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#July"><span class="nav-number">5.</span> <span class="nav-text">July</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#【02-07-2018】"><span class="nav-number">5.1.</span> <span class="nav-text">【02/07/2018】</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Construct-resnet-by-keras"><span class="nav-number">5.1.1.</span> <span class="nav-text">Construct resnet by keras</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#【03-07-2018】"><span class="nav-number">5.2.</span> <span class="nav-text">【03/07/2018】</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#load-pre-trained-model-of-resnet50"><span class="nav-number">5.2.1.</span> <span class="nav-text">load pre-trained model of resnet50</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#【04-07-2018】"><span class="nav-number">5.3.</span> <span class="nav-text">【04/07/2018】</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Loading-pre-trained-model"><span class="nav-number">5.3.1.</span> <span class="nav-text">Loading pre-trained model</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#【05-06-07-2018】"><span class="nav-number">5.4.</span> <span class="nav-text">【05~06/07/2018】</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#construct-faster-rcnn-net"><span class="nav-number">5.4.1.</span> <span class="nav-text">construct faster rcnn net</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#August"><span class="nav-number">6.</span> <span class="nav-text">August</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#September"><span class="nav-number">7.</span> <span class="nav-text">September</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Junming Zhang</span>
</div>


<div class="powered-by">
  由 <a class="theme-link" href="https://hexo.io">Hexo</a> 强力驱动
</div>

<div class="theme-info">
  主题 -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Gemini
  </a>
</div>


        
<div class="busuanzi-count">
  <script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="site-uv">
      <i class="fa fa-user"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
      
    </span>
  

  
    <span class="site-pv">
      <i class="fa fa-eye"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
      
    </span>
  
</div>


        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  


  











  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>

  
  <script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.2"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.2"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.2"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.2"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.2"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.2"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.2"></script>



  


  








  
    <script type="text/javascript">
      (function(d, s) {
        var j, e = d.getElementsByTagName(s)[0];
        if (typeof LivereTower === 'function') { return; }
        j = d.createElement(s);
        j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
        j.async = true;
        e.parentNode.insertBefore(j, e);
      })(document, 'script');
    </script>
  






  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  
  <script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.4.js"></script>
  <script>AV.initialize("OCX7qN1ae9flChFhUYTRGsS8-gzGzoHsz", "iUTQuhaKI2lh81hnVNhGbHDd");</script>
  <script>
    function showTime(Counter) {
      var query = new AV.Query(Counter);
      var entries = [];
      var $visitors = $(".leancloud_visitors");

      $visitors.each(function () {
        entries.push( $(this).attr("id").trim() );
      });

      query.containedIn('url', entries);
      query.find()
        .done(function (results) {
          var COUNT_CONTAINER_REF = '.leancloud-visitors-count';

          if (results.length === 0) {
            $visitors.find(COUNT_CONTAINER_REF).text(0);
            return;
          }

          for (var i = 0; i < results.length; i++) {
            var item = results[i];
            var url = item.get('url');
            var time = item.get('time');
            var element = document.getElementById(url);

            $(element).find(COUNT_CONTAINER_REF).text(time);
          }
          for(var i = 0; i < entries.length; i++) {
            var url = entries[i];
            var element = document.getElementById(url);
            var countSpan = $(element).find(COUNT_CONTAINER_REF);
            if( countSpan.text() == '') {
              countSpan.text(0);
            }
          }
        })
        .fail(function (object, error) {
          console.log("Error: " + error.code + " " + error.message);
        });
    }

    function addCount(Counter) {
      var $visitors = $(".leancloud_visitors");
      var url = $visitors.attr('id').trim();
      var title = $visitors.attr('data-flag-title').trim();
      var query = new AV.Query(Counter);

      query.equalTo("url", url);
      query.find({
        success: function(results) {
          if (results.length > 0) {
            var counter = results[0];
            counter.fetchWhenSave(true);
            counter.increment("time");
            counter.save(null, {
              success: function(counter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(counter.get('time'));
              },
              error: function(counter, error) {
                console.log('Failed to save Visitor num, with error message: ' + error.message);
              }
            });
          } else {
            var newcounter = new Counter();
            /* Set ACL */
            var acl = new AV.ACL();
            acl.setPublicReadAccess(true);
            acl.setPublicWriteAccess(true);
            newcounter.setACL(acl);
            /* End Set ACL */
            newcounter.set("title", title);
            newcounter.set("url", url);
            newcounter.set("time", 1);
            newcounter.save(null, {
              success: function(newcounter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(newcounter.get('time'));
              },
              error: function(newcounter, error) {
                console.log('Failed to create');
              }
            });
          }
        },
        error: function(error) {
          console.log('Error:' + error.code + " " + error.message);
        }
      });
    }

    $(function() {
      var Counter = AV.Object.extend("Counter");
      if ($('.leancloud_visitors').length == 1) {
        addCount(Counter);
      } else if ($('.post-title-link').length > 1) {
        showTime(Counter);
      }
    });
  </script>



  

  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
